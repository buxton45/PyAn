{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns\n",
    "from packaging import version\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import adjustText\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "import Utilities_dt\n",
    "from Utilities_df import DFConstructType\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import Plot_Hist\n",
    "import GrubbsTest\n",
    "import DataFrameSubsetSlicer\n",
    "from DataFrameSubsetSlicer import DataFrameSubsetSlicer as DFSlicer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71266c0c-8d82-4513-b478-7d34a99a1d48",
   "metadata": {},
   "source": [
    "# Pretty much everything from this notebook has been moved to other classes/modules/whatever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63af59",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# BEGIN ACQUISITION METHODS\n",
    "# ==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb0857",
   "metadata": {},
   "source": [
    "# TODO Need to update build_active_MP_for_outages_df in other notebooks, as the method below is the most recent\n",
    "Also need to add build_active_MP_for_xfmrs_in_outages_df (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO !!!!!!!!!!!! IMPORTANT !!!!!!!!!!!!!!\n",
    "# In order for this to function as desired, reduce_active_MP_for_outages must group by transformer pole number\n",
    "#   for this case, not outage number as in the build_active_MP_for_outages_df case\n",
    "# Some other adjustment may be needed as well!\n",
    "# NOTE: There are some instances where build_active_MP_for_outages_df will find PNs not found by \n",
    "#         build_active_MP_for_xfmrs_in_outages_df.\n",
    "#       The reason being that the former doesn't care whether or not the trsf_pole_nb is NaN, but this function does!\n",
    "# May want to, e.g., add in some sort of dropna option\n",
    "# However, what would one do in such a situation?  I guess you would have to make an or statement, allowing\n",
    "#   a PN to have the correct transformer OR be listed in df_outage is partaking in the outage?\n",
    "# Not too sure about this, but it will obviously need some work.\n",
    "# For example where build_active_MP_for_outages_df finds additional PN, see outg_rec_nb 13297062, where\n",
    "#   mfr_devc_ser_nbr=995582422 is found by build_active_MP_for_outages_df but not by build_active_MP_for_xfmrs_in_outages_df\n",
    "#   (because only found in historic, and therefore the trsf_pole_nb could not be determined!)\n",
    "#\n",
    "# May need to combine those with trsf_pole_nb=na in mp to those in mp_df_curr_hist_dict\n",
    "#   One could, e.g., call MeterPremise.build_mp_df_curr_hist_for_PNs for any PNs without a transformer \n",
    "#   pole number, and still use MeterPremise.build_mp_df_curr_hist_for_xfmrs for others\n",
    "def build_active_MP_for_xfmrs_in_outages_df(\n",
    "    df_outage, \n",
    "    prem_nb_col, \n",
    "    df_outage_trsf_pole_nb_col=None, \n",
    "    drop_inst_rmvl_cols=False, \n",
    "    outg_rec_nb_idfr='OUTG_REC_NB', \n",
    "    is_slim=False, \n",
    "    addtnl_mp_df_curr_cols=None, \n",
    "    addtnl_mp_df_hist_cols=None, \n",
    "    dt_on_ts_col='DT_ON_TS', \n",
    "    df_off_ts_full_col='DT_OFF_TS_FULL', \n",
    "    consolidate_PNs_batch_size=1000, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb', \n",
    "    early_return=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Similar to build_active_MP_for_outages/build_active_MP_for_outages_df\n",
    "    \n",
    "    df_outage_trsf_pole_nb_col:\n",
    "        If trsf_pole_nb already present in df_outage, set this equal to the column.\n",
    "        Otherwise, the trsf_pole_nbs will be acquired from the current Meter Premise table\n",
    "        \n",
    "    If addtnl_mp_df_curr_cols and addtnl_mp_df_hist_cols are included (i.e., are not None), their intersection will be added\n",
    "      as addtnl_groupby_cols input argument to drop_approx_duplicates (otherwise, they would be returned in the DF as list\n",
    "      objects, and would likely need to be exploded)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(prem_nb_col in df_outage.columns and \n",
    "           dt_on_ts_col in df_outage.columns and \n",
    "           df_off_ts_full_col in df_outage.columns)\n",
    "    #-------------------------\n",
    "    # If trsf_pole_nbs present in df_outage, simply grab.\n",
    "    # Otherwise, use current Meter Premise table together with PNs from df_outage to acquire trsf_pole_nbs\n",
    "    if df_outage_trsf_pole_nb_col is not None and df_outage_trsf_pole_nb_col in df_outage.columns:\n",
    "        trsf_pole_nbs = df_outage[df_outage_trsf_pole_nb_col].unique().tolist()\n",
    "    else:\n",
    "        if not is_slim:\n",
    "            PNs = df_outage[prem_nb_col].unique().tolist()\n",
    "        else:\n",
    "            PNs = Utilities_df.consolidate_column_of_lists(\n",
    "                df=df_outage, \n",
    "                col=prem_nb_col, \n",
    "                sort=True,\n",
    "                include_None=False,\n",
    "                batch_size=consolidate_PNs_batch_size, \n",
    "                verbose=False\n",
    "            )\n",
    "        #-----\n",
    "        PNs = [x for x in PNs if pd.notna(x)]\n",
    "        PNs_type = type(PNs[0])\n",
    "        assert(Utilities.are_all_list_elements_of_type(PNs, PNs_type))\n",
    "        #-------------------------\n",
    "        mp_cols_of_interest=[df_mp_prem_nb_col, df_mp_trsf_pole_nb_col]\n",
    "        mp = MeterPremise(\n",
    "            df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "            init_df_in_constructor=True, \n",
    "            build_sql_function=MeterPremise.build_sql_meter_premise, \n",
    "            build_sql_function_kwargs=dict(\n",
    "                cols_of_interest=mp_cols_of_interest, \n",
    "                premise_nbs=PNs, \n",
    "                table_name='meter_premise'\n",
    "            )\n",
    "        )\n",
    "        mp_df = mp.df\n",
    "        #-------------------------\n",
    "        trsf_pole_nbs = mp_df[df_mp_trsf_pole_nb_col].unique().tolist()\n",
    "    #--------------------------------------------------\n",
    "    if addtnl_mp_df_curr_cols is not None and addtnl_mp_df_hist_cols is not None:\n",
    "        assert(Utilities.is_object_one_of_types(addtnl_mp_df_curr_cols, [list, tuple]))\n",
    "        assert(Utilities.is_object_one_of_types(addtnl_mp_df_hist_cols, [list, tuple]))\n",
    "        drop_approx_duplicates_args = dict(\n",
    "            addtnl_groupby_cols=list(set(addtnl_mp_df_curr_cols).intersection(set(addtnl_mp_df_hist_cols)))\n",
    "        )\n",
    "    else:\n",
    "        drop_approx_duplicates_args=None\n",
    "    #-----\n",
    "    mp_df_curr_hist_dict = MeterPremise.build_mp_df_curr_hist_for_xfmrs(\n",
    "        trsf_pole_nbs=trsf_pole_nbs, \n",
    "        join_curr_hist=False, \n",
    "        addtnl_mp_df_curr_cols=addtnl_mp_df_curr_cols, \n",
    "        addtnl_mp_df_hist_cols=addtnl_mp_df_hist_cols, \n",
    "        assume_one_xfmr_per_PN=True, \n",
    "        drop_approx_duplicates=True, \n",
    "        drop_approx_duplicates_args=drop_approx_duplicates_args, \n",
    "        df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "        df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "        df_mp_install_time_col=df_mp_install_time_col, \n",
    "        df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "        df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "    )\n",
    "    df_mp_curr = mp_df_curr_hist_dict['mp_df_curr']\n",
    "    df_mp_hist = mp_df_curr_hist_dict['mp_df_hist']\n",
    "    if early_return:\n",
    "        return df_mp_curr, df_mp_hist\n",
    "    #-------------------------\n",
    "    active_SNs_df = reduce_active_MP_for_outages(\n",
    "        df_outage=df_outage, \n",
    "        prem_nb_col=prem_nb_col, \n",
    "        df_mp_curr=df_mp_curr, \n",
    "        df_mp_hist=df_mp_hist, \n",
    "        drop_inst_rmvl_cols=drop_inst_rmvl_cols, \n",
    "        outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "        is_slim=is_slim, \n",
    "        drop_approx_duplicates=True, \n",
    "        drop_approx_duplicates_args=drop_approx_duplicates_args, \n",
    "        dt_on_ts_col=dt_on_ts_col, \n",
    "        df_off_ts_full_col=df_off_ts_full_col, \n",
    "        df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "        df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "        df_mp_install_time_col=df_mp_install_time_col, \n",
    "        df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "        df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    return active_SNs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c9b2e",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# END ACQUISITION METHODS\n",
    "# ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8d631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_est_outg_times_to_axis(\n",
    "    ax, \n",
    "    est_outg_times, \n",
    "    t_beg_kwargs=None,\n",
    "    t_end_kwargs=None,\n",
    "    include='both', \n",
    "    include_outage_limits_text=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    At vertical lines showing estimates outage times.\n",
    "    \n",
    "    Note: out_t_beg and out_t_end aren't really necessary, they simply direct which side of\n",
    "          the estimated lines to print the text\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(include in ['both', 'beg', 'end'])\n",
    "    #-------------------------\n",
    "    if t_beg_kwargs is None:\n",
    "        t_beg_kwargs={}\n",
    "    if t_end_kwargs is None:\n",
    "        t_end_kwargs={}\n",
    "    #-----\n",
    "    dflt_t_beg_kwargs=dict(color='red', linestyle='--')\n",
    "    dflt_t_end_kwargs=dict(color='green', linestyle='--')\n",
    "    #-----\n",
    "    t_beg_kwargs = Utilities.supplement_dict_with_default_values(to_supplmnt_dict=t_beg_kwargs, default_values_dict=dflt_t_beg_kwargs, inplace=False)\n",
    "    t_end_kwargs = Utilities.supplement_dict_with_default_values(to_supplmnt_dict=t_end_kwargs, default_values_dict=dflt_t_end_kwargs, inplace=False)\n",
    "    #-------------------------\n",
    "    for dt_off_i, dt_on_i in est_outg_times:\n",
    "        if include=='both' or include=='beg':\n",
    "            ax.axvline(dt_off_i, **t_beg_kwargs)\n",
    "        if include=='both' or include=='end':\n",
    "            ax.axvline(dt_on_i, **t_end_kwargs)\n",
    "        #-----\n",
    "        if include_outage_limits_text:\n",
    "            ax.text(dt_off_i, ax.get_ylim()[0], 'Est. outg. beg', \n",
    "                    rotation=90, verticalalignment='bottom', \n",
    "                    horizontalalignment='right' if dt_off_i<out_t_beg else 'left')\n",
    "            ax.text(dt_on_i, ax.get_ylim()[0], 'Est. outg. end', \n",
    "                    rotation=90, verticalalignment='bottom', \n",
    "                    horizontalalignment='left' if dt_on_i>out_t_end else 'right')\n",
    "    #-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8cd0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1787f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d7fddf8",
   "metadata": {},
   "source": [
    "# COMBINE AMI AND EDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ed600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_multiindex_duplicate_idxs_for_csv_output(df):\n",
    "    r\"\"\"\n",
    "    Eliminate duplicate values from MultiIndex index before writing to CSV to make reading by eye easier.\n",
    "    IMPORTANT!!!!! The CSV generated here isn't useful for futher manipulation using code, only for inspection by eye\n",
    "                   Thus, it's intended more for when I deliver results to others, not for use myself\n",
    "    -----\n",
    "    Essentially, instead of the output looking like:\n",
    "        idx0     idx1     col1   col2   col3\n",
    "        a        1        ...    ...    ...     \n",
    "        a        2        ...    ...    ...\n",
    "        b        2        ...    ...    ...\n",
    "        b        3        ...    ...    ...\n",
    "    \n",
    "    It will instead look like:\n",
    "        idx0     idx1     col1   col2   col3\n",
    "        a        1        ...    ...    ... \n",
    "                 2        ...    ...    ...\n",
    "        b        2        ...    ...    ...\n",
    "                 3        ...    ...    ...\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure df has MultiIndex index, and each level has a name\n",
    "    assert(df.index.nlevels>1)\n",
    "    assert((len(df.index.names)==df.index.nlevels))\n",
    "    assert(all([True if x else False for x in df.index.names]))\n",
    "    idx_names = list(df.index.names)\n",
    "    #-------------------------\n",
    "    df = df.reset_index()\n",
    "    #-------------------------\n",
    "    dupl_srs_list = []\n",
    "    for i_idx_lvl in range(len(idx_names)):\n",
    "        dupl_srs_i = df.duplicated(subset=idx_names[:i_idx_lvl+1])\n",
    "        dupl_srs_list.append(dupl_srs_i)\n",
    "    assert(len(dupl_srs_list)==len(idx_names))\n",
    "    #-------------------------\n",
    "    for i_idx_lvl in range(len(idx_names)):\n",
    "        df[idx_names[i_idx_lvl]] = df[idx_names[i_idx_lvl]].where(~dupl_srs_list[i_idx_lvl], '')\n",
    "    #-------------------------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c39fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fa71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
