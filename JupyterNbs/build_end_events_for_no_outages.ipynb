{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "# NOTE: To reload a class imported as, e.g., \n",
    "# from module import class\n",
    "# One must call:\n",
    "#   1. import module\n",
    "#   2. reload module\n",
    "#   3. from module import class\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "import string\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns, natsort_keygen\n",
    "from packaging import version\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL, DfToSqlMap\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "from OutageDAQ import OutageDAQ, OutageDAQOtBL\n",
    "from OutageDAQ import OutageDataInfo as ODI\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "#sys.path.insert(0, os.path.join(os.path.realpath('..'), 'Utilities'))\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "from Utilities_df import DFConstructType\n",
    "import Utilities_dt\n",
    "from CustomJSON import CustomEncoder, CustomWriter\n",
    "import DataFrameSubsetSlicer\n",
    "from DataFrameSubsetSlicer import DataFrameSubsetSlicer as DFSlicer\n",
    "from DataFrameSubsetSlicer import DataFrameSubsetSingleSlicer as DFSingleSlicer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d57880",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c6673",
   "metadata": {},
   "source": [
    "# Will have two methods for building.  \n",
    "# One using a supplied df_outage with (or, I suppose, without) accompanying meter premise\n",
    "# One building from ground up, given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# VARIABLES TO BE SET BY USER!\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Unless absolutely certain df_mp in csv has all necessary data, use read_dfs_from_file=False\n",
    "save_dfs_to_file   = True\n",
    "read_dfs_from_file = False\n",
    "save_end_events    = True\n",
    "\n",
    "#-------------------------\n",
    "# run_date is used to collect all results from a given acquisiton run together.\n",
    "# As such, run_date should be set to the first date of the acquisition run, and\n",
    "#   SHOULD NOT be changed for each individual date in a run (which typically lasts\n",
    "#   over the course of days/weeks)\n",
    "run_date = '20240906' # Date of data acquisition\n",
    "\n",
    "#-------------------------\n",
    "date_0 = '2023-04-01' # Lower limit for end events\n",
    "date_1 = '2024-08-31' # Upper limit for end events\n",
    "\n",
    "#-------------------------\n",
    "# dataset = 'outg'\n",
    "dataset = 'otbl'\n",
    "# dataset = 'prbl'\n",
    "\n",
    "#-------------------------\n",
    "min_window_width = pd.Timedelta('31 days')\n",
    "buffer_time_left = pd.Timedelta('1 days')\n",
    "buffer_time_rght = pd.Timedelta('31 days')\n",
    "\n",
    "#-------------------------\n",
    "run_using_slim = False\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# If df_mp is read from csv, it will typically contain an outg_rec_nb column\n",
    "#   and entries which are duplicates except for outg_rec_nb\n",
    "# For this process to work correctly, these duplicates must be removed.\n",
    "df_mp_outg_rec_nb_col = 'OUTG_REC_NB'\n",
    "\n",
    "#-------------------------\n",
    "groupby_col = 'trsf_pole_nb'\n",
    "# groupby_col = 'PREMISE_NB'\n",
    "assert(groupby_col in ['trsf_pole_nb', 'PREMISE_NB'])\n",
    "\n",
    "#-------------------------\n",
    "pd_selection_stategy = 'all'\n",
    "# search_window_strategy = 'centered'\n",
    "# search_window_strategy = pd.Timedelta('1 day')\n",
    "search_window_strategy = 'all_subwindows'\n",
    "\n",
    "#--------------------------------------------------\n",
    "# NOTE: below, states and opcos should be consistent!\n",
    "#       i.e., e.g., if states='OH', then opcos should be 'oh' (or None, I suppose)\n",
    "#-------------------------\n",
    "# states used to \n",
    "#   (1) find transformers which suffered at least one outage from DOVS\n",
    "#   (2) find all transformers from MeterPremise\n",
    "# states can be:\n",
    "#   - a single string, e.g. 'OH'\n",
    "#   - a list of strings, e.g., ['OH', 'WV']\n",
    "#   - None\n",
    "# NOTE: states tend to be upper-case!\n",
    "states=['OH']\n",
    "\n",
    "#-------------------------\n",
    "# opcos used with AMIEndEvents to\n",
    "#  (1) find the premise numbers which recorded an event between date_0 and date_1.\n",
    "#  (2) selection/acquisiton of end_device_events\n",
    "# opcos can be:\n",
    "#   - a single string, e.g. 'oh'\n",
    "#   - a list of strings, e.g., ['oh', 'tx']\n",
    "#   - None\n",
    "# NOTE: opcos tend to be lower-case!\n",
    "# NOTE: Acceptable opcos appear to be: ['ap', 'im', 'oh', 'pso', 'swp', 'tx']\n",
    "opcos='oh'\n",
    "\n",
    "#-------------------------\n",
    "# cities = None\n",
    "cities = ['COLUMBUS']\n",
    "\n",
    "#--------------------------------------------------\n",
    "trsf_pole_nbs_to_ignore = [' ', 'TRANSMISSION', 'PRIMARY', 'NETWORK']\n",
    "\n",
    "# TODO!!!!!!!!!!!!!!!!!!\n",
    "single_zip_xfmrs_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550ac01-56b8-4eb8-ab65-0201bfc6723d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3c7ad-352f-498b-b063-1fd6a29f1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT AFTER DONE CHECKING EVS_SUM\n",
    "# ############### NEW\n",
    "# outg_daq = OutageDAQOtBL(\n",
    "#     run_date                = run_date, \n",
    "#     date_0                  = date_0, \n",
    "#     date_1                  = date_1, \n",
    "#     collect_evs_sum_vw      = False,  \n",
    "#     save_sub_dir            = ODI.get_subdir(dataset), \n",
    "#     groupby_col             = groupby_col, \n",
    "#     min_window_width        = min_window_width, \n",
    "#     buffer_time_left        = buffer_time_left, \n",
    "#     buffer_time_rght        = buffer_time_rght, \n",
    "#     pd_selection_stategy    = pd_selection_stategy, \n",
    "#     search_window_strategy  = search_window_strategy, \n",
    "#     states                  = states, \n",
    "#     opcos                   = opcos, \n",
    "#     cities                  = cities, \n",
    "#     single_zip_xfmrs_only   = single_zip_xfmrs_only, \n",
    "#     save_end_events         = save_end_events, \n",
    "#     save_dfs_to_file        = save_dfs_to_file, \n",
    "#     read_dfs_from_file      = read_dfs_from_file, \n",
    "#     base_dir                = os.path.join(\n",
    "#         Utilities.get_local_data_dir(), \n",
    "#         r'dovs_and_end_events_data'\n",
    "#     ), \n",
    "#     run_using_slim          = run_using_slim \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeec6bc-138d-4895-b5b4-6443241fd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT AFTER DONE CHECKING EVS_SUM\n",
    "# #--------------------------------------------------\n",
    "# # OUTAGES\n",
    "# #--------------------------------------------------\n",
    "# outg_daq.build_or_load_df_outage_OG(verbose=True)\n",
    "\n",
    "# #--------------------------------------------------\n",
    "# # Find all outages between date_0 and date_1 for PNs in df_outage\n",
    "# #   Need to find all PNs, which consist not only of those directly from df_outage, \n",
    "# #   but also those not in df_outage who were connected to transformers having entries in df_outage\n",
    "# #--------------------------------------------------\n",
    "# outg_daq.build_or_load_mp_df_xfmrs(verbose=True)\n",
    "\n",
    "# #--------------------------------------------------\n",
    "# # Build zip code information for transformers, \n",
    "# # and possibly restrict data to single zipcode transformers only\n",
    "# #--------------------------------------------------\n",
    "# outg_daq.build_or_load_trsf_pole_zips_info(verbose=True)\n",
    "\n",
    "# #--------------------------------------------------\n",
    "# # Now, for all premise numbers in df_outage, we want to find all outages suffered between date_0 and date_1\n",
    "# # There are two methods for achieving this, using either method='query_pns_only' or method='query_all' in find_all_outages_for_pns\n",
    "# # NOTE: It is possible that all needed info already contained in df_outage! \n",
    "# #       But that is reliant upon the user having everything set up perfectly....so safest just to build...\n",
    "# #--------------------------------------------------\n",
    "# outg_daq.build_or_load_all_outages_df(\n",
    "#     verbose               = True, \n",
    "#     n_update              = 10, \n",
    "#     batch_size            = 1000, \n",
    "#     method                = 'decide_at_runtime'\n",
    "# )\n",
    "\n",
    "# #--------------------------------------------------\n",
    "# # Find the clean windows for each group and build df_no_outage\n",
    "# #--------------------------------------------------\n",
    "# outg_daq.build_or_load_df_no_outage(verbose = True)\n",
    "\n",
    "# #--------------------------------------------------\n",
    "# # Convert df_no_outage to consolidated (slim) verion if run_using_slim\n",
    "# #--------------------------------------------------\n",
    "# if run_using_slim:\n",
    "#     outg_daq.build_or_load_df_no_outage_slim(verbose = True)\n",
    "\n",
    "# #--------------------------------------------------\n",
    "# # Collect Events\n",
    "# #--------------------------------------------------\n",
    "# outg_daq.collect_events(\n",
    "#     batch_size = None, \n",
    "#     verbose    = True, \n",
    "#     n_update   = 1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9b93d-1b9d-4c17-9d6e-7c198865baa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec9a4384-3b2c-414a-8d8c-4b6f635b674e",
   "metadata": {},
   "source": [
    "# CHECKING EVS_SUM METHOD VS ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530ad3a-3693-4fe0-b47e-bf1fbf9773d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### NEW\n",
    "outg_daq = OutageDAQOtBL(\n",
    "    run_date                = run_date, \n",
    "    date_0                  = date_0, \n",
    "    date_1                  = date_1, \n",
    "    collect_evs_sum_vw      = False,  \n",
    "    save_sub_dir            = ODI.get_subdir(dataset), \n",
    "    groupby_col             = groupby_col, \n",
    "    min_window_width        = min_window_width, \n",
    "    buffer_time_left        = buffer_time_left, \n",
    "    buffer_time_rght        = buffer_time_rght, \n",
    "    pd_selection_stategy    = pd_selection_stategy, \n",
    "    search_window_strategy  = search_window_strategy, \n",
    "    states                  = states, \n",
    "    opcos                   = opcos, \n",
    "    cities                  = cities, \n",
    "    single_zip_xfmrs_only   = single_zip_xfmrs_only, \n",
    "    save_end_events         = save_end_events, \n",
    "    save_dfs_to_file        = save_dfs_to_file, \n",
    "    read_dfs_from_file      = read_dfs_from_file, \n",
    "    base_dir                = os.path.join(\n",
    "        Utilities.get_local_data_dir(), \n",
    "        r'dovs_and_end_events_data'\n",
    "    ), \n",
    "    run_using_slim          = run_using_slim \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0808cae-9b2e-4daa-b6fd-c6d96141bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# OUTAGES\n",
    "#--------------------------------------------------\n",
    "outg_daq.build_or_load_df_outage_OG(verbose=True)\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Find all outages between date_0 and date_1 for PNs in df_outage\n",
    "#   Need to find all PNs, which consist not only of those directly from df_outage, \n",
    "#   but also those not in df_outage who were connected to transformers having entries in df_outage\n",
    "#--------------------------------------------------\n",
    "outg_daq.build_or_load_mp_df_xfmrs(verbose=True)\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Build zip code information for transformers, \n",
    "# and possibly restrict data to single zipcode transformers only\n",
    "#--------------------------------------------------\n",
    "outg_daq.build_or_load_trsf_pole_zips_info(verbose=True)\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Now, for all premise numbers in df_outage, we want to find all outages suffered between date_0 and date_1\n",
    "# There are two methods for achieving this, using either method='query_pns_only' or method='query_all' in find_all_outages_for_pns\n",
    "# NOTE: It is possible that all needed info already contained in df_outage! \n",
    "#       But that is reliant upon the user having everything set up perfectly....so safest just to build...\n",
    "#--------------------------------------------------\n",
    "outg_daq.build_or_load_all_outages_df(\n",
    "    verbose               = True, \n",
    "    n_update              = 10, \n",
    "    batch_size            = 1000, \n",
    "    method                = 'decide_at_runtime'\n",
    ")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Find the clean windows for each group and build df_no_outage\n",
    "#--------------------------------------------------\n",
    "outg_daq.build_or_load_df_no_outage(verbose = True)\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Convert df_no_outage to consolidated (slim) verion if run_using_slim\n",
    "#--------------------------------------------------\n",
    "if run_using_slim:\n",
    "    outg_daq.build_or_load_df_no_outage_slim(verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da78016-d90a-4526-9b55-ed30ea2e8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop down DF size for quick run comp\n",
    "if run_using_slim:\n",
    "    outg_daq.df_no_outage_slim = outg_daq.df_no_outage_slim.iloc[:1000]\n",
    "else:\n",
    "    outg_daq.df_no_outage = outg_daq.df_no_outage.sort_values(by=['no_outg_rec_nb', 'trsf_pole_nb', 'prem_nb', 't_search_min'], ignore_index=True).iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67232a84-7604-4338-8e30-d8153fd5cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = outg_daq.df_no_outage.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fe3b3-6b1c-4fab-9b8d-a1abd81a2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# Collect Events\n",
    "#--------------------------------------------------\n",
    "outg_daq.collect_events(\n",
    "    batch_size = None, \n",
    "    verbose    = True, \n",
    "    n_update   = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e09f5-3ace-4ee5-9098-ad92c010897a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc1f29-ea84-4805-82a2-e6ac8de82fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a7d806-09f8-4279-978d-c407deef9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### NEWEST\n",
    "outg_daq = OutageDAQOtBL(\n",
    "    run_date                = run_date, \n",
    "    date_0                  = date_0, \n",
    "    date_1                  = date_1, \n",
    "    collect_evs_sum_vw      = True,  \n",
    "    save_sub_dir            = ODI.get_subdir(dataset), \n",
    "    groupby_col             = groupby_col, \n",
    "    min_window_width        = min_window_width, \n",
    "    buffer_time_left        = buffer_time_left, \n",
    "    buffer_time_rght        = buffer_time_rght, \n",
    "    pd_selection_stategy    = pd_selection_stategy, \n",
    "    search_window_strategy  = search_window_strategy, \n",
    "    states                  = states, \n",
    "    opcos                   = opcos, \n",
    "    cities                  = cities, \n",
    "    single_zip_xfmrs_only   = single_zip_xfmrs_only, \n",
    "    save_end_events         = save_end_events, \n",
    "    save_dfs_to_file        = save_dfs_to_file, \n",
    "    read_dfs_from_file      = read_dfs_from_file, \n",
    "    base_dir                = os.path.join(\n",
    "        Utilities.get_local_data_dir(), \n",
    "        r'dovs_and_end_events_data'\n",
    "    ), \n",
    "    run_using_slim          = run_using_slim \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdf54b-013c-4dc0-aa82-b344a0c793b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# OUTAGES\n",
    "#--------------------------------------------------\n",
    "outg_daq.build_or_load_df_outage_OG(verbose=True)\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Find all outages between date_0 and date_1 for PNs in df_outage\n",
    "#   Need to find all PNs, which consist not only of those directly from df_outage, \n",
    "#   but also those not in df_outage who were connected to transformers having entries in df_outage\n",
    "#--------------------------------------------------\n",
    "outg_daq.build_or_load_mp_df_xfmrs(verbose=True)\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Build zip code information for transformers, \n",
    "# and possibly restrict data to single zipcode transformers only\n",
    "#--------------------------------------------------\n",
    "outg_daq.build_or_load_trsf_pole_zips_info(verbose=True)\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Now, for all premise numbers in df_outage, we want to find all outages suffered between date_0 and date_1\n",
    "# There are two methods for achieving this, using either method='query_pns_only' or method='query_all' in find_all_outages_for_pns\n",
    "# NOTE: It is possible that all needed info already contained in df_outage! \n",
    "#       But that is reliant upon the user having everything set up perfectly....so safest just to build...\n",
    "#--------------------------------------------------\n",
    "outg_daq.build_or_load_all_outages_df(\n",
    "    verbose               = True, \n",
    "    n_update              = 10, \n",
    "    batch_size            = 1000, \n",
    "    method                = 'decide_at_runtime'\n",
    ")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Find the clean windows for each group and build df_no_outage\n",
    "#--------------------------------------------------\n",
    "outg_daq.build_or_load_df_no_outage(verbose = True)\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Convert df_no_outage to consolidated (slim) verion if run_using_slim\n",
    "#--------------------------------------------------\n",
    "if run_using_slim:\n",
    "    outg_daq.build_or_load_df_no_outage_slim(verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae8b23-06e3-4dfa-b2ec-d6809b45391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop down DF size for quick run comp\n",
    "if run_using_slim:\n",
    "    outg_daq.df_no_outage_slim = outg_daq.df_no_outage_slim.iloc[:1000]\n",
    "else:\n",
    "    outg_daq.df_no_outage = outg_daq.df_no_outage.sort_values(by=['no_outg_rec_nb', 'trsf_pole_nb', 'prem_nb', 't_search_min'], ignore_index=True).iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca819b-9f72-4572-aadf-b600740cfb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_daq.df_no_outage.equals(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd34ee-60dd-4f94-a9b0-b59f5cc8c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# Collect Events\n",
    "#--------------------------------------------------\n",
    "outg_daq.collect_events(\n",
    "    batch_size = None, \n",
    "    verbose    = True, \n",
    "    n_update   = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f5b81-8f9e-47b7-8ae4-bb48be5b4833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882f6d1-876d-45a7-829e-5563c45d133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a476e1-8e0c-47be-8671-77078032dd78",
   "metadata": {},
   "source": [
    "# END CHECKING EVS_SUM METHOD VS ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc6c6d-8137-40a9-a7bb-065d7fc1a8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba479c5-0227-4f06-bcf8-dafdead1b6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64cb3b6c-b399-466b-9a37-0da5f55ee5c5",
   "metadata": {},
   "source": [
    "# OLD DEV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffeb6a1-f811-4a30-b0a0-efd8025b815d",
   "metadata": {},
   "source": [
    "### OutagesDAQ constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9424bab-7a01-4f09-91a5-f07e1a030a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b18eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# DFs will be saved in save_dir_base\n",
    "# Collection of end events files will be saved in os.path.join(save_dir_base, 'EndEvents')\n",
    "save_dir_base = os.path.join(\n",
    "    Utilities.get_local_data_dir(), \n",
    "    r'dovs_and_end_events_data', \n",
    "    run_date, \n",
    "    f\"{date_0.replace('-','')}_{date_1.replace('-','')}\", \n",
    "    'NoOutgs_OLD'\n",
    ")\n",
    "#-------------------------\n",
    "end_events_save_args = dict(\n",
    "    save_to_file=save_end_events, \n",
    "    save_dir = os.path.join(save_dir_base, 'EndEvents'), \n",
    "    save_name=r'end_events.csv', \n",
    "    index=True\n",
    ")\n",
    "#-------------------------\n",
    "print(f\"save_dir_base = {save_dir_base}\")\n",
    "print('end_events_save_args')\n",
    "for k,v in end_events_save_args.items():\n",
    "    print(f\"\\t{k} : {v}\")\n",
    "#-------------------------\n",
    "if save_dfs_to_file or save_end_events:\n",
    "    if not os.path.exists(save_dir_base):\n",
    "        os.makedirs(save_dir_base)\n",
    "    #-----\n",
    "    if save_end_events and not os.path.exists(end_events_save_args['save_dir']):\n",
    "        os.makedirs(end_events_save_args['save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "assert(save_dfs_to_file+read_dfs_from_file <=1) # Should never both read and write!\n",
    "assert(pd.to_datetime(date_1)-pd.to_datetime(date_0) > min_window_width+buffer_time_left+buffer_time_rght)\n",
    "#--------------------------------------------------\n",
    "if not read_dfs_from_file:\n",
    "    conn_outages = Utilities.get_utldb01p_oracle_connection()\n",
    "    conn_aws = Utilities.get_athena_prod_aws_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc7f28-6582-4da2-8a75-7414bc982960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dfs_to_file   = False\n",
    "# read_dfs_from_file = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ad41a",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------\n",
    "# OUTAGES\n",
    "# ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f0bc-de1c-4722-af13-3a8a361641c4",
   "metadata": {},
   "source": [
    "### build_or_load_df_outage_OG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_dfs_from_file:\n",
    "    df_outage_OG = pd.read_pickle(os.path.join(save_dir_base, 'df_outage_OG.pkl'))\n",
    "    csv_cols_and_types_to_convert_dict = {'CI_NB':np.int32, 'CMI_NB':np.float64, 'OUTG_REC_NB':[np.float64, np.int32]}\n",
    "    df_outage_OG = Utilities_df.convert_col_types(df_outage_OG, csv_cols_and_types_to_convert_dict)\n",
    "else:\n",
    "    sql_outage_full = DOVSOutages_SQL.build_sql_std_outage(\n",
    "        mjr_mnr_cause   = None, \n",
    "        include_premise = True, \n",
    "        date_range      = [date_0, date_1], \n",
    "        states          = states, \n",
    "        opcos           = opcos, \n",
    "        cities          = cities\n",
    "    ).get_sql_statement()\n",
    "    print(sql_outage_full)\n",
    "    #-------------------------\n",
    "    df_outage_OG = pd.read_sql_query(\n",
    "        sql_outage_full, \n",
    "        conn_outages, \n",
    "        dtype = {\n",
    "            'CI_NB':np.int32, \n",
    "            'CMI_NB':np.float64, \n",
    "            'OUTG_REC_NB':np.int32\n",
    "        }\n",
    "    )\n",
    "    csv_cols_and_types_to_convert_dict = {'CI_NB':np.int32, 'CMI_NB':np.float64, 'OUTG_REC_NB':[np.float64, np.int32]}\n",
    "    df_outage_OG = Utilities_df.convert_col_types(df_outage_OG, csv_cols_and_types_to_convert_dict)\n",
    "    #-------------------------\n",
    "    if save_dfs_to_file:\n",
    "        df_outage_OG.to_pickle(os.path.join(save_dir_base, 'df_outage_OG.pkl'))\n",
    "#-------------------------\n",
    "print(f'df_outage_OG.shape = {df_outage_OG.shape}')\n",
    "# df_outage = df_outage_OG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53da5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage = df_outage_OG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b69e40-62d4-4e34-bcd1-d39503f5038f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ffa6697",
   "metadata": {},
   "source": [
    "# Find all outages between date_0 and date_1 for PNs in df_outage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0dfcd2",
   "metadata": {},
   "source": [
    "# Need to find all PNs, which consist not only of those directly from df_outage, but also those not in df_outage who were connected to transformers having entries in df_outage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482dad34-b516-43fd-b513-4d4b8b178a9d",
   "metadata": {},
   "source": [
    "### build_or_load_mp_df_xfmrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988da14-7a16-4f16-b74b-0ac299aeaae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_dfs_from_file:\n",
    "    mp_df_PNs   = pd.read_pickle(os.path.join(save_dir_base, 'mp_df_PNs_no_outg.pkl'))\n",
    "    mp_df_xfmrs = pd.read_pickle(os.path.join(save_dir_base, 'mp_df_xfmrs_no_outg.pkl'))\n",
    "else:\n",
    "    conn_aws = Utilities.get_athena_prod_aws_connection()\n",
    "    PNs = df_outage['PREMISE_NB'].unique().tolist()\n",
    "    #-------------------------\n",
    "    start=time.time()\n",
    "    mp_df_PNs, sql_stmnts = MeterPremise.get_distinct_trsf_pole_nbs_for_PNs(\n",
    "        PNs        = PNs, \n",
    "        batch_size = 10000, \n",
    "        conn_aws   = conn_aws, \n",
    "        return_sql = True, \n",
    "        states     = states, \n",
    "        opcos      = opcos, \n",
    "        cities     = cities\n",
    "    )\n",
    "    print(f\"mp_df_PNs: {time.time()-start}\")\n",
    "    #-------------------------\n",
    "    start=time.time()\n",
    "    mp_xfmrs = GenAn(\n",
    "        df_construct_type         = DFConstructType.kRunSqlQuery, \n",
    "        contstruct_df_args        = dict(conn_db=Utilities.get_athena_prod_aws_connection()), \n",
    "        init_df_in_constructor    = True, \n",
    "        build_sql_function        = MeterPremise.build_sql_meter_premise, \n",
    "        build_sql_function_kwargs = dict(\n",
    "            cols_of_interest = ['trsf_pole_nb', 'prem_nb', 'mfr_devc_ser_nbr', 'inst_ts', 'rmvl_ts', 'state_cd'], \n",
    "            trsf_pole_nb     = [x for x in mp_df_PNs['trsf_pole_nb'].unique() if x!='TRANSMISSION'], \n",
    "            field_to_split   = 'trsf_pole_nb', \n",
    "            states           = states, \n",
    "            opcos            = opcos, \n",
    "            cities           = cities\n",
    "        )\n",
    "    )\n",
    "    print(f\"mp_df_xfmrs: {time.time()-start}\")\n",
    "    mp_df_xfmrs = mp_xfmrs.df.copy()\n",
    "    #-------------------------\n",
    "    if save_dfs_to_file:\n",
    "        mp_df_PNs.to_pickle(os.path.join(save_dir_base, 'mp_df_PNs_no_outg.pkl'))\n",
    "        mp_df_xfmrs.to_pickle(os.path.join(save_dir_base, 'mp_df_xfmrs_no_outg.pkl'))\n",
    "#---------------------------------------------------------------------------\n",
    "# PNs = df_outage['PREMISE_NB'].unique().tolist()\n",
    "PNs = list(set(df_outage['PREMISE_NB'].unique().tolist() + mp_df_xfmrs['prem_nb'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfe8ad-6941-458b-b734-a8e08f123a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b62b094-c4e3-4b0f-a7ec-87fa631b1e4a",
   "metadata": {},
   "source": [
    "# Build zip code information for transformers, and possibly restrict data to single zipcode transformers only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db34b9-fbca-441f-af1d-8d0cbebdc176",
   "metadata": {},
   "source": [
    "### build_or_load_trsf_pole_zips_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab3433-f2ac-41f1-9d6e-c85bd304e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_dfs_from_file:\n",
    "    mp_for_zips_df    = pd.read_pickle(os.path.join(save_dir_base, 'mp_for_zips_df.pkl'))\n",
    "    trsf_pole_df_full = pd.read_pickle(os.path.join(save_dir_base, 'trsf_location_info_df.pkl'))\n",
    "    trsf_pole_zips_df = pd.read_pickle(os.path.join(save_dir_base, 'trsf_pole_zips_df.pkl'))\n",
    "else:\n",
    "    zips_dict = OutageDAQ.build_trsf_pole_zips_df(\n",
    "        field_to_split_and_val = ('premise_nbs', PNs), \n",
    "        states                 = states, \n",
    "        opcos                  = opcos, \n",
    "        cities                 = cities\n",
    "    )\n",
    "    #----------\n",
    "    trsf_pole_zips_df = zips_dict['trsf_pole_zips_df']\n",
    "    trsf_pole_df_full = zips_dict['trsf_pole_df_full']\n",
    "    mp_for_zips_df    = zips_dict['mp_for_zips_df']\n",
    "    #--------------------------------------------------\n",
    "    if save_dfs_to_file:\n",
    "        mp_for_zips_df.to_pickle(   os.path.join(save_dir_base, 'mp_for_zips_df.pkl'))\n",
    "        trsf_pole_df_full.to_pickle(os.path.join(save_dir_base, 'trsf_location_info_df.pkl'))\n",
    "        trsf_pole_zips_df.to_pickle(os.path.join(save_dir_base, 'trsf_pole_zips_df.pkl'))\n",
    "#--------------------------------------------------\n",
    "if single_zip_xfmrs_only:\n",
    "    trsf_pole_nzips   = trsf_pole_zips_df.drop(columns=['zip+4']).drop_duplicates()['trsf_pole_nb'].value_counts()\n",
    "    single_zip_poles  = trsf_pole_nzips[trsf_pole_nzips==1].index.tolist()\n",
    "    #-----\n",
    "    trsf_pole_zips_df = trsf_pole_zips_df[trsf_pole_zips_df['trsf_pole_nb'].isin(single_zip_poles)]\n",
    "    #-----\n",
    "    PNs = mp_for_zips_df[mp_for_zips_df['trsf_pole_nb'].isin(single_zip_poles)]['prem_nb'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cecc3e-39ea-4c05-bae8-9ca366b86e78",
   "metadata": {},
   "source": [
    "### build_or_load_all_outages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, for all premise numbers in df_outage, we want to find all outages suffered between date_0 and date_1\n",
    "# There are two methods for achieving this, using either method='query_pns_only' or method='query_all' in find_all_outages_for_pns\n",
    "# NOTE: It is possible that all needed info already contained in df_outage! \n",
    "#       But that is reliant upon the user having everything set up perfectly....so safest just to build...\n",
    "verbose    = True\n",
    "n_update   = 10\n",
    "batch_size = 1000\n",
    "\n",
    "method = 'decide_at_runtime'\n",
    "# addtnl_build_sql_std_outage_kwargs=None\n",
    "addtnl_build_sql_std_outage_kwargs=dict(\n",
    "    states = states, \n",
    "    opcos  = opcos, \n",
    "    cities = cities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10491456-b519-40ca-8f82-e0abdc1e5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(PNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------*************************-------------------------*************************\n",
    "start = time.time()\n",
    "all_outages_df = OutageDAQOtBL.find_all_outages_for_pns(\n",
    "    PNs                                = PNs, \n",
    "    date_0                             = date_0, \n",
    "    date_1                             = date_1, \n",
    "    cols_of_interest                   = None, \n",
    "    mjr_mnr_cause                      = None, \n",
    "    method                             = method, \n",
    "    addtnl_build_sql_std_outage_kwargs = addtnl_build_sql_std_outage_kwargs, \n",
    "    verbose                            = verbose, \n",
    "    n_update                           = n_update, \n",
    "    batch_size                         = batch_size\n",
    ")\n",
    "print(f\"Time to run OutageDAQOtBL.find_all_outages_for_pns: {time.time()-start}\")\n",
    "print(f\"# Unique PNs in df_outage:      {df_outage['PREMISE_NB'].nunique()}\")\n",
    "print(f\"# Unique PNs in all_outages_df: {all_outages_df['PREMISE_NB'].nunique()}\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86875cc-64d6-4d85-ad1a-696090f8c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dfs_to_file   = True\n",
    "# read_dfs_from_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------*************************-------------------------*************************\n",
    "# If grouping by transformer, the trsf_pole_nb from MeterPremise must be merged with all_outages_df\n",
    "# Also, the active meters at the time of outage must be selected by comparing inst_ts,rmvl_ts to \n",
    "#   DT_OFF_TS_FULL,DT_ON_TS.\n",
    "# This is documented in the code below\n",
    "#-------------------------\n",
    "if groupby_col=='trsf_pole_nb':\n",
    "    if read_dfs_from_file:\n",
    "        df_mp = pd.read_pickle(os.path.join(save_dir_base, 'df_mp_no_outg.pkl'))\n",
    "        #-----\n",
    "        # Check for df_mp_outg_rec_nb_col in df_mp, regardless of case\n",
    "        #   If contained, must be dropped and duplicates removed\n",
    "        #   *See comment above df_mp_outg_rec_nb_col initial assignment\n",
    "        df_mp = Utilities_df.drop_col_case_insensitive(\n",
    "            df        = df_mp, \n",
    "            col       = df_mp_outg_rec_nb_col, \n",
    "            inplace   = True, \n",
    "            drop_dups = True\n",
    "        )\n",
    "    else:\n",
    "        df_mp = MeterPremise.build_mp_df_curr_hist_for_PNs(\n",
    "            PNs                    =  PNs, \n",
    "            mp_df_curr             = None,\n",
    "            mp_df_hist             = None, \n",
    "            join_curr_hist         = True, \n",
    "            addtnl_mp_df_curr_cols = None, \n",
    "            addtnl_mp_df_hist_cols = None, \n",
    "            assert_all_PNs_found   = False, \n",
    "            assume_one_xfmr_per_PN = True, \n",
    "            drop_approx_duplicates = True\n",
    "        )\n",
    "        if save_dfs_to_file:\n",
    "            df_mp.to_pickle(os.path.join(save_dir_base, 'df_mp_no_outg.pkl'))\n",
    "    #--------------------------------------------------\n",
    "    # Some premise numbers from DOVS are missing from df_mp.\n",
    "    # This is not an issue with the code, I checked. \n",
    "    # This means DOVS says a premise was affected by an outage, but at the time of the outage there were \n",
    "    #   no active meters on the premise.\n",
    "    # My question is: How did DOVS therefore know the premise was affected?\n",
    "    # How are premise numbers in DOVS determined?\n",
    "    #-------------------------\n",
    "    # I want to at least get a count to quantify the situation described above, i.e., how many premises from DOVS\n",
    "    #   did not have any active meters at the time of the outage.\n",
    "    # Note, for this, I cannot simply do, e.g., \n",
    "    #     set(all_outages_df['PREMISE_NB'].unique()).difference(set(df_mp['prem_nb'].unique()))\n",
    "    #   as this might reflect a smaller number of missing PNs than in reality, as df_mp has not yet been chopped down\n",
    "    #   to only those present at time of outage (which is done below comparing 'inst_ts' to 'DT_OFF_TS_FULL' and \n",
    "    #   'rmvl_ts' to 'DT_ON_TS')\n",
    "    #-------------------------\n",
    "    # The meters present at the time of the outages can only be select after all_outages_df and df_mp are merged.\n",
    "    #-------------------------\n",
    "    # Note: A left merge is used below instead of an inner to protect against the case of a df_mp (being read in from a CSV \n",
    "    #       file) which contains extra entries than in all_outages_df\n",
    "    #-------------------------\n",
    "    all_outages_df = DOVSOutages.merge_df_outage_with_mp(\n",
    "        df_outage          = all_outages_df, \n",
    "        df_mp              = df_mp,  \n",
    "        merge_on_outg      = ['PREMISE_NB'], \n",
    "        merge_on_mp        = ['prem_nb'], \n",
    "        cols_to_include_mp = None, \n",
    "        drop_cols          = None, \n",
    "        rename_cols        = None, \n",
    "        how                = 'left', \n",
    "        inplace            = True\n",
    "    )\n",
    "\n",
    "    #-------------------------\n",
    "    # Only include serial numbers which were present at the time of the outage.\n",
    "    #-----\n",
    "    # NOTE the use of .fillna(pd.Timestamp.min) (YES, MIN) below, as this is different from MeterPremise.get_active_SNs_for_PNs_at_datetime_interval\n",
    "    #   and MeterPremise.merge_df_with_active_mp.\n",
    "    # This is needed so the premises missing from df_mp are not removed at this stage (yes, they will ultimately be removed, \n",
    "    #   but I don't want them removed yet because I want to track them!)\n",
    "    # Without this, any entry with 'inst_ts'=NaT would be removed, as a comparison of NaT to anything returns False\n",
    "    #-----\n",
    "    all_outages_df = Utilities_df.convert_col_types(\n",
    "        df                  = all_outages_df, \n",
    "        cols_and_types_dict = {\n",
    "            'inst_ts' : datetime.datetime, \n",
    "            'rmvl_ts' : datetime.datetime\n",
    "        }, \n",
    "        to_numeric_errors   = 'coerce', \n",
    "        inplace             = True\n",
    "    )\n",
    "    #-----\n",
    "    all_outages_df = all_outages_df[\n",
    "        (all_outages_df['inst_ts'].fillna(pd.Timestamp.min) <= all_outages_df['DT_OFF_TS_FULL']) & \n",
    "        (all_outages_df['rmvl_ts'].fillna(pd.Timestamp.max) >  all_outages_df['DT_ON_TS'])\n",
    "    ]\n",
    "\n",
    "    #-------------------------\n",
    "    # Find the entries with missing df_mp data, i.e., find the entries where DOVS says a premise was affected by an outage, \n",
    "    #   but at the time of the outage there were no active meters on the premise.\n",
    "    all_outages_df_for_non_active_pns = all_outages_df[all_outages_df[df_mp.columns].isna().all(axis=1)].copy()\n",
    "    non_active_pns_from_DOVS          = all_outages_df_for_non_active_pns['PREMISE_NB'].unique().tolist()\n",
    "\n",
    "    # And remove the entries with missing df_mp data from all_outages_df\n",
    "    all_outages_df = all_outages_df.dropna(subset=df_mp.columns, how='all')\n",
    "    #-------------------------\n",
    "    print(\"\"\"\n",
    "    Some premise numbers from DOVS are missing from df_mp\n",
    "    This is not an issue with the code, I checked.  \n",
    "    This means DOVS says a premise was affected by an outage, but at the time of the outage there were no active meters on the premise.\n",
    "    My question is: How did DOVS therefore know the premise was affected?\n",
    "    How are premise numbers in DOVS determined?\n",
    "    \"\"\")\n",
    "    print(f\"Number of premise numbers from DOVS without an active meter at outage time: {len(non_active_pns_from_DOVS)}\") \n",
    "    #-------------------------\n",
    "    # At this point, any trsf_pole_nbs to be excluded can be removed\n",
    "    # Remove 'TRANSMISSION', 'PRIMARY', and 'NETWORK' transformers\n",
    "    all_outages_df = all_outages_df[~all_outages_df['trsf_pole_nb'].isin(['TRANSMISSION', 'NETWORK', 'PRIMARY'])] \n",
    "    #--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6410b2-5b12-4336-a3eb-6d9eafe57d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ed00e88-9e9f-4837-bad0-74b684f63200",
   "metadata": {},
   "source": [
    "### build_or_load_df_no_outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e3108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------*************************-------------------------*************************\n",
    "# Find the clean windows for each group and build df_no_outage\n",
    "#-------------------------\n",
    "start = time.time()\n",
    "clean_windows_by_grp = OutageDAQOtBL.find_clean_windows(\n",
    "    df                     = all_outages_df, \n",
    "    groupby_col            = groupby_col, \n",
    "    min_window_width       = min_window_width, \n",
    "    buffer_time_left       = buffer_time_left, \n",
    "    buffer_time_rght       = buffer_time_rght, \n",
    "    date_1                 = date_1, \n",
    "    set_search_window      = True, \n",
    "    pd_selection_stategy   = pd_selection_stategy, \n",
    "    search_window_strategy = search_window_strategy, \n",
    "    outg_beg_col           = 'DT_OFF_TS_FULL', \n",
    "    outg_end_col           = 'DT_ON_TS'\n",
    ")\n",
    "print(time.time()-start)\n",
    "#-------------------------\n",
    "# All groups (trsf_pole_nbs, PREMISE_NBs, etc.) in clean_windows_by_grp should also be found in all_outages_df, \n",
    "#   but the reverse is not true\n",
    "assert(len(set(clean_windows_by_grp[groupby_col].unique()).difference(set(all_outages_df[groupby_col].unique())))==0)\n",
    "\n",
    "# Groups where no clean time period was found\n",
    "grps_with_no_clean = all_outages_df[~all_outages_df[groupby_col].isin(clean_windows_by_grp[groupby_col].unique())]\n",
    "\n",
    "print(f\"groupby_col = {groupby_col}\")\n",
    "print(f\"a. # Groups:                      {all_outages_df[groupby_col].nunique()}\")\n",
    "print(f\"b. # Groups with clean period:    {clean_windows_by_grp[groupby_col].nunique()}\")\n",
    "print(f\"c. # Groups without clean period: {len(set(all_outages_df[groupby_col].unique()).difference(set(clean_windows_by_grp[groupby_col].unique())))}\")\n",
    "print(\"NOTE: There may be a difference of 1 between a and b+c due to fact that nunique() does not including NaNs but unique does\")\n",
    "#--------------------------------------------------\n",
    "if save_dfs_to_file:\n",
    "    all_outages_df.to_pickle(os.path.join(save_dir_base, 'all_outages_df.pkl'))\n",
    "    clean_windows_by_grp.to_pickle(os.path.join(save_dir_base, 'clean_windows_by_grp.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0782350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_outages_df=pd.read_pickle(os.path.join(save_dir_base, 'all_outages_df.pkl'))\n",
    "# clean_windows_by_grp=pd.read_pickle(os.path.join(save_dir_base, 'clean_windows_by_grp.pkl'))\n",
    "# df_no_outage=pd.read_pickle(os.path.join(save_dir_base, 'df_no_outage.pkl'))\n",
    "# df_mp = pd.read_pickle(os.path.join(save_dir_base, 'df_mp_no_outg.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43723a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------*************************-------------------------*************************\n",
    "# Merge clean_windows_by_grp with df_mp\n",
    "#-------------------------\n",
    "clean_windows_by_grp_mrg_mp = pd.merge(\n",
    "    clean_windows_by_grp, \n",
    "    df_mp, \n",
    "    left_on='trsf_pole_nb', \n",
    "    right_on='trsf_pole_nb', \n",
    "    how='left'\n",
    ")\n",
    "clean_windows_by_grp_mrg_mp = Utilities_df.convert_col_types(\n",
    "    df                  = clean_windows_by_grp_mrg_mp, \n",
    "    cols_and_types_dict = {\n",
    "        'inst_ts' : datetime.datetime, \n",
    "        'rmvl_ts' : datetime.datetime\n",
    "    }, \n",
    "    to_numeric_errors   = 'coerce', \n",
    "    inplace             = True\n",
    ")\n",
    "\n",
    "\n",
    "clean_windows_by_grp_mrg_mp = clean_windows_by_grp_mrg_mp[\n",
    "    (clean_windows_by_grp_mrg_mp['inst_ts'].fillna(pd.Timestamp.min) <= clean_windows_by_grp_mrg_mp['t_search_min']) & \n",
    "    (clean_windows_by_grp_mrg_mp['rmvl_ts'].fillna(pd.Timestamp.max)  > clean_windows_by_grp_mrg_mp['t_search_max'])\n",
    "]\n",
    "#--------------------------------------------------\n",
    "if save_dfs_to_file:\n",
    "    clean_windows_by_grp_mrg_mp.to_pickle(os.path.join(save_dir_base, 'clean_windows_by_grp_mrg_mp.pkl'))\n",
    "#--------------------------------------------------\n",
    "df_no_outage = clean_windows_by_grp_mrg_mp.copy()\n",
    "df_no_outage = df_no_outage.sort_values(by=[groupby_col, 'prem_nb', 't_search_min'], ignore_index=True)\n",
    "#--------------------------------------------------\n",
    "# Add no_outg_rec_nb column to allow easier grouping when building rcpo_dfs\n",
    "rand_pfx                       = Utilities.generate_random_string(str_len=5, letters=string.ascii_letters + string.digits)\n",
    "df_no_outage['no_outg_rec_nb'] = df_no_outage.groupby(['trsf_pole_nb', 't_search_min', 't_search_max']).ngroup()\n",
    "df_no_outage['no_outg_rec_nb'] = rand_pfx + df_no_outage['no_outg_rec_nb'].astype(str)\n",
    "#--------------------------------------------------\n",
    "if save_dfs_to_file:\n",
    "    df_no_outage.to_pickle(os.path.join(save_dir_base, 'df_no_outage_FINAL.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e71a3-bc5b-44fb-a864-783cdd442864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "646df26d-15d4-47a5-b8a6-4df2bd3fad8e",
   "metadata": {},
   "source": [
    "### build_or_load_df_no_outage_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------*************************-------------------------*************************\n",
    "# Convert df_no_outage to consolidated (slim) verion if run_using_slim\n",
    "#-------------------------\n",
    "if run_using_slim:\n",
    "    #--------------------------------------------------\n",
    "    # Convert to slim \n",
    "    cols_shared_by_group     = None\n",
    "    cols_to_collect_in_lists = ['prem_nb']\n",
    "    rename_cols              = {'prem_nb':'premise_nbs'}\n",
    "    if groupby_col=='trsf_pole_nb':\n",
    "        cols_to_collect_in_lists.append('mfr_devc_ser_nbr')\n",
    "        rename_cols['mfr_devc_ser_nbr'] = 'serial_numbers'\n",
    "    #-------------------------\n",
    "    if groupby_col=='trsf_pole_nb':\n",
    "        consol_groupby_cols = ['no_outg_rec_nb', groupby_col, 't_search_min', 't_search_max']\n",
    "    elif groupby_col=='PREMISE_NB':\n",
    "        consol_groupby_cols =[ 'no_outg_rec_nb', 't_search_min', 't_search_max']\n",
    "    else:\n",
    "        assert(0)\n",
    "    #-------------------------\n",
    "    if search_window_strategy=='all_subwindows':\n",
    "        consol_groupby_cols.append('is_first_after_outg')\n",
    "    #-------------------------\n",
    "    df_no_outage_slim = Utilities_df.consolidate_df(\n",
    "        df                       = df_no_outage, \n",
    "        groupby_cols             = consol_groupby_cols, \n",
    "        cols_shared_by_group     = cols_shared_by_group, \n",
    "        cols_to_collect_in_lists = cols_to_collect_in_lists, \n",
    "        rename_cols              = rename_cols, \n",
    "        verbose                  = True\n",
    "    )\n",
    "    #-------------------------\n",
    "    df_no_outage_slim=df_no_outage_slim.reset_index()\n",
    "    if groupby_col=='trsf_pole_nb':\n",
    "        df_no_outage_slim = df_no_outage_slim.set_index(['no_outg_rec_nb', 'trsf_pole_nb'], drop=False)\n",
    "        df_no_outage_slim.index.names=['idx_no_outg_rec_nb', 'idx_trsf_pole_nb']\n",
    "    #-------------------------\n",
    "    df_no_outage_slim['premise_nbs'] = df_no_outage_slim['premise_nbs'].apply(sorted)\n",
    "    if groupby_col=='trsf_pole_nb':\n",
    "        df_no_outage_slim['serial_numbers'] = df_no_outage_slim['serial_numbers'].apply(sorted)\n",
    "    #-------------------------\n",
    "    if save_dfs_to_file:\n",
    "        df_no_outage_slim.to_pickle(os.path.join(save_dir_base, 'df_no_outage_slim.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e981b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9965a5e1",
   "metadata": {},
   "source": [
    "# Collect events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ebbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_no_outage = pd.read_pickle(os.path.join(save_dir_base, 'df_no_outage_FINAL.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726515ed-4079-4464-b3c1-cb50dfa84c1d",
   "metadata": {},
   "source": [
    "### collect_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------\n",
    "df_construct_type             = DFConstructType.kRunSqlQuery\n",
    "contstruct_df_args_end_events = None\n",
    "\n",
    "\n",
    "\n",
    "cols_of_interest_end_dev_event = TableInfos.AMIEndEvents_TI.std_columns_of_interest\n",
    "verbose  = True\n",
    "n_update = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if groupby_col=='trsf_pole_nb':\n",
    "    addtnl_groupby_cols = ['trsf_pole_nb', 'no_outg_rec_nb']\n",
    "if groupby_col=='PREMISE_NB':\n",
    "    addtnl_groupby_cols = ['no_outg_rec_nb']\n",
    "#-------------------------\n",
    "if search_window_strategy=='all_subwindows':\n",
    "    addtnl_groupby_cols.append('is_first_after_outg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c007a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "end_events_sql_function_kwargs = dict(\n",
    "    cols_of_interest                  = cols_of_interest_end_dev_event, \n",
    "    join_mp_args                      = False, \n",
    "    df_args                           = dict(\n",
    "        addtnl_groupby_cols = addtnl_groupby_cols, \n",
    "        t_search_min_col    = 't_search_min', \n",
    "        t_search_max_col    = 't_search_max'\n",
    "    ), \n",
    "    field_to_split                    = 'df_mp_no_outg', \n",
    "    field_to_split_location_in_kwargs = ['df_mp_no_outg'], \n",
    "    sort_coll_to_split                = False,\n",
    "    verbose                           = verbose, \n",
    "    n_update                          = n_update\n",
    ")\n",
    "#----------\n",
    "# if opcos is not None:\n",
    "addtnl_end_events_sql_function_kwargs = dict(\n",
    "    build_sql_function_kwargs = dict(\n",
    "        states = states, \n",
    "        opcos  = opcos, \n",
    "        cities = cities\n",
    "    )\n",
    ")\n",
    "end_events_sql_function_kwargs = {\n",
    "    **end_events_sql_function_kwargs, \n",
    "    **addtnl_end_events_sql_function_kwargs\n",
    "}\n",
    "#--------------------------------------------------\n",
    "if run_using_slim:\n",
    "    batch_size = 10\n",
    "    #----------\n",
    "    end_events_sql_function_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "        to_supplmnt_dict    = end_events_sql_function_kwargs, \n",
    "        default_values_dict = dict(\n",
    "            df_mp_no_outg = df_no_outage_slim, \n",
    "            batch_size    = batch_size, \n",
    "            df_args       = dict(\n",
    "                mapping_to_ami     = DfToSqlMap(df_col='premise_nbs', kwarg='premise_nbs', sql_col='aep_premise_nb'), \n",
    "                is_df_consolidated = True\n",
    "            )\n",
    "        ), \n",
    "        extend_any_lists    = True,\n",
    "        inplace             = True\n",
    "    )\n",
    "#-------------------------\n",
    "else:\n",
    "    batch_size=200\n",
    "    #----------\n",
    "    if groupby_col=='trsf_pole_nb':\n",
    "        df_no_outage=df_no_outage.sort_values(by=['no_outg_rec_nb', 'trsf_pole_nb', 'prem_nb', 't_search_min'], ignore_index=True)\n",
    "    if groupby_col=='PREMISE_NB':\n",
    "        df_no_outage=df_no_outage.sort_values(by=['no_outg_rec_nb', 'PREMISE_NB', 't_search_min'], ignore_index=True)\n",
    "    #----------\n",
    "    end_events_sql_function_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "        to_supplmnt_dict = end_events_sql_function_kwargs, \n",
    "        default_values_dict = dict(\n",
    "            df_mp_no_outg = df_no_outage, \n",
    "            batch_size    = batch_size, \n",
    "            df_args       = dict(\n",
    "                mapping_to_ami     = DfToSqlMap(df_col='prem_nb', kwarg='premise_nbs', sql_col='aep_premise_nb'), \n",
    "                is_df_consolidated = False\n",
    "            )\n",
    "        ), \n",
    "        extend_any_lists = True,\n",
    "        inplace          = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb519a-4928-44f5-a7e1-23583b9d03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "exit_status = Utilities.run_tryexceptwhile_process(\n",
    "    func                = AMIEndEvents,\n",
    "    func_args_dict      = dict(\n",
    "        df_construct_type         = df_construct_type, \n",
    "        contstruct_df_args        = contstruct_df_args_end_events, \n",
    "        build_sql_function        = AMIEndEvents_SQL.build_sql_end_events_for_no_outages, \n",
    "        build_sql_function_kwargs = end_events_sql_function_kwargs, \n",
    "        init_df_in_constructor    = True, \n",
    "        save_args                 = end_events_save_args\n",
    "    ), \n",
    "    max_calls_per_min   = 1, \n",
    "    lookback_period_min = 15, \n",
    "    max_calls_absolute  = 1000, \n",
    "    verbose             = True\n",
    ")\n",
    "print(f'exit_status = {exit_status}')\n",
    "\n",
    "build_time = time.time()-start\n",
    "print(build_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d14af-5f2e-419e-982d-0ee1a6a4c5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104dee3-73dd-4f5a-9e2d-374d725a55c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86ce09-340e-455c-be0e-baf486ede7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0330dab-d3f5-420f-bd06-411736985f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
