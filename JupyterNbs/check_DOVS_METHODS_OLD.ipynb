{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39904fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns\n",
    "from packaging import version\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import adjustText\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "import Utilities_dt\n",
    "from Utilities_df import DFConstructType\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import Plot_Hist\n",
    "import GrubbsTest\n",
    "import DataFrameSubsetSlicer\n",
    "from DataFrameSubsetSlicer import DataFrameSubsetSlicer as DFSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025651d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a63af59",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# BEGIN ACQUISITION METHODS\n",
    "# ==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb0857",
   "metadata": {},
   "source": [
    "# TODO Need to update build_active_MP_for_outages_df in other notebooks, as the method below is the most recent\n",
    "Also need to add build_active_MP_for_xfmrs_in_outages_df (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d128dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_active_MP_for_outages(\n",
    "    df_outage, \n",
    "    prem_nb_col, \n",
    "    df_mp_curr, \n",
    "    df_mp_hist, \n",
    "    drop_inst_rmvl_cols=False, \n",
    "    outg_rec_nb_col='OUTG_REC_NB',  #TODO!!!!!!!!!!!!!!!!!!!!!!! what if index?!\n",
    "    is_slim=False, \n",
    "    drop_approx_duplicates=True, \n",
    "    drop_approx_duplicates_args=None, \n",
    "    dt_on_ts_col='DT_ON_TS', \n",
    "    df_off_ts_full_col='DT_OFF_TS_FULL', \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Intended to be used within build_active_MP_for_outages_df and build_active_MP_for_xfmrs_in_outages_df.\n",
    "    This does not mean it cannot be used elsewhere, but use with caution (understand functionality before use!)\n",
    "    \"\"\"\n",
    "    # Only reason for making dict is to ensure outg_rec_nbs are not repeated \n",
    "    active_SNs_in_outgs_dfs_dict = {}\n",
    "\n",
    "    if not is_slim:\n",
    "        for outg_rec_nb_i, df_i in df_outage.groupby(outg_rec_nb_col):\n",
    "            # Don't want to include outg_rec_nb_i=-2147483648\n",
    "            if int(outg_rec_nb_i) < 0:\n",
    "                continue\n",
    "            # There should only be a single unique dt_on_ts and dt_off_ts_full for each outage\n",
    "            if(df_i[dt_on_ts_col].nunique()!=1 or \n",
    "               df_i[df_off_ts_full_col].nunique()!=1):\n",
    "                print(f'outg_rec_nb_i = {outg_rec_nb_i}')\n",
    "                print(f'df_i[dt_on_ts_col].nunique()       = {df_i[dt_on_ts_col].nunique()}')\n",
    "                print(f'df_i[df_off_ts_full_col].nunique() = {df_i[df_off_ts_full_col].nunique()}')\n",
    "                print('CRASH IMMINENT!')\n",
    "                assert(0)\n",
    "            # Grab power out/on time and PNs from df_i\n",
    "            dt_on_ts_i       = df_i[dt_on_ts_col].unique()[0]\n",
    "            df_off_ts_full_i = df_i[df_off_ts_full_col].unique()[0]\n",
    "            PNs_i            = df_i[prem_nb_col].unique().tolist()\n",
    "\n",
    "            # Just as was done above for PNs, NaN values must be removed from PNs_i\n",
    "            #   The main purpose here is to remove instances where PNs_i = [nan]\n",
    "            #   NOTE: For case of slim df, the NaNs should already be removed\n",
    "            # After removal, if len(PNs_i)==0, contine\n",
    "            PNs_i = [x for x in PNs_i if pd.notna(x)]\n",
    "            if len(PNs_i)==0:\n",
    "                continue\n",
    "            \n",
    "            # Build active_SNs_df_i and add it to active_SNs_in_outgs_dfs_dict\n",
    "            # NOTE: assume_one_xfmr_per_PN=True above in MeterPremise.build_mp_df_curr_hist_for_PNs,\n",
    "            #       so does not need to be set again (i.e., assume_one_xfmr_per_PN=False below)\n",
    "            active_SNs_df_i = MeterPremise.get_active_SNs_for_PNs_at_datetime_interval(\n",
    "                PNs=PNs_i,\n",
    "                df_mp_curr=df_mp_curr, \n",
    "                df_mp_hist=df_mp_hist, \n",
    "                dt_0=df_off_ts_full_i,\n",
    "                dt_1=dt_on_ts_i,\n",
    "                addtnl_mp_df_curr_cols=None, \n",
    "                addtnl_mp_df_hist_cols=None,\n",
    "                assume_one_xfmr_per_PN=False, \n",
    "                output_index=None,\n",
    "                output_groupby=None, \n",
    "                include_prems_wo_active_SNs_when_groupby=True, \n",
    "                assert_all_PNs_found=False, \n",
    "                drop_approx_duplicates=drop_approx_duplicates, \n",
    "                drop_approx_duplicates_args=drop_approx_duplicates_args, \n",
    "                df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "                df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "                df_mp_install_time_col=df_mp_install_time_col, \n",
    "                df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "                df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "            )\n",
    "            active_SNs_df_i[outg_rec_nb_col] = outg_rec_nb_i\n",
    "            assert(outg_rec_nb_i not in active_SNs_in_outgs_dfs_dict)\n",
    "            active_SNs_in_outgs_dfs_dict[outg_rec_nb_i] = active_SNs_df_i\n",
    "    else:\n",
    "        for outg_rec_nb_i, row_i in df_outage.iterrows():\n",
    "            # NOTE: assume_one_xfmr_per_PN=True above in MeterPremise.build_mp_df_curr_hist_for_PNs,\n",
    "            #       so does not need to be set again (i.e., assume_one_xfmr_per_PN=False below)\n",
    "            active_SNs_df_i = MeterPremise.get_active_SNs_for_PNs_at_datetime_interval(\n",
    "                PNs=row_i[prem_nb_col],\n",
    "                df_mp_curr=df_mp_curr, \n",
    "                df_mp_hist=df_mp_hist, \n",
    "                dt_0=row_i[df_off_ts_full_col],\n",
    "                dt_1=row_i[dt_on_ts_col],\n",
    "                addtnl_mp_df_curr_cols=None, \n",
    "                addtnl_mp_df_hist_cols=None,\n",
    "                assume_one_xfmr_per_PN=False, \n",
    "                output_index=None,\n",
    "                output_groupby=None, \n",
    "                include_prems_wo_active_SNs_when_groupby=True, \n",
    "                assert_all_PNs_found=False, \n",
    "                drop_approx_duplicates=drop_approx_duplicates, \n",
    "                drop_approx_duplicates_args=drop_approx_duplicates_args, \n",
    "                df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "                df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "                df_mp_install_time_col=df_mp_install_time_col, \n",
    "                df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "                df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col                \n",
    "            )\n",
    "            active_SNs_df_i[outg_rec_nb_col] = outg_rec_nb_i\n",
    "            assert(outg_rec_nb_i not in active_SNs_in_outgs_dfs_dict)\n",
    "            active_SNs_in_outgs_dfs_dict[outg_rec_nb_i] = active_SNs_df_i\n",
    "    #-------------------------\n",
    "    active_SNs_df = pd.concat(list(active_SNs_in_outgs_dfs_dict.values()))\n",
    "    #-------------------------\n",
    "    if drop_inst_rmvl_cols:\n",
    "        active_SNs_df = active_SNs_df.drop(columns=[df_mp_install_time_col, df_mp_removal_time_col])\n",
    "    #-------------------------\n",
    "    return active_SNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33c870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In MeterPremise, can I create a function which will build mp_df_hist given mp_df_curr?\n",
    "In DOVSOutages I need to build new build_mp_for_outg (and update its use throughout)\n",
    "    APPARENTLY I already have this, build_active_MP_for_outages or one of other similar functions\n",
    "    \n",
    "I need to basically replace everything in DOVSOutages which uses build_mp_for_outg\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_active_MP_for_outages_df(\n",
    "    df_outage, \n",
    "    prem_nb_col, \n",
    "    df_mp_curr=None, \n",
    "    df_mp_hist=None, \n",
    "    assert_all_PNs_found=True, \n",
    "    drop_inst_rmvl_cols=False, \n",
    "    outg_rec_nb_col='OUTG_REC_NB',  #TODO!!!!!!!!!!!!!!!!!!!!!!! what if index?!\n",
    "    is_slim=False, \n",
    "    addtnl_mp_df_curr_cols=None, \n",
    "    addtnl_mp_df_hist_cols=None, \n",
    "    dt_on_ts_col='DT_ON_TS', \n",
    "    df_off_ts_full_col='DT_OFF_TS_FULL', \n",
    "    consolidate_PNs_batch_size=1000, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb', \n",
    "    early_return=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Similar to build_active_MP_for_outages\n",
    "    \n",
    "    If addtnl_mp_df_curr_cols and addtnl_mp_df_hist_cols are included (i.e., are not None), their intersection will be added\n",
    "      as addtnl_groupby_cols input argument to drop_approx_duplicates (otherwise, they would be returned in the DF as list\n",
    "      objects, and would likely need to be exploded)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(prem_nb_col in df_outage.columns and \n",
    "           dt_on_ts_col in df_outage.columns and \n",
    "           df_off_ts_full_col in df_outage.columns)\n",
    "    #-------------------------\n",
    "    if not is_slim:\n",
    "        PNs = df_outage[prem_nb_col].unique().tolist()\n",
    "    else:\n",
    "        PNs = Utilities_df.consolidate_column_of_lists(\n",
    "            df=df_outage, \n",
    "            col=prem_nb_col, \n",
    "            sort=True,\n",
    "            include_None=False,\n",
    "            batch_size=consolidate_PNs_batch_size, \n",
    "            verbose=False\n",
    "        )\n",
    "    #-----\n",
    "    PNs = [x for x in PNs if pd.notna(x)]\n",
    "    PNs_type = type(PNs[0])\n",
    "    assert(Utilities.are_all_list_elements_of_type(PNs, PNs_type))\n",
    "    #-------------------------\n",
    "    if addtnl_mp_df_curr_cols is not None and addtnl_mp_df_hist_cols is not None:\n",
    "        assert(Utilities.is_object_one_of_types(addtnl_mp_df_curr_cols, [list, tuple]))\n",
    "        assert(Utilities.is_object_one_of_types(addtnl_mp_df_hist_cols, [list, tuple]))\n",
    "        drop_approx_duplicates_args = dict(\n",
    "            addtnl_groupby_cols=list(set(addtnl_mp_df_curr_cols).intersection(set(addtnl_mp_df_hist_cols)))\n",
    "        )\n",
    "    else:\n",
    "        drop_approx_duplicates_args=None\n",
    "    #-----\n",
    "    mp_df_curr_hist_dict = MeterPremise.build_mp_df_curr_hist_for_PNs(\n",
    "        PNs=PNs, \n",
    "        mp_df_curr=df_mp_curr,\n",
    "        mp_df_hist=df_mp_hist, \n",
    "        join_curr_hist=False, \n",
    "        addtnl_mp_df_curr_cols=addtnl_mp_df_curr_cols, \n",
    "        addtnl_mp_df_hist_cols=addtnl_mp_df_hist_cols, \n",
    "        assert_all_PNs_found=assert_all_PNs_found, \n",
    "        assume_one_xfmr_per_PN=True, \n",
    "        drop_approx_duplicates=True, \n",
    "        drop_approx_duplicates_args=drop_approx_duplicates_args, \n",
    "        df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "        df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "        df_mp_install_time_col=df_mp_install_time_col, \n",
    "        df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "        df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "    )\n",
    "    df_mp_curr = mp_df_curr_hist_dict['mp_df_curr']\n",
    "    df_mp_hist = mp_df_curr_hist_dict['mp_df_hist']\n",
    "    if early_return:\n",
    "        return df_mp_curr, df_mp_hist\n",
    "    print(f'df_mp_curr.shape = {df_mp_curr.shape}')\n",
    "    print(f'df_mp_hist.shape = {df_mp_hist.shape}')\n",
    "    #-------------------------\n",
    "    active_SNs_df = reduce_active_MP_for_outages(\n",
    "        df_outage=df_outage, \n",
    "        prem_nb_col=prem_nb_col, \n",
    "        df_mp_curr=df_mp_curr, \n",
    "        df_mp_hist=df_mp_hist, \n",
    "        drop_inst_rmvl_cols=drop_inst_rmvl_cols, \n",
    "        outg_rec_nb_col=outg_rec_nb_col, \n",
    "        is_slim=is_slim, \n",
    "        drop_approx_duplicates=True, \n",
    "        drop_approx_duplicates_args=drop_approx_duplicates_args, \n",
    "        dt_on_ts_col=dt_on_ts_col, \n",
    "        df_off_ts_full_col=df_off_ts_full_col, \n",
    "        df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "        df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "        df_mp_install_time_col=df_mp_install_time_col, \n",
    "        df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "        df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    return active_SNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9557b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO !!!!!!!!!!!! IMPORTANT !!!!!!!!!!!!!!\n",
    "# In order for this to function as desired, reduce_active_MP_for_outages must group by transformer pole number\n",
    "#   for this case, not outage number as in the build_active_MP_for_outages_df case\n",
    "# Some other adjustment may be needed as well!\n",
    "# NOTE: There are some instances where build_active_MP_for_outages_df will find PNs not found by \n",
    "#         build_active_MP_for_xfmrs_in_outages_df.\n",
    "#       The reason being that the former doesn't care whether or not the trsf_pole_nb is NaN, but this function does!\n",
    "# May want to, e.g., add in some sort of dropna option\n",
    "# However, what would one do in such a situation?  I guess you would have to make an or statement, allowing\n",
    "#   a PN to have the correct transformer OR be listed in df_outage is partaking in the outage?\n",
    "# Not too sure about this, but it will obviously need some work.\n",
    "# For example where build_active_MP_for_outages_df finds additional PN, see outg_rec_nb 13297062, where\n",
    "#   mfr_devc_ser_nbr=995582422 is found by build_active_MP_for_outages_df but not by build_active_MP_for_xfmrs_in_outages_df\n",
    "#   (because only found in historic, and therefore the trsf_pole_nb could not be determined!)\n",
    "#\n",
    "# May need to combine those with trsf_pole_nb=na in mp to those in mp_df_curr_hist_dict\n",
    "#   One could, e.g., call MeterPremise.build_mp_df_curr_hist_for_PNs for any PNs without a transformer \n",
    "#   pole number, and still use MeterPremise.build_mp_df_curr_hist_for_xfmrs for others\n",
    "def build_active_MP_for_xfmrs_in_outages_df(\n",
    "    df_outage, \n",
    "    prem_nb_col, \n",
    "    df_outage_trsf_pole_nb_col=None, \n",
    "    drop_inst_rmvl_cols=False, \n",
    "    outg_rec_nb_col='OUTG_REC_NB',  #TODO!!!!!!!!!!!!!!!!!!!!!!! what if index?!\n",
    "    is_slim=False, \n",
    "    addtnl_mp_df_curr_cols=None, \n",
    "    addtnl_mp_df_hist_cols=None, \n",
    "    dt_on_ts_col='DT_ON_TS', \n",
    "    df_off_ts_full_col='DT_OFF_TS_FULL', \n",
    "    consolidate_PNs_batch_size=1000, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb', \n",
    "    early_return=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Similar to build_active_MP_for_outages/build_active_MP_for_outages_df\n",
    "    \n",
    "    df_outage_trsf_pole_nb_col:\n",
    "        If trsf_pole_nb already present in df_outage, set this equal to the column.\n",
    "        Otherwise, the trsf_pole_nbs will be acquired from the current Meter Premise table\n",
    "        \n",
    "    If addtnl_mp_df_curr_cols and addtnl_mp_df_hist_cols are included (i.e., are not None), their intersection will be added\n",
    "      as addtnl_groupby_cols input argument to drop_approx_duplicates (otherwise, they would be returned in the DF as list\n",
    "      objects, and would likely need to be exploded)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(prem_nb_col in df_outage.columns and \n",
    "           dt_on_ts_col in df_outage.columns and \n",
    "           df_off_ts_full_col in df_outage.columns)\n",
    "    #-------------------------\n",
    "    # If trsf_pole_nbs present in df_outage, simply grab.\n",
    "    # Otherwise, use current Meter Premise table together with PNs from df_outage to acquire trsf_pole_nbs\n",
    "    if df_outage_trsf_pole_nb_col is not None and df_outage_trsf_pole_nb_col in df_outage.columns:\n",
    "        trsf_pole_nbs = df_outage[df_outage_trsf_pole_nb_col].unique().tolist()\n",
    "    else:\n",
    "        if not is_slim:\n",
    "            PNs = df_outage[prem_nb_col].unique().tolist()\n",
    "        else:\n",
    "            PNs = Utilities_df.consolidate_column_of_lists(\n",
    "                df=df_outage, \n",
    "                col=prem_nb_col, \n",
    "                sort=True,\n",
    "                include_None=False,\n",
    "                batch_size=consolidate_PNs_batch_size, \n",
    "                verbose=False\n",
    "            )\n",
    "        #-----\n",
    "        PNs = [x for x in PNs if pd.notna(x)]\n",
    "        PNs_type = type(PNs[0])\n",
    "        assert(Utilities.are_all_list_elements_of_type(PNs, PNs_type))\n",
    "        #-------------------------\n",
    "        mp_cols_of_interest=[df_mp_prem_nb_col, df_mp_trsf_pole_nb_col]\n",
    "        mp = MeterPremise(\n",
    "            df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "            init_df_in_constructor=True, \n",
    "            build_sql_function=MeterPremise.build_sql_meter_premise, \n",
    "            build_sql_function_kwargs=dict(\n",
    "                cols_of_interest=mp_cols_of_interest, \n",
    "                premise_nbs=PNs, \n",
    "                table_name='meter_premise'\n",
    "            )\n",
    "        )\n",
    "        mp_df = mp.df\n",
    "        #-------------------------\n",
    "        trsf_pole_nbs = mp_df[df_mp_trsf_pole_nb_col].unique().tolist()\n",
    "    #--------------------------------------------------\n",
    "    if addtnl_mp_df_curr_cols is not None and addtnl_mp_df_hist_cols is not None:\n",
    "        assert(Utilities.is_object_one_of_types(addtnl_mp_df_curr_cols, [list, tuple]))\n",
    "        assert(Utilities.is_object_one_of_types(addtnl_mp_df_hist_cols, [list, tuple]))\n",
    "        drop_approx_duplicates_args = dict(\n",
    "            addtnl_groupby_cols=list(set(addtnl_mp_df_curr_cols).intersection(set(addtnl_mp_df_hist_cols)))\n",
    "        )\n",
    "    else:\n",
    "        drop_approx_duplicates_args=None\n",
    "    #-----\n",
    "    mp_df_curr_hist_dict = MeterPremise.build_mp_df_curr_hist_for_xfmrs(\n",
    "        trsf_pole_nbs=trsf_pole_nbs, \n",
    "        join_curr_hist=False, \n",
    "        addtnl_mp_df_curr_cols=addtnl_mp_df_curr_cols, \n",
    "        addtnl_mp_df_hist_cols=addtnl_mp_df_hist_cols, \n",
    "        assume_one_xfmr_per_PN=True, \n",
    "        drop_approx_duplicates=True, \n",
    "        drop_approx_duplicates_args=drop_approx_duplicates_args, \n",
    "        df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "        df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "        df_mp_install_time_col=df_mp_install_time_col, \n",
    "        df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "        df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "    )\n",
    "    df_mp_curr = mp_df_curr_hist_dict['mp_df_curr']\n",
    "    df_mp_hist = mp_df_curr_hist_dict['mp_df_hist']\n",
    "    if early_return:\n",
    "        return df_mp_curr, df_mp_hist\n",
    "    print(f'df_mp_curr.shape = {df_mp_curr.shape}')\n",
    "    print(f'df_mp_hist.shape = {df_mp_hist.shape}')\n",
    "    #-------------------------\n",
    "    active_SNs_df = reduce_active_MP_for_outages(\n",
    "        df_outage=df_outage, \n",
    "        prem_nb_col=prem_nb_col, \n",
    "        df_mp_curr=df_mp_curr, \n",
    "        df_mp_hist=df_mp_hist, \n",
    "        drop_inst_rmvl_cols=drop_inst_rmvl_cols, \n",
    "        outg_rec_nb_col=outg_rec_nb_col, \n",
    "        is_slim=is_slim, \n",
    "        drop_approx_duplicates=True, \n",
    "        drop_approx_duplicates_args=drop_approx_duplicates_args, \n",
    "        dt_on_ts_col=dt_on_ts_col, \n",
    "        df_off_ts_full_col=df_off_ts_full_col, \n",
    "        df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "        df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "        df_mp_install_time_col=df_mp_install_time_col, \n",
    "        df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "        df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    return active_SNs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c9b2e",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# END ACQUISITION METHODS\n",
    "# ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f09761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664e1e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MOVED TO Utilities_df\n",
    "# import statistics\n",
    "# def determine_freq_in_df_col(\n",
    "#     df, \n",
    "#     groupby_SN=True, \n",
    "#     return_val_if_not_found='error', \n",
    "#     assert_single_freq=True, \n",
    "#     assert_expected_freq_found=False, \n",
    "#     expected_freq=pd.Timedelta('15 minutes'), \n",
    "#     time_col='starttimeperiod_local', \n",
    "#     SN_col='serialnumber'\n",
    "# ):\n",
    "#     r\"\"\"\n",
    "#     Determine the frequency of the data in df[time_col]\n",
    "#     If the df contains multiple serial numbers, IT IS HIGHLY recommended that one groups by serialnumber,\n",
    "#       finding the frequency of each subset, and inferring the overall frequency of the data from that collection.\n",
    "#         In most cases, not grouping the data for, e.g., and outage by serial number will be fine.\n",
    "#         BUT, there are some instances where this will give unexpected results (more info below).\n",
    "#         THUS, to be safe, follow the recommendation above.\n",
    "        \n",
    "#     return_val_if_not_found:\n",
    "#         This directs what occurs if no frequency is determined.\n",
    "#         Possible values (case insensitive):\n",
    "#             NaN:      return np.nan (which is converted by Python/pandas to NaT)\n",
    "#             expected: return input argument expected_freq\n",
    "#             error:    throw an error\n",
    "#         This is especially useful when finding the frequency for a group of SNs. \n",
    "#             For this case, return_val_if_not_found is set to NaN within the lambda function call to \n",
    "#               determine_freq_in_df_col, allowing some flexibility of not every single SN needing to return \n",
    "#               a frequency for the overall frequency to be determined.\n",
    "#             However, after the groupby method, the overall returned result behaves according to the value of\n",
    "#               return_val_if_not_found, as stated above.\n",
    "        \n",
    "#     assert_single_freq:\n",
    "#         NOTE: This is only enforced if frequencies are found; i.e., if no frequency is determined, the function\n",
    "#               progressed according to the return_val_if_not_found argument.\n",
    "#         -----\n",
    "#         For a single SN level or groupby_SN==False:\n",
    "#             Self-explanatory; if True, assert only a single frequency is found.\n",
    "#             If False (not recommended), and multiple frequencies are found, take the first\n",
    "#         -----\n",
    "#         For multiple SNs with groupby_SN==True:\n",
    "#             True:\n",
    "#                 Enforce a single frequency is found for each SN\n",
    "#                 Enforce all SNs share the same frequency\n",
    "#             False:\n",
    "#                 Do not enforce single frequency found for each SN (i.e., if multiple are found for a single SN, \n",
    "#                   the first is returned).\n",
    "#                 If the collection of frequencies found from SNs contains multiple values, take the mode (if multiple\n",
    "#                   modes found, take the first one)\n",
    "                  \n",
    "#     assert_expected_freq_found:\n",
    "#         Assert the frequency found equals that which is expected.\n",
    "#         NOTE: This is only enforced if a frequency is found; i.e., if no frequency is determined, the function\n",
    "#               progressed according to the return_val_if_not_found argument.        \n",
    "        \n",
    "#     -----\n",
    "#     Potential for unexpected results:\n",
    "#         Consider the following simple example of a pd.DataFrame (ex_df) containing two serial numbers whose times are offset \n",
    "#           relative to each other by one-minute (BTW, this is not some made-up scenario, this is observed in the data)\n",
    "#                 -------------------------\n",
    "#                     serialnumber starttimeperiod_local\n",
    "#                 0               1   2023-01-14 20:45:00\n",
    "#                 1               2   2023-01-14 20:46:00\n",
    "#                 2               1   2023-01-14 21:00:00\n",
    "#                 3               2   2023-01-14 21:01:00\n",
    "#                 4               1   2023-01-14 21:15:00\n",
    "#                 5               2   2023-01-14 21:16:00\n",
    "#                 6               1   2023-01-14 21:30:00\n",
    "#                 7               2   2023-01-14 21:31:00\n",
    "#                 8               1   2023-01-14 21:45:00\n",
    "#                 9               2   2023-01-14 21:46:00\n",
    "#                 10              1   2023-01-14 22:00:00\n",
    "#                 -------------------------                \n",
    "#         Pandas has no idea what the frequency of the data is, and the call \n",
    "#           freq = pd.infer_freq(ex_df['starttimeperiod_local'].unique()) will return a value of None\n",
    "#         Therefore, one falls back to the secondary method of looking for the most frequent difference in the data (i.e.,\n",
    "#           the method freq=stats.mode(np.diff(natsorted(df[time_col].unique()))).mode[0])\n",
    "#         However, the call np.diff(natsorted(ex_df['starttimeperiod_local'].unique())) will return:\n",
    "#           (NOTE: Actually, to make numbers easy to interpret, the call was: \n",
    "#                  [pd.Timedelta(x) for x in np.diff(natsorted(ex_df['starttimeperiod_local'].unique()))])\n",
    "#             -------------------------                  \n",
    "#             [Timedelta('0 days 00:01:00'),\n",
    "#              Timedelta('0 days 00:14:00'),\n",
    "#              Timedelta('0 days 00:01:00'),\n",
    "#              Timedelta('0 days 00:14:00'),\n",
    "#              Timedelta('0 days 00:01:00'),\n",
    "#              Timedelta('0 days 00:14:00'),\n",
    "#              Timedelta('0 days 00:01:00'),\n",
    "#              Timedelta('0 days 00:14:00'),\n",
    "#              Timedelta('0 days 00:01:00'),\n",
    "#              Timedelta('0 days 00:14:00')]\n",
    "#             -------------------------\n",
    "#         The example I have chosen actually has two modes (1 minutes and 14 minutes), both of which are incorrect!\n",
    "#         Using scipy.stats.mode, only a single value is returned, and this appears to be the first, i.e. 1 minute\n",
    "#             ==> freq = stats.mode(np.diff(natsorted(ex_df['starttimeperiod_local'].unique()))).mode[0]\n",
    "#                 freq = pd.Timedelta(freq)\n",
    "#                 GIVES THE RESULT: freq = 1 minute!\n",
    "#         We only expect there to be one frequency of the data, but one could also use statistics.multimode to obtain:\n",
    "#             ==> freq = statistics.multimode(np.diff(natsorted(ex_df['starttimeperiod_local'].unique())))\n",
    "#                 freq = [pd.Timedelta(x) for x in freq]\n",
    "#                 GIVES THE RESULT: freq = [1 minute, 14 minutes]\n",
    "                \n",
    "#         ----------\n",
    "#         SOLUTION: Group the data by serial number, and find the frequency wrt each SN\n",
    "#             ------------------------- \n",
    "#             SN==1\n",
    "#             -----\n",
    "#                 serialnumber starttimeperiod_local\n",
    "#             0              1   2023-01-14 20:45:00\n",
    "#             2              1   2023-01-14 21:00:00\n",
    "#             4              1   2023-01-14 21:15:00\n",
    "#             6              1   2023-01-14 21:30:00\n",
    "#             8              1   2023-01-14 21:45:00\n",
    "#             10             1   2023-01-14 22:00:00\n",
    "#             ------------------------- \n",
    "#             ------------------------- \n",
    "#             SN==2\n",
    "#             -----\n",
    "#                serialnumber starttimeperiod_local\n",
    "#             1             2   2023-01-14 20:46:00\n",
    "#             3             2   2023-01-14 21:01:00\n",
    "#             5             2   2023-01-14 21:16:00\n",
    "#             7             2   2023-01-14 21:31:00\n",
    "#             9             2   2023-01-14 21:46:00\n",
    "#             ------------------------- \n",
    "#         ==> freq_1 == freq_2 == 15 minutes! (with pd.infer_freq or secondary method!)\n",
    "#         ----------\n",
    "#     \"\"\"\n",
    "#     #-------------------------\n",
    "#     if not groupby_SN and df[SN_col].nunique()>1:\n",
    "#         print('Multiple SNs exist in df.\\nGrouping by SN is HIGHLY RECOMMENDED!')\n",
    "#     #-------------------------\n",
    "#     return_val_if_not_found = return_val_if_not_found.lower()\n",
    "#     assert(return_val_if_not_found in ['nan', 'expected', 'error'])\n",
    "#     #-------------------------\n",
    "#     # Strategy below will be to develop the code as if being written for case where df has a single SN (or, the case\n",
    "#     #   where groupby_SN==False).\n",
    "#     # Then, if groupby_SN is True, one can call the function from inside a groupby lambda function (with groupby_SN\n",
    "#     #   set to False within the lambda call)\n",
    "#     #----------------------------------------------------------------------------------------------------\n",
    "#     if not groupby_SN:\n",
    "#         # First, try pd.infer_freq method\n",
    "#         # NOTE: infer_freq returns None if no frequency could be determined and raises a TypeError \n",
    "#         #         if data are not datetime-like and ValueError is few than 3 values\n",
    "#         #       try/except below eliminates errors from being raise, setting freq to None in such cases instead.\n",
    "#         try:\n",
    "#             freq=pd.infer_freq(natsorted(df[time_col].unique()))\n",
    "#             # If freq is, e.g., 1 hour, this will return 'H', whereas pd.Timedelta wants\n",
    "#             #   its input to be '1H' and will complain if fed 'H'\n",
    "#             # Thus, check if string starts with digit.  It it doesn't, prepend '1' to it\n",
    "#             if not freq[0].isdigit():\n",
    "#                 freq = '1'+freq\n",
    "#         except:\n",
    "#             freq=None\n",
    "#         #-------------------------\n",
    "#         # If pd.infer_freq was unsuccessful, use the secondary method of looking for the most frequent \n",
    "#         #   difference in the data\n",
    "#         # NOTE!: Cannot enforce assert_single_freq in the try block below, as if the assertion is tripped this\n",
    "#         #        will simply switch the code to the except block!!!!!\n",
    "#         if freq is None:\n",
    "#             try:\n",
    "#                 # NOTE: Below, freq will always be a list\n",
    "#                 freq=statistics.multimode(np.diff(natsorted(df[time_col].unique())))\n",
    "#             except:\n",
    "#                 freq=[]\n",
    "#             #-----\n",
    "#             if len(freq)==0:\n",
    "#                 if return_val_if_not_found=='nan':\n",
    "#                     return np.nan\n",
    "#                 elif return_val_if_not_found=='expected':\n",
    "#                     return pd.Timedelta(expected_freq)\n",
    "#                 elif return_val_if_not_found=='error':\n",
    "#                     assert(0)\n",
    "#                 else:\n",
    "#                     assert(0)\n",
    "#             else:\n",
    "#                 # Enforce assertion and/or set freq equal to first value (as \n",
    "#                 #   freq returned from statistics.multimode is a list)\n",
    "#                 if assert_single_freq:\n",
    "#                     assert(len(freq)==1)\n",
    "#                 freq=freq[0]\n",
    "#         #-------------------------\n",
    "#         freq=pd.Timedelta(freq)\n",
    "#         if assert_expected_freq_found:\n",
    "#             assert(freq==pd.Timedelta(expected_freq))\n",
    "#         #-------------------------\n",
    "#         return freq\n",
    "#     #----------------------------------------------------------------------------------------------------\n",
    "#     # Now, for the more typical case when groupby_SN is True\n",
    "#     # In this case, we essentially call the above methods from within a groupby lambda function\n",
    "#     #-------------------------\n",
    "#     # Get the frequency for each serial number, which will be a pd.Series object\n",
    "#     # NOTE: groupby_SN MUST BE FALSE in the lambda function below, otherwise infinite loop!!!!!\n",
    "#     freqs_srs = df.groupby([SN_col]).apply(\n",
    "#         lambda x: determine_freq_in_df_col(\n",
    "#             df=x, \n",
    "#             groupby_SN=False, \n",
    "#             assert_single_freq=assert_single_freq, \n",
    "#             expected_freq=expected_freq, \n",
    "#             return_val_if_not_found='nan', \n",
    "#             assert_expected_freq_found=assert_expected_freq_found, \n",
    "#             time_col=time_col, \n",
    "#             SN_col=SN_col\n",
    "#         )\n",
    "#     )\n",
    "#     #-------------------------\n",
    "#     freqs_unq_vals = freqs_srs.dropna().unique()\n",
    "#     freqs_modes = statistics.multimode(freqs_unq_vals)\n",
    "#     #-----        \n",
    "#     if len(freqs_modes)==0:\n",
    "#         if return_val_if_not_found=='nan':\n",
    "#             return np.nan\n",
    "#         elif return_val_if_not_found=='expected':\n",
    "#             return pd.Timedelta(expected_freq)\n",
    "#         elif return_val_if_not_found=='error':\n",
    "#             assert(0)\n",
    "#         else:\n",
    "#             assert(0)\n",
    "#     #-----\n",
    "#     if assert_single_freq:\n",
    "#         assert(len(freqs_modes)==1)\n",
    "#     freq=freqs_modes[0]\n",
    "#     freq=pd.Timedelta(freq)\n",
    "#     #-------------------------\n",
    "#     return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6a24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24595a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pd_pu_times_for_meter(\n",
    "    ede_df_i, \n",
    "    t_search_min_max=None, \n",
    "    pd_ids=['3.26.0.47', '3.26.136.47'], \n",
    "    pu_ids=['3.26.0.216', '3.26.136.216'], \n",
    "    SN_col='serialnumber', \n",
    "    valuesinterval_col='valuesinterval_local', \n",
    "    edetypeid_col='enddeviceeventtypeid'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Get the power-down (pd) and power-up (pu) events from ede_df_i within t_search_min_max.\n",
    "    ede_df_i must be a meter events DataFrame containing data from a single meter.\n",
    "    \n",
    "    Returns a dict with two keys, pd_times and pu_times, the values of which are lists containing\n",
    "      the associated times.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(ede_df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    # Make sure valuesinterval_col contains time data\n",
    "    if not is_datetime64_dtype(ede_df_i[valuesinterval_col]):\n",
    "        ede_df_i[valuesinterval_col] = pd.to_datetime(ede_df_i[valuesinterval_col])\n",
    "    ede_df_i = ede_df_i.sort_values(by=[valuesinterval_col])\n",
    "\n",
    "    #------------------------- \n",
    "    if t_search_min_max is not None:\n",
    "        assert(Utilities.is_object_one_of_types(t_search_min_max, [list, tuple]))\n",
    "        assert(len(t_search_min_max)==2)\n",
    "        #-----\n",
    "        # At least one of t_search_min_max should not be None\n",
    "        assert(t_search_min_max[0] is not None or t_search_min_max[1] is not None)\n",
    "        if t_search_min_max[0] is None:\n",
    "            t_search_min_max[0] = pd.Timestamp.min\n",
    "        if t_search_min_max[1] is None:\n",
    "            t_search_min_max[1] = pd.Timestamp.max\n",
    "        #-----\n",
    "        ede_df_i = ede_df_i[\n",
    "            (ede_df_i[valuesinterval_col]>=t_search_min_max[0]) & \n",
    "            (ede_df_i[valuesinterval_col]<=t_search_min_max[1])\n",
    "        ]\n",
    "        \n",
    "    #------------------------- \n",
    "    ede_df_i = ede_df_i.sort_values(by=valuesinterval_col)\n",
    "    #-----\n",
    "    pd_times = ede_df_i[ede_df_i[edetypeid_col].isin(pd_ids)][valuesinterval_col].tolist()\n",
    "    pu_times = ede_df_i[ede_df_i[edetypeid_col].isin(pu_ids)][valuesinterval_col].tolist()\n",
    "    #------------------------- \n",
    "    return dict(\n",
    "        pd_times=pd_times, \n",
    "        pu_times=pu_times\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565d0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is not used much anymore\n",
    "# Typically, I just use best_ests_df to grab which PNs/SNs participate in outage and which do not.\n",
    "def get_full_part_not_outage_SNs(\n",
    "    df, \n",
    "    out_t_beg,\n",
    "    out_t_end, \n",
    "    pct_time_out_required_for_outage=25, \n",
    "    exclusive_partials=False, \n",
    "    t_int_beg_col='starttimeperiod_local', \n",
    "    t_int_end_col='endtimeperiod_local', \n",
    "    value_col='value', \n",
    "    SN_col='serialnumber'    \n",
    "):\n",
    "    r\"\"\"\n",
    "    During the outage time (typically taken from DOVS and defined by out_t_beg and out_t_end), find the serial numbers which were\n",
    "      out (value==0) for the entire period, for part of the period (threshold set by pct_time_out_required_for_outage), \n",
    "      and for none of the period.\n",
    "      \n",
    "    NOTE: By default, the partial outage SNs will include the full outage SNs.\n",
    "          See exclusive_partials below\n",
    "      \n",
    "    pct_time_out_required_for_outage:\n",
    "        In order for a SN to be considered partially out, it must be out (i.e., have value==0) for at least \n",
    "          pct_time_out_required_for_outage percent of the time.\n",
    "          \n",
    "    exclusive_partials:\n",
    "        By default, the partial outage SNs will include the full outage SNs.\n",
    "        If this functionality is not desired, and one wants the two sets to be mutually exclusive, set\n",
    "          exclusive_partials=True\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # First, grab the subset of df occurring during the outage\n",
    "    df_sub = df[\n",
    "        (df[t_int_beg_col]>=out_t_beg) & \n",
    "        (df[t_int_end_col]<out_t_end)\n",
    "    ]\n",
    "    #-------------------------\n",
    "    full_SNs = df_sub.groupby([SN_col]).filter(lambda x: (x[value_col]==0).all())[SN_col].unique().tolist() \n",
    "    #-----\n",
    "    not_SNs = df_sub.groupby([SN_col]).filter(lambda x: (x[value_col]>0).all())[SN_col].unique().tolist()\n",
    "    #-----\n",
    "    part_SNs = df_sub.groupby([SN_col]).filter(lambda x: 100*(x[value_col]==0).sum()/x.shape[0]>pct_time_out_required_for_outage)[SN_col].unique().tolist()\n",
    "    if exclusive_partials:\n",
    "        part_SNs = list(set(part_SNs).difference(set(full_SNs)))\n",
    "    #-------------------------\n",
    "    return dict(\n",
    "        full_SNs=full_SNs, \n",
    "        part_SNs=part_SNs, \n",
    "        not_SNs=not_SNs\n",
    "    )\n",
    "    \n",
    "\n",
    "# This function is not used much anymore\n",
    "# Typically, I just use best_ests_df to grab which PNs/SNs participate in outage and which do not.\n",
    "def get_full_part_not_outage_subset_dfs(\n",
    "    df, \n",
    "    out_t_beg,\n",
    "    out_t_end, \n",
    "    pct_time_out_required_for_outage=0, \n",
    "    return_SNs=False, \n",
    "    exclusive_partials=False, \n",
    "    t_int_beg_col='starttimeperiod_local', \n",
    "    t_int_end_col='endtimeperiod_local', \n",
    "    value_col='value', \n",
    "    SN_col='serialnumber'    \n",
    "):\n",
    "    r\"\"\"\n",
    "    The full, partial, and not outage SNs are determined in the outage period defined by out_t_beg and out_t_end, but the returned\n",
    "      subsets are for all times in df (i.e., not restricted to outage time)\n",
    "    i.e., the SNs for the full, partial, and not are found using get_full_part_not_outage_SNs during outage times, and then\n",
    "      the subsets for df containing those SNs are projected out and returned.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    full_part_not_dict = get_full_part_not_outage_SNs(\n",
    "        df=df, \n",
    "        out_t_beg=out_t_beg,\n",
    "        out_t_end=out_t_end, \n",
    "        pct_time_out_required_for_outage=pct_time_out_required_for_outage, \n",
    "        exclusive_partials=exclusive_partials, \n",
    "        t_int_beg_col=t_int_beg_col, \n",
    "        t_int_end_col=t_int_end_col, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col   \n",
    "    )\n",
    "    #-------------------------\n",
    "    full_df = df[df[SN_col].isin(full_part_not_dict['full_SNs'])].copy()\n",
    "    part_df = df[df[SN_col].isin(full_part_not_dict['part_SNs'])].copy()\n",
    "    not_df  = df[df[SN_col].isin(full_part_not_dict['not_SNs'])].copy()\n",
    "    #-------------------------\n",
    "    return_dict = dict(\n",
    "        full_df=full_df, \n",
    "        part_df=part_df, \n",
    "        not_df=not_df\n",
    "    )\n",
    "    if return_SNs:\n",
    "        return_dict = return_dict|full_part_not_dict\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc800d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3599239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_broad_outage_time_using_ede(\n",
    "    ede_df_i, \n",
    "    dovs_out_t_beg, \n",
    "    dovs_out_t_end, \n",
    "    \n",
    "    pd_ids=['3.26.0.47', '3.26.136.47'], \n",
    "    pu_ids=['3.26.0.216', '3.26.136.216'], \n",
    "    \n",
    "    return_pct_SNs_close=False, \n",
    "    out_time_thresh=pd.Timedelta('10 minutes'), \n",
    "    \n",
    "    SN_col='serialnumber', \n",
    "    valuesinterval_col='valuesinterval_local', \n",
    "    edetypeid_col='enddeviceeventtypeid', \n",
    "    verbose=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    This function essentially finds the closest power-down event to dovs_out_t_beg and the \n",
    "      closest power-up event to dovs_out_t_end\n",
    "    NOTE: I use the terms power-up and power-down events broadly here.  \n",
    "          In general, these also contain reasons such as last gasps, power restore, etc.\n",
    "          What is considered power-up is defined by pu_ids, and what is considered power-down is defined by pd_ids,\n",
    "            where pu_ids and pd_ids contain enddeviceeventtypeids.\n",
    "          Might want to use 'reason' column and run a further reduce method, but it seems enddeviceeventtypeids will\n",
    "            work well for this purpose.\n",
    "          \n",
    "    This can be used with an ede_df_i containing data for all meters in an outage, or for an ede_df_i containing\n",
    "      events for a single meter.\n",
    "    If ede_df_i contains data from multiple meters, one can check the percentage of SNs with a power-down close to the\n",
    "      found outage begin time and the percentage with a power-up close to the found outage end time.\n",
    "      The definition of 'close' is set by out_time_thresh\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # The methodology relies on ede_df_i being sorted by time\n",
    "    # But, first, make sure valuesinterval_col contains time data\n",
    "    if not is_datetime64_dtype(ede_df_i[valuesinterval_col]):\n",
    "        ede_df_i[valuesinterval_col] = pd.to_datetime(ede_df_i[valuesinterval_col])\n",
    "    ede_df_i = ede_df_i.sort_values(by=[valuesinterval_col])\n",
    "    #-------------------------\n",
    "    # Get the power-up and power-down subsets from ede_df_i\n",
    "    ede_df_i_pu = ede_df_i[ede_df_i[edetypeid_col].isin(pu_ids)]\n",
    "    ede_df_i_pd = ede_df_i[ede_df_i[edetypeid_col].isin(pd_ids)]\n",
    "    if ede_df_i_pd.shape[0]==0 or ede_df_i_pu.shape[0]==0:\n",
    "        if return_pct_SNs_close:\n",
    "            return [], 0., 0.\n",
    "        else:\n",
    "            return []\n",
    "    #-----\n",
    "    # Get the time values for the power-up and power-down events\n",
    "    vals_pu = ede_df_i_pu[valuesinterval_col].drop_duplicates().sort_values().tolist()\n",
    "    vals_pd = ede_df_i_pd[valuesinterval_col].drop_duplicates().sort_values().tolist()\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    # Estimate beginning of outage, out_t_beg\n",
    "    #--------------------------------------------------\n",
    "    try:\n",
    "        out_t_beg_le = Utilities.find_le(vals_pd, dovs_out_t_beg)\n",
    "    except:\n",
    "        if verbose:\n",
    "            print('Warning: Unable to find out_t_beg_le, so using vals_pd[0]')\n",
    "        out_t_beg_le = vals_pd[0]\n",
    "    #-----\n",
    "    try:\n",
    "        out_t_beg_ge = Utilities.find_ge(vals_pd, dovs_out_t_beg)\n",
    "    except:\n",
    "        if verbose:\n",
    "            print('Warning: Unable to find out_t_beg_ge, so using out_t_beg_le')\n",
    "        out_t_beg_ge = out_t_beg_le\n",
    "    #-----\n",
    "    if out_t_beg_le==out_t_beg_ge:\n",
    "        out_t_beg = out_t_beg_le\n",
    "    else:\n",
    "        # Note: If out_t_beg_le and out_t_beg_ge are equidistant from dovs_out_t_beg\n",
    "        #         out_t_beg_le is favored (through <= operator)\n",
    "        if (dovs_out_t_beg-out_t_beg_le) <= (out_t_beg_ge-dovs_out_t_beg):\n",
    "            out_t_beg = out_t_beg_le\n",
    "        else:\n",
    "            out_t_beg = out_t_beg_ge\n",
    "            \n",
    "    #--------------------------------------------------\n",
    "    # Estimate end of outage, out_t_end\n",
    "    #--------------------------------------------------\n",
    "    try:\n",
    "        out_t_end_ge = Utilities.find_ge(vals_pu, dovs_out_t_end)\n",
    "    except:\n",
    "        if verbose:\n",
    "            print('Warning: Unable to find out_t_end_ge, so using vals_pu[-1]')\n",
    "        out_t_end_ge = vals_pu[-1]\n",
    "    #-----\n",
    "    try:\n",
    "        out_t_end_le = Utilities.find_le(vals_pu, dovs_out_t_end)\n",
    "    except:\n",
    "        if verbose:\n",
    "            print('Warning: Unable to find out_t_end_le, so using out_t_end_ge')\n",
    "        out_t_end_le = out_t_end_ge\n",
    "    #-----\n",
    "    if out_t_end_le==out_t_end_ge:\n",
    "        out_t_end = out_t_end_le\n",
    "    else:\n",
    "        # Note: If out_t_end_le and out_t_end_ge are equidistant from dovs_out_t_end\n",
    "        #         out_t_beg_ge is favored (through <= operator)\n",
    "        if (out_t_end_ge-dovs_out_t_end) <= (dovs_out_t_end-out_t_end_le):\n",
    "            out_t_end = out_t_end_ge\n",
    "        else:\n",
    "            out_t_end = out_t_end_le\n",
    "            \n",
    "    #--------------------------------------------------\n",
    "    # If out_t_beg is greater than or equal to out_t_end, try changing out_t_beg to out_t_beg_le and/or \n",
    "    #   out_t_end to out_t_end_ge\n",
    "    #-----\n",
    "    # Before vals were split into vals_pd and vals_pu, it seemed like the only way this could happen was if\n",
    "    #   out_t_beg equals out_t_end, but now I supposed it could happen in general?\n",
    "    #   In any case, if the assertion hits below, then I should investigate further\n",
    "    #-----\n",
    "    if out_t_beg>=out_t_end:\n",
    "#         assert(out_t_end==out_t_beg)\n",
    "        # Change either out_t_beg to out_t_beg_le or out_t_end to out_t_end_ge first according to which \n",
    "        #   is closer (to make minimal adjustment first)\n",
    "        # Note: abs calls below should really be necessary, but also no harm\n",
    "        if np.abs(out_t_beg-out_t_beg_le) <= np.abs(out_t_end_ge-out_t_end):\n",
    "            out_t_beg = out_t_beg_le\n",
    "        else:\n",
    "            out_t_end = out_t_end_ge\n",
    "        #-----\n",
    "        # If out_t_beg still >= out_t_end, set both out_t_beg=out_t_beg_le and out_t_end=out_t_end_ge\n",
    "        # Note: Don't know which was set in the above if/else, so simply set both\n",
    "        if out_t_beg>=out_t_end:\n",
    "            out_t_beg = out_t_beg_le\n",
    "            out_t_end = out_t_end_ge\n",
    "    #-------------------------\n",
    "    if out_t_beg>=out_t_end:\n",
    "        if verbose:\n",
    "            print('In estimate_broad_outage_time_using_ede, found out_t_beg<out_t_end.')\n",
    "            print('Returning []')\n",
    "        return ()\n",
    "    assert(out_t_beg<out_t_end)\n",
    "    #--------------------------------------------------\n",
    "    if not return_pct_SNs_close:\n",
    "        return (out_t_beg, out_t_end)\n",
    "    #--------------------------------------------------\n",
    "    # Find percentage of SNs with PD close to out_t_beg and percentage with PU close to out_t_end\n",
    "    n_SNs_w_pd_close_to_beg = ede_df_i_pd[np.abs(ede_df_i_pd[valuesinterval_col]-out_t_beg)<out_time_thresh][SN_col].nunique()\n",
    "    pct_SNs_w_pd_close_to_beg = 100*n_SNs_w_pd_close_to_beg/ede_df_i[SN_col].nunique()\n",
    "    #-----\n",
    "    n_SNs_w_pu_close_to_end = ede_df_i_pu[np.abs(ede_df_i_pu[valuesinterval_col]-out_t_end)<out_time_thresh][SN_col].nunique()\n",
    "    pct_SNs_w_pu_close_to_end = 100*n_SNs_w_pu_close_to_end/ede_df_i[SN_col].nunique()\n",
    "    #-------------------------\n",
    "    return (out_t_beg, out_t_end), pct_SNs_w_pd_close_to_beg, pct_SNs_w_pu_close_to_end\n",
    "\n",
    "\n",
    "def estimate_outage_times_using_ede_for_meter(\n",
    "    ede_df_i, \n",
    "    broad_out_t_beg, \n",
    "    broad_out_t_end, \n",
    "    expand_search_time=pd.Timedelta('10 minutes'), \n",
    "    \n",
    "    pd_ids=['3.26.0.47', '3.26.136.47'], \n",
    "    pu_ids=['3.26.0.216', '3.26.136.216'], \n",
    "    \n",
    "    SN_col='serialnumber', \n",
    "    valuesinterval_col='valuesinterval_local', \n",
    "    edetypeid_col='enddeviceeventtypeid', \n",
    "    verbose=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given the broad outage time, find any times during the main outage time when power was restored for a single meter.\n",
    "    NOTE: ede_df_i must be a meter events DataFrame containing data from a single meter.\n",
    "\n",
    "    Essentially, I am finding any sub-outages within the outage as a whole\n",
    "    This only makes sense at a serial number level (i.e., ede_df_i must contain events for only a single SN)\n",
    "    This method, in contrast to the work I put together for Patrick, doesn't care about mismatched power-up/power-down \n",
    "      pairs (i.e., e.g., multiple power down events can occur in a row)\n",
    "    Basically, first find a power down event, then find the next power up.  Call this the first sub-outage\n",
    "    Then, find the next power down after the first sub-outage, then find the next power up.  Call this the second sub-outage.\n",
    "    Repeat.\n",
    "    -----    \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # As described above, designed to work for DF containing data from a single meter.\n",
    "    assert(ede_df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    # First, get a better estimate for the broad outage times (which are usually estimated from all meters in an outage \n",
    "    #  or simply taken from DOVS)\n",
    "    out_t_beg_end = estimate_broad_outage_time_using_ede(\n",
    "        ede_df_i=ede_df_i, \n",
    "        dovs_out_t_beg=broad_out_t_beg-expand_search_time, \n",
    "        dovs_out_t_end=broad_out_t_end+expand_search_time, \n",
    "\n",
    "        pd_ids=pd_ids, \n",
    "        pu_ids=pu_ids, \n",
    "\n",
    "        return_pct_SNs_close=False, \n",
    "        out_time_thresh=pd.Timedelta('10 minutes'), \n",
    "\n",
    "        SN_col=SN_col, \n",
    "        valuesinterval_col=valuesinterval_col, \n",
    "        edetypeid_col=edetypeid_col, \n",
    "        verbose=verbose\n",
    "    )\n",
    "    if out_t_beg_end:\n",
    "        assert(len(out_t_beg_end)==2)\n",
    "        out_t_beg = out_t_beg_end[0]\n",
    "        out_t_end = out_t_beg_end[1]\n",
    "    else:\n",
    "        return []\n",
    "#     return [[out_t_beg, out_t_end]]\n",
    "    #-------------------------\n",
    "    # Get the subset of ede_df_i within the broad outage time (including the outage beginning and end) having power-up\n",
    "    #   or power-down type IDs.\n",
    "    ede_df_i_sub = ede_df_i[\n",
    "        (ede_df_i[valuesinterval_col]>=out_t_beg) & \n",
    "        (ede_df_i[valuesinterval_col]<=out_t_end) &\n",
    "        (ede_df_i[edetypeid_col].isin(pd_ids+pu_ids))\n",
    "    ].copy()\n",
    "    #-------------------------\n",
    "    # The methodology relies on ede_df_i_sub being sorted by time\n",
    "    # But, first, make sure valuesinterval_col contains time data\n",
    "    if not is_datetime64_dtype(ede_df_i_sub[valuesinterval_col]):\n",
    "        ede_df_i_sub[valuesinterval_col] = pd.to_datetime(ede_df_i_sub[valuesinterval_col])\n",
    "    ede_df_i_sub = ede_df_i_sub.sort_values(by=[valuesinterval_col])\n",
    "    #-------------------------\n",
    "    # The procedure is to essentially mark and power-ups with a value of 1 and any power-downs with a value of 0\n",
    "    # These values are stored in a temporary column (tmp_id_type_col).\n",
    "    # Then, .diff can be called on this column, and the results (stored in tmp_diff_col) can be used to identify\n",
    "    #   the beginning and end of the sub-outages.\n",
    "    #-----\n",
    "    tmp_id_type_col = Utilities.generate_random_string()\n",
    "    ede_df_i_sub[tmp_id_type_col] = np.nan\n",
    "    ede_df_i_sub.loc[ede_df_i_sub[edetypeid_col].isin(pu_ids), tmp_id_type_col] = 1\n",
    "    ede_df_i_sub.loc[ede_df_i_sub[edetypeid_col].isin(pd_ids), tmp_id_type_col] = 0\n",
    "    #-----\n",
    "    tmp_diff_col = Utilities.generate_random_string()\n",
    "    ede_df_i_sub[tmp_diff_col] = ede_df_i_sub[tmp_id_type_col].diff()\n",
    "    #-------------------------\n",
    "    # By definition, the first entry should be a power down event and the last should be a power up event\n",
    "    #  (because of the use of >= and <= in ede_df_i_sub declaration above)\n",
    "    assert(ede_df_i_sub.shape[0]>=2)\n",
    "    #-----\n",
    "    # If first value is not in pd_ids, remove until found\n",
    "    if ede_df_i_sub.iloc[0][edetypeid_col] not in pd_ids:\n",
    "        if verbose:\n",
    "            print('Warning, first value of ede_df_i_sub not in pd_ids')\n",
    "        while ede_df_i_sub.iloc[0][edetypeid_col] not in pd_ids:\n",
    "            ede_df_i_sub = ede_df_i_sub.iloc[1:]\n",
    "    # If last value is not in pu_ids, remove until found\n",
    "    if ede_df_i_sub.iloc[-1][edetypeid_col] not in pu_ids:\n",
    "        if verbose:\n",
    "            print('Warning, last value of ede_df_i_sub not in pu_ids')\n",
    "        while ede_df_i_sub.iloc[-1][edetypeid_col] not in pu_ids:\n",
    "            ede_df_i_sub = ede_df_i_sub.iloc[:-1]\n",
    "    #-----\n",
    "    if ede_df_i_sub.shape[0]<2:\n",
    "        print(f'Could not determine outage times for SN={ede_df_i[SN_col].unique()[0]}')\n",
    "        return []\n",
    "    assert(ede_df_i_sub.shape[0]>=2)\n",
    "    assert(ede_df_i_sub.iloc[0][edetypeid_col] in pd_ids)\n",
    "    assert(ede_df_i_sub.iloc[-1][edetypeid_col] in pu_ids)\n",
    "    if ede_df_i_sub.shape[0]==2:\n",
    "        outg_times = [(ede_df_i_sub.iloc[0][valuesinterval_col], ede_df_i_sub.iloc[-1][valuesinterval_col])]\n",
    "        # return outg_times\n",
    "    #-------------------------\n",
    "    # The .diff operation above leaves the first entry (which is a power-down event) as NaN\n",
    "    # Make it equal to -1 instead, like all other power-down events\n",
    "    ede_df_i_sub.loc[ede_df_i_sub.index[0], tmp_diff_col]=-1\n",
    "\n",
    "    # Get the sub-outages begin (diff = -1) and end (diff = +1) times\n",
    "    outg_times_beg = ede_df_i_sub[ede_df_i_sub[tmp_diff_col]==-1][valuesinterval_col].tolist()\n",
    "    outg_times_end = ede_df_i_sub[ede_df_i_sub[tmp_diff_col]==1][valuesinterval_col].tolist()\n",
    "    assert(len(outg_times_beg)==len(outg_times_end))\n",
    "\n",
    "    # Combine the begin and end times\n",
    "    outg_times = list(zip(outg_times_beg, outg_times_end))\n",
    "    #-------------------------\n",
    "    # Sanity check on outage times\n",
    "    for i_outg, outg_time in enumerate(outg_times):\n",
    "        # Each element should contain a beginning and ending time\n",
    "        assert(len(outg_time)==2)\n",
    "        # The beginning time should occur before the ending time\n",
    "        assert(outg_time[0]<=outg_time[1])\n",
    "        # This outage should occur after the last outage\n",
    "        if i_outg>0:\n",
    "            assert(outg_time[0]>=outg_times[i_outg-1][1])\n",
    "    #-------------------------\n",
    "    return outg_times\n",
    "\n",
    "\n",
    "def estimate_cmi_using_ede_for_meter(\n",
    "    ede_df_i, \n",
    "    broad_out_t_beg, \n",
    "    broad_out_t_end, \n",
    "    expand_search_time=pd.Timedelta('1 hour'), \n",
    "    \n",
    "    pd_ids=['3.26.0.47', '3.26.136.47'], \n",
    "    pu_ids=['3.26.0.216', '3.26.136.216'], \n",
    "    \n",
    "    SN_col='serialnumber', \n",
    "    valuesinterval_col='valuesinterval_local', \n",
    "    edetypeid_col='enddeviceeventtypeid', \n",
    "    return_est_outage_times=False, \n",
    "    verbose=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Estimate the outage times using estimate_outage_times_using_ede_for_meter, then calculate the CMI for the customer\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    outg_times = estimate_outage_times_using_ede_for_meter(\n",
    "        ede_df_i=ede_df_i, \n",
    "        broad_out_t_beg=broad_out_t_beg, \n",
    "        broad_out_t_end=broad_out_t_end,\n",
    "        expand_search_time=expand_search_time, \n",
    "        pd_ids=pd_ids, \n",
    "        pu_ids=pu_ids, \n",
    "        SN_col=SN_col, \n",
    "        valuesinterval_col=valuesinterval_col, \n",
    "        edetypeid_col=edetypeid_col, \n",
    "        verbose=verbose\n",
    "    )\n",
    "    #-------------------------\n",
    "    cmi_i = pd.Timedelta(0)\n",
    "    for outg_time in outg_times:\n",
    "        cmi_i += (outg_time[1]-outg_time[0])\n",
    "    #-------------------------\n",
    "    if return_est_outage_times:\n",
    "        return cmi_i, outg_times\n",
    "    else:\n",
    "        return cmi_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b78ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outg_rec_nb_in_outg_rec_nbs_in_files_dict(\n",
    "    outg_rec_nb, \n",
    "    outg_rec_nbs_in_files,\n",
    "    crash_if_not_found=True\n",
    "):\n",
    "    #-------------------------\n",
    "    for file_i, outg_rec_nbs_i in outg_rec_nbs_in_files.items():\n",
    "        if outg_rec_nb in outg_rec_nbs_i:\n",
    "            return file_i\n",
    "    print(f'outg_rec_nb = {outg_rec_nb} not found!')\n",
    "    if crash_if_not_found:\n",
    "        assert(0)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f7f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_file_to_outg_rec_nbs_dict(file_to_outg_rec_nbs_dict):\n",
    "    r\"\"\"\n",
    "    Input:\n",
    "        keys   = file paths\n",
    "        values = outg_rec_nbs contained in given file path\n",
    "        \n",
    "    Output:\n",
    "        keys   = outg_rec_nbs\n",
    "        values = files containing given outg_rec_nb\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    outg_rec_nb_to_files_dict = dict()\n",
    "    for file_i, outg_rec_nbs_i in file_to_outg_rec_nbs_dict.items():\n",
    "        for outg_rec_nb_ij in outg_rec_nbs_i:\n",
    "            if outg_rec_nb_ij in outg_rec_nb_to_files_dict.keys():\n",
    "                outg_rec_nb_to_files_dict[outg_rec_nb_ij].append(file_i)\n",
    "            else:\n",
    "                outg_rec_nb_to_files_dict[outg_rec_nb_ij] = [file_i]\n",
    "    #-------------------------\n",
    "    return outg_rec_nb_to_files_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586bd17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f1063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_data_in_search_window(\n",
    "    df_i, \n",
    "    t_search_min_max=None, \n",
    "    t_int_beg_col='starttimeperiod_local', \n",
    "    t_int_end_col='endtimeperiod_local'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Checks whether or not data are present within the search window\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if t_search_min_max is not None:\n",
    "        assert(Utilities.is_object_one_of_types(t_search_min_max, [list, tuple]))\n",
    "        assert(len(t_search_min_max)==2)\n",
    "        #-----\n",
    "        # At least one of t_search_min_max should not be None\n",
    "        assert(t_search_min_max[0] is not None or t_search_min_max[1] is not None)\n",
    "        if t_search_min_max[0] is None:\n",
    "            t_search_min_max[0] = pd.Timestamp.min\n",
    "        if t_search_min_max[1] is None:\n",
    "            t_search_min_max[1] = pd.Timestamp.max\n",
    "        #-----\n",
    "        return df_i[\n",
    "            (df_i[t_int_beg_col]>=t_search_min_max[0]) & \n",
    "            (df_i[t_int_end_col]<=t_search_min_max[1])\n",
    "        ].shape[0]>0\n",
    "    else:\n",
    "        return df_i.shape[0]>0\n",
    "\n",
    "def estimate_outage_times_for_meter(\n",
    "    df_i, \n",
    "    t_search_min_max=None, \n",
    "    t_int_beg_col='starttimeperiod_local', \n",
    "    t_int_end_col='endtimeperiod_local', \n",
    "    value_col='value', \n",
    "    SN_col='serialnumber', \n",
    "    verbose=True,\n",
    "    outg_rec_nb_col=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    Try to determine when outages occur by looking for periods of time where the meters show values of 0.\n",
    "    df_i should contain AMI data for a single meter.\n",
    "    Designed to find multipe groups of zero readings.\n",
    "    \n",
    "    Returns a list of dict objects, each having keys: cnsrvtv_t_beg, cnsrvtv_t_end, zero_t_beg, zero_t_end.\n",
    "    \n",
    "    IMPORTANT: Due to the 15-minute granularity of the data, the raw output will underestimate the length of the outage\n",
    "                 between 0-30 minutes (i.e., the outage actually began 0-15 minutes before AMI data received showing meters\n",
    "                 with 0 values and the outage actually ended 0-15 minutes after last data-point received with meters \n",
    "                 showing 0 values).\n",
    "                ACTUALLY, to be 100% correct, the raw output will underestimate the length of the outage by between \n",
    "                  0 and 2*freq of data!!\n",
    "                  \n",
    "    REMINDER: The original method, which found zero times and combined them using Utilities.get_fuzzy_overlap, \n",
    "                was no good because if there are gaps in the data multiple sub-outages would be split off\n",
    "              e.g., if a period of zero values lasts for an hour, but there is a 15-minute interval missing data,\n",
    "                the original method would have split this into two sub-outages\n",
    "                  \n",
    "    t_search_min_max:\n",
    "        Allows the user to set of time subset of the data from which to search for the outages.\n",
    "        If t_search_min_max is not None, it should be a two-element list/tuple.\n",
    "        However, one of the elements of the list may be None if, e.g., one wants limits on only the min or max\n",
    "        \n",
    "    outg_rec_nb_col:\n",
    "        Only used for outputting warnings etc. when verbose==True\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # If df_i doesn't have any data within search window, return []\n",
    "    if not check_for_data_in_search_window(\n",
    "        df_i=df_i, \n",
    "        t_search_min_max=t_search_min_max, \n",
    "        t_int_beg_col=t_int_beg_col, \n",
    "        t_int_end_col=t_int_end_col\n",
    "    ):\n",
    "        return []\n",
    "    #-------------------------\n",
    "    # As described above, designed to work for DF containing data from a single meter.\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    SN_i = df_i[SN_col].unique().tolist()[0] # Used only for error messages\n",
    "    \n",
    "    # outg_rec_nb only used for error messages\n",
    "    if outg_rec_nb_col is not None and outg_rec_nb_col in df_i.columns.tolist():\n",
    "        outg_rec_nb = df_i[outg_rec_nb_col].unique().tolist()[0]\n",
    "    else:\n",
    "        outg_rec_nb = None\n",
    "    \n",
    "    #-------------------------\n",
    "    # For this procedure, only really need the t_int_beg_col, t_int_end_col, value_col (and, SN_col for assertion check)\n",
    "    df_i = df_i.drop(\n",
    "        columns=list(set(df_i.columns.tolist()).difference(set([t_int_beg_col, t_int_end_col, value_col, SN_col]))), \n",
    "        inplace=False\n",
    "    )\n",
    "    \n",
    "    #-------------------------\n",
    "    # First, make sure df_i is sorted by t_int_beg_col!\n",
    "    df_i = df_i.sort_values(by=t_int_beg_col)\n",
    "    \n",
    "    #-------------------------\n",
    "    # It is easier to work with the index in terms of numbers between 0 and df_i.shape[0], therefore\n",
    "    #   I call reset_index()\n",
    "    # NOTE: Is storing idx_col really necessary?  I'm thinking it's not...\n",
    "    assert(df_i.index.nlevels==1)\n",
    "    if df_i.index.name is not None and df_i.index.name not in df_i.columns.tolist():\n",
    "        idx_col = df_i.index.name\n",
    "    else:\n",
    "        idx_col = Utilities.generate_random_string()\n",
    "        df_i.index.name = idx_col\n",
    "    assert(idx_col not in df_i.columns.tolist())\n",
    "    #-----\n",
    "    df_i = df_i.reset_index()\n",
    "    \n",
    "    \n",
    "    #-------------------------\n",
    "    # Find the rows in df_i in which to search for outages\n",
    "    if t_search_min_max is not None:\n",
    "        assert(Utilities.is_object_one_of_types(t_search_min_max, [list, tuple]))\n",
    "        assert(len(t_search_min_max)==2)\n",
    "        #-----\n",
    "        # At least one of t_search_min_max should not be None\n",
    "        assert(t_search_min_max[0] is not None or t_search_min_max[1] is not None)\n",
    "        #-----\n",
    "        if t_search_min_max[0] is None:\n",
    "            idx_search_min = 0\n",
    "        else:\n",
    "            idx_search_min = df_i[df_i[t_int_beg_col]>=t_search_min_max[0]].index[0]\n",
    "        #-----\n",
    "        if t_search_min_max[1] is None:\n",
    "            idx_search_max = df_i.shape[0]-1\n",
    "        else:\n",
    "            idx_search_max = df_i[df_i[t_int_end_col]<=t_search_min_max[1]].index[-1]\n",
    "    else:\n",
    "        idx_search_min = 0\n",
    "        idx_search_max = df_i.shape[0]-1\n",
    "        t_search_min_max=[None, None] # Needed only for verbose printing below\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    # If there is no power (value==0) at the beginning or end of the search window\n",
    "    #   expand the search window\n",
    "    #-----\n",
    "    # NOTE: Below, I want sub_df_i to include idx_search_min and idx_search_max, which is why\n",
    "    #         the +1 is needed with idx_search_min\n",
    "    # NOTE: Even if idx_search_max was set to df_i.shape[0]-1, there not no harm\n",
    "    #         in calling df_i.iloc[i:df_i.shape[0]], whereas calling df_i.iloc[df_i.shape[0]]\n",
    "    #         would result in an index out-of-bound error!\n",
    "    #-------------------------\n",
    "    # If no power at the beginning of the search window\n",
    "    if df_i.iloc[idx_search_min][value_col]==0:\n",
    "        # Portion of df_i previous to search window\n",
    "        df_i_pre = df_i.iloc[:idx_search_min]\n",
    "        if (df_i_pre[value_col]!=0).any():\n",
    "            # I want the non-zero value closest to the search window, which, in this case\n",
    "            #   means the largest index having a non-zero value\n",
    "            # To achieve this, reverse the order of df_i_pre and use idxmax\n",
    "            # NOTE: This only works because the indices are integers (and argmax would give the\n",
    "            #       wrong answer in any case)!  \n",
    "            idx_search_min = (df_i_pre.iloc[::-1][value_col]!=0).idxmax()\n",
    "            if verbose:\n",
    "                print('\\n\\nExpanding search window to include beginning of first outage')\n",
    "        else:\n",
    "            idx_search_min = 0\n",
    "            if verbose:\n",
    "                print('\\n\\nWARNING: could not find beginning of first outage!')\n",
    "                print('Search window expanded to include minimum time of full df_i')\n",
    "        #-----\n",
    "        if verbose:\n",
    "            print(f'SN = {SN_i}')\n",
    "            if outg_rec_nb is not None:\n",
    "                print(f'OUTG_REC_NB = {outg_rec_nb}')\n",
    "            print(f'\\tOrg = {t_search_min_max[0]}\\n\\tNew = {df_i.iloc[idx_search_min][t_int_beg_col]}')\n",
    "    #-------------------------\n",
    "    # If no power at the end of the search window\n",
    "    if df_i.iloc[idx_search_max][value_col]==0:\n",
    "        # Portion of df_i following search window\n",
    "        df_i_post = df_i.iloc[idx_search_max+1:]\n",
    "        if (df_i_post[value_col]!=0).any():\n",
    "            # I want the non-zero value closest to the search window, which, in this case\n",
    "            #   means the smallest index having a non-zero value\n",
    "            idx_search_max = (df_i_post[value_col]!=0).idxmax()\n",
    "            if verbose:\n",
    "                print('\\n\\nExpanding search window to include ending of last outage')\n",
    "        else:\n",
    "            idx_search_max = df_i.shape[0]-1\n",
    "            if verbose:\n",
    "                print('\\n\\nWARNING: could not find ending of last outage')\n",
    "                print('Search window expanded to include maximum time of full df_i')\n",
    "        #-----\n",
    "        if verbose:\n",
    "            print(f'SN = {SN_i}')\n",
    "            if outg_rec_nb is not None:\n",
    "                print(f'OUTG_REC_NB = {outg_rec_nb}')\n",
    "            print(f'\\tOrg = {t_search_min_max[1]}\\n\\tNew = {df_i.iloc[idx_search_max][t_int_end_col]}')\n",
    "    #-------------------------\n",
    "    sub_df_i = df_i.iloc[idx_search_min:idx_search_max+1].copy()\n",
    "    #--------------------------------------------------\n",
    "    #-------------------------\n",
    "    # If no periods with zero values, return empty list\n",
    "    if not (sub_df_i[value_col]==0).any():\n",
    "        return []\n",
    "    \n",
    "    #-------------------------\n",
    "    # To be consistent with methods in estimate_outage_times_using_ede_for_meter, where power-up events are assigned\n",
    "    #   a value of +1 and power-down event assigned a value of 0, I should use !=0 below\n",
    "    tmp_neq0_col = Utilities.generate_random_string()\n",
    "    tmp_diff_col = Utilities.generate_random_string()\n",
    "    #-----\n",
    "    sub_df_i[tmp_neq0_col] = (sub_df_i[value_col] != 0).astype(int)\n",
    "    sub_df_i[tmp_diff_col] = sub_df_i[tmp_neq0_col].diff()\n",
    "    #-------------------------\n",
    "    # The .diff() operation always leaves the first element as a NaN\n",
    "    # However, if the first element was the beginning of an outage (i.e., if the value==0)\n",
    "    #   then the diff should be +1\n",
    "    if sub_df_i.iloc[0][value_col]==0:\n",
    "        sub_df_i.loc[sub_df_i.index[0], tmp_diff_col]=-1  \n",
    "    \n",
    "    #-------------------------\n",
    "    # Find the power-down (pd) and power-up (pu) rows\n",
    "    #-----\n",
    "    # Outage times are defined by a power loss followed by a power restore.\n",
    "    # The beginning of a power loss occurs when the value in value_col goes from non-zero to zero\n",
    "    #   and is located in rows for which sub_df_i[tmp_diff_col]==-1\n",
    "    # The ending of a power loss occurs when the value in value_col goes from zero to non-zero\n",
    "    #   and is located in rows for which sub_df_i[tmp_diff_col]==+1\n",
    "    # NOTE: The use of .reset_index() below ensures that the values returned for rows are numerical index\n",
    "    #         values from 0 to sub_df_i.shape[0]-1\n",
    "    # NOTE: Below pd==power-down and pu==power-up.  The pd times are when the first zero values occur (following non-zero\n",
    "    #         values) and the pu times are when the first non-zero values occur (following zero values)\n",
    "    #-----\n",
    "    pd_row_idxs = sub_df_i.reset_index()[sub_df_i.reset_index()[tmp_diff_col]==-1].index.tolist()\n",
    "    pu_row_idxs = sub_df_i.reset_index()[sub_df_i.reset_index()[tmp_diff_col]==1].index.tolist()\n",
    "    #-----\n",
    "    # Sanity checks\n",
    "    assert((sub_df_i.iloc[pd_row_idxs][tmp_diff_col]==-1).all())\n",
    "    assert((sub_df_i.iloc[pd_row_idxs][value_col]==0).all())\n",
    "    #-----\n",
    "    assert((sub_df_i.iloc[pu_row_idxs][tmp_diff_col]==1).all())\n",
    "    assert((sub_df_i.iloc[pu_row_idxs][value_col]!=0).all())\n",
    "    #-------------------------    \n",
    "    \n",
    "    #-------------------------\n",
    "    # As stated above, the pd times are when the first zero values occur (following non-zero\n",
    "    #   values) and the pu times are when the first non-zero values occur (following zero values)\n",
    "    #-----\n",
    "    # pd_row_idxs/zeros_beg_row_idxs/cnsrvtv_beg_row_idxs:\n",
    "    #   - the idxs denoting when the periods of zeros begin (zeros_beg_row_idxs) is simply pd_row_idxs.\n",
    "    #     At these times, we know for certain the outage is ongoing, so these are essentially the maximum\n",
    "    #       times at which the outages could begin\n",
    "    #   - the conservative estimates for the beginning of the outages is the time period preceding the first zero,\n",
    "    #       which should be a non-zero value.\n",
    "    #     At these times, we know for certain (unless the outage is ongoing at the beginning of the data) the outage is not \n",
    "    #       ongoing, so these are essentially the minumum times at which the outages could begin\n",
    "    #     If the first outage is ongoing at the beginning of the data, then zeros_beg_row_idxs[0]==0, in which case the best\n",
    "    #       we can do is set the conservative row equal to 0 as well\n",
    "    #-----\n",
    "    # pu_row_idxs/zeros_end_row_idxs/cnsrvtv_end_row_idxs:\n",
    "    #   - the idxs denoting when a period of zeros ends with the first non-zero value (cnsrvtv_end_row_idxs) is simply pu_row_idxs.\n",
    "    #     At these times, we know for certain the outages are no longer ongoin (unless the outage is ongoing at the end of the data).\n",
    "    #     These are essentially the maximum times at which the outages could end.\n",
    "    #   - The last zero value (zeros_end_row_idxs) in a group of zeros should be the time period preceding the first non-zero (which \n",
    "    #       should be a zero value).\n",
    "    #     At these times, we know for certain the outage is ongoing, so these are essentially the minimum times at which the outages\n",
    "    #       could end.\n",
    "    #----------\n",
    "    zeros_beg_row_idxs   = pd_row_idxs\n",
    "    cnsrvtv_beg_row_idxs = [x-1 if x>0 else 0 for x in pd_row_idxs]\n",
    "    #-----\n",
    "    cnsrvtv_end_row_idxs = pu_row_idxs\n",
    "    zeros_end_row_idxs = [x-1 if x>0 else 0 for x in pu_row_idxs]\n",
    "    #-------------------------\n",
    "    \n",
    "    #-------------------------\n",
    "    # All the conservative estimates should have non-zero values\n",
    "    # NOTE: Cannot simply include, e.g.,  only values from cnsrvtv_beg_row_idxs which\n",
    "    #         are greater than zero, as the first pd could be at i=1, making\n",
    "    #         i=0 a perfectly acceptable value\n",
    "    #       Basically, one needs to include those in cnsrvtv_beg_row_idxs not \n",
    "    #         equal to their counterparts in pd_row_idxs\n",
    "    assert((sub_df_i.iloc[\n",
    "        [cnsrvtv_beg_row_idxs[i] for i in range(len(pd_row_idxs)) \n",
    "         if cnsrvtv_beg_row_idxs[i]!=pd_row_idxs[i]]\n",
    "    ][value_col]!=0).all())\n",
    "    assert((sub_df_i.iloc[cnsrvtv_end_row_idxs][value_col]!=0).all())\n",
    "    #-----\n",
    "    # Similarly, the zero times should have zero values!\n",
    "    assert((sub_df_i.iloc[zeros_beg_row_idxs][value_col]==0).all())\n",
    "    assert((sub_df_i.iloc[zeros_end_row_idxs][value_col]==0).all())\n",
    "    \n",
    "    #-------------------------\n",
    "    # Sanity check: all conservative beg(end) values should have matching zeros beg(end) value\n",
    "    assert(len(cnsrvtv_beg_row_idxs)==len(zeros_beg_row_idxs))\n",
    "    assert(len(zeros_end_row_idxs)==len(cnsrvtv_end_row_idxs))\n",
    "    #-------------------------\n",
    "    # In the case where an outage is ongoing at the end of the data, there will be\n",
    "    #   one more beginning time than ending time.\n",
    "    # In such a case, the best we can do is set the ending times (zeros and conservative)\n",
    "    #   equal to the end of the data\n",
    "    # If verbose, the user will have already been notified of this\n",
    "    if len(zeros_end_row_idxs)<len(zeros_beg_row_idxs):\n",
    "        assert(sub_df_i.iloc[-1][value_col]==0)\n",
    "        zeros_end_row_idxs.append(sub_df_i.shape[0]-1)\n",
    "        cnsrvtv_end_row_idxs.append(sub_df_i.shape[0]-1)\n",
    "    assert(len(zeros_end_row_idxs)==len(zeros_beg_row_idxs))\n",
    "    \n",
    "    #-------------------------\n",
    "    # Convert the row indices to times\n",
    "    cnsrvtv_beg_times = sub_df_i.iloc[cnsrvtv_beg_row_idxs][t_int_beg_col].tolist()\n",
    "    zeros_beg_times   = sub_df_i.iloc[zeros_beg_row_idxs][t_int_beg_col].tolist()\n",
    "    #-----\n",
    "    cnsrvtv_end_times = sub_df_i.iloc[cnsrvtv_end_row_idxs][t_int_end_col].tolist()\n",
    "    zeros_end_times   = sub_df_i.iloc[zeros_end_row_idxs][t_int_end_col].tolist()\n",
    "    \n",
    "    # Assertion not really needed\n",
    "    assert(\n",
    "        len(cnsrvtv_beg_times)==\n",
    "        len(zeros_beg_times)==\n",
    "        len(cnsrvtv_end_times)==\n",
    "        len(zeros_end_times)\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Construct return_list of dict objects\n",
    "    return_list = []\n",
    "    for i_outg in range(len(cnsrvtv_beg_times)):\n",
    "        return_list.append(dict(\n",
    "            cnsrvtv_t_beg = cnsrvtv_beg_times[i_outg], \n",
    "            zero_t_beg    = zeros_beg_times[i_outg], \n",
    "            zero_t_end    = zeros_end_times[i_outg], \n",
    "            cnsrvtv_t_end = cnsrvtv_end_times[i_outg]\n",
    "        ))\n",
    "    #-------------------------\n",
    "    # Sanity check on outage times\n",
    "    for i_outg, outg_dict_i in enumerate(return_list):\n",
    "        # Time ordering should be: cnsrvtv_t_beg, zero_t_beg, zero_t_end, cnsrvtv_t_end\n",
    "        assert(\n",
    "            outg_dict_i['cnsrvtv_t_beg']<=\n",
    "            outg_dict_i['zero_t_beg']<=\n",
    "            outg_dict_i['zero_t_end']<=\n",
    "            outg_dict_i['cnsrvtv_t_end']\n",
    "        )\n",
    "        # This outage should occur after the last outage\n",
    "        # NOTE: We are using zero_t_beg/_end instead of cnsrvtv_t_beg/_end.\n",
    "        #       For the case where a meter regains power briefly (briefly here meaning during a single\n",
    "        #         15T interval), the cnsrvtv_t_beg of the right outage period will actually be BEFORE the\n",
    "        #         cnsrvtv_t_end of the left outage.\n",
    "        #       Visually, this situation will result in a triangular structure in the voltage signal data (zeros\n",
    "        #         to the left, one single peak, followed by zeros to the right).\n",
    "        #       We use zero_t_beg/_end because the zero_t_beg of the right outage will always be after\n",
    "        #         the zero_t_end of the left outage period.\n",
    "        if i_outg>0:\n",
    "            assert(outg_dict_i['zero_t_beg']>=return_list[i_outg-1]['zero_t_end'])\n",
    "    #-------------------------\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0deb9e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_est_outg_times_with_overlap_cnsrvtv(\n",
    "    est_outg_times,\n",
    "    overlap_to_enforce=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    The conservative beginning time of an outage can overlap with the convervative ending time of\n",
    "      the previous outage.\n",
    "    This will occur, e.g., when, during an outage, a meter regains power for one interval (visually, this\n",
    "      corresponds to a triangular shape in the voltage data).\n",
    "    In such instances, this function can be used to combine the two into a single est_outg_time.\n",
    "    \n",
    "    overlap_to_enforce:\n",
    "        If not None:\n",
    "            Should be a pd.Timedelta.\n",
    "            It is enforced that overlaps between any two est_outg_times must equal this value\n",
    "            A typical value is overlap_to_enforce=pd.Timedelta('15T')\n",
    "        \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # We are going to manipulate est_outg_times (e.g., after popping off first element, subsequent changes\n",
    "    #   to that object will actually change to underlying dict values as well!\n",
    "    # So make a copy so original is not altered\n",
    "    est_outg_times = copy.deepcopy(est_outg_times)\n",
    "    #-------------------------\n",
    "    if overlap_to_enforce is not None:\n",
    "        assert(isinstance(overlap_to_enforce, pd.Timedelta))\n",
    "    #-------------------------\n",
    "    # First, make sure all elements of est_outg_times are dicts with the expected keys\n",
    "    expected_keys = ['cnsrvtv_t_beg', 'zero_t_beg', 'zero_t_end', 'cnsrvtv_t_end']\n",
    "    for est_outg_times_i in est_outg_times:\n",
    "        assert(isinstance(est_outg_times_i, dict))\n",
    "        assert(len(set(expected_keys).difference(set(est_outg_times_i.keys())))==0)\n",
    "    #-----\n",
    "    # Next, make sure est_outg_times is sorted\n",
    "    est_outg_times = natsorted(est_outg_times, key=lambda x: x['cnsrvtv_t_beg'])\n",
    "    #-------------------------\n",
    "    return_est_outg_times = []\n",
    "    # Set the first range in return_est_outg_times simply as the first range in the list\n",
    "    est_outg_times_curr = est_outg_times.pop(0)\n",
    "    return_est_outg_times.append(est_outg_times_curr)\n",
    "    #-----\n",
    "    for est_outg_times_i in est_outg_times:\n",
    "        if est_outg_times_i['cnsrvtv_t_beg'] > est_outg_times_curr['cnsrvtv_t_end']:\n",
    "            # cnsrvtv_t_beg_i is after current end, so new interval needed\n",
    "            return_est_outg_times.append(est_outg_times_i)\n",
    "            est_outg_times_curr = est_outg_times_i\n",
    "        else:\n",
    "            # cnsrvtv_t_beg_i <= current_end, so overlap.\n",
    "            # The beg of return_est_outg_times[-1] remains the same, \n",
    "            #   but the end of return_est_outg_times[-1] should be changed to\n",
    "            #   the max of current_end and end\n",
    "            #-----\n",
    "            if overlap_to_enforce is not None:\n",
    "                overlap_i = est_outg_times_curr['cnsrvtv_t_end']-est_outg_times_i['cnsrvtv_t_beg']\n",
    "                assert(overlap_i==overlap_to_enforce)\n",
    "            #-----\n",
    "            return_est_outg_times[-1]['zero_t_end'] = max(\n",
    "                return_est_outg_times[-1]['zero_t_end'], \n",
    "                est_outg_times_i['zero_t_end']\n",
    "            )\n",
    "            return_est_outg_times[-1]['cnsrvtv_t_end'] = max(\n",
    "                return_est_outg_times[-1]['cnsrvtv_t_end'], \n",
    "                est_outg_times_i['cnsrvtv_t_end']\n",
    "            )\n",
    "    return return_est_outg_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a6e58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_outage_times(\n",
    "    df, \n",
    "    t_search_min_max=None, \n",
    "    pct_SNs_required_for_outage=0, \n",
    "    relax_pct_SNs_required_if_no_outgs_found=False, \n",
    "    t_int_beg_col='starttimeperiod_local', \n",
    "    t_int_end_col='endtimeperiod_local', \n",
    "    value_col='value', \n",
    "    combine_overlaps=True, \n",
    "    verbose=True,\n",
    "    outg_rec_nb_col=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    Try to determine when outages occur by looking for periods of time where some minimum percentage\n",
    "      of the meters (defined by pct_SNs_required_for_outage) show values of 0.\n",
    "    Designed to find multipe groups of zero readings.\n",
    "    Strategy: Basically, reduce df down to one entry per timestamp (instead of multiple SNs per timestamp) whose\n",
    "                value signifies whether or not the the timestamp can be considered as a zero value or a non-zero value.\n",
    "              A throw-away SN_col is then added on so that the estimate_outage_times_for_meter method may be used.\n",
    "              \n",
    "    !!!!! IMPORTANT !!!!!\n",
    "        pct_SNs_required_for_outage is the minimum percentage of available data at each timestamp needed to be zero\n",
    "          for the timestamp to be considered a zero value.\n",
    "        In many cases, this is equal to the percentage of SNs with a zero value, BUT NOT ALWAYS.\n",
    "        If, e.g., one meter in an outage has timestamps offset by a minute compared to the others, this will \n",
    "          not be the case!\n",
    "    \n",
    "    Returns a list of dict objects, each having keys: cnsrvtv_t_beg, cnsrvtv_t_end, zero_t_beg, zero_t_end.\n",
    "    \n",
    "    IMPORTANT: Due to the 15-minute granularity of the data, the raw output will underestimate the length of the outage\n",
    "                 between 0-30 minutes (i.e., the outage actually began 0-15 minutes before AMI data received showing meters\n",
    "                 with 0 values and the outage actually ended 0-15 minutes after last data-point received with meters \n",
    "                 showing 0 values).\n",
    "                ACTUALLY, to be 100% correct, the raw output will underestimate the length of the outage by between \n",
    "                  0 and 2*freq of data!!\n",
    "                  \n",
    "    t_search_min_max:\n",
    "        Allows the user to set of time subset of the data from which to search for the outages.\n",
    "        If t_search_min_max is not None, it should be a two-element list/tuple.\n",
    "        However, one of the elements of the list may be None if, e.g., one wants limits on only the min or max\n",
    "        \n",
    "    outg_rec_nb_col:\n",
    "        Only used for outputting warnings etc. when verbose==True\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # If df doesn't have any data within search window, return []\n",
    "    if not check_for_data_in_search_window(\n",
    "        df_i=df, \n",
    "        t_search_min_max=t_search_min_max, \n",
    "        t_int_beg_col=t_int_beg_col, \n",
    "        t_int_end_col=t_int_end_col\n",
    "    ):\n",
    "        return []\n",
    "    #------------------------- \n",
    "    # outg_rec_nb only used for error messages\n",
    "    if outg_rec_nb_col is not None and outg_rec_nb_col in df.columns.tolist():\n",
    "        outg_rec_nb = df[outg_rec_nb_col].unique().tolist()[0]\n",
    "    else:\n",
    "        outg_rec_nb = None\n",
    "        \n",
    "    #------------------------- \n",
    "    if t_search_min_max is not None:\n",
    "        assert(Utilities.is_object_one_of_types(t_search_min_max, [list, tuple]))\n",
    "        assert(len(t_search_min_max)==2)\n",
    "        #-----\n",
    "        # At least one of t_search_min_max should not be None\n",
    "        assert(t_search_min_max[0] is not None or t_search_min_max[1] is not None)\n",
    "        if t_search_min_max[0] is None:\n",
    "            t_search_min_max[0] = pd.Timestamp.min\n",
    "        if t_search_min_max[1] is None:\n",
    "            t_search_min_max[1] = pd.Timestamp.max\n",
    "        #-----\n",
    "        df = df[\n",
    "            (df[t_int_beg_col]>=t_search_min_max[0]) & \n",
    "            (df[t_int_end_col]<=t_search_min_max[1])\n",
    "        ]\n",
    "    \n",
    "    #-------------------------\n",
    "    # For this procedure, only really need the t_int_beg_col, t_int_end_col, value_col\n",
    "    df = df.drop(\n",
    "        columns=list(set(df.columns.tolist()).difference(set([t_int_beg_col, t_int_end_col, value_col]))), \n",
    "        inplace=False\n",
    "    )\n",
    "    \n",
    "    #-------------------------\n",
    "    # Make sure df is sorted by t_int_beg_col!\n",
    "    df = df.sort_values(by=t_int_beg_col)\n",
    "    \n",
    "    #-------------------------\n",
    "    # NOTE: Cannot use >= below!\n",
    "    #       A typical value is to set pct_SNs_required_for_outage=0, and if using >= below,\n",
    "    #         everything would come back as true, regardless of whether or not any 0 values\n",
    "    #         were registered.\n",
    "    zeros_df = df.groupby([t_int_beg_col, t_int_end_col]).apply(\n",
    "        lambda x: 100*((x[value_col]==0).sum()/x.shape[0])>pct_SNs_required_for_outage\n",
    "    )\n",
    "\n",
    "    # To be able to feed into estimate_outage_times_for_meter, we actually want the opposite of zeros_df,\n",
    "    #   where non-zero values are assigned 1 and zero values are assigned 0\n",
    "    neq0_df = (~zeros_df).astype(int)\n",
    "\n",
    "    #-------------------------\n",
    "    # At this point, neq0_df is a series with indices equal to starttimeperiod_local and values equal to 0 or 1, \n",
    "    #   indicating whether or not the time period as a whole can be treated as value 0 (without power) or 1 (with power)\n",
    "    # Name the series value_col, so after calling reset_index to column will equal value_col, as needed for input into\n",
    "    #   estimate_outage_times_for_meter\n",
    "    neq0_df.name = value_col\n",
    "    neq0_df = neq0_df.reset_index()\n",
    "\n",
    "    #-------------------------\n",
    "    # estimate_outage_times_for_meter requires a SN_col with one unique value, so satisfy that\n",
    "    SN_col='SN'\n",
    "    if outg_rec_nb is None:\n",
    "        SN_val = 'AGG_OUTAGE'\n",
    "    else:\n",
    "        SN_val = f'AGG_OUTAGE {outg_rec_nb}'\n",
    "    neq0_df[SN_col] = SN_val\n",
    "\n",
    "    #-------------------------\n",
    "    # Now, simply feed into estimate_outage_times_for_meter\n",
    "    return_list = estimate_outage_times_for_meter(\n",
    "        df_i=neq0_df, \n",
    "        t_search_min_max=t_search_min_max, \n",
    "        t_int_beg_col=t_int_beg_col, \n",
    "        t_int_end_col=t_int_end_col, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col, \n",
    "        verbose=verbose,\n",
    "        outg_rec_nb_col=outg_rec_nb_col\n",
    "    )\n",
    "\n",
    "    #-------------------------\n",
    "    if len(return_list)==0 and relax_pct_SNs_required_if_no_outgs_found and pct_SNs_required_for_outage>0:\n",
    "        # NOTE: Important below that input relax_pct_SNs_required_if_no_outgs_found be set to False\n",
    "        #       OTHERWISE INFINITE LOOP WHEN NO OUTAGE TIMES FOUND!\n",
    "        print('No outage_times found in estimate_outage_times, so relaxing the pct_SNs_required contraint')\n",
    "        return_list = estimate_outage_times(\n",
    "            df=df, \n",
    "            t_search_min_max=t_search_min_max, \n",
    "            pct_SNs_required_for_outage=0, \n",
    "            relax_pct_SNs_required_if_no_outgs_found=False, \n",
    "            t_int_beg_col=t_int_beg_col, \n",
    "            t_int_end_col=t_int_end_col, \n",
    "            value_col=value_col, \n",
    "            verbose=verbose,\n",
    "            outg_rec_nb_col=outg_rec_nb_col\n",
    "        )\n",
    "        \n",
    "    #-------------------------\n",
    "    if combine_overlaps and len(return_list)>0:\n",
    "        return_list = combine_est_outg_times_with_overlap_cnsrvtv(\n",
    "            est_outg_times=return_list,\n",
    "            overlap_to_enforce=None\n",
    "        )\n",
    "    \n",
    "    #-------------------------\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1787f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d7fddf8",
   "metadata": {},
   "source": [
    "# COMBINE AMI AND EDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a64c3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outg_time_audit(\n",
    "    outg_est_times_i, \n",
    "    dovs_outg_t_beg_end=None, \n",
    "    outg_times_ede=None, \n",
    "    selection_method='min', \n",
    "    return_all_best_ests=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given the zero times for the (sub)outage, frequency of the data, and DOVS outage data and/or EDE outage times,\n",
    "    return the best estimate for the outage time.\n",
    "    \n",
    "    NOTE: To be certain, this explicitly places preference on the zero times as found by the 15-minute interval.\n",
    "          The conservative values found using that data are replaced on if dovs and/or ede data supply a value that is\n",
    "            consistent with the 15-minute interval data (i.e., in the uncertainty intervals, see below for more info).\n",
    "          Therefore, if dovs or ede complete disagree with 15-minute interval, the latter is always chosen.\n",
    "    \n",
    "    The conservative estimates for the outage time are found by subtracting the frequency of the data (freq) from the first\n",
    "      zero time and adding freq to the last zero time.\n",
    "    The uncertainty intervals are defined as:\n",
    "        [first zero - freq, first zero]\n",
    "        [last zero,         last zero + freq]\n",
    "    If dovs and/or ede supply times within the uncertainty region, the values used are chosen according to selection_method: \n",
    "        selection_method=='min':       those minimizing the outage time are chosen\n",
    "        selection_method=='max':       those maximizing the outage time are chosen\n",
    "        selection_method=='dovs pref': dovs times preferred, meaning dovs times are chosen if both dovs and ede available,\n",
    "                                         but ede times are taken if available when dovs are not\n",
    "        selection_method=='dovs only':  dovs times are chosen if available.  \n",
    "                                         If dovs not available but ede is, ede ignored\n",
    "        selection_method=='ede pref':  ede times preferred, meaning ede times are chosen if both dovs and ede available\n",
    "                                         but dovs times are taken if available when ede are not\n",
    "        selection_method=='ede only':  ede times are chosen if available.  \n",
    "                                         If ede not available but dovs is, dovs ignored\n",
    "                                         \n",
    "    outg_times_ede:\n",
    "        this can be a list of 2-element lists representing multiple outage beg/end pairs (if, e.g., found using\n",
    "          estimate_outage_times_using_ede_for_meter)\n",
    "        OR\n",
    "        this can be a dict with two keys, pd_times and pu_times, the values of which are lists containing the \n",
    "          associated times (if, e.g., found using get_pd_pu_times_for_meter)\n",
    "    \n",
    "    Given the conservative estimates for the outage time (and frequency of the data), and DOVS outage data and/or EDE outage times,\n",
    "    return the best estimate for the outage time.\n",
    "    \n",
    "    cnsrvtv_beg_end:\n",
    "        The conservative estimates for the beginning and end of the outage.\n",
    "        These are typically the period before the first zero and the period after the last zero. \n",
    "    \n",
    "    NOTE: This assumes the user wants a conservative estimate.! \n",
    "            The non-conservative estimate essentially counts to times the meter registered a zero count, so no outside audit needed.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(isinstance(outg_est_times_i, dict))\n",
    "    assert(len(set(outg_est_times_i.keys()).difference(set(['cnsrvtv_t_beg', 'cnsrvtv_t_end', 'zero_t_beg', 'zero_t_end'])))==0)\n",
    "    \n",
    "    #-------------------------\n",
    "    # First, find the conservative outage beginning and ending\n",
    "    cnsrvtv_beg_end = [outg_est_times_i['cnsrvtv_t_beg'], outg_est_times_i['cnsrvtv_t_end']]\n",
    "    zero_t_beg_end  = [outg_est_times_i['zero_t_beg'], outg_est_times_i['zero_t_end']]\n",
    "    #-------------------------\n",
    "    if not dovs_outg_t_beg_end and not outg_times_ede:\n",
    "        return cnsrvtv_beg_end\n",
    "    #-------------------------\n",
    "    assert(selection_method in ['min', 'max', 'dovs pref', 'dovs only', 'ede pref', 'ede only'])\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    dovs_best_est = [None, None]\n",
    "    if dovs_outg_t_beg_end:\n",
    "        # If the outage information from DOVS is between the conservative beginning/end and the\n",
    "        #   first/last recorded zero, DOVSs may be used in audit\n",
    "        if dovs_outg_t_beg_end[0]>cnsrvtv_beg_end[0] and dovs_outg_t_beg_end[0]<zero_t_beg_end[0]:\n",
    "            dovs_best_est[0] = dovs_outg_t_beg_end[0]\n",
    "        #-----\n",
    "        if dovs_outg_t_beg_end[1]<cnsrvtv_beg_end[1] and dovs_outg_t_beg_end[1]>zero_t_beg_end[1]:\n",
    "            dovs_best_est[1] = dovs_outg_t_beg_end[1]\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    ede_best_est = [None, None]\n",
    "    if outg_times_ede is not None:\n",
    "        # Expecting outg_times_ede to be a list of outage times or a dict with pd_times, pu_times keys\n",
    "        assert(Utilities.is_object_one_of_types(outg_times_ede, [list, dict]))\n",
    "        #----------\n",
    "        if isinstance(outg_times_ede, list) and len(outg_times_ede)>0:\n",
    "            # Expecting outg_times_ede to be a list of outage times, so list of two-element lists\n",
    "            outg_times_ede_shape = np.array(outg_times_ede).shape\n",
    "            assert(outg_times_ede_shape[1]==2)\n",
    "            #-----\n",
    "            # Unzip outg_times_ede, taking it from a list of outage times (2-element lists) to a list of two lists,\n",
    "            #   one containing the beginning times and one containing the end times\n",
    "            # This is essentially a transpose\n",
    "            outg_times_beg_end_ede = list(zip(*outg_times_ede))\n",
    "            #-----\n",
    "            # Expecting outg_times_beg_end_ede to be a list containing two lists\n",
    "            outg_times_beg_end_ede_shape = np.array(outg_times_beg_end_ede).shape\n",
    "            assert(outg_times_beg_end_ede_shape[0]==2)\n",
    "\n",
    "            # Since this is a transpose...\n",
    "            assert(outg_times_ede_shape[0]==outg_times_beg_end_ede_shape[1])\n",
    "            #-----\n",
    "            outg_times_beg_ede = outg_times_beg_end_ede[0]\n",
    "            outg_times_end_ede = outg_times_beg_end_ede[1]\n",
    "        else:\n",
    "            outg_times_beg_ede=[]\n",
    "            outg_times_end_ede=[]\n",
    "        #----------\n",
    "        if isinstance(outg_times_ede, dict):\n",
    "            assert('pd_times' in outg_times_ede.keys() and 'pu_times' in outg_times_ede.keys())\n",
    "            outg_times_beg_ede = outg_times_ede['pd_times']\n",
    "            outg_times_end_ede = outg_times_ede['pu_times']\n",
    "\n",
    "        #--------------------------------------------------\n",
    "        # If multiple suitable beg/ends are found, those which maximize the outage time are chosen\n",
    "        #   i.e., min of ede_best_ests_beg and max of ede_best_ests_end\n",
    "        # This keeps in line with our strategy of being conservative at each step.\n",
    "        #-----\n",
    "        # To be certain, if outg_times_ede is a list, meaning it is a collection of (outg_beg,outg_end pairs) \n",
    "        #   for different sub-outages, the method below allows one sub-outage to be used to resolve the ambiguity\n",
    "        #   in the start time and another to resolve that of the end time.\n",
    "        #-------------------------\n",
    "        ede_best_ests_beg = []\n",
    "        for beg_t_i in outg_times_beg_ede:\n",
    "            if beg_t_i>cnsrvtv_beg_end[0] and beg_t_i<zero_t_beg_end[0]:\n",
    "                ede_best_ests_beg.append(beg_t_i)\n",
    "        #-----\n",
    "        if ede_best_ests_beg:\n",
    "            ede_best_est_beg = np.min(ede_best_ests_beg)\n",
    "        else:\n",
    "            ede_best_est_beg = None #excplicitly make None for later\n",
    "        #-------------------------\n",
    "        ede_best_ests_end = []\n",
    "        for end_t_i in outg_times_end_ede:\n",
    "            if end_t_i<cnsrvtv_beg_end[1] and end_t_i>zero_t_beg_end[1]:\n",
    "                ede_best_ests_end.append(end_t_i)\n",
    "        #-----\n",
    "        if ede_best_ests_end:\n",
    "            ede_best_est_end = np.max(ede_best_ests_end)\n",
    "        else:\n",
    "            ede_best_est_end = None #excplicitly make None for later\n",
    "        #--------------------------------------------------\n",
    "        ede_best_est = [ede_best_est_beg, ede_best_est_end]\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # NOTE: dovs always 0th element, ede 1st element\n",
    "    best_ests_beg = [dovs_best_est[0], ede_best_est[0]]\n",
    "    best_ests_end = [dovs_best_est[1], ede_best_est[1]]\n",
    "    #--------------------------------------------------\n",
    "    if np.all([x is None for x in best_ests_beg]):\n",
    "        best_est_beg = None\n",
    "    else:\n",
    "        # NOTE: because of if statement above, we can be certain here there is at least one\n",
    "        #       non-null value, so taking min/max should be safe (since not finding min/max of empty list)\n",
    "        if selection_method=='min':\n",
    "            best_est_beg = np.max([x for x in best_ests_beg if x])\n",
    "        elif selection_method=='max':\n",
    "            best_est_beg = np.min([x for x in best_ests_beg if x])\n",
    "        elif selection_method=='dovs pref' or selection_method=='dovs only':\n",
    "            if best_ests_beg[0]:\n",
    "                best_est_beg = best_ests_beg[0]\n",
    "            else:\n",
    "                if selection_method=='dovs pref':\n",
    "                    assert(best_ests_beg[1])\n",
    "                    best_est_beg = best_ests_beg[1]\n",
    "                else:\n",
    "                    #selection_method=='dovs only' case\n",
    "                    best_est_beg = None\n",
    "        elif selection_method=='ede pref' or selection_method=='ede only':\n",
    "            if best_ests_beg[1]:\n",
    "                best_est_beg = best_ests_beg[1]\n",
    "            else:\n",
    "                if selection_method=='ede pref':\n",
    "                    assert(best_ests_beg[0])\n",
    "                    best_est_beg = best_ests_beg[0]\n",
    "                else:\n",
    "                    #selection_method=='ede only' case\n",
    "                    best_est_beg = None\n",
    "        else:\n",
    "            assert(0)\n",
    "    #-------------------------\n",
    "    if np.all([x is None for x in best_ests_end]):\n",
    "        best_est_end = None\n",
    "    else:\n",
    "        # NOTE: because of if statement above, we can be certain here there is at least one\n",
    "        #       non-null value, so taking min/max should be safe (since not finding min/max of empty list)\n",
    "        if selection_method=='min':\n",
    "            best_est_end = np.min([x for x in best_ests_end if x])\n",
    "        elif selection_method=='max':\n",
    "            best_est_end = np.max([x for x in best_ests_end if x])\n",
    "        elif selection_method=='dovs pref' or selection_method=='dovs only':\n",
    "            if best_ests_end[0]:\n",
    "                best_est_end = best_ests_end[0]\n",
    "            else:\n",
    "                if selection_method=='dovs pref':\n",
    "                    assert(best_ests_end[1])\n",
    "                    best_est_end = best_ests_end[1]\n",
    "                else:\n",
    "                    #selection_method=='dovs only' case\n",
    "                    best_est_end = None\n",
    "        elif selection_method=='ede pref' or selection_method=='ede only':\n",
    "            if best_ests_end[1]:\n",
    "                best_est_end = best_ests_end[1]\n",
    "            else:\n",
    "                if selection_method=='ede pref':\n",
    "                    assert(best_ests_end[0])\n",
    "                    best_est_end = best_ests_end[0]\n",
    "                else:\n",
    "                    #selection_method=='ede only' case\n",
    "                    best_est_end = None\n",
    "        else:\n",
    "            assert(0)\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # By definition, if any best estimates are found, they are in the uncertainty intervals, and therefore\n",
    "    #  reduce the outage time in comparison to the conservative estimates\n",
    "    if best_est_beg:\n",
    "        assert(best_est_beg > cnsrvtv_beg_end[0])\n",
    "        final_out_t_beg = best_est_beg\n",
    "    else:\n",
    "        final_out_t_beg = cnsrvtv_beg_end[0]\n",
    "    #-----\n",
    "    if best_est_end:\n",
    "        assert(best_est_end < cnsrvtv_beg_end[1])\n",
    "        final_out_t_end = best_est_end\n",
    "    else:\n",
    "        final_out_t_end = cnsrvtv_beg_end[1]\n",
    "    #-------------------------\n",
    "    if return_all_best_ests:\n",
    "        all_best_est_dict = dict(\n",
    "            ede=ede_best_est, \n",
    "            dovs=dovs_best_est,\n",
    "            conservative=cnsrvtv_beg_end, \n",
    "            zero_times=zero_t_beg_end,\n",
    "            winner = [final_out_t_beg, final_out_t_end]\n",
    "        )\n",
    "        return final_out_t_beg, final_out_t_end, all_best_est_dict\n",
    "    else:\n",
    "        return final_out_t_beg, final_out_t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8abeed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default arg for ede_df_i will be None, and if None, old functionality without EDE in place\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# TODO : Don't need freq anymore@@!!!!!!!!!\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "def calculate_mi_for_meter_ami_w_ede_help(\n",
    "    df_i, \n",
    "    ede_df_i, \n",
    "    t_search_min_max=None, \n",
    "    conservative_estimate=True, \n",
    "    dovs_outg_t_beg_end=None, \n",
    "    freq=None, \n",
    "    est_ede_kwargs=None, \n",
    "    audit_selection_method='min', \n",
    "    t_int_beg_col='starttimeperiod_local', \n",
    "    t_int_end_col='endtimeperiod_local', \n",
    "    value_col='value', \n",
    "    SN_col='serialnumber', \n",
    "    return_all_best_ests=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Find the times for which df_i registers a value of 0 and use that information to calculate the Minutes Interrupted (MI)\n",
    "      for the meter.\n",
    "    df_i should contain AMI data for a single meter.\n",
    "      \n",
    "    t_search_min_max:\n",
    "        Allows the user to set of time subset of the data from which to search for the outages/calculate mi.\n",
    "        If t_search_min_max is not None, it should be a two-element list/tuple.\n",
    "        However, one of the elements of the list may be None if, e.g., one wants limits on only the min or max\n",
    "        \n",
    "    conservative_estimate:\n",
    "        It appears that AMI meters only report values of 0 if all readings in the 15-minute period are zero.\n",
    "        I believe that if at least one value is non-zero, the AMI will report a non-zero value.\n",
    "            e.g., if power out from 12:00 to 12:14, but turns on from 12:14 to 12:15, the 12:15 reading will be non-zero\n",
    "        Thus, simply calculating the period of time when the measurements are 0 inherently underestimates the actual outage time.\n",
    "        The more conservative approach, which typically overestimates the MI, would be to add 2*freq to MI, where freq is the \n",
    "          frequency of the data (typically 15 minutes).\n",
    "            To elaborate a bit more: the outage actually begins in the interval before the first 0 value is regiesterd and ends\n",
    "              in the interval after the last 0 is registered.\n",
    "        If conservative_estimate==True, 2*freq is added to the MI UNLESS dovs_outg_t_beg_end is supplied (see description below)\n",
    "        \n",
    "    dovs_outg_t_beg_end:\n",
    "        Only has an affect if conservative_estimate==True\n",
    "        If the outage beginning and ending times are supplied from DOVS, they can sometimes be used in place of the conservative\n",
    "          starting and ending times to help resolve the uncertainty.\n",
    "        Essentially, if a DOVS time falls within the uncertainty interval, it can be used in place of the conservative time (where\n",
    "          the uncertainty interval is the time before/after the first/last recorded zero value and the preceding/following interval).\n",
    "            i.e., on the front end, use DOVS if time between 0-freq before last zero value\n",
    "                  on the back end, use DOVS if time between 0-freq after last zero value\n",
    "        If a DOVS time is well outside of the uncertainty interval (e.g., if the DOVS start time is well before or well after the first\n",
    "          recorded 0 value), it cannot be used.\n",
    "          \n",
    "    freq:\n",
    "        The user can supply the frequency of the data if desired or needed.\n",
    "        If freq is None, it will be inferred from df_i.\n",
    "        In general, probably safest to just let the algorithm infer the frequency.\n",
    "        I suppose this could be useful to save some time in large data runs over many outages and serial numbers (although I doubt\n",
    "          the savings will be significant).\n",
    "        It could also be useful if for whatever reason there is an issue inferring the frequency from the data (e.g., maybe a measurement\n",
    "          falls slightly outside of the normal 15-minute intervals, like 12:44:59 or similar)\n",
    "          \n",
    "    est_ede_kwargs:\n",
    "        Keyword arguments to input in estimate_outage_times_using_ede_for_meter.\n",
    "        NOTE: ede_df_i, broad_out_t_beg, broad_out_t_end, and expand_search_time SHOULD NOT be included\n",
    "                in est_ede_kwargs, as the function will set these        \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # As described above, designed to work for DF containing data from a single meter.\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    if ede_df_i is not None:\n",
    "        assert(ede_df_i[SN_col].nunique()==1)\n",
    "        assert(df_i[SN_col].unique().tolist()[0]==ede_df_i[SN_col].unique().tolist()[0])\n",
    "    \n",
    "    #-------------------------\n",
    "    if t_search_min_max is not None:\n",
    "        assert(Utilities.is_object_one_of_types(t_search_min_max, [list, tuple]))\n",
    "        assert(len(t_search_min_max)==2)\n",
    "        #-----\n",
    "        # At least one of t_search_min_max should not be None\n",
    "        assert(t_search_min_max[0] is not None or t_search_min_max[1] is not None)\n",
    "        if t_search_min_max[0] is None:\n",
    "            t_search_min_max[0] = pd.Timestamp.min\n",
    "        if t_search_min_max[1] is None:\n",
    "            t_search_min_max[1] = pd.Timestamp.max\n",
    "    #-------------------------\n",
    "    # Make sure values sorted\n",
    "    df_i = df_i.sort_values(by=t_int_beg_col)\n",
    "    #-------------------------\n",
    "    # Need frequency of the data for conservative estimates\n",
    "    if freq is None:\n",
    "        freq = Utilities_df.determine_freq_in_df_col(\n",
    "            df=df_i, \n",
    "            groupby_SN=False, \n",
    "            return_val_if_not_found='error', \n",
    "            assert_single_freq=True, \n",
    "            assert_expected_freq_found=False, \n",
    "            expected_freq=pd.Timedelta('15 minutes'), \n",
    "            time_col=t_int_beg_col, \n",
    "            SN_col=SN_col\n",
    "        )\n",
    "    #--------------------------------------------------\n",
    "    # Find the time periods where all values are zero\n",
    "    outg_est_times = estimate_outage_times_for_meter(\n",
    "        df_i=df_i, \n",
    "        t_search_min_max=t_search_min_max, \n",
    "        t_int_beg_col=t_int_beg_col, \n",
    "        t_int_end_col=t_int_end_col, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    # Grab the outage times from ede\n",
    "    if ede_df_i is not None:\n",
    "        if not est_ede_kwargs:\n",
    "            est_ede_kwargs = {}\n",
    "        #broad_out_t_beg = t_search_min_max[0]\n",
    "        #broad_out_t_end = t_search_min_max[1]\n",
    "        # Since estimate_outage_times_for_meter is able to expand the search window, instead of\n",
    "        #   using broad_out_t_beg_end = t_search_min_max, I should set these values equal to the\n",
    "        #   min/max of the conservative estimates from outg_est_times\n",
    "        if len(outg_est_times)>0:\n",
    "            broad_out_t_beg = np.min([x['cnsrvtv_t_beg'] for x in outg_est_times])\n",
    "            broad_out_t_end = np.max([x['cnsrvtv_t_end'] for x in outg_est_times])\n",
    "        else:\n",
    "            broad_out_t_beg = t_search_min_max[0]\n",
    "            broad_out_t_end = t_search_min_max[1]\n",
    "        dflt_est_ede_kwargs = dict(\n",
    "            ede_df_i=ede_df_i, \n",
    "            broad_out_t_beg=broad_out_t_beg, \n",
    "            broad_out_t_end=broad_out_t_end,\n",
    "            expand_search_time=pd.Timedelta(0), \n",
    "            use_full_ede_outgs=False, \n",
    "            pd_ids=['3.26.0.47', '3.26.136.47'], \n",
    "            pu_ids=['3.26.0.216', '3.26.136.216'], \n",
    "            SN_col='serialnumber', \n",
    "            valuesinterval_col='valuesinterval_local', \n",
    "            edetypeid_col='enddeviceeventtypeid', \n",
    "            verbose=False\n",
    "        )\n",
    "        #-----\n",
    "        # Make sure none of the kwargs set by function are included in est_ede_kwargs\n",
    "        supplied_est_ede_kwargs = ['ede_df_i', 'broad_out_t_beg', 'broad_out_t_end', 'expand_search_time']\n",
    "        assert(len(set(est_ede_kwargs.keys()).intersection(set(supplied_est_ede_kwargs)))==0)\n",
    "        #-----\n",
    "        # Want est_ede_kwargs to be kept over dflt_est_ede_kwargs, so use to_supplmnt_dict=est_ede_kwargs in the following\n",
    "        est_ede_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "            to_supplmnt_dict=est_ede_kwargs, \n",
    "            default_values_dict=dflt_est_ede_kwargs, \n",
    "            extend_any_lists=False, \n",
    "            inplace=False\n",
    "        )\n",
    "        use_full_ede_outgs = est_ede_kwargs.pop('use_full_ede_outgs')\n",
    "        #-----\n",
    "        if use_full_ede_outgs:\n",
    "            outg_times_ede = estimate_outage_times_using_ede_for_meter(**est_ede_kwargs)\n",
    "        else:\n",
    "            outg_times_ede = get_pd_pu_times_for_meter(\n",
    "                ede_df_i=ede_df_i, \n",
    "                t_search_min_max=[est_ede_kwargs['broad_out_t_beg'], est_ede_kwargs['broad_out_t_end']], \n",
    "                pd_ids=est_ede_kwargs['pd_ids'], \n",
    "                pu_ids=est_ede_kwargs['pu_ids'], \n",
    "                SN_col=est_ede_kwargs['SN_col'], \n",
    "                valuesinterval_col=est_ede_kwargs['valuesinterval_col'], \n",
    "                edetypeid_col=est_ede_kwargs['edetypeid_col']\n",
    "            )\n",
    "    else:\n",
    "        outg_times_ede = None\n",
    "    #--------------------------------------------------\n",
    "    # Iterate through outg_est_times, calculate mi_j for each, and add to total mi\n",
    "    mi=pd.Timedelta(0)\n",
    "    all_best_ests = []\n",
    "    for outg_est_times_j in outg_est_times:\n",
    "        #-------------------------\n",
    "        #TODO: Is there really any reason to find times from df_i?\n",
    "        #      Can't I just directly use outg_est_times_j?\n",
    "        #      Only point seems to be to enforce assertion, but probably \n",
    "        #        the definition of estimate_outage_times_for_meter requires that outcome?\n",
    "        # First, grab the subset of the data corresponding to the zero time\n",
    "        df_i_zr_j = df_i[\n",
    "            (df_i[t_int_beg_col]>=outg_est_times_j['zero_t_beg']) & \n",
    "            (df_i[t_int_end_col]<=outg_est_times_j['zero_t_end'])\n",
    "        ]\n",
    "        #-------------------------\n",
    "        # Since zero_t_end/zero_t_beg keys used above, all values should be zero\n",
    "        assert((df_i_zr_j[value_col]==0).all())\n",
    "\n",
    "        #-------------------------\n",
    "        # Non-conservative MI, simply the difference in time between the last zero value and the first\n",
    "        # As mentioned elsehwere, this inherently underestimates the MI\n",
    "        mi_j = df_i_zr_j.iloc[-1][t_int_end_col]-df_i_zr_j.iloc[0][t_int_beg_col]\n",
    "\n",
    "        if conservative_estimate:\n",
    "            outg_j_beg_end = outg_time_audit(\n",
    "                outg_est_times_i=outg_est_times_j, \n",
    "                dovs_outg_t_beg_end=dovs_outg_t_beg_end, \n",
    "                outg_times_ede=outg_times_ede, \n",
    "                selection_method=audit_selection_method, \n",
    "                return_all_best_ests=return_all_best_ests\n",
    "            )\n",
    "            outg_j_beg, outg_j_end = outg_j_beg_end[0], outg_j_beg_end[1]\n",
    "            mi_j = outg_j_end-outg_j_beg\n",
    "            #-----\n",
    "            if return_all_best_ests:\n",
    "                assert(len(outg_j_beg_end)==3)\n",
    "                all_best_ests_j = outg_j_beg_end[2]\n",
    "                #-----\n",
    "                all_best_ests.append(all_best_ests_j)\n",
    "        #-------------------------\n",
    "        mi += mi_j\n",
    "    #-------------------------\n",
    "    mi = mi.total_seconds()/60\n",
    "    if return_all_best_ests:\n",
    "        return mi, all_best_ests\n",
    "    else:\n",
    "        return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef517b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonoverlapping_search_windows(\n",
    "    est_outg_times, \n",
    "    expand_outg_search_time\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a list of estimated outage times and expand search times, return a list of non-overlapping t_search_min_max values.\n",
    "    Returns a list of two-element lists (0th element is t_search_min, 1st is t_search_max).\n",
    "    The length of the return list will equal that of est_outg_times\n",
    "    \n",
    "    NOTE: If I ever decide to move to asymmetric expand_outg_search_time, I believe this method should still work.\n",
    "          However, one should carefully verify instead of assuming.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(\n",
    "        Utilities.is_object_one_of_types(est_outg_times, [list, tuple]) and\n",
    "        Utilities.are_all_list_elements_one_of_types(est_outg_times, [list, tuple]) and\n",
    "        Utilities.are_list_elements_lengths_homogeneous(est_outg_times, length=2)\n",
    "    )\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    # First, go through and naively set t_search values\n",
    "    # To more easily find overlapping regions, define a left, center, and right search window for each outage, where:\n",
    "    #   left (left)   = search period preceding outage, ending at est. outage begin\n",
    "    #   center (cntr) = est. outage begin to est. outage end\n",
    "    #   right (rght)  = search period following outage, beginning at est. outage end\n",
    "    # The left and right windows will be of length expand_outg_search_time UNLESS they overlap with another outage, in which\n",
    "    #   case the endpoint of the window will be defined by the overlapping outage.\n",
    "    #     left - check if overlaps with est. outage end of preceding outage.\n",
    "    #            If overlaps, minimum of left set to est. outage end of preceding outage.\n",
    "    #     rght - check if overlaps with est. outage beginning of following outage.\n",
    "    #            If overlaps, maximum of right set to est. outage beginning of following outage.\n",
    "    # NOTE: Ensuring the search windows themselves do not overlap occurs in the next for loop, not this one!\n",
    "    #       This is just a rough chopping\n",
    "    # NOTE: After completing the code, no real need to keep 'cntr' windows, but no harm in keeping (might be useful later?)\n",
    "    #-----\n",
    "    search_windows_coll = []\n",
    "    est_outg_times = natsorted(est_outg_times)\n",
    "    for i_outg in range(len(est_outg_times)):\n",
    "        est_outg_times_i = est_outg_times[i_outg]\n",
    "        #-----\n",
    "        # Simple sanity check: ensure outage end occurs after outage beginning!\n",
    "        assert(len(est_outg_times_i)==2)\n",
    "        assert(est_outg_times_i[1] > est_outg_times_i[0])\n",
    "        # Sanity check: the given estimate outage times should not overlap!\n",
    "        # Note: Sufficient to check that beginning of current is greater than ending of preceding\n",
    "        if i_outg>0:\n",
    "            assert(est_outg_times_i[0] >= est_outg_times[i_outg-1][1])\n",
    "        #-------------------------\n",
    "        # No adjustment needed on center window\n",
    "        search_windows_i = dict(cntr=est_outg_times_i)\n",
    "        #-------------------------\n",
    "        # Left window\n",
    "        if i_outg==0:\n",
    "            # First element, so no preceding outage to compare\n",
    "            search_windows_i['left']=[\n",
    "                est_outg_times_i[0]-expand_outg_search_time, \n",
    "                est_outg_times_i[0]\n",
    "            ]\n",
    "        else:\n",
    "            # Desired left window = est_outg_times_i[0]-expand_outg_search_time to est_outg_times_i[0]\n",
    "            # Check if minimum value of desired left window overlaps with est. outage end of preceding outage\n",
    "            # NOTE: im1 == i minus 1\n",
    "            left_i = [est_outg_times_i[0]-expand_outg_search_time, est_outg_times_i[0]]\n",
    "            est_outg_times_im1 = est_outg_times[i_outg-1]\n",
    "            if left_i[0]<est_outg_times_im1[1]:\n",
    "                left_i[0] = est_outg_times_im1[1]\n",
    "            #-----\n",
    "            search_windows_i['left']=left_i\n",
    "        #-------------------------\n",
    "        # Right window\n",
    "        if i_outg==len(est_outg_times)-1:\n",
    "            # Last element, so no following outage to compare\n",
    "            search_windows_i['rght']=[\n",
    "                est_outg_times_i[1], \n",
    "                est_outg_times_i[1]+expand_outg_search_time\n",
    "            ]\n",
    "        else:\n",
    "            # Desired right window = est_outg_times_i[1] to est_outg_times_i[1]+expand_outg_search_time\n",
    "            # Check if maximum value of desired right window overlaps with est. outage beginning of following outage\n",
    "            # NOTE: ip1 == i plus 1\n",
    "            rght_i = [est_outg_times_i[1], est_outg_times_i[1]+expand_outg_search_time]\n",
    "            est_outg_times_ip1 = est_outg_times[i_outg+1]\n",
    "            if rght_i[1]>est_outg_times_ip1[0]:\n",
    "                rght_i[1] = est_outg_times_ip1[0]\n",
    "            #-----\n",
    "            search_windows_i['rght']=rght_i\n",
    "        #-------------------------\n",
    "        search_windows_coll.append(search_windows_i)\n",
    "    #--------------------------------------------------\n",
    "    # Now, go back through and resolve any overlaps.\n",
    "    # If overlaps exist, set the boundary equal to the midpoint of the overlap.\n",
    "    # It should be sufficient to go through and compare the left search window of the\n",
    "    #   current element to the right of the previous\n",
    "    assert(len(search_windows_coll)==len(est_outg_times))\n",
    "    for i_outg in range(len(search_windows_coll)):\n",
    "        if i_outg==0:\n",
    "            continue\n",
    "        search_windows_i   = search_windows_coll[i_outg]\n",
    "        search_windows_im1 = search_windows_coll[i_outg-1]\n",
    "        #-----\n",
    "        left_i   = search_windows_i['left']\n",
    "        rght_im1 = search_windows_im1['rght']\n",
    "        #-----\n",
    "        # Sanity check: Due to rough clipping done in first iteration:\n",
    "        #   max of left_i (outg. beg. i) should be greater than (or eq. to) max of rght_im1\n",
    "        #   min of rght_im1 (outg. end i-1) should be less than (or eq. to) min of left_i\n",
    "        assert(left_i[1] >= rght_im1[1])\n",
    "        assert(rght_im1[0] <= left_i[0])\n",
    "        #-------------------------\n",
    "        # If there is no overlap, continue\n",
    "        if left_i[0] >= rght_im1[1]:\n",
    "            continue\n",
    "        #-------------------------\n",
    "        # Overlap exists, so resolve by setting endpoints equal to midpoint of overlap\n",
    "        #-----\n",
    "        # The overlap min is the maximum of the mins\n",
    "        # The overlap max is the minimum of the maxs\n",
    "        ovrlp_min = np.max([left_i[0], rght_im1[0]])\n",
    "        ovrlp_max = np.min([left_i[1], rght_im1[1]])\n",
    "        #-----\n",
    "        ovrlp_mid = Utilities_dt.calc_dt_mean([ovrlp_min, ovrlp_max])\n",
    "        #-----\n",
    "        # Update values in search_windows_coll (update min of left_i and max of right_im1 to equal ovrlp_mid)\n",
    "        search_windows_coll[i_outg]['left'][0]   = ovrlp_mid\n",
    "        search_windows_coll[i_outg-1]['rght'][1] = ovrlp_mid\n",
    "        \n",
    "    #--------------------------------------------------\n",
    "    # Finally, get the final search windows as the min of left and max of right\n",
    "    t_search_min_max_coll = []\n",
    "    for search_windows_i in search_windows_coll:\n",
    "        left_i = search_windows_i['left']\n",
    "        rght_i = search_windows_i['rght']\n",
    "        t_search_min_max_coll.append([left_i[0], rght_i[1]])\n",
    "        \n",
    "    #--------------------------------------------------\n",
    "    # Final sanity check!\n",
    "    assert(len(est_outg_times)==len(t_search_min_max_coll))\n",
    "    for i_outg in range(len(t_search_min_max_coll)):\n",
    "        assert(t_search_min_max_coll[i_outg][1]>t_search_min_max_coll[i_outg][0])\n",
    "        if i_outg>0:\n",
    "            assert(t_search_min_max_coll[i_outg][0]>=t_search_min_max_coll[i_outg-1][1])\n",
    "    #--------------------------------------------------\n",
    "    return t_search_min_max_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3630bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_best_ests_dict_to_df(all_best_ests_dict):\n",
    "    r\"\"\"\n",
    "    Convert all_best_ests_dict to pd.DataFrame object.\n",
    "    all_best_ests_dict is returned by, e.g., calculate_ci_cmi_w_ami_w_ede_help.\n",
    "    This is a pretty specific function, so it should probably only be used within calculate_ci_cmi_w_ami_w_ede_help\n",
    "      or directly on an object returned by it.\n",
    "    NOTE: SNs for which no suboutages were found are EXCLUDED from df!\n",
    "    -----\n",
    "    all_best_ests_dict:\n",
    "        It should be a dictionary whose keys equal serial numbers in an outage.\n",
    "        Each key is a list of dict objects.\n",
    "            Each dict object represents on suboutage time, and contains keys for the various estimates.\n",
    "            The length of the list equals the number of suboutages found (can be zero if none found)\n",
    "            Typical keys are: 'zero_times', 'conservative', 'winner', 'ede', and 'dovs'\n",
    "            For each key, there is a two-element list representing the beg/end times.\n",
    "    EX:\n",
    "        {'645792023': [],\n",
    "         '645779036': \n",
    "           [\n",
    "             {\n",
    "              'ede': [Timestamp('2023-01-01 01:22:37'), None],\n",
    "              'dovs': [Timestamp('2023-01-01 01:22:00'), None],\n",
    "              'conservative': [Timestamp('2023-01-01 01:15:00'), Timestamp('2023-01-01 02:30:00')],\n",
    "              'winner': [Timestamp('2023-01-01 01:22:37'), Timestamp('2023-01-01 02:30:00')],\n",
    "              'zero_times': [Timestamp('2023-01-01 01:30:00'), Timestamp('2023-01-01 02:15:00')]\n",
    "              },\n",
    "             {\n",
    "              'ede': [None, None],\n",
    "              'dovs': [None, Timestamp('2023-01-01 04:13:00')],\n",
    "              'conservative': [Timestamp('2023-01-01 03:00:00'), Timestamp('2023-01-01 04:15:00')],\n",
    "              'winner': [Timestamp('2023-01-01 03:00:00'), Timestamp('2023-01-01 04:13:00')],\n",
    "              'zero_times': [Timestamp('2023-01-01 03:15:00'), Timestamp('2023-01-01 04:00:00')]\n",
    "             }\n",
    "          ],\n",
    "        ...\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    return_df = pd.DataFrame()\n",
    "    #-----\n",
    "    for (SN_i, PN_i), best_ests_list_i in all_best_ests_dict.items():\n",
    "        if len(best_ests_list_i)==0:\n",
    "            continue\n",
    "        for outg_j, best_est_dict_ij in enumerate(best_ests_list_i):\n",
    "            best_est_dict_ij_split=dict()\n",
    "            best_est_dict_ij_split['SN'] = SN_i\n",
    "            best_est_dict_ij_split['PN'] = PN_i\n",
    "            best_est_dict_ij_split['i_outg'] = outg_j\n",
    "            for est_key, beg_end_vals in best_est_dict_ij.items():\n",
    "                best_est_dict_ij_split[f\"{est_key}_min\"] = beg_end_vals[0]\n",
    "                best_est_dict_ij_split[f\"{est_key}_max\"] = beg_end_vals[1]\n",
    "            return_df = pd.concat([return_df, pd.DataFrame(best_est_dict_ij_split, index=[return_df.shape[0]])])\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c514277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: I don't think I need n_SNs_not_in_ede anymore\n",
    "def calculate_ci_cmi_w_ami_w_ede_help(\n",
    "    df, \n",
    "    ede_df, \n",
    "    dovs_outg_t_beg_end, \n",
    "    expand_outg_search_time=pd.Timedelta('1 hour'), \n",
    "    conservative_estimate=True, \n",
    "    est_ede_kwargs=None, \n",
    "    audit_selection_method='min', \n",
    "    return_CI_SNs=False, \n",
    "    use_est_outg_times=False, \n",
    "    pct_SNs_required_for_outage_est=0, \n",
    "    expand_outg_est_search_time=pd.Timedelta('1 hour'), \n",
    "    use_only_overall_endpoints_of_est_outg_times=False, \n",
    "    t_int_beg_col='starttimeperiod_local', \n",
    "    t_int_end_col='endtimeperiod_local', \n",
    "    value_col='value', \n",
    "    SN_col='serialnumber', \n",
    "    PN_col='aep_premise_nb', \n",
    "    return_all_best_ests=False, \n",
    "    return_all_best_ests_type='dict'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a DF containing AMI data together with outage begin and end times from DOVS, calculate the CI (customers interrupted) \n",
    "      and CMI (customer minutes interrupted).\n",
    "      \n",
    "    NOTE: The times from DOVS are used simply as time markers around which the algorithm searches for outages events.\n",
    "          The DOVS times aren't really necessary aside from setting this time search region (defined by both dovs_outg_t_beg_end\n",
    "            and expand_outg_search_time)\n",
    "          So, by no means is this a simple calculation using only DOVS.\n",
    "          Moreover, any beg/end time together with expand_outg_search_time could be used.\n",
    "          \n",
    "    NOTE: if ede_df is supplied, it should be for the same outage as that of df.\n",
    "      \n",
    "    Return value is tuple containing (CI, CMI)\n",
    "      UNLESS return_CI_SNs==True, in which case return (CI, CMI, outg_SNs)\n",
    "    \n",
    "    NOTE:  If one wants to calculate CI/CMI using PNs (premise numbers) instead of SNs (serial numbers), one must be careful\n",
    "           to first drop duplicate values (after dropping SNs col), or else the same answer will be found for PNs as SNs\n",
    "           \n",
    "    dovs_outg_t_beg_end:\n",
    "        Should be a list/tuple of length two containing the outage beginning and ending times from DOVS.\n",
    "        This, together with expand_outg_search_time sets the region around which the algorithm searches for outages.\n",
    "    \n",
    "    expand_outg_search_time:\n",
    "        Sets the region around each outage (either found using estimate_outage_times if use_est_outg_times==True, or taken\n",
    "          directly from dovs_outg_t_beg_end) to search for individual meter outages (using calculate_mi_for_meter).\n",
    "        If use_est_outg_times==True, probably want expand_outg_search_time tighter and expand_outg_est_search_time looser.\n",
    "        If use_est_outg_times==False, probably want expand_outg_search_time looser\n",
    "          \n",
    "    conservative_estimate:\n",
    "        Should likely have this set to True, otherwise one will certainly be underestimating the CI/CMI\n",
    "        If False, CI/CMI are calculated only using the periods with values of 0.\n",
    "          As documented elsewhere, it appears that AMI meters only report values of 0 if all readings in the 15-minute period are zero.\n",
    "          I believe that if at least one value is non-zero, the AMI will report a non-zero value.\n",
    "              e.g., if power out from 12:00 to 12:14, but turns on from 12:14 to 12:15, the 12:15 reading will be non-zero\n",
    "          Thus, simply calculating the period of time when the measurements are 0 inherently underestimates the actual outage time.\n",
    "        If True, a more conservative approach is used, in which the CI/CMI include the period preceding the first 0 value and that\n",
    "          following the last 0 value.\n",
    "          If applicable, the endpoint will be taken from DOVS (via dovs_outg_t_beg_end) instead of the full period preceding/following\n",
    "            as described above.\n",
    "          Find more information regarding this functionality in the dovs_outg_t_beg_end section of the calculate_mi_for_meter documentation.\n",
    "        \n",
    "    return_CI_SNs:\n",
    "        If True, also return outg_SNs.\n",
    "        This feature is mainly included for functionality where out_t_beg_end contains multiple outage times.\n",
    "        \n",
    "    use_est_outg_times:\n",
    "        If True, estimate the outage times by looking at the collection of all SNs in df\n",
    "          i.e., try to determine when outages occur by looking for periods of time where the meters show values of 0.\n",
    "        If a single outage is actually comprised of smaller suboutages, the suboutage times will be returned and used.\n",
    "        NOTE: If no estimate outages are found, dovs_outg_t_beg_end will be used (assuming it is not None)\n",
    "        -----\n",
    "        pct_SNs_required_for_outage_est:\n",
    "            The percentage of serial numbers which must show a reading of 0 for a time period to be considered an outage.\n",
    "              e.g., if pct_SNs_required_for_outage==0, all serial numbers must have a reading of 0\n",
    "              e.g., if pct_SNs_required_for_outage==25, 25% of the serial numbers must have a reading of zero.\n",
    "            NOTE: pct_SNs_required_for_outage_est=0 essentially means at least one SN must register a value of 0\n",
    "        \n",
    "        expand_outg_est_search_time=pd.Timedelta('1 hour'):\n",
    "            The interval around dovs_outg_t_beg_end to use when trying to estimate the outage times.\n",
    "            This should typically be looser than expand_outg_search_time.\n",
    "            \n",
    "        use_only_overall_endpoints_of_est_outg_times:\n",
    "            As stated above, if a single outage is actually comprised of smaller suboutages, the suboutage times \n",
    "              will be returned.\n",
    "            If use_only_overall_endpoints_of_est_outg_times==True, the beginning of the first suboutage and ending\n",
    "              of the last suboutage will only be used.\n",
    "            If use_only_overall_endpoints_of_est_outg_times==False, all suboutages will be used fully.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Probably easier/faster to supply calculate_mi_for_meter with freq, so infer here\n",
    "    freq = Utilities_df.determine_freq_in_df_col(\n",
    "        df=df, \n",
    "        groupby_SN=True, \n",
    "        return_val_if_not_found='error', \n",
    "        assert_single_freq=True, \n",
    "        assert_expected_freq_found=False, \n",
    "        expected_freq=pd.Timedelta('15 minutes'), \n",
    "        time_col=t_int_beg_col, \n",
    "        SN_col=SN_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    if use_est_outg_times:\n",
    "        # Find an estimate of the outage time using the AMI data\n",
    "        # NOTE: Below, conservative_estimate should always be True (not necessarily equal to input parameter)\n",
    "        est_outg_times = estimate_outage_times(\n",
    "            df = df, \n",
    "            t_search_min_max=[\n",
    "                dovs_outg_t_beg_end[0]-expand_outg_est_search_time, \n",
    "                dovs_outg_t_beg_end[1]+expand_outg_est_search_time\n",
    "            ], \n",
    "            pct_SNs_required_for_outage=pct_SNs_required_for_outage_est, \n",
    "            relax_pct_SNs_required_if_no_outgs_found=True, \n",
    "            t_int_beg_col=t_int_beg_col, \n",
    "            t_int_end_col=t_int_end_col, \n",
    "            value_col=value_col, \n",
    "            verbose=False,\n",
    "            outg_rec_nb_col=None\n",
    "        )\n",
    "        if len(est_outg_times)==0:\n",
    "            if dovs_outg_t_beg_end is not None:\n",
    "                outg_times = [dovs_outg_t_beg_end]\n",
    "            else:\n",
    "                outg_times = []\n",
    "        else:\n",
    "            if use_only_overall_endpoints_of_est_outg_times:\n",
    "                outg_times = [[est_outg_times[0]['cnsrvtv_t_beg'], est_outg_times[-1]['cnsrvtv_t_end']]]\n",
    "            else:\n",
    "                outg_times = [[x['cnsrvtv_t_beg'], x['cnsrvtv_t_end']] for x in est_outg_times]\n",
    "    else:\n",
    "        assert(dovs_outg_t_beg_end is not None)\n",
    "        outg_times = [dovs_outg_t_beg_end]\n",
    "    #-------------------------\n",
    "    # Get the search min/max times from the est. outage times and expand search time\n",
    "    t_search_min_max_coll = get_nonoverlapping_search_windows(\n",
    "        est_outg_times=outg_times, \n",
    "        expand_outg_search_time=expand_outg_search_time\n",
    "    )\n",
    "    assert(len(t_search_min_max_coll)==len(outg_times))\n",
    "    #-----\n",
    "    CMI=0\n",
    "    outg_SNs=[]\n",
    "    all_best_ests_dict = dict()\n",
    "    for t_search_min_max_i in t_search_min_max_coll:\n",
    "        df_outg_i=df \n",
    "        # Get the SNs and PNs for outg_i\n",
    "        SNs_PNs_i = df_outg_i[[SN_col, PN_col]].value_counts().index.tolist()\n",
    "        #-------------------------\n",
    "        for SN_ij, PN_ij in SNs_PNs_i:\n",
    "            df_outg_i_SN_j = df_outg_i[df_outg_i[SN_col]==SN_ij]\n",
    "            assert(df_outg_i_SN_j[PN_col].nunique()==1)\n",
    "            #-----\n",
    "            if ede_df is not None:\n",
    "                if SN_ij in ede_df[SN_col].tolist():\n",
    "                    ede_df_outg_i_SN_j = ede_df[ede_df[SN_col]==SN_ij]\n",
    "                else:\n",
    "                    ede_df_outg_i_SN_j=None\n",
    "            else:\n",
    "                ede_df_outg_i_SN_j=None\n",
    "            #-----\n",
    "            mi_ij = calculate_mi_for_meter_ami_w_ede_help(\n",
    "                df_i=df_outg_i_SN_j, \n",
    "                ede_df_i=ede_df_outg_i_SN_j, \n",
    "                t_search_min_max=t_search_min_max_i, \n",
    "                conservative_estimate=conservative_estimate, \n",
    "                dovs_outg_t_beg_end=dovs_outg_t_beg_end, \n",
    "                freq=freq, \n",
    "                est_ede_kwargs=est_ede_kwargs, \n",
    "                audit_selection_method=audit_selection_method, \n",
    "                t_int_beg_col=t_int_beg_col, \n",
    "                t_int_end_col=t_int_end_col, \n",
    "                value_col=value_col, \n",
    "                SN_col=SN_col, \n",
    "                return_all_best_ests=return_all_best_ests\n",
    "            )\n",
    "            #-------------------------\n",
    "            if return_all_best_ests:\n",
    "                assert(isinstance(mi_ij, tuple) and len(mi_ij)==2)\n",
    "                # NOTE: mi_ij must be grabbed from tuple second!\n",
    "                all_best_ests_ij = mi_ij[1]\n",
    "                mi_ij = mi_ij[0]\n",
    "                #-----\n",
    "                if (SN_ij, PN_ij) in all_best_ests_dict.keys():\n",
    "                    all_best_ests_dict[(SN_ij, PN_ij)].extend(all_best_ests_ij)\n",
    "                else:\n",
    "                    all_best_ests_dict[(SN_ij, PN_ij)] = all_best_ests_ij\n",
    "            #-------------------------                \n",
    "            CMI += mi_ij\n",
    "            if mi_ij>0:\n",
    "                outg_SNs.append(SN_ij) \n",
    "    #-------------------------\n",
    "    # If there are multiple sub-outage periods, likely repeat values in outg_SNs\n",
    "    # So need to perform set operation\n",
    "    outg_SNs = list(set(outg_SNs))\n",
    "    CI = len(outg_SNs)\n",
    "    #-------------------------\n",
    "    if not return_CI_SNs and not return_all_best_ests:\n",
    "        return CI, CMI\n",
    "    else:\n",
    "        return_dict = dict(\n",
    "            CI=CI, \n",
    "            CMI=CMI\n",
    "        )\n",
    "        if return_CI_SNs:\n",
    "            return_dict['CI_SNs'] = outg_SNs\n",
    "        if return_all_best_ests:\n",
    "            assert(return_all_best_ests_type in ['dict', 'pd.DataFrame'])\n",
    "            if return_all_best_ests_type=='dict':\n",
    "                return_dict['all_best_ests'] = all_best_ests_dict\n",
    "            else:\n",
    "                return_dict['all_best_ests'] = convert_all_best_ests_dict_to_df(all_best_ests_dict)\n",
    "        #-----\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9dd18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_est_outg_times_to_axis(\n",
    "    ax, \n",
    "    est_outg_times, \n",
    "    t_beg_kwargs=None,\n",
    "    t_end_kwargs=None,\n",
    "    include='both', \n",
    "    include_outage_limits_text=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    At vertical lines showing estimates outage times.\n",
    "    \n",
    "    Note: out_t_beg and out_t_end aren't really necessary, they simply direct which side of\n",
    "          the estimated lines to print the text\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(include in ['both', 'beg', 'end'])\n",
    "    #-------------------------\n",
    "    if t_beg_kwargs is None:\n",
    "        t_beg_kwargs={}\n",
    "    if t_end_kwargs is None:\n",
    "        t_end_kwargs={}\n",
    "    #-----\n",
    "    dflt_t_beg_kwargs=dict(color='red', linestyle='--')\n",
    "    dflt_t_end_kwargs=dict(color='green', linestyle='--')\n",
    "    #-----\n",
    "    t_beg_kwargs = Utilities.supplement_dict_with_default_values(to_supplmnt_dict=t_beg_kwargs, default_values_dict=dflt_t_beg_kwargs, inplace=False)\n",
    "    t_end_kwargs = Utilities.supplement_dict_with_default_values(to_supplmnt_dict=t_end_kwargs, default_values_dict=dflt_t_end_kwargs, inplace=False)\n",
    "    #-------------------------\n",
    "    for dt_off_i, dt_on_i in est_outg_times:\n",
    "        if include=='both' or include=='beg':\n",
    "            ax.axvline(dt_off_i, **t_beg_kwargs)\n",
    "        if include=='both' or include=='end':\n",
    "            ax.axvline(dt_on_i, **t_end_kwargs)\n",
    "        #-----\n",
    "        if include_outage_limits_text:\n",
    "            ax.text(dt_off_i, ax.get_ylim()[0], 'Est. outg. beg', \n",
    "                    rotation=90, verticalalignment='bottom', \n",
    "                    horizontalalignment='right' if dt_off_i<out_t_beg else 'left')\n",
    "            ax.text(dt_on_i, ax.get_ylim()[0], 'Est. outg. end', \n",
    "                    rotation=90, verticalalignment='bottom', \n",
    "                    horizontalalignment='left' if dt_on_i>out_t_end else 'right')\n",
    "    #-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdbf8c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_best_est_to_axis(\n",
    "    ax, \n",
    "    est_val_beg,\n",
    "    est_val_end,\n",
    "    line_kwargs, \n",
    "    expand_ax_to_accommodate=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Mainly a helper function for add_all_best_ests_to_axis, but can certainly be used on its own\n",
    "    \n",
    "    NOTE:\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Set color_beg equal to line_kwargs['color_beg'] if exists (and remove), else set equal to line_kwargs['color']\n",
    "    # Similar for color_end\n",
    "    color_beg = line_kwargs.pop('color_beg', line_kwargs.get('color', 'black'))\n",
    "    color_end = line_kwargs.pop('color_end', line_kwargs.get('color', 'black'))\n",
    "    #-----\n",
    "    # Same idea for linestyles as used above for colors\n",
    "    linestyle_beg = line_kwargs.pop('linestyle_beg', line_kwargs.get('linestyle', '-'))\n",
    "    linestyle_end = line_kwargs.pop('linestyle_end', line_kwargs.get('linestyle', '-'))\n",
    "\n",
    "    # Need to strip off 'color' since explicitly setting via beg(end)_color\n",
    "    line_kwargs.pop('color', None)\n",
    "    line_kwargs.pop('linestyle', None)\n",
    "    # Note: Below, pd.isna() catches values of None, NaN, and NaT\n",
    "    if not pd.isna(est_val_beg):\n",
    "        if (\n",
    "            not expand_ax_to_accommodate and \n",
    "            (mpl.dates.date2num(est_val_beg)<ax.get_xlim()[0] or mpl.dates.date2num(est_val_beg)>ax.get_xlim()[1])\n",
    "        ):\n",
    "            pass\n",
    "        else:\n",
    "            ax.axvline(est_val_beg, color=color_beg, linestyle=linestyle_beg, **line_kwargs)\n",
    "    if not pd.isna(est_val_end):\n",
    "        if (\n",
    "            not expand_ax_to_accommodate and \n",
    "            (mpl.dates.date2num(est_val_end)<ax.get_xlim()[0] or mpl.dates.date2num(est_val_end)>ax.get_xlim()[1])\n",
    "        ):\n",
    "            pass\n",
    "        else:\n",
    "            ax.axvline(est_val_end, color=color_end, linestyle=linestyle_end, **line_kwargs)\n",
    "\n",
    "def add_all_best_ests_to_axis(\n",
    "    ax, \n",
    "    all_best_ests, \n",
    "    line_kwargs_by_est_key=None, \n",
    "    keys_to_include=['winner', 'conservative', 'zero_times'], \n",
    "    expand_ax_to_accommodate=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    At vertical lines showing estimates outage times.\n",
    "    NOTE: This will also work with means_df from get_mean_times_w_dbscan\n",
    "    -----\n",
    "    all_best_ests:\n",
    "        This should either be a list or a pd.DataFrame object containing estimated values.\n",
    "        If a list, each element should be a dictionary with the keys_to_include/expctd_keys (defined in code below).\n",
    "        If a pd.DataFrame, the columns should contain f'{x}_min', f'{x}_max' for x in keys_to_include/expctd_keys\n",
    "        -----\n",
    "        At the time of writing, one could use the results from calculate_ci_cmi_w_ami_w_ede_help in the following ways:\n",
    "        If return_all_best_ests_type=='dict':\n",
    "            all_best_ests = list(itertools.chain.from_iterable(list(results['all_best_ests'].values())))\n",
    "        If return_all_best_ests_type=='pd.DataFrame':\n",
    "            all_best_ests = results['all_best_ests']\n",
    "    \n",
    "    keys_to_include:\n",
    "        See expctd_keys in code below for list of acceptable keys\n",
    "        Set equal to 'all' to include all keys.\n",
    "        Otherwise, set equal to a single key or list of keys\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if all_best_ests is None:\n",
    "        return\n",
    "    #-------------------------\n",
    "    assert(Utilities.is_object_one_of_types(all_best_ests, [list, tuple, pd.DataFrame]))\n",
    "    #-------------------------\n",
    "    expctd_keys = ['conservative', 'zero_times', 'winner', 'ede', 'dovs']\n",
    "    #-------------------------\n",
    "    dflt_line_kwargs = dict(\n",
    "        conservative=dict(color_beg='red', color_end='green', linestyle=':'),\n",
    "        zero_times=dict(color_beg='red', color_end='green', linestyle=':'),\n",
    "#         winner=dict(color='goldenrod', linestyle='--'),\n",
    "        winner=dict(color_beg='red', color_end='green', linestyle='--'),\n",
    "        ede=dict(color_beg='red', color_end='green', linestyle='-.'),\n",
    "        dovs=dict(color_beg='red', color_end='green', linestyle='-'),\n",
    "\n",
    "    )\n",
    "    #-------------------------\n",
    "    if keys_to_include=='all':\n",
    "        keys_to_include=expctd_keys\n",
    "    assert(Utilities.is_object_one_of_types(keys_to_include, [str, list, tuple]))\n",
    "    if isinstance(keys_to_include, str):\n",
    "        keys_to_include = [keys_to_include]\n",
    "    assert(len(set(keys_to_include).difference(set(expctd_keys)))==0)\n",
    "    #-------------------------\n",
    "    if line_kwargs_by_est_key is None:\n",
    "        line_kwargs_by_est_key=dflt_line_kwargs\n",
    "    else:\n",
    "        line_kwargs_by_est_key = Utilities.supplement_dict_with_default_values(\n",
    "            to_supplmnt_dict=line_kwargs_by_est_key, \n",
    "            default_values_dict=dflt_line_kwargs, \n",
    "            extend_any_lists=True, \n",
    "            inplace=False\n",
    "        )\n",
    "    #-------------------------\n",
    "    if Utilities.is_object_one_of_types(all_best_ests, [list, tuple]):\n",
    "        for all_best_ests_i in all_best_ests:\n",
    "            for k,v in all_best_ests_i.items():\n",
    "                assert(k in expctd_keys)\n",
    "                if k not in keys_to_include:\n",
    "                    continue\n",
    "                beg_ik, end_ik = v[0], v[1]\n",
    "                line_kwargs_k = copy.deepcopy(line_kwargs_by_est_key[k])\n",
    "                #-----\n",
    "                add_best_est_to_axis(\n",
    "                    ax=ax, \n",
    "                    est_val_beg=beg_ik,\n",
    "                    est_val_end=end_ik,\n",
    "                    line_kwargs=line_kwargs_k, \n",
    "                    expand_ax_to_accommodate=expand_ax_to_accommodate\n",
    "                )\n",
    "    else:\n",
    "        assert(isinstance(all_best_ests, pd.DataFrame))\n",
    "        #-----\n",
    "        cols_to_include = [[f'{x}_min', f'{x}_max'] for x in keys_to_include]\n",
    "        cols_to_include = list(itertools.chain.from_iterable(cols_to_include))\n",
    "        assert(len(set(cols_to_include).difference(all_best_ests.columns.tolist()))==0)\n",
    "        #-----\n",
    "        for idx_i, row_i in all_best_ests.iterrows():\n",
    "            for k in keys_to_include:\n",
    "                beg_ik, end_ik = row_i[[f\"{k}_min\", f\"{k}_max\"]]\n",
    "                line_kwargs_k = copy.deepcopy(line_kwargs_by_est_key[k])\n",
    "                #-----\n",
    "                add_best_est_to_axis(\n",
    "                    ax=ax, \n",
    "                    est_val_beg=beg_ik,\n",
    "                    est_val_end=end_ik,\n",
    "                    line_kwargs=line_kwargs_k, \n",
    "                    expand_ax_to_accommodate=expand_ax_to_accommodate\n",
    "                )\n",
    "                \n",
    "                \n",
    "# SHOULD NOT BE NEEDED ANYMORE BECAUSE add_all_best_ests_to_axis can be used instead!                \n",
    "# def add_mean_best_ests_to_axis(\n",
    "#     ax, \n",
    "#     means_df, \n",
    "#     line_kwargs_by_est_key=None, \n",
    "#     keys_to_include=['winner', 'conservative', 'zero_times'], \n",
    "#     expand_ax_to_accommodate=True\n",
    "# ):\n",
    "#     r\"\"\"\n",
    "#     At vertical lines showing estimates outage times.\n",
    "#     Note: all_best_ests_list will contain more than one element if multiple suboutages\n",
    "#     Each element in all_best_ests_list should be a dictionary with the expctd_keys\n",
    "    \n",
    "#     keys_to_include:\n",
    "#         See expctd_keys in code below for list of acceptable keys\n",
    "#         Set equal to 'all' to include all keys.\n",
    "#         Otherwise, set equal to a single key or list of keys\n",
    "#     \"\"\"\n",
    "#     #-------------------------\n",
    "#     expctd_keys = ['conservative', 'zero_times', 'winner', 'ede', 'dovs']\n",
    "#     #-------------------------\n",
    "#     dflt_line_kwargs = dict(\n",
    "#         conservative=dict(color_beg='red', color_end='green', linestyle=':'),\n",
    "#         zero_times=dict(color_beg='red', color_end='green', linestyle=':'),\n",
    "# #         winner=dict(color='goldenrod', linestyle='--'),\n",
    "#         winner=dict(color_beg='red', color_end='green', linestyle='--'),\n",
    "#         ede=dict(color_beg='red', color_end='green', linestyle='-.'),\n",
    "#         dovs=dict(color_beg='red', color_end='green', linestyle='-'),\n",
    "\n",
    "#     )\n",
    "#     #-------------------------\n",
    "#     if keys_to_include=='all':\n",
    "#         keys_to_include=expctd_keys\n",
    "#     assert(Utilities.is_object_one_of_types(keys_to_include, [str, list, tuple]))\n",
    "#     if isinstance(keys_to_include, str):\n",
    "#         keys_to_include = [keys_to_include]\n",
    "#     assert(len(set(keys_to_include).difference(set(expctd_keys)))==0)\n",
    "#     #-------------------------\n",
    "#     if line_kwargs_by_est_key is None:\n",
    "#         line_kwargs_by_est_key=dflt_line_kwargs\n",
    "#     else:\n",
    "#         line_kwargs_by_est_key = Utilities.supplement_dict_with_default_values(\n",
    "#             to_supplmnt_dict=line_kwargs_by_est_key, \n",
    "#             default_values_dict=dflt_line_kwargs, \n",
    "#             extend_any_lists=True, \n",
    "#             inplace=False\n",
    "#         )\n",
    "#     #-------------------------\n",
    "#     for key in keys_to_include:\n",
    "#         assert(f\"{key}_min\" in means_df.columns.tolist())\n",
    "#         assert(f\"{key}_max\" in means_df.columns.tolist())    \n",
    "#     #-------------------------\n",
    "#     for idx, row in means_df.iterrows():\n",
    "#     #     if str(idx).startswith('Unclustered'):\n",
    "#     #         # Unclustered specific code:\n",
    "#         for key in keys_to_include:\n",
    "#             beg_ik, end_ik = row[[f\"{key}_min\", f\"{key}_max\"]]\n",
    "#             line_kwargs_k = copy.deepcopy(line_kwargs_by_est_key[key])\n",
    "#             #-----\n",
    "#             add_best_est_to_axis(\n",
    "#                 ax=ax, \n",
    "#                 est_val_beg=beg_ik,\n",
    "#                 est_val_end=end_ik,\n",
    "#                 line_kwargs=line_kwargs_k, \n",
    "#                 expand_ax_to_accommodate=expand_ax_to_accommodate\n",
    "#             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "309565d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_mean_times_w_dbscan(\n",
    "    best_ests_df, \n",
    "    eps_min=5, \n",
    "    min_samples=2, \n",
    "    ests_to_include_in_clustering=['winner_min', 'winner_max'],\n",
    "    ests_to_include_in_output=[\n",
    "        'winner_min', 'winner_max', \n",
    "        'conservative_min', 'conservative_max', \n",
    "        'zero_times_min', 'zero_times_max'\n",
    "    ], \n",
    "    return_labelled_best_ests_df=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # For this procedure to work, they cannot be and NaNs in ests_to_include_in_clustering.\n",
    "    # Therefore, it may be best to use winner, as there will always be winner values\n",
    "    if best_ests_df.dropna(subset=ests_to_include_in_clustering).shape[0]==0:\n",
    "        print(\"All rows contain at least one NaN in ests_to_include_in_clustering!\")\n",
    "        print(\"Resetting ests_to_include_in_clustering to only includer winners\")\n",
    "        ests_to_include_in_clustering=['winner_min', 'winner_max']\n",
    "    #-----\n",
    "    best_ests_df = best_ests_df.dropna(subset=ests_to_include_in_clustering)\n",
    "    #-------------------------\n",
    "    # Although it is apparently possible to feed dbscan the raw Datetime objects, it appears that they are essentially\n",
    "    #   converted to timestamps, meaning they are converted to numbers on the order of 1.6e9, which makes them somewhat\n",
    "    #   difficult to deal with (e.g., in setting the eps parameter for dbscan)\n",
    "    # Therefore, for my purposes it is preferable to convert these to smaller numbers, such as the difference between the row's\n",
    "    #   datetime and the approximate beginning of the outage (NOTE, the comparison value doesn't matter too much, we just want\n",
    "    #   something in the ballpark so the differences are not too big).\n",
    "    # I will express these differences in terms of minutes, so the eps value should be set in terms of minutes\n",
    "    comp_val = best_ests_df[ests_to_include_in_clustering[0]].min()\n",
    "    #-----\n",
    "    db_cols = []\n",
    "    for clust_col in ests_to_include_in_clustering:\n",
    "        best_ests_df[f'{clust_col}_db'] = (best_ests_df[clust_col]-comp_val).dt.total_seconds()/60\n",
    "        db_cols.append(f'{clust_col}_db')\n",
    "    #-------------------------    \n",
    "    db = DBSCAN(eps=eps_min, min_samples=min_samples).fit(best_ests_df[db_cols])\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    #-------------------------\n",
    "    # Set the label values in best_ests_df\n",
    "    best_ests_df['db_label'] = labels\n",
    "    #-------------------------\n",
    "    means_df = best_ests_df.groupby(['db_label'])[ests_to_include_in_output].mean()\n",
    "    if n_noise_>0:\n",
    "        # Unclustered data will be handled separately, and not aggregated.  So remove from means_df\n",
    "        means_df=means_df.drop(index=[-1])\n",
    "        #-----\n",
    "        unclstrd_df = best_ests_df[best_ests_df['db_label']==-1][ests_to_include_in_output].copy()\n",
    "        unclstrd_df['db_label'] = [f\"Unclustered {i}\" for i in range(unclstrd_df.shape[0])]\n",
    "        unclstrd_df = unclstrd_df.set_index('db_label')\n",
    "        #-----\n",
    "        # Want to set the correct db_label value for these in best_ests_df in case return_labelled_best_ests_df\n",
    "        best_ests_df.loc[best_ests_df['db_label']==-1, 'db_label'] = [f\"Unclustered {i}\" for i in range(best_ests_df[best_ests_df['db_label']==-1].shape[0])]\n",
    "        #-----\n",
    "        means_df = pd.concat([means_df, unclstrd_df])\n",
    "    #-------------------------\n",
    "    if return_labelled_best_ests_df:\n",
    "        return means_df, best_ests_df.drop(columns=db_cols)\n",
    "    else:\n",
    "        return means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c8051d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nPNs_to_means_df(\n",
    "    means_df, \n",
    "    best_ests_df_w_db_lbl,\n",
    "    db_label_col='db_label', \n",
    "    PN_col='PN', \n",
    "    n_PNs_col = 'n_PNs'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Add the number of premise numbers per db_label group to means_df\n",
    "    NOTE: The index of means_df should be the db_labels (db_label_col is to locate the labels\n",
    "            within best_ests_df_w_db_lbl)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    n_PNs_dict = {}\n",
    "    for db_label_i, row_i in means_df.iterrows():\n",
    "        n_PNs_i = best_ests_df_w_db_lbl[best_ests_df_w_db_lbl[db_label_col]==db_label_i][PN_col].nunique()\n",
    "        n_PNs_dict[db_label_i] = n_PNs_i\n",
    "    #-----\n",
    "    n_PNs_srs = pd.Series(n_PNs_dict)\n",
    "    n_PNs_srs.name = n_PNs_col\n",
    "    #-------------------------\n",
    "    means_df = pd.merge(means_df, n_PNs_srs, left_index=True, right_index=True, how='inner')\n",
    "    return means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09a8b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_PNs_in_best_ests_df_i(\n",
    "    best_ests_df_for_PN_i, \n",
    "    min_cols, \n",
    "    max_cols, \n",
    "    return_col_order, \n",
    "    likeness_thresh = pd.Timedelta('1 minutes'), \n",
    "    SN_col = 'SN', \n",
    "    PN_col = 'PN', \n",
    "    i_outg_col = 'i_outg' \n",
    "):\n",
    "    r\"\"\"\n",
    "    !!!!! THIS IS A HELPER FUNCTION FOR combine_PNs_in_best_ests_df. !!!!!\n",
    "    !!!!! IT IS NOT RECOMMENDED TO USE THIS FUNCTION OUTSIDE OF combine_PNs_in_best_ests_df !!!!!\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if best_ests_df_for_PN_i.shape[0]==1:\n",
    "        return best_ests_df_for_PN_i[return_col_order].squeeze()\n",
    "    #-------------------------\n",
    "    # best_ests_df_for_PN_i should come from best_ests_df being grouped by PN_col and i_outg_col\n",
    "    # Therefore, there should be a single unique value for these!\n",
    "    #   NOTE: Could also do nunique for each column sparately instead of using value_counts\n",
    "    assert(best_ests_df_for_PN_i[[PN_col, i_outg_col]].value_counts().shape[0]==1)\n",
    "    #-------------------------\n",
    "    ranges = best_ests_df_for_PN_i[min_cols+max_cols].apply(lambda x: x.dropna().max()-x.dropna().min())\n",
    "    #-----\n",
    "    # NOTE: NaN values are possible if all values are None for given column\n",
    "    if (ranges>likeness_thresh).any():\n",
    "        print('Outage estimates for serial numbers in given premise are not similar!')\n",
    "        print('CRASH IMMINENT')\n",
    "        print(f\"PN = {best_ests_df_for_PN_i[PN_col].unique()[0]}\")\n",
    "        print(f\"threshold = {likeness_thresh}\")\n",
    "        print(f\"Violators:\\n{ranges[ranges>likeness_thresh]}\")\n",
    "    assert(((ranges.isna()) | (ranges<=likeness_thresh)).all())\n",
    "    #-------------------------\n",
    "    return_srs = best_ests_df_for_PN_i[min_cols+max_cols].agg({x:'min' for x in min_cols}|{x:'max' for x in max_cols})\n",
    "    return_srs = return_srs[return_col_order]\n",
    "    #-------------------------\n",
    "    return return_srs\n",
    "\n",
    "\n",
    "def combine_PNs_in_best_ests_df(\n",
    "    best_ests_df, \n",
    "    likeness_thresh = pd.Timedelta('1 minutes'), \n",
    "    SN_col = 'SN', \n",
    "    PN_col = 'PN', \n",
    "    i_outg_col = 'i_outg'     \n",
    "):\n",
    "    r\"\"\"\n",
    "    Combine all serial numbers (SNs) for each premise number (PN) in best_ests_df.\n",
    "    The purpose is so that CI/CMI may be calculated at the premise level, instead of meter level.\n",
    "    When there are multiple meters for a given premise:\n",
    "        1. If there are multiple sub-outages, it is checked to ensure each sub-outage contains the same\n",
    "             set of serial numbers, see EXPLANATION OF STACKED GROUPBY OPERATIONS below for more info.\n",
    "        2. For each sub-outage, it is ensured that the time estimates from all meters is roughly equal,\n",
    "             as defined by the likeness_thresh parameter.\n",
    "           e.g., winner_min should be roughly the same for all meters on a premise, as should winner_max, etc.\n",
    "        3. For each premise and sub-outage:\n",
    "               for all minimum time estimates, the minimum value amongst the meters on a premise is kept.\n",
    "               for all maximum time estimates, the maximum value amongst the meters on a premise is kept.\n",
    "           This maintains our strategy of reporting the most conservative value for the outage time, although,\n",
    "             due to the tightness of likeness_thresh, this doesn't make a huge difference.\n",
    "    \n",
    "    best_ests_df:\n",
    "        Should contain SN_col, PN_col, i_outg_col and various min/max time estimate columns.\n",
    "        In a typical situation, best_ests_df will have the following columns:\n",
    "            ['SN', 'PN', 'i_outg',\n",
    "             'ede_min', 'ede_max',\n",
    "             'dovs_min', 'dovs_max',\n",
    "             'conservative_min', 'conservative_max',\n",
    "             'zero_times_min', 'zero_times_max',\n",
    "             'winner_min', 'winner_max']\n",
    "        NOTE: minimum columns must end with '_min' and maxima with '_max'\n",
    "              Also, best_ests_df should only contain 'SN', 'PN', 'i_outg' and min/max columns\n",
    "    \n",
    "    likeness_thresh:\n",
    "        Sets the maximum allowed difference between time estimates for meters of a given premise.\n",
    "        All SNs on a given PN should lose and regain power at the same time, although slight differences exist (typically\n",
    "          only a few seconds difference, and likely due to delays in meters registering events)\n",
    "          \n",
    "    --------------------------------------------------\n",
    "    EXPLANATION OF STACKED GROUPBY OPERATIONS\n",
    "    --------------------------------------------------\n",
    "    The purpose of the multiple groupby operations is to ensure that, when multiple sub-outages exist for a premise (i.e., \n",
    "      when i_outg=0, 1, ...), all sub-outages share the same set of meters.\n",
    "    One would not expect, e.g., the first sub-outage to only affect one meter on a given premise and the second sub-outage\n",
    "      to affect multiple meters on the premise.\n",
    "    The full chain of groupby commands I am referencing is:\n",
    "    \n",
    "        (best_ests_df\n",
    "         .groupby([PN_col, i_outg_col])[SN_col].apply(list)\n",
    "         .groupby([PN_col]).apply(list)\n",
    "         .apply(lambda x: Utilities.are_all_lists_eq(x))\n",
    "        )  \n",
    "        \n",
    "    Suppose best_ests_df has the following form:\n",
    "    \n",
    "              SN    PN  i_outg\n",
    "        0  SN_11  PN_1       0\n",
    "        1  SN_11  PN_1       1\n",
    "        2  SN_12  PN_1       0\n",
    "        3  SN_12  PN_1       1\n",
    "        4  SN_21  PN_2       0\n",
    "        5  SN_21  PN_2       1\n",
    "        6  SN_22  PN_2       0\n",
    "        7  SN_22  PN_2       1\n",
    "        \n",
    "    In this example, there are two premises (PN_1 and PN_2), each having two meters (SN_11, SN_12 associated with PN_1 \n",
    "      and SN_21, SN_22 associated with PN_2), and each having an outage split into two sub-outages (i_outg 0 and 1).\n",
    "    The output for the steps of the chain of groupby commands is as follows:\n",
    "    -------------------------\n",
    "    1. Get a list of the meters (SNs) associated with each premise and sub-outage\n",
    "        ----------\n",
    "        (best_ests_df\n",
    "         .groupby([PN_col, i_outg_col])[SN_col].apply(list)\n",
    "        )\n",
    "        ----------\n",
    "        PN    i_outg\n",
    "        PN_1  0         [SN_11, SN_12]\n",
    "              1         [SN_11, SN_12]\n",
    "        PN_2  0         [SN_21, SN_22]\n",
    "              1         [SN_21, SN_22]        \n",
    "    -------------------------\n",
    "    2. Combine these into a list of lists for each premise\n",
    "        ----------\n",
    "        (best_ests_df\n",
    "         .groupby([PN_col, i_outg_col])[SN_col].apply(list)\n",
    "         .groupby([PN_col]).apply(list)\n",
    "        )\n",
    "        ----------\n",
    "        PN\n",
    "        PN_1    [[SN_11, SN_12], [SN_11, SN_12]]\n",
    "        PN_2    [[SN_21, SN_22], [SN_21, SN_22]]        \n",
    "    -------------------------\n",
    "    3. Use Utilities.are_all_lists_eq function to ensure the collection of meters for each premise is consistent\n",
    "         for all sub-outages\n",
    "        ----------\n",
    "        (best_ests_df\n",
    "         .groupby([PN_col, i_outg_col])[SN_col].apply(list)\n",
    "         .groupby([PN_col]).apply(list)\n",
    "         .apply(lambda x: Utilities.are_all_lists_eq(x))\n",
    "        )\n",
    "        ----------\n",
    "        PN\n",
    "        PN_1    True\n",
    "        PN_2    True\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure SN_col, PN_col, and i_outg_col all in best_ests_df\n",
    "    assert(len(set([SN_col, PN_col, i_outg_col]).difference(best_ests_df.columns.tolist()))==0)\n",
    "    \n",
    "    # Determine which cols are mins and which are maxs\n",
    "    # NOTE: The method used for finding minmax_cols together with the assertion below assures that \n",
    "    #       best_ests_df contains only SN_col, PN_col, i_outg_col and min/max cols\n",
    "    minmax_cols = [x for x in best_ests_df.columns.tolist() if x not in [SN_col, PN_col, i_outg_col]]\n",
    "    min_cols = [x for x in best_ests_df.columns.tolist() if x.endswith('_min')]\n",
    "    max_cols = [x for x in best_ests_df.columns.tolist() if x.endswith('_max')]\n",
    "    assert(len(set(minmax_cols).symmetric_difference(set(min_cols+max_cols)))==0)\n",
    "    assert(len(min_cols)==len(max_cols))\n",
    "    \n",
    "    # Maintain the original order of columns in best_ests_df in the returned df\n",
    "    return_col_order = [x for x in best_ests_df.columns.tolist() if x in min_cols+max_cols]\n",
    "    \n",
    "    #-------------------------\n",
    "    # All columns in minmax_cols must be datetime type\n",
    "    # Below ensures all min/max columns are datetime\n",
    "    for col_i in min_cols+max_cols:\n",
    "        dtype_i = best_ests_df[col_i].dtype\n",
    "        if not (dtype_i is datetime.datetime or is_datetime64_dtype(dtype_i)):\n",
    "            best_ests_df = Utilities_df.convert_col_type_w_pd_to_datetime(\n",
    "                df=best_ests_df, \n",
    "                column=col_i, \n",
    "                inplace=True\n",
    "            )\n",
    "    \n",
    "    #-------------------------\n",
    "    # Really, the reduction operations below only need to be performed on PNs with more than one SN\n",
    "    # To save resources and time, split apart those needing reduced and those not\n",
    "    SNs_per_PN = best_ests_df[[SN_col, PN_col]].drop_duplicates()[PN_col].value_counts()\n",
    "    PNs_w_mult_SNs = SNs_per_PN[SNs_per_PN>1].index.tolist()\n",
    "    \n",
    "    # If there are no PNs with multiple SNs, return best_ests_df (without SN_col, as the purpose of this\n",
    "    #   is to eliminate SN_col by grouping by premise)\n",
    "    if len(PNs_w_mult_SNs)==0:\n",
    "        return best_ests_df.drop(columns=[SN_col]).sort_values(by=[PN_col, i_outg_col], ignore_index=True)\n",
    "    \n",
    "    best_ests_df_w_mult  = best_ests_df[best_ests_df[PN_col].isin(PNs_w_mult_SNs)]\n",
    "    best_ests_df_wo_mult = best_ests_df[~best_ests_df[PN_col].isin(PNs_w_mult_SNs)]\n",
    "    assert(best_ests_df_w_mult.shape[0]+best_ests_df_wo_mult.shape[0]==best_ests_df.shape[0])\n",
    "    \n",
    "    #-------------------------\n",
    "    # Ensure that, when multiple sub-outages exist for a premise (i.e., when i_outg=0, 1, ...), \n",
    "    #   all sub-outages share the same set of meters.\n",
    "    # See function documentation above for explanation of operations\n",
    "    assert(\n",
    "        (best_ests_df_w_mult\n",
    "         .groupby([PN_col, i_outg_col])[SN_col].apply(list)\n",
    "         .groupby([PN_col]).apply(list)\n",
    "         .apply(lambda x: Utilities.are_all_lists_eq(x))\n",
    "        ).all()\n",
    "    )\n",
    "    \n",
    "    #-------------------------\n",
    "    # Perform reduction\n",
    "    #   For all minimum time estimates, the minimum value amongst the meters on a premise is kept.\n",
    "    #   For all maximum time estimates, the maximum value amongst the meters on a premise is kept.\n",
    "    return_df = best_ests_df_w_mult.groupby([PN_col, i_outg_col], as_index=False, group_keys=False).apply(\n",
    "        lambda x: combine_PNs_in_best_ests_df_i(\n",
    "            best_ests_df_for_PN_i=x, \n",
    "            min_cols=min_cols, \n",
    "            max_cols=max_cols, \n",
    "            return_col_order=return_col_order, \n",
    "            likeness_thresh=likeness_thresh, \n",
    "            SN_col=SN_col, \n",
    "            PN_col=PN_col, \n",
    "            i_outg_col=i_outg_col\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #-------------------------\n",
    "    # Combine together with collection of PNs having a single SN\n",
    "    # NOTE: Since we are grouping by PN_col, i_outg_col and aggregating the min/max cols,\n",
    "    #       the SN_col should not be present in the final output\n",
    "    best_ests_df_wo_mult = best_ests_df_wo_mult.drop(columns=[SN_col])\n",
    "    assert(return_df.columns.tolist()==best_ests_df_wo_mult.columns.tolist())\n",
    "    return_df = pd.concat([return_df, best_ests_df_wo_mult])\n",
    "    \n",
    "    #-------------------------\n",
    "    # Sometimes it seems the operation can change the type of data in min/max cols\n",
    "    #   e.g., I have seen epoch times returned, instead of datetime object\n",
    "    #     (e.g., 1673095676000000000, which is actually 2023-01-07 12:47:56)\n",
    "    # This was occurred in the older version, prior to splitting best_ests_df_w_mult and \n",
    "    #   best_ests_df_wo_mult, but there's no harm in keeping this check in place\n",
    "    # Below ensures all min/max columns are datetime\n",
    "    for col_i in min_cols+max_cols:\n",
    "        dtype_i = return_df[col_i].dtype\n",
    "        if not (dtype_i is datetime.datetime or is_datetime64_dtype(dtype_i)):\n",
    "            return_df = Utilities_df.convert_col_type_w_pd_to_datetime(\n",
    "                df=return_df, \n",
    "                column=col_i, \n",
    "                inplace=True\n",
    "            )\n",
    "    \n",
    "    #-------------------------\n",
    "    return_df = return_df.sort_values(by=[PN_col, i_outg_col], ignore_index=True)\n",
    "    \n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0841faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_best_ests_df_using_dovs_outg_t_beg(\n",
    "    best_ests_df,\n",
    "    dovs_df, \n",
    "    outg_rec_nb, \n",
    "    outg_rec_nb_idfr='index', \n",
    "    dt_off_ts_full_col='DT_OFF_TS_FULL', \n",
    "    winner_min_col='winner_min', \n",
    "    winner_max_col='winner_max', \n",
    "    i_outg_col='i_outg'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Alter best_ests_df to use the outage beginning time as given by DOVS and the ending times as estimated using AMI.\n",
    "    To summarize:\n",
    "        Take dovs_outg_t_beg to be the outage starting time as given by DOVS.\n",
    "        Any sub-outages which end before dovs_outg_t_beg will be completely removed.\n",
    "        Any sub-outages which begin after dovs_outg_t_beg will be left alone.\n",
    "        Sub-outages with beginning times before dovs_outg_t_beg and ending times after dovs_outg_t_beg will\n",
    "          have their beginning times altered to equal dovs_outg_t_beg\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # First, get the needed info from DOVS\n",
    "    dovs_df_i = DOVSOutages.retrieve_outage_from_dovs_df(\n",
    "        dovs_df=dovs_df, \n",
    "        outg_rec_nb=outg_rec_nb, \n",
    "        outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "        assert_outg_rec_nb_found=True\n",
    "    )\n",
    "    assert(dovs_df_i.shape[0]==1)\n",
    "    # Get the outage time from DOVS\n",
    "    dovs_outg_t_beg = dovs_df_i.iloc[0][dt_off_ts_full_col]\n",
    "    #-------------------------\n",
    "    # Since we are using the DOVS beginning time, we only consider sub-outages that end after this time\n",
    "    #   Put differently, get rid of any sub-outages which end before the posted DOVS beginning time\n",
    "    return_df = best_ests_df[best_ests_df[winner_max_col]>dovs_outg_t_beg].copy()\n",
    "    #-------------------------\n",
    "    # Make sure return_df is sorted properly, and reset i_outg\n",
    "    return_df=return_df.sort_values(by=[winner_min_col, winner_max_col], ignore_index=True)\n",
    "    return_df[i_outg_col]=return_df.index\n",
    "    #-------------------------\n",
    "    # Any sub-outages which begin after dovs_outg_t_beg should be left alone\n",
    "    #   Only those with starting times before dovs_outg_t_beg should be altered\n",
    "    return_df.loc[return_df[winner_min_col]<dovs_outg_t_beg, winner_min_col] = dovs_outg_t_beg\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8101ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e178a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do I need to update this function to use t_int_beg_col, t_int_end_col instead of time_col?\n",
    "def group_ami_df_by_PN(\n",
    "    ami_df, \n",
    "    SN_col='serialnumber', \n",
    "    PN_col='aep_premise_nb', \n",
    "    time_col='starttimeperiod_local', \n",
    "    value_col='value', \n",
    "    include_index_in_shared_cols=True, \n",
    "    gpby_dropna=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Relatively simple-minded method to group ami_df by premise number (i.e., instead of the possibility of\n",
    "      multiple entries for a premise at a given datetime corresponding the multiple meters on the premise, the\n",
    "      DF will be collapsed down to a single value for the premise).\n",
    "    The DF will be grouped by PN_col and time_col.\n",
    "    All columns outside of SN_col, PN_col, time_col, and value_col must have a single value per group (i.e., per\n",
    "      unique combination of PN_col and time_col).\n",
    "    The value_col will be averaged.\n",
    "    The SN_col will be dropped\n",
    "    \n",
    "    include_index_in_shared_cols:\n",
    "        As various elements will be grouped/rows collapsed/however you want to think about it, the index will\n",
    "          necessarily be lost, \n",
    "        In many cases (e.g., when the indices are time stamps) the indices for those being combined should be \n",
    "          shared (but, pandas doesn't know that).\n",
    "        If this is the case, set include_index_in_shared_cols==True\n",
    "        NOTE: For the foreseeable applications, this only really makes sense if the indices are datetime objects,\n",
    "              so this will be enforced.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure all of the needed columns are present\n",
    "    assert(len(set([SN_col, PN_col, time_col, value_col]).difference(set(ami_df.columns.tolist())))==0)\n",
    "    \n",
    "    #-------------------------\n",
    "    grp_by_cols = [PN_col, time_col]\n",
    "    cols_shared_by_groups = [x for x in ami_df.columns.tolist() \n",
    "                             if x not in grp_by_cols+[SN_col, value_col]]\n",
    "    return_col_order = [x for x in ami_df.columns.tolist() if x != SN_col]\n",
    "    \n",
    "    #-------------------------\n",
    "    # Really, the aggregation operation only needs to be performed on PNs with more than one SN\n",
    "    # To save resources and time, split apart those needing reduced and those not\n",
    "    #-----\n",
    "    # To be most correct, for this application one should include time_col, so this is slightly different to what is \n",
    "    #   done in combine_PNs_in_best_ests_df\n",
    "    SNs_per_PN_and_time = ami_df[[SN_col, PN_col, time_col]].drop_duplicates()[[PN_col, time_col]].value_counts()\n",
    "    PNs_w_mult_SNs = SNs_per_PN_and_time[SNs_per_PN_and_time>1].index.get_level_values(0).unique().tolist()\n",
    "    \n",
    "    # If there are no PNs with multiple SNs, return ami_df (without SN_col, as the purpose of this\n",
    "    #   is to eliminate SN_col by grouping by premise)\n",
    "    if len(PNs_w_mult_SNs)==0:\n",
    "        return ami_df.drop(columns=[SN_col]).sort_values(by=[time_col, PN_col])\n",
    "\n",
    "    ami_df_w_mult  = ami_df[ami_df[PN_col].isin(PNs_w_mult_SNs)]\n",
    "    ami_df_wo_mult = ami_df[~ami_df[PN_col].isin(PNs_w_mult_SNs)]\n",
    "    assert(ami_df_w_mult.shape[0]+ami_df_wo_mult.shape[0]==ami_df.shape[0])\n",
    "    \n",
    "    #-------------------------\n",
    "    if include_index_in_shared_cols:\n",
    "        # As various elements will be grouped/rows collapsed/however you want to think about it, the index will\n",
    "        #   necessarily be lost, even though the indices for those being combined should be shared (but, pandas\n",
    "        #   doesn't know that).\n",
    "        # In order to retain the index, stored it in a temporary column and re-set it later\n",
    "        # NOTE: This assumes there is a single index level.  If MultiIndex, method will need re-worked\n",
    "        assert(ami_df.index.nlevels==1)\n",
    "        assert(is_datetime64_dtype(ami_df.index.dtype))\n",
    "        if ami_df.index.name is not None and ami_df.index.name not in ami_df.columns.tolist():\n",
    "            tmp_idx_col = ami_df.index.name\n",
    "        else:\n",
    "            tmp_idx_col = Utilities.generate_random_string()\n",
    "        ami_df_w_mult = ami_df_w_mult.reset_index(drop=False, names=tmp_idx_col)\n",
    "        cols_shared_by_groups.append(tmp_idx_col)\n",
    "    \n",
    "    #-------------------------\n",
    "    # Make sure all columns in cols_shared_by_groups have a single value per group\n",
    "    assert((ami_df_w_mult.groupby(grp_by_cols, dropna=gpby_dropna)[cols_shared_by_groups].nunique()<=1).all().all())\n",
    "    \n",
    "    #-------------------------\n",
    "    # Perform aggregation\n",
    "    return_df = ami_df_w_mult.groupby(\n",
    "        grp_by_cols, dropna=gpby_dropna, as_index=False, group_keys=False\n",
    "    ).agg(\n",
    "        {x:'first' for x in cols_shared_by_groups}|\n",
    "        {value_col:'mean'}\n",
    "    )\n",
    "    \n",
    "    #-------------------------\n",
    "    if include_index_in_shared_cols:\n",
    "        # Set index back to original values\n",
    "        return_df = return_df.set_index(tmp_idx_col, drop=True)\n",
    "    \n",
    "    #-------------------------\n",
    "    # Make sure columns are as expected\n",
    "    assert(len(set(return_df.columns.tolist()).symmetric_difference(set(return_col_order)))==0)\n",
    "    return_df = return_df[return_col_order]\n",
    "    \n",
    "    #-------------------------\n",
    "    # Combine together with collection of PNs having a single SN\n",
    "    # NOTE: The SN_col should not be present in the final output\n",
    "    ami_df_wo_mult = ami_df_wo_mult.drop(columns=[SN_col])\n",
    "    assert(return_df.columns.tolist()==ami_df_wo_mult.columns.tolist())\n",
    "    return_df = pd.concat([return_df, ami_df_wo_mult])\n",
    "    \n",
    "    #-------------------------\n",
    "    return_df = return_df.sort_values(by=[time_col, PN_col])\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce0294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4422eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16cf30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_found_ami_for_SN(\n",
    "    df_i, \n",
    "    ts_req, \n",
    "    time_col='starttimeperiod_local', \n",
    "    SN_col='serialnumber', \n",
    "):\n",
    "    r\"\"\"\n",
    "    Intended for use in check_found_ami_for_all_SNs_in_outage.\n",
    "    df_i must be for a single SN\n",
    "    Just checks that all required timestamps (ts_req) are found in df_i[time_col]\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    if len(set(ts_req).difference(set(df_i[time_col].tolist())))==0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f6bcc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_found_ami_for_all_SNs_in_outage(\n",
    "    df, \n",
    "    t_search_min_max, \n",
    "    requirement='all', \n",
    "    time_col='starttimeperiod_local', \n",
    "    value_col='value', \n",
    "    SN_col='serialnumber', \n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(requirement in ['all', 'endpoints'])\n",
    "    #-------------------------\n",
    "    # Need frequency for rounding and generating required timestamps (if requirement=='all')\n",
    "    freq = Utilities_df.determine_freq_in_df_col(\n",
    "        df=df, \n",
    "        groupby_SN=True, \n",
    "        return_val_if_not_found='error', \n",
    "        assert_single_freq=True, \n",
    "        assert_expected_freq_found=False, \n",
    "        expected_freq=pd.Timedelta('15 minutes'), \n",
    "        time_col=time_col, \n",
    "        SN_col=SN_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    assert(Utilities.is_object_one_of_types(t_search_min_max, [list, tuple]) and len(t_search_min_max)==2)\n",
    "    # t_search_min_max must be datetime object\n",
    "    if not isinstance(t_search_min_max[0], datetime.datetime) or not isinstance(t_search_min_max[1], datetime.datetime):\n",
    "        t_search_min_max = [pd.to_datetime(t_search_min_max[0]), pd.to_datetime(t_search_min_max[1])]\n",
    "    # Need to round t_search_min down to nearest freq and t_search_max up to nearest freq\n",
    "    t_search_min_max = [t_search_min_max[0].floor(freq=freq), t_search_min_max[1].ceil(freq=freq)]\n",
    "    #-------------------------\n",
    "    # Generate the needed timestamps according to requirement argument\n",
    "    if requirement=='all':\n",
    "        ts_req = pd.date_range(start=t_search_min_max[0], end=t_search_min_max[1], freq=freq)\n",
    "    elif requirement=='endpoints':\n",
    "        ts_req = t_search_min_max\n",
    "    else:\n",
    "        assert(0)\n",
    "    #-------------------------\n",
    "    # Generate the series containing all pass values (index is SN, value is boolean representing pass/fail)\n",
    "    pass_srs = df.groupby(SN_col).apply(\n",
    "        lambda x: check_found_ami_for_SN(\n",
    "            df_i=x, \n",
    "            ts_req=ts_req, \n",
    "            time_col=time_col, \n",
    "            SN_col=SN_col\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #-------------------------\n",
    "    # In order to pass, all SNs must have the required AMI\n",
    "    if all(pass_srs):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0b40212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_found_ami_for_all_SNs_in_outage_2(\n",
    "    df, \n",
    "    t_search_min_max, \n",
    "    requirement='all', \n",
    "    time_col='starttimeperiod_local', \n",
    "    value_col='value', \n",
    "    SN_col='serialnumber', \n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(requirement in ['all', 'endpoints'])\n",
    "    #-------------------------\n",
    "    # Need frequency for rounding and generating required timestamps (if requirement=='all')\n",
    "    freq = Utilities_df.determine_freq_in_df_col(\n",
    "        df=df, \n",
    "        groupby_SN=True, \n",
    "        return_val_if_not_found='error', \n",
    "        assert_single_freq=True, \n",
    "        assert_expected_freq_found=False, \n",
    "        expected_freq=pd.Timedelta('15 minutes'), \n",
    "        time_col=time_col, \n",
    "        SN_col=SN_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    assert(Utilities.is_object_one_of_types(t_search_min_max, [list, tuple]) and len(t_search_min_max)==2)\n",
    "    # t_search_min_max must be datetime object\n",
    "    if not isinstance(t_search_min_max[0], datetime.datetime) or not isinstance(t_search_min_max[1], datetime.datetime):\n",
    "        t_search_min_max = [pd.to_datetime(t_search_min_max[0]), pd.to_datetime(t_search_min_max[1])]\n",
    "    # Need to round t_search_min down to nearest freq and t_search_max up to nearest freq\n",
    "    t_search_min_max = [t_search_min_max[0].floor(freq=freq), t_search_min_max[1].ceil(freq=freq)]\n",
    "    #-------------------------\n",
    "    # Generate the needed timestamps according to requirement argument\n",
    "    if requirement=='all':\n",
    "        ts_req = pd.date_range(start=t_search_min_max[0], end=t_search_min_max[1], freq=freq)\n",
    "    elif requirement=='endpoints':\n",
    "        ts_req = t_search_min_max\n",
    "    else:\n",
    "        assert(0)\n",
    "    #-------------------------\n",
    "    # Generate a list containing the pass values\n",
    "    pass_vals = []\n",
    "    for SN_i in df[SN_col].unique().tolist():\n",
    "        df_i = df[df[SN_col]==SN_i]\n",
    "        pass_i = check_found_ami_for_SN(\n",
    "            df_i=df_i, \n",
    "            ts_req=ts_req, \n",
    "            time_col=time_col, \n",
    "            SN_col=SN_col, \n",
    "        )\n",
    "        pass_vals.append(pass_i)\n",
    "    #-------------------------\n",
    "    # In order to pass, all SNs must have the required AMI\n",
    "    if all(pass_vals):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627d243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e03117d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to DOVSAudit\n",
    "def assess_outage_inclusion_requirements(\n",
    "    ami_df_i, \n",
    "    outg_rec_nb, \n",
    "    dovs_df, \n",
    "    max_pct_PNs_missing_allowed=0, \n",
    "    ami_df_i_info_dict=None, \n",
    "    dovs_df_info_dict=None, \n",
    "    check_found_ami_for_all_SNs_kwargs=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    Check whether or not outage should be included in analysis.\n",
    "    Returns boolean; True if the outage should be included, False if not\n",
    "    This check includes:\n",
    "        - ensure minimum percentage of PNs listed in DOVS are found in AMI\n",
    "            - By default, all PNs must be included for an outage to be deemed suitable\n",
    "            - Value set by max_pct_PNs_missing_allowed parameter, which should be between 0 and 100\n",
    "        - ensure usable AMI data are found for each meter\n",
    "            - Enforced using the function check_found_ami_for_all_SNs_in_outage, which has a\n",
    "                requirement parameter which (at the time of writing) may be set to 'all' or 'endpoints'\n",
    "        \n",
    "    ami_df_i:\n",
    "        AMI DataFrame for a single outage.\n",
    "        This should be reduced down to only the data of interest, e.g., only data with 'aep_derived_uom'=='VOLT'\n",
    "          and 'aep_srvc_qlty_idntfr'=='AVG' or instantaneous voltage data after reduction is run (using, e.g.,\n",
    "          the function reduce_INSTV_ABC_1_vals_in_df)\n",
    "          \n",
    "    outg_rec_nb:\n",
    "        The outage record number, used to identify the outage information in dovs_df\n",
    "        \n",
    "    dovs_df:\n",
    "        DOVS DataFrame containing information about the outage.\n",
    "        The preferred form for this DF is consolidated form, meaning one row per outage with the elements\n",
    "          of the premise column being list objects\n",
    "          \n",
    "    max_pct_PNs_missing_allowed:\n",
    "        The maximum percent of PNs found in DOVS missing from AMI allowed for the outage to be included (to return True)\n",
    "        default: 0 (meaning all PNs must be found)\n",
    "        \n",
    "          \n",
    "    ami_df_i_info_dict:\n",
    "        Gives necessary information regarding ami_df_i.\n",
    "        See default values in dflt_ami_df_i_info_dict below\n",
    "        PN_col:\n",
    "            default: 'aep_premise_nb'\n",
    "        time_col:\n",
    "            default: 'starttimeperiod_local', \n",
    "        value_col:\n",
    "            default: 'value', \n",
    "        SN_col: \n",
    "            default: 'serialnumber', \n",
    "        outg_rec_nb_col:\n",
    "            default: 'OUTG_REC_NB_GPD_FOR_SQL'\n",
    "            Not necessary in ami_df_i, but if present, the value will be compared to outg_rec_nb to ensure consistency\n",
    "        outg_t_beg_col:\n",
    "            default: 'DT_OFF_TS_FULL'\n",
    "            Not necessary in ami_df_i, but if present, the value will be compared to the value extracted from dovs_df\n",
    "        outg_t_end_col:\n",
    "            default: 'DT_ON_TS'\n",
    "            Not necessary in ami_df_i, but if present, the value will be compared to the value extracted from dovs_df\n",
    "          \n",
    "    dovs_df_info_dict:\n",
    "        Gives necessary information regarding dovs_df, such as whether or not it is consolidated and the\n",
    "          names of needed columns (e.g., PN_col)\n",
    "        Default values given in code below in dflt_dovs_df_info_dict variable.\n",
    "            is_consolidated:\n",
    "                default: True\n",
    "            PN_col:\n",
    "                default: 'premise_nbs'\n",
    "            outg_rec_nb_idfr:\n",
    "                default: 'index'\n",
    "                This directs where the outg_rec_nbs are stored in dovs_df, which can be a column or the index.\n",
    "                This should be a string, list, or tuple.\n",
    "                If the outg_rec_nbs are located in a column, idfr should simply be the column\n",
    "                    - Single index columns --> simple string\n",
    "                    - MultiIndex columns   --> appropriate tuple to identify column\n",
    "                If the outg_rec_nbs are located in the index:\n",
    "                    - Single level index --> simple string 'index' or 'index_0'\n",
    "                    - MultiIndex index:  --> \n",
    "                        - string f'index_{level}', where level is the index level containing the outg_rec_nbs\n",
    "                        - tuple of length 2, with 0th element ='index' and 1st element = idx_level_name where\n",
    "                            idx_level_name is the name of the index level containing the outg_rec_nbs\n",
    "            outg_t_beg_col:\n",
    "                default: 'DT_OFF_TS_FULL'\n",
    "            outg_t_end_col:\n",
    "                default: 'DT_ON_TS'\n",
    "                \n",
    "    check_found_ami_for_all_SNs_kwargs:\n",
    "        Arguments to be fed into check_found_ami_for_all_SNs_in_outage function.\n",
    "        See default values in dflt_check_found_ami_for_all_SNs_kwargs in code below.\n",
    "        User should really only set t_search_min_max and requirement\n",
    "            df:\n",
    "                Does not need to be input by user, as it will be set to ami_df_i\n",
    "            t_search_min_max:\n",
    "                If not set by user, this will be set to [out_t_beg-pd.Timedelta('1hour'), out_t_end+pd.Timedelta('1hour')]\n",
    "            requirement:\n",
    "                default: 'all'\n",
    "            time_col:\n",
    "                Does not need to be input by user\n",
    "            value_col:\n",
    "                Does not need to be input by user\n",
    "            SN_col:\n",
    "                Does not need to be input by user\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if ami_df_i.shape[0]==0:\n",
    "        return False\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    # Grab the relevant info from dovs_df\n",
    "    #-------------------------\n",
    "    dflt_dovs_df_info_dict = dict(\n",
    "        is_consolidated=True,\n",
    "        PN_col='premise_nbs', \n",
    "        outg_rec_nb_idfr='index', \n",
    "        outg_t_beg_col='DT_OFF_TS_FULL', \n",
    "        outg_t_end_col='DT_ON_TS'\n",
    "    )\n",
    "    dovs_df_info_dict = Utilities.supplement_dict_with_default_values(dovs_df_info_dict, dflt_dovs_df_info_dict, inplace=False)\n",
    "    \n",
    "    #-------------------------\n",
    "    dovs_df_i = DOVSOutages.retrieve_outage_from_dovs_df(\n",
    "        dovs_df=dovs_df, \n",
    "        outg_rec_nb=outg_rec_nb, \n",
    "        outg_rec_nb_idfr=dovs_df_info_dict['outg_rec_nb_idfr'], \n",
    "        assert_outg_rec_nb_found=True\n",
    "    )\n",
    "    \n",
    "    #-------------------------\n",
    "    # Make sure the necessary columns are in dovs_df_i\n",
    "    necessary_dovs_cols = [\n",
    "        dovs_df_info_dict['PN_col'], \n",
    "        dovs_df_info_dict['outg_t_beg_col'], \n",
    "        dovs_df_info_dict['outg_t_end_col']\n",
    "    ]\n",
    "    assert(len(set(necessary_dovs_cols).difference(set(dovs_df_i.columns.tolist())))==0)\n",
    "    \n",
    "    #-------------------------\n",
    "    # Grab the needed info from dovs_df_i\n",
    "    if dovs_df_info_dict['is_consolidated']:\n",
    "        assert(dovs_df_i.shape[0]==1)\n",
    "        out_t_beg = dovs_df_i.iloc[0][dovs_df_info_dict['outg_t_beg_col']]\n",
    "        out_t_end = dovs_df_i.iloc[0][dovs_df_info_dict['outg_t_end_col']]\n",
    "        PNs_dovs  = dovs_df_i.iloc[0][dovs_df_info_dict['PN_col']]\n",
    "        assert(Utilities.is_object_one_of_types(PNs_dovs, [list, tuple, set]))\n",
    "    else:\n",
    "        assert(dovs_df_i[dovs_df_info_dict['outg_t_beg_col']].nunique()==1)\n",
    "        assert(dovs_df_i[dovs_df_info_dict['outg_t_end_col']].nunique()==1)\n",
    "        out_t_beg = dovs_df_i[dovs_df_info_dict['outg_t_beg_col']].unique().tolist()[0]\n",
    "        out_t_end = dovs_df_i[dovs_df_info_dict['outg_t_end_col']].unique().tolist()[0]\n",
    "        PNs_dovs  = dovs_df_i[dovs_df_info_dict['PN_col']].unique().tolist()\n",
    "        \n",
    "    #--------------------------------------------------\n",
    "    # Determine the PNs found in ami_df and compare to those from DOVS\n",
    "    #-------------------------\n",
    "    dflt_ami_df_i_info_dict = dict(\n",
    "        PN_col='aep_premise_nb', \n",
    "        time_col='starttimeperiod_local', \n",
    "        value_col='value', \n",
    "        SN_col='serialnumber', \n",
    "        outg_rec_nb_col='OUTG_REC_NB_GPD_FOR_SQL', \n",
    "        outg_t_beg_col='DT_OFF_TS_FULL', \n",
    "        outg_t_end_col='DT_ON_TS'        \n",
    "    )\n",
    "    ami_df_i_info_dict = Utilities.supplement_dict_with_default_values(ami_df_i_info_dict, dflt_ami_df_i_info_dict, inplace=False)\n",
    "    \n",
    "    #-------------------------\n",
    "    # Make sure the necessary columns are present in ami_df_i\n",
    "    necessary_ami_cols = [\n",
    "        ami_df_i_info_dict['PN_col'], \n",
    "        ami_df_i_info_dict['time_col'], \n",
    "        ami_df_i_info_dict['value_col'], \n",
    "        ami_df_i_info_dict['SN_col']\n",
    "    ]\n",
    "    assert(len(set(necessary_ami_cols).difference(set(ami_df_i.columns.tolist())))==0)\n",
    "    \n",
    "    #-------------------------\n",
    "    # If outage information also contained in ami_df, compare against expected values\n",
    "    if ami_df_i_info_dict['outg_rec_nb_col'] in ami_df_i.columns.tolist():\n",
    "        assert(ami_df_i[ami_df_i_info_dict['outg_rec_nb_col']].nunique()==1)\n",
    "        assert(ami_df_i[ami_df_i_info_dict['outg_rec_nb_col']].unique().tolist()[0]==outg_rec_nb)\n",
    "    if ami_df_i_info_dict['outg_t_beg_col'] in ami_df_i.columns.tolist():\n",
    "        assert(ami_df_i[ami_df_i_info_dict['outg_t_beg_col']].nunique()==1)\n",
    "        assert(ami_df_i[ami_df_i_info_dict['outg_t_beg_col']].unique().tolist()[0]==out_t_beg)\n",
    "    if ami_df_i_info_dict['outg_t_end_col'] in ami_df_i.columns.tolist():\n",
    "        assert(ami_df_i[ami_df_i_info_dict['outg_t_end_col']].nunique()==1)\n",
    "        assert(ami_df_i[ami_df_i_info_dict['outg_t_end_col']].unique().tolist()[0]==out_t_end)\n",
    "    \n",
    "    #-------------------------\n",
    "    # Grab PNs_ami\n",
    "    PNs_ami = ami_df_i[ami_df_i_info_dict['PN_col']].unique().tolist()\n",
    "    \n",
    "    #-----*****-----*****-----*****-----*****-----\n",
    "    # Find difference in PNs between DOVS and AMI\n",
    "    dff_pns = list(set(PNs_dovs).difference(set(PNs_ami)))\n",
    "    \n",
    "    # NOTE: For the current analysis, where data are queried according to premise numbers, there should\n",
    "    #         never be a situation where PNs found in AMI not present in DOVS.\n",
    "    #       For now, assert this to be true.\n",
    "    #       In the future, if I instead query at the transformer level to try to pick up PNs possible missed\n",
    "    #         by DOVS, this may not be the case.\n",
    "    assert(len(set(PNs_ami).difference(set(PNs_dovs)))==0)\n",
    "    \n",
    "    # Check if difference within allowable range, as set by max_pct_PNs_missing_allowed\n",
    "    pct_pns_missing = 100.*len(dff_pns)/len(PNs_dovs)\n",
    "    if pct_pns_missing > max_pct_PNs_missing_allowed:\n",
    "        return False\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    # Check that usable AMI data are found for each meter\n",
    "    #   At this point, the minimum percentage of PNs listed in DOVS found in AMI criterion passed\n",
    "    #-------------------------\n",
    "    dflt_check_found_ami_for_all_SNs_kwargs = dict(\n",
    "        t_search_min_max=[out_t_beg-pd.Timedelta('1hour'), out_t_end+pd.Timedelta('1hour')], \n",
    "        requirement='all', \n",
    "        time_col=ami_df_i_info_dict['time_col'], \n",
    "        value_col=ami_df_i_info_dict['value_col'], \n",
    "        SN_col=ami_df_i_info_dict['SN_col'], \n",
    "    )\n",
    "    check_found_ami_for_all_SNs_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "        check_found_ami_for_all_SNs_kwargs, \n",
    "        dflt_check_found_ami_for_all_SNs_kwargs, \n",
    "        inplace=False\n",
    "    )\n",
    "    # Make sure the user didn't accidentally set time_col, value_col, or SN_col incorrectly\n",
    "    assert(check_found_ami_for_all_SNs_kwargs['time_col']==ami_df_i_info_dict['time_col'])\n",
    "    assert(check_found_ami_for_all_SNs_kwargs['value_col']==ami_df_i_info_dict['value_col'])\n",
    "    assert(check_found_ami_for_all_SNs_kwargs['SN_col']==ami_df_i_info_dict['SN_col'])\n",
    "    \n",
    "    check_found_ami_for_all_SNs_kwargs['df'] = ami_df_i\n",
    "    \n",
    "    #-------------------------\n",
    "    # Run ami_df_i through check_found_ami_for_all_SNs_in_outage\n",
    "    all_found_ami_i = check_found_ami_for_all_SNs_in_outage(**check_found_ami_for_all_SNs_kwargs)\n",
    "    \n",
    "    # Since minimum percentage of PNs already passed, all_found_ami_i can simply be returned here\n",
    "    return all_found_ami_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4fb2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO DOVSAudit\n",
    "def choose_best_slicer_and_perform_slicing(\n",
    "    df, \n",
    "    slicers, \n",
    "    groupby_SN=False, \n",
    "    t_search_min_max=None, \n",
    "    time_col='starttimeperiod_local', \n",
    "    value_col=None, \n",
    "    SN_col='serialnumber', \n",
    "    return_sorted=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    From slicers, choose the best option, slice the full df, and return.\n",
    "    This can be done for the entire df as a whole, or by serial number, depending on the value of groupby_SN\n",
    "    If t_search_min_max is set, the best slicer is chosen from the subset of df, df_choose_slcr, with\n",
    "      time values within the constraints of t_search_min_max\n",
    "    Although the best slicer is determined from the subset, the slicing is performed on the entirety of df.\n",
    "    How is the best slicer chosen?\n",
    "        If value_col is None, the best slicer is that with the most rows after slicing df_choose_slcr.\n",
    "        If value_col is set, the best slicer is that with the most not-NA value_col entries after slicing df_choose_slcr.\n",
    "        NOTE: If multiple best slicers exists, the first is chosen\n",
    "        \n",
    "    return_sorted:\n",
    "        If true, the returned df is sorted according to time_col, SN_col\n",
    "    \"\"\"\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # NOTE: Need to exercise groupby_SN==True option here (instead of, e.g., after df_choose_slcr is found),\n",
    "    #         so that slicing is done on entirety of df, as described in documentation\n",
    "    if groupby_SN:\n",
    "        # NOTE: groupby_SN MUST BE SET TO FALSE BELOW, otherwise infinite loop!\n",
    "        return_df = df.groupby([SN_col], as_index=False, group_keys=False).apply(\n",
    "            lambda x: choose_best_slicer_and_perform_slicing(\n",
    "                df=x, \n",
    "                slicers=slicers, \n",
    "                groupby_SN=False, \n",
    "                t_search_min_max=t_search_min_max, \n",
    "                time_col=time_col, \n",
    "                value_col=value_col, \n",
    "                SN_col=SN_col, \n",
    "                return_sorted=return_sorted\n",
    "            )\n",
    "        )\n",
    "        #-----\n",
    "        # Sort, if return_sorted==True\n",
    "        if return_sorted:\n",
    "            return_df = return_df.sort_values(by=[time_col, SN_col])\n",
    "        #-----\n",
    "        return return_df\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    assert(Utilities.is_object_one_of_types(slicers, [list, tuple]))\n",
    "    #-------------------------\n",
    "    if t_search_min_max is not None:\n",
    "        assert(Utilities.is_object_one_of_types(t_search_min_max, [list, tuple]))\n",
    "        assert(len(t_search_min_max)==2)\n",
    "        #-----\n",
    "        # At least one of t_search_min_max should not be None\n",
    "        assert(t_search_min_max[0] is not None or t_search_min_max[1] is not None)\n",
    "        if t_search_min_max[0] is None:\n",
    "            t_search_min_max[0] = pd.Timestamp.min\n",
    "        if t_search_min_max[1] is None:\n",
    "            t_search_min_max[1] = pd.Timestamp.max\n",
    "        #-----\n",
    "        df_choose_slcr = df[\n",
    "            (df[time_col]>=t_search_min_max[0]) & \n",
    "            (df[time_col]<=t_search_min_max[1])\n",
    "        ]\n",
    "    else:\n",
    "        df_choose_slcr = df\n",
    "    #-------------------------\n",
    "    # Construct slcrs_w_counts to choose best slicer\n",
    "    slcrs_w_counts = []\n",
    "    for slcr_i in slicers:\n",
    "        if value_col is None:\n",
    "            counts_i = slcr_i.perform_slicing(df_choose_slcr).shape[0]\n",
    "        else:\n",
    "            counts_i = slcr_i.perform_slicing(df_choose_slcr)[value_col].notna().sum()\n",
    "        slcrs_w_counts.append((slcr_i, counts_i))\n",
    "    #-------------------------\n",
    "    # Find the winner from slcrs_w_counts\n",
    "    slicer = max(slcrs_w_counts, key=lambda x: x[1])[0]\n",
    "    \n",
    "    #-------------------------\n",
    "    # Perform the slicing\n",
    "    return_df = slicer.perform_slicing(df).copy()\n",
    "    \n",
    "    #-------------------------\n",
    "    # Sort, if return_sorted==True\n",
    "    if return_sorted:\n",
    "        return_df = return_df.sort_values(by=[time_col, SN_col])\n",
    "    \n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9940e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO DOVSAudit\n",
    "def reduce_INSTV_ABC_1_vals_in_df(\n",
    "    df, \n",
    "    value_col='value', \n",
    "    aep_derived_uom_col='aep_derived_uom', \n",
    "    aep_srvc_qlty_idntfr_col='aep_srvc_qlty_idntfr', \n",
    "    output_aep_srvc_qlty_idntfr = 'INSTV(ABC)1', \n",
    "    include_index_in_shared_cols=True, \n",
    "    gpby_dropna=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Function to reduce down multiple INSTVA1/INSTVB1/INSTVC1 for each timestamp into a single average value.\n",
    "    -----\n",
    "    In general, it is desired that each serial number have a single value per time stamp.\n",
    "    For this function, df can have a mix of INSTV(ABC)1 and AVG voltage readings, HOWEVER, it is expected that for\n",
    "      each SN/timestamp combination, there should only be INSTV(ABC)1 values or a single AVG value.\n",
    "    This is built only for aep_derived_uom==VOLT, and aep_srvc_qlty_idntfr in [INSTVA1, INSTVB1, INSTVC1, AVG]\n",
    "    \n",
    "    include_index_in_shared_cols:\n",
    "        As various elements will be grouped/rows collapsed/however you want to think about it, the index will\n",
    "          necessarily be lost, \n",
    "        In many cases (e.g., when the indices are time stamps) the indices for those being combined should be \n",
    "          shared (but, pandas doesn't know that).\n",
    "        If this is the case, set include_index_in_shared_cols==True\n",
    "        NOTE: For the foreseeable applications, this only really makes sense if the indices are datetime objects,\n",
    "              so this will be enforced.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df[aep_derived_uom_col].nunique()==1)\n",
    "    assert(df[aep_derived_uom_col].unique()[0]=='VOLT')\n",
    "    #-----\n",
    "    assert(len(set(df[aep_srvc_qlty_idntfr_col].unique().tolist()).difference(\n",
    "        set(['INSTVA1', 'INSTVB1', 'INSTVC1', 'AVG'])\n",
    "    ))==0)\n",
    "    #-------------------------\n",
    "    # Grab og_cols to maintain column ordering in output\n",
    "    og_cols = df.columns.tolist()\n",
    "    \n",
    "    #-------------------------\n",
    "    grp_by_cols = [x for x in df.columns.tolist() \n",
    "                   if x not in [value_col, aep_derived_uom_col, aep_srvc_qlty_idntfr_col]]\n",
    "    cols_shared_by_groups = [aep_derived_uom_col]\n",
    "    agg_dict = {\n",
    "        value_col:               'mean', \n",
    "        aep_derived_uom_col:     'first', \n",
    "        aep_srvc_qlty_idntfr_col: lambda x: ' '.join(natsorted(set(x)))\n",
    "    }\n",
    "    if include_index_in_shared_cols:\n",
    "        # As various elements will be grouped/rows collapsed/however you want to think about it, the index will\n",
    "        #   necessarily be lost, even though the indices for those being combined should be shared (but, pandas\n",
    "        #   doesn't know that).\n",
    "        # In order to retain the index, stored it in a temporary column and re-set it later\n",
    "        # NOTE: This assumes there is a single index level.  If MultiIndex, method will need re-worked\n",
    "        assert(df.index.nlevels==1)\n",
    "        assert(is_datetime64_dtype(df.index.dtype))\n",
    "        if df.index.name is not None and df.index.name not in df.columns.tolist():\n",
    "            tmp_idx_col = df.index.name\n",
    "        else:\n",
    "            tmp_idx_col = Utilities.generate_random_string()\n",
    "        df = df.reset_index(drop=False, names=tmp_idx_col)\n",
    "        cols_shared_by_groups.append(tmp_idx_col)\n",
    "        agg_dict[tmp_idx_col] = 'first'\n",
    "    #----------\n",
    "    # Make sure all columns in cols_shared_by_groups have a single value per group\n",
    "    assert((df.groupby(grp_by_cols, dropna=gpby_dropna)[cols_shared_by_groups].nunique()<=1).all().all())    \n",
    "    \n",
    "    #----------\n",
    "    # Do aggregation\n",
    "    return_df = df.groupby(grp_by_cols, dropna=gpby_dropna, as_index=False, group_keys=False).agg(agg_dict)\n",
    "    \n",
    "    #-------------------------\n",
    "    # Due to join operation about with natsorted and set, the only possible values for aep_srvc_qlty_idntfr_col\n",
    "    #   in return_df are the sorted combinations of ['INSTVA1', 'INSTVB1', 'INSTVC1']\n",
    "    #   e.g., 'INSTVA1', 'INSTVB1', 'INSTVC1', 'INSTVA1 INSTVB1', ... 'INSTVA1 INSTVB1 INSTVC1'\n",
    "    # Assert that this is true\n",
    "    accptbl_fnl_srvc_qlty_idntfrs = ['INSTVA1', 'INSTVB1', 'INSTVC1']\n",
    "    #-----\n",
    "    combs = []\n",
    "    for i in range(1, len(accptbl_fnl_srvc_qlty_idntfrs)+1):\n",
    "        els = [list(x) for x in itertools.combinations(accptbl_fnl_srvc_qlty_idntfrs, i)]\n",
    "        combs.extend(els)\n",
    "    #-----\n",
    "    accptbl_fnl_srvc_qlty_idntfrs = [' '.join(x) for x in combs]\n",
    "    accptbl_fnl_srvc_qlty_idntfrs.append('AVG')\n",
    "    #-----\n",
    "    assert(len(set(df[aep_srvc_qlty_idntfr_col].unique().tolist()).difference(set(accptbl_fnl_srvc_qlty_idntfrs)))==0)\n",
    "    #-------------------------\n",
    "    # Set rows with aep_srvc_qlty_idntfr!='VOLT' equal to output_aep_srvc_qlty_idntfr\n",
    "    return_df.loc[return_df[aep_srvc_qlty_idntfr_col]!='AVG', aep_srvc_qlty_idntfr_col] = output_aep_srvc_qlty_idntfr\n",
    "    #-------------------------\n",
    "    if include_index_in_shared_cols:\n",
    "        # Set index back to original values\n",
    "        return_df = return_df.set_index(tmp_idx_col, drop=True)\n",
    "    #-------------------------\n",
    "    assert(len(set(og_cols).symmetric_difference(set(return_df.columns.tolist())))==0)\n",
    "    return_df = return_df[og_cols]\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49597a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814d0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47293d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91308016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fbb69d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outg_rec_nbs_to_remove(\n",
    "    paths, \n",
    "    slicers, \n",
    "    verbose=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # First, need to iterate through paths to retrieve list of all outg_rec_nbs\n",
    "    # This is necessary as outages can be split across multiple files, so one cannot\n",
    "    #   simply iterate through the files\n",
    "    paths=natsorted(paths)\n",
    "    outg_rec_nbs_in_files = dict()\n",
    "    for path in paths:\n",
    "        assert(path not in outg_rec_nbs_in_files.keys())\n",
    "        df = GenAn.read_df_from_csv(path)\n",
    "        outg_rec_nbs_in_files[path] = df['OUTG_REC_NB_GPD_FOR_SQL'].unique().tolist()\n",
    "    outg_rec_nb_to_files_dict = invert_file_to_outg_rec_nbs_dict(outg_rec_nbs_in_files)\n",
    "    all_outg_rec_nbs = list(outg_rec_nb_to_files_dict.keys())\n",
    "    \n",
    "    #-------------------------\n",
    "    # Build dovs_df\n",
    "    dovs = DOVSOutages(\n",
    "        df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "        contstruct_df_args=None, \n",
    "        init_df_in_constructor=True,\n",
    "        build_sql_function=DOVSOutages_SQL.build_sql_outage, \n",
    "        build_sql_function_kwargs=dict(\n",
    "            outg_rec_nbs=all_outg_rec_nbs, \n",
    "            field_to_split='outg_rec_nbs', \n",
    "            include_premise=True\n",
    "        ), \n",
    "        build_consolidated=True\n",
    "    )\n",
    "    dovs_df = dovs.df.copy()\n",
    "    \n",
    "    #-------------------------\n",
    "    # Now, iterate through all outages\n",
    "    outg_rec_nbs_to_remove = []\n",
    "    for i_outg, outg_rec_nb in enumerate(all_outg_rec_nbs):\n",
    "        if verbose:\n",
    "            print(f'\\n\\ti_outg: {i_outg+1}/{len(all_outg_rec_nbs)}')\n",
    "            print(f'\\toutg_rec_nb = {outg_rec_nb}')\n",
    "        ami_df = GenAn.read_df_from_csv_batch(outg_rec_nb_to_files_dict[outg_rec_nb])\n",
    "\n",
    "        #--------------------------------------------------\n",
    "        ami_df_i = ami_df[ami_df['OUTG_REC_NB_GPD_FOR_SQL']==outg_rec_nb].copy()     \n",
    "        \n",
    "        # Although I cannot yet call choose_best_slicer_and_perform_slicing and reduce_INSTV_ABC_1_vals_in_df, \n",
    "        #   as the standard cleaning and conversions must be done first, I am able to cut down the size of\n",
    "        #   ami_df_i by joining the slicers with 'or' statements.\n",
    "        # Thus, ami_df_i will be reduced to only the subset of data which will be considered in \n",
    "        #   choose_best_slicer_and_perform_slicing\n",
    "        # As mentioned, this will cut down the size of ami_df_i and will also save time and resources by not having\n",
    "        #   to run entire DF through cleaning and conversions procedures.\n",
    "        ami_df_i = DFSlicer.combine_slicers_and_perform_slicing(\n",
    "            df=ami_df_i, \n",
    "            slicers=slicers, \n",
    "            join_slicers='or'\n",
    "        )\n",
    "        if ami_df_i.shape[0]==0:\n",
    "            outg_rec_nbs_to_remove.append(outg_rec_nb)\n",
    "            continue        \n",
    "        \n",
    "        #--------------------------------------------------\n",
    "        ami_df_i = AMINonVee.perform_std_initiation_and_cleaning(ami_df_i)\n",
    "        #-----\n",
    "        # Should the following be added to AMINonVee.perform_std_initiation_and_cleaning?\n",
    "        ami_df_i = Utilities_dt.strip_tz_info_and_convert_to_dt(\n",
    "            df=ami_df_i, \n",
    "            time_col='starttimeperiod', \n",
    "            placement_col='starttimeperiod_local', \n",
    "            run_quick=True, \n",
    "            n_strip=6, \n",
    "            inplace=False\n",
    "        )\n",
    "        ami_df_i = Utilities_dt.strip_tz_info_and_convert_to_dt(\n",
    "            df=ami_df_i, \n",
    "            time_col='endtimeperiod', \n",
    "            placement_col='endtimeperiod_local', \n",
    "            run_quick=True, \n",
    "            n_strip=6, \n",
    "            inplace=False\n",
    "        )\n",
    "        #--------------------------------------------------\n",
    "        ami_df_i = choose_best_slicer_and_perform_slicing(\n",
    "            df=ami_df_i, \n",
    "            slicers=slicers, \n",
    "            groupby_SN=True, \n",
    "            t_search_min_max=None, \n",
    "            time_col='starttimeperiod_local', \n",
    "            value_col=None, \n",
    "            SN_col='serialnumber', \n",
    "            return_sorted=True\n",
    "        )\n",
    "\n",
    "        ami_df_i = reduce_INSTV_ABC_1_vals_in_df(\n",
    "            df=ami_df_i, \n",
    "            value_col='value', \n",
    "            aep_derived_uom_col='aep_derived_uom', \n",
    "            aep_srvc_qlty_idntfr_col='aep_srvc_qlty_idntfr', \n",
    "            output_aep_srvc_qlty_idntfr = 'INSTV(ABC)1'\n",
    "        )\n",
    "        \n",
    "        if ami_df_i.shape[0]==0:\n",
    "            outg_rec_nbs_to_remove.append(outg_rec_nb)\n",
    "            continue\n",
    "            \n",
    "        to_include_i = assess_outage_inclusion_requirements(\n",
    "            ami_df_i=ami_df_i, \n",
    "            outg_rec_nb=outg_rec_nb, \n",
    "            dovs_df=dovs_df, \n",
    "            max_pct_PNs_missing_allowed=0\n",
    "\n",
    "        )\n",
    "        if not to_include_i:\n",
    "            outg_rec_nbs_to_remove.append(outg_rec_nb)\n",
    "    #--------------------------------------------------\n",
    "    return outg_rec_nbs_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c198977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a81c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e261e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9975e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f746677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "795aa824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_detailed_summary_df(\n",
    "    means_df, \n",
    "    best_ests_df_w_db_lbl,\n",
    "    CI_tot, \n",
    "    CMI_tot, \n",
    "    n_PNs_ami, \n",
    "    outg_rec_nb, \n",
    "    dovs_df_i, \n",
    "    db_label_col='db_label', \n",
    "    winner_min_col='winner_min', \n",
    "    winner_max_col='winner_max', \n",
    "    PN_col='PN',\n",
    "    i_outg_col='i_outg'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Build a summmary_df for outage.\n",
    "    This summary is really just a single-row DataFrame (i.e., a series).\n",
    "    The intent is for this to be used when running over a large number of outages.\n",
    "    \n",
    "    PN_col:\n",
    "        In the most typical case, where CI/CMI is set by premise number, this should be set to the\n",
    "          premise number column.\n",
    "        However, if one wants to calculate CI/CMI by serial number, this should be set to the serial number column.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Don't want to alter means_df, best_ests_df_w_db_lbl outside of function\n",
    "    means_df=means_df.copy()\n",
    "    best_ests_df_w_db_lbl=best_ests_df_w_db_lbl.copy()\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    # Get DOVS info\n",
    "    assert(\n",
    "        isinstance(dovs_df_i, pd.Series) or \n",
    "        (isinstance(dovs_df_i, pd.DataFrame) and dovs_df_i.shape[0]==1)\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Make sure OUTG_REC_NB in dovs_df_i agrees with input value safecheck\n",
    "    if isinstance(dovs_df_i, pd.DataFrame):\n",
    "        if dovs_df_i.index.name=='OUTG_REC_NB':\n",
    "            assert(dovs_df_i.index[0]==outg_rec_nb)\n",
    "        else:\n",
    "            assert('OUTG_REC_NB' in dovs_df_i.columns)\n",
    "            assert(dovs_df_i.iloc[0]['OUTG_REC_NB']==outg_rec_nb)\n",
    "    else:\n",
    "        if 'OUTG_REC_NB' in dovs_df_i.index:\n",
    "            assert(dovs_df_i['OUTG_REC_NB']==outg_rec_nb)\n",
    "        else:\n",
    "            assert(dovs_df_i.name==outg_rec_nb)\n",
    "\n",
    "    #-------------------------\n",
    "    # Make dovs_df_i a pd.Series object (if pd.DataFrame, this will collapse to pd.Series, if\n",
    "    #   pd.Series, this will have no effect)\n",
    "    # This isn't necessary, it just eliminate the need to use .loc[0] all over the place\n",
    "    dovs_df_i = dovs_df_i.squeeze()\n",
    "\n",
    "    #-------------------------\n",
    "    # Grab needed entries from DOVS data\n",
    "    dovs_outg_t_beg = dovs_df_i['DT_OFF_TS_FULL']\n",
    "    dovs_outg_t_end = dovs_df_i['DT_ON_TS']\n",
    "    ci_dovs         = dovs_df_i['CI_NB']\n",
    "    cmi_dovs        = dovs_df_i['CMI_NB']\n",
    "    n_PNs_dovs      = len(set(dovs_df_i['premise_nbs']))\n",
    "    outage_nb       = dovs_df_i['OUTAGE_NB']\n",
    "    mjr_cause_cd    = dovs_df_i['MJR_CAUSE_CD']\n",
    "    mnr_cause_cd    = dovs_df_i['MNR_CAUSE_CD']\n",
    "    dvc_typ_nb      = dovs_df_i['DVC_TYP_NM']   \n",
    "    opco_id         = dovs_df_i['OPERATING_UNIT_ID']\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    # If only a single sub-outage, only return 'Full Outage' entry\n",
    "    if means_df.shape[0]==1:\n",
    "        return_df = pd.DataFrame(\n",
    "            data=dict(\n",
    "                outg_i_beg=means_df.iloc[0][winner_min_col], \n",
    "                outg_i_end=means_df.iloc[0][winner_max_col], \n",
    "                CI_i=CI_tot,\n",
    "                CMI_i=CMI_tot\n",
    "            ), \n",
    "            index=pd.MultiIndex.from_tuples(\n",
    "                [(outage_nb, outg_rec_nb, 'Full Outage')], \n",
    "                names=['OUTAGE_NB','OUTG_REC_NB', 'Outage Subset']\n",
    "            )\n",
    "        )\n",
    "        # Remove fractional seconds\n",
    "        return_df['outg_i_beg']=return_df['outg_i_beg'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return_df['outg_i_end']=return_df['outg_i_end'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    else:  \n",
    "        #-------------------------\n",
    "        # The following protects against the case where one inputs means_df/best_ests_df_w_db_lbl which were built\n",
    "        #   at the serial number level, but tries to run this function at the premise number level (by, e.g., setting\n",
    "        #   PN_col='PN')\n",
    "        # In such a case, the CI would still come out correct (due to how it is calculated below), but the CMI would be \n",
    "        #   larger than in reality, as premises with multiple SNs would get counted multiple times.\n",
    "        if (best_ests_df_w_db_lbl.groupby([PN_col, i_outg_col]).size()>1).any():\n",
    "            print('Each combination of PN_col, i_outg_col in best_ests_df_w_db_lbl should have a single entry')\n",
    "            print('The input violates this!  CRASH IMMINENT!!!!!')\n",
    "            print('Likely, one is trying to calculate CI/CMI by premise, but forgot to run combine_PNs_in_best_ests_df')\n",
    "        assert((best_ests_df_w_db_lbl.groupby([PN_col, i_outg_col]).size()==1).all())\n",
    "\n",
    "        #-------------------------\n",
    "        # Add nPNs column to means_df, as this will be used for CI of the sub-outages\n",
    "        n_PNs_col = f'n_{PN_col}s'\n",
    "        #-----\n",
    "        means_df = add_nPNs_to_means_df(\n",
    "            means_df=means_df, \n",
    "            best_ests_df_w_db_lbl=best_ests_df_w_db_lbl,\n",
    "            db_label_col=db_label_col, \n",
    "            PN_col=PN_col, \n",
    "            n_PNs_col=n_PNs_col\n",
    "        )\n",
    "\n",
    "        #-------------------------\n",
    "        # Sort means_df\n",
    "        means_df.sort_values(by=[winner_min_col, winner_max_col])\n",
    "\n",
    "        #-------------------------\n",
    "        # Create winner_max-winner_min column (winner_delta_col), used to calculate CMI\n",
    "        winner_delta_col = Utilities.generate_random_string()\n",
    "        best_ests_df_w_db_lbl[winner_delta_col] = best_ests_df_w_db_lbl[winner_max_col]-best_ests_df_w_db_lbl[winner_min_col]\n",
    "\n",
    "        #-------------------------\n",
    "        # Build a series containing the CMI values by db_label\n",
    "        cmi_by_label = best_ests_df_w_db_lbl.groupby([db_label_col])[winner_delta_col].apply(lambda x: x.sum().total_seconds()/60)\n",
    "        cmi_by_label.name = 'CMI_i'\n",
    "        assert(len(set(cmi_by_label.index).symmetric_difference(means_df.index))==0)\n",
    "\n",
    "        # The CMI_tot input should equal cmi_by_label.sum() (likely they won't be EXACTLY the same, hence\n",
    "        #   the use of Utilities.are_approx_equal)\n",
    "        # NOTE: The same cannot be said for CI, as individual meters can suffer multiple sub-outages\n",
    "        assert(Utilities.are_approx_equal(CMI_tot, cmi_by_label.sum(), precision=0.00001))\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct return_df\n",
    "        return_df = means_df[[winner_min_col, winner_max_col, n_PNs_col]].copy()\n",
    "        return_df = return_df.merge(cmi_by_label, left_index=True, right_index=True, how='left')\n",
    "\n",
    "        # Remove fractional seconds\n",
    "        return_df[winner_min_col]=return_df[winner_min_col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return_df[winner_max_col]=return_df[winner_max_col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Adjust column names\n",
    "        return_df=return_df.rename(columns={\n",
    "            winner_min_col:'outg_i_beg', \n",
    "            winner_max_col:'outg_i_end', \n",
    "            n_PNs_col:'CI_i'\n",
    "        })\n",
    "\n",
    "        # Change index to sub-outage numbers\n",
    "        return_df.index = [f'Sub-outage {i}' for i in range(1, return_df.shape[0]+1)]\n",
    "\n",
    "        #-------------------------\n",
    "        # Build a row containing the net outage information\n",
    "        net_outg_row = pd.DataFrame(\n",
    "            data=dict(\n",
    "                outg_i_beg=return_df['outg_i_beg'].min(), \n",
    "                outg_i_end=return_df['outg_i_end'].max(), \n",
    "                CI_i=CI_tot,\n",
    "                CMI_i=CMI_tot\n",
    "            ), \n",
    "            index=['Full Outage']\n",
    "        )\n",
    "\n",
    "        # Add net_outg_row to beginning of return_df\n",
    "        return_df = pd.concat([net_outg_row, return_df])\n",
    "        return_df.index.name = 'Outage Subset'\n",
    "\n",
    "        #-------------------------\n",
    "        # Include the OUTG_REC_NB and OUTAGE_NB\n",
    "        return_df = Utilities_df.prepend_level_to_MultiIndex(return_df, level_val=outg_rec_nb, level_name='OUTG_REC_NB', axis=0)\n",
    "        return_df = Utilities_df.prepend_level_to_MultiIndex(return_df, level_val=outage_nb, level_name='OUTAGE_NB', axis=0)\n",
    "        \n",
    "    #--------------------------------------------------\n",
    "    # Add n_PNs_ami, n_PNs_dovs, and %\n",
    "    return_df['n_PNs_ami']     = n_PNs_ami\n",
    "    return_df['n_PNs_DOVS']    = n_PNs_dovs\n",
    "    return_df['pct_PNs_found'] = 100*return_df['n_PNs_ami']/return_df['n_PNs_DOVS']\n",
    "    #-------------------------\n",
    "    # Add DOVS\n",
    "    return_df['DT_OFF_TS_FULL']    = dovs_outg_t_beg\n",
    "    return_df['DT_ON_TS']          = dovs_outg_t_end\n",
    "    return_df['CI_NB']             = ci_dovs\n",
    "    return_df['CMI_NB']            = cmi_dovs\n",
    "    return_df['MJR_CAUSE_CD']      = mjr_cause_cd\n",
    "    return_df['MNR_CAUSE_CD']      = mnr_cause_cd\n",
    "    return_df['DVC_TYP_NM']        = dvc_typ_nb\n",
    "    return_df['OPERATING_UNIT_ID'] = opco_id\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    # For new format, I still want 'OUTAGE_NB' and 'OUTG_REC_NB' as indices, but I'm going to move 'Outage Subset'\n",
    "    #   into the columns\n",
    "    # I also want to list all of the DOVS attributes first\n",
    "    return_df=return_df.reset_index(level='Outage Subset')\n",
    "    #-----\n",
    "    return_df = Utilities_df.move_cols_to_front(\n",
    "        return_df, \n",
    "        [\n",
    "            'DT_OFF_TS_FULL', \n",
    "            'DT_ON_TS', \n",
    "            'CI_NB', \n",
    "            'CMI_NB', \n",
    "            'MJR_CAUSE_CD', \n",
    "            'MNR_CAUSE_CD', \n",
    "            'DVC_TYP_NM', \n",
    "            'OPERATING_UNIT_ID', \n",
    "            'n_PNs_DOVS'\n",
    "        ]\n",
    "    )\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d8a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "302ed600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_multiindex_duplicate_idxs_for_csv_output(df):\n",
    "    r\"\"\"\n",
    "    Eliminate duplicate values from MultiIndex index before writing to CSV to make reading by eye easier.\n",
    "    IMPORTANT!!!!! The CSV generated here isn't useful for futher manipulation using code, only for inspection by eye\n",
    "                   Thus, it's intended more for when I deliver results to others, not for use myself\n",
    "    -----\n",
    "    Essentially, instead of the output looking like:\n",
    "        idx0     idx1     col1   col2   col3\n",
    "        a        1        ...    ...    ...     \n",
    "        a        2        ...    ...    ...\n",
    "        b        2        ...    ...    ...\n",
    "        b        3        ...    ...    ...\n",
    "    \n",
    "    It will instead look like:\n",
    "        idx0     idx1     col1   col2   col3\n",
    "        a        1        ...    ...    ... \n",
    "                 2        ...    ...    ...\n",
    "        b        2        ...    ...    ...\n",
    "                 3        ...    ...    ...\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure df has MultiIndex index, and each level has a name\n",
    "    assert(df.index.nlevels>1)\n",
    "    assert((len(df.index.names)==df.index.nlevels))\n",
    "    assert(all([True if x else False for x in df.index.names]))\n",
    "    idx_names = list(df.index.names)\n",
    "    #-------------------------\n",
    "    df = df.reset_index()\n",
    "    #-------------------------\n",
    "    dupl_srs_list = []\n",
    "    for i_idx_lvl in range(len(idx_names)):\n",
    "        dupl_srs_i = df.duplicated(subset=idx_names[:i_idx_lvl+1])\n",
    "        dupl_srs_list.append(dupl_srs_i)\n",
    "    assert(len(dupl_srs_list)==len(idx_names))\n",
    "    #-------------------------\n",
    "    for i_idx_lvl in range(len(idx_names)):\n",
    "        df[idx_names[i_idx_lvl]] = df[idx_names[i_idx_lvl]].where(~dupl_srs_list[i_idx_lvl], '')\n",
    "    #-------------------------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404885f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19cd5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:  THERE EXISTS A NEW VERSSION OF THE BELOW FUNCTION\n",
    "def plot_all_out_not(\n",
    "    fig_num, \n",
    "    ami_df_i, \n",
    "    ami_df_i_out, \n",
    "    ami_df_i_not_out, \n",
    "    dovs_outg_t_beg, \n",
    "    dovs_outg_t_end, \n",
    "    cnsrvtv_out_t_beg, \n",
    "    cnsrvtv_out_t_end, \n",
    "    means_df, \n",
    "    outg_rec_nb, \n",
    "    outage_nb, \n",
    "    n_PNs_dovs, \n",
    "    n_PNs, \n",
    "    n_SNs, \n",
    "    ci_dovs, \n",
    "    cmi_dovs, \n",
    "    ci_ami, \n",
    "    cmi_ami, \n",
    "    ci_ami_dovs_beg, \n",
    "    cmi_ami_dovs_beg, \n",
    "    expand_time=pd.Timedelta('1 hour'), \n",
    "    mean_keys_to_include=['winner', 'conservative', 'zero_times']\n",
    "):\n",
    "    #-------------------------\n",
    "    fig, axs = Plot_General.default_subplots(\n",
    "        n_x=1,\n",
    "        n_y=3,\n",
    "        fig_num=fig_num,\n",
    "        sharex=False,\n",
    "        sharey=False,\n",
    "        unit_figsize_width=14,\n",
    "        unit_figsize_height=6, \n",
    "        return_flattened_axes=True,\n",
    "        row_major=True\n",
    "    )\n",
    "    Plot_General.adjust_subplots_args(fig, dict(hspace=0.30))\n",
    "\n",
    "    palette = Plot_General.get_standard_colors_dict(\n",
    "        keys=ami_df_i['serialnumber'].unique().tolist(), \n",
    "        palette='colorblind'\n",
    "    )\n",
    "\n",
    "    #-------------------------\n",
    "    i_subplot=0\n",
    "    fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "        fig=fig, \n",
    "        ax=axs[i_subplot], \n",
    "        data=ami_df_i, \n",
    "        x='starttimeperiod_local', \n",
    "        y='value', \n",
    "        hue='serialnumber', \n",
    "        out_t_beg=dovs_outg_t_beg, \n",
    "        out_t_end=dovs_outg_t_end, \n",
    "        expand_time=expand_time, \n",
    "        plot_time_beg_end=[cnsrvtv_out_t_beg, cnsrvtv_out_t_end], \n",
    "        data_label='', \n",
    "        title_args=dict(label=f\"All (#SNs = {ami_df_i['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "        ax_args=None, \n",
    "        xlabel_args=None, \n",
    "        ylabel_args=None, \n",
    "        df_mean=None, \n",
    "        df_mean_col=None, \n",
    "        mean_args=None, \n",
    "        draw_outage_limits=True, \n",
    "        draw_outage_limits_kwargs=dict(alpha=1.0, linewidth=5.0, ymax=0.1), \n",
    "        include_outage_limits_text=dict(\n",
    "            out_t_beg_text='DOVS Beg.', \n",
    "            out_t_beg_ypos=(0.12, 'ax_coord'), \n",
    "            out_t_beg_va='bottom', \n",
    "            out_t_beg_ha='center', \n",
    "            out_t_beg_color='red', \n",
    "            #-----\n",
    "            out_t_end_text='DOVS End', \n",
    "            out_t_end_ypos=(0.12, 'ax_coord'), \n",
    "            out_t_end_va='bottom', \n",
    "            out_t_end_ha='center', \n",
    "            out_t_end_color='green', \n",
    "        ), \n",
    "        draw_without_hue_also=False, \n",
    "        seg_line_freq=None, \n",
    "        palette=palette\n",
    "    )\n",
    "    axs[i_subplot].legend().set_visible(False)\n",
    "    Plot_General.set_general_plotting_args(\n",
    "        ax=axs[i_subplot], \n",
    "        tick_args =[\n",
    "            dict(axis='x', labelrotation=0, labelsize=14.0, direction='out'), \n",
    "            dict(axis='y', labelrotation=0, labelsize=14.0, direction='out')\n",
    "        ], \n",
    "        xlabel_args=dict(xlabel=axs[i_subplot].get_xlabel(), fontsize=16), \n",
    "        ylabel_args=dict(ylabel=axs[i_subplot].get_ylabel(), fontsize=16)\n",
    "    )\n",
    "\n",
    "\n",
    "    #-------------------------\n",
    "    i_subplot=1\n",
    "    if ami_df_i_out.shape[0]>0:\n",
    "        fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "            fig=fig, \n",
    "            ax=axs[i_subplot], \n",
    "            data=ami_df_i_out, \n",
    "            x='starttimeperiod_local', \n",
    "            y='value', \n",
    "            hue='serialnumber', \n",
    "            out_t_beg=dovs_outg_t_beg, \n",
    "            out_t_end=dovs_outg_t_end, \n",
    "            expand_time=expand_time, \n",
    "            plot_time_beg_end=[cnsrvtv_out_t_beg, cnsrvtv_out_t_end], \n",
    "            data_label='', \n",
    "            title_args=dict(label=f\"Out (#SNs = {ami_df_i_out['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "            ax_args=None, \n",
    "            xlabel_args=None, \n",
    "            ylabel_args=None, \n",
    "            df_mean=None, \n",
    "            df_mean_col=None, \n",
    "            mean_args=None, \n",
    "            draw_outage_limits=True, \n",
    "            draw_outage_limits_kwargs=dict(alpha=1.0, linewidth=5.0, ymax=0.1), \n",
    "            include_outage_limits_text=dict(\n",
    "                out_t_beg_text='DOVS Beg.', \n",
    "                out_t_beg_ypos=(0.12, 'ax_coord'), \n",
    "                out_t_beg_va='bottom', \n",
    "                out_t_beg_ha='center', \n",
    "                out_t_beg_color='red', \n",
    "                #-----\n",
    "                out_t_end_text='DOVS End', \n",
    "                out_t_end_ypos=(0.12, 'ax_coord'), \n",
    "                out_t_end_va='bottom', \n",
    "                out_t_end_ha='center', \n",
    "                out_t_end_color='green', \n",
    "            ), \n",
    "            draw_without_hue_also=False, \n",
    "            seg_line_freq=None, \n",
    "            palette=palette\n",
    "        )\n",
    "        axs[i_subplot].legend().set_visible(False)\n",
    "        add_all_best_ests_to_axis(\n",
    "            axs[i_subplot], \n",
    "            means_df, \n",
    "            line_kwargs_by_est_key=dict(\n",
    "                conservative=dict(alpha=0.25, linewidth=5.0, ymax=0.6), \n",
    "                zero_times=dict(alpha=0.25, linewidth=5.0, ymax=0.4) \n",
    "            ), \n",
    "            keys_to_include=mean_keys_to_include\n",
    "        )\n",
    "        Plot_General.set_general_plotting_args(\n",
    "            ax=axs[i_subplot], \n",
    "            tick_args =[\n",
    "                dict(axis='x', labelrotation=0, labelsize=14.0, direction='out'), \n",
    "                dict(axis='y', labelrotation=0, labelsize=14.0, direction='out')\n",
    "            ], \n",
    "            xlabel_args=dict(xlabel=axs[i_subplot].get_xlabel(), fontsize=16), \n",
    "            ylabel_args=dict(ylabel=axs[i_subplot].get_ylabel(), fontsize=16)\n",
    "        )\n",
    "    else:\n",
    "        axs[i_subplot].set_title(\n",
    "            label=f'Out', \n",
    "            fontdict=dict(fontsize=24)\n",
    "        )\n",
    "\n",
    "    #-------------------------\n",
    "    i_subplot=2\n",
    "    if ami_df_i_not_out.shape[0]>0:\n",
    "        fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "            fig=fig, \n",
    "            ax=axs[i_subplot], \n",
    "            data=ami_df_i_not_out, \n",
    "            x='starttimeperiod_local', \n",
    "            y='value', \n",
    "            hue='serialnumber', \n",
    "            out_t_beg=dovs_outg_t_beg, \n",
    "            out_t_end=dovs_outg_t_end, \n",
    "            expand_time=expand_time, \n",
    "            plot_time_beg_end=[cnsrvtv_out_t_beg, cnsrvtv_out_t_end], \n",
    "            data_label='', \n",
    "            title_args=dict(label=f\"Not Out (#SNs = {ami_df_i_not_out['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "            ax_args=None, \n",
    "            xlabel_args=None, \n",
    "            ylabel_args=None, \n",
    "            df_mean=None, \n",
    "            df_mean_col=None, \n",
    "            mean_args=None, \n",
    "            draw_outage_limits=True, \n",
    "            draw_outage_limits_kwargs=dict(alpha=1.0, linewidth=5.0, ymax=0.1), \n",
    "            include_outage_limits_text=dict(\n",
    "                out_t_beg_text='DOVS Beg.', \n",
    "                out_t_beg_ypos=(0.12, 'ax_coord'), \n",
    "                out_t_beg_va='bottom', \n",
    "                out_t_beg_ha='center', \n",
    "                out_t_beg_color='red', \n",
    "                #-----\n",
    "                out_t_end_text='DOVS End', \n",
    "                out_t_end_ypos=(0.12, 'ax_coord'), \n",
    "                out_t_end_va='bottom', \n",
    "                out_t_end_ha='center', \n",
    "                out_t_end_color='green', \n",
    "            ), \n",
    "            draw_without_hue_also=False, \n",
    "            seg_line_freq=None, \n",
    "            palette=palette\n",
    "        )\n",
    "        axs[i_subplot].legend().set_visible(False)\n",
    "        Plot_General.set_general_plotting_args(\n",
    "            ax=axs[i_subplot], \n",
    "            tick_args =[\n",
    "                dict(axis='x', labelrotation=0, labelsize=14.0, direction='out'), \n",
    "                dict(axis='y', labelrotation=0, labelsize=14.0, direction='out')\n",
    "            ], \n",
    "            xlabel_args=dict(xlabel=axs[i_subplot].get_xlabel(), fontsize=16), \n",
    "            ylabel_args=dict(ylabel=axs[i_subplot].get_ylabel(), fontsize=16)\n",
    "        )\n",
    "    else:\n",
    "        axs[i_subplot].set_title(label='Not Out', fontdict=dict(fontsize=24))\n",
    "\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    # Add legend to first plot\n",
    "    patch_dovs_beg = Line2D(\n",
    "        [0], [0], color='red', \n",
    "        alpha=1.0, linewidth=5.0, linestyle='-', \n",
    "        label='DOVS Beg.'\n",
    "    )\n",
    "    patch_dovs_end = Line2D(\n",
    "        [0], [0], color='green', \n",
    "        alpha=1.0, linewidth=5.0, linestyle='-', \n",
    "        label='DOVS End'\n",
    "    )\n",
    "    #-----\n",
    "    patch_ui_beg =  Line2D(\n",
    "        [0], [0], color='red', \n",
    "        alpha=1.0, linewidth=5.0, linestyle=':', \n",
    "        label='Beg. Uncertainty Interval'\n",
    "    )\n",
    "    patch_ui_end =  Line2D(\n",
    "        [0], [0], color='green', \n",
    "        alpha=1.0, linewidth=5.0, linestyle=':', \n",
    "        label='End Uncertainty Interval'\n",
    "    )\n",
    "    #-----\n",
    "    patch_best_beg =  Line2D(\n",
    "        [0], [0], color='red', \n",
    "        alpha=1.0, linewidth=1.0, linestyle='--', \n",
    "        label='Best Est. Beg.'\n",
    "    )\n",
    "    patch_best_end =  Line2D(\n",
    "        [0], [0], color='green', \n",
    "        alpha=1.0, linewidth=1.0, linestyle='--', \n",
    "        label='Best Est. End'\n",
    "    )\n",
    "    #-------------------------\n",
    "    if len(set(['conservative', 'zero_times']).difference(mean_keys_to_include))==0:\n",
    "        handles=[patch_dovs_beg, patch_dovs_end, patch_ui_beg, patch_ui_end, patch_best_beg, patch_best_end]\n",
    "    else:\n",
    "        handles=[patch_dovs_beg, patch_dovs_end, patch_best_beg, patch_best_end]\n",
    "    #-------------------------\n",
    "    leg_1 = axs[0].legend(\n",
    "        title=None, \n",
    "        handles=handles, \n",
    "        bbox_to_anchor=(1, 1.025), \n",
    "        loc='upper left', \n",
    "        fontsize=15\n",
    "    )\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    ci_info_fontsize = 20\n",
    "    left_text_x=0.95\n",
    "    shift_text_down = 0\n",
    "\n",
    "    fig.text(left_text_x, 0.745-shift_text_down, f'OUTG_REC_NB: {outg_rec_nb}', fontsize=ci_info_fontsize+4)\n",
    "    fig.text(left_text_x, 0.715-shift_text_down, f\"OUTAGE_NB:     {outage_nb}\", fontsize=ci_info_fontsize+4)\n",
    "\n",
    "    fig.text(left_text_x, 0.675-shift_text_down, f\"#PNs from DOVS = {n_PNs_dovs}\", fontsize=ci_info_fontsize)\n",
    "\n",
    "    fig.text(left_text_x, 0.640-shift_text_down, \"----- Found in AMI -----\", fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.615-shift_text_down, f\"#PNs = {n_PNs}\", fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.590-shift_text_down, f\"#SNs = {n_SNs}\", fontsize=ci_info_fontsize)\n",
    "\n",
    "    fig.text(left_text_x, 0.525-shift_text_down, '-----'*5+'\\nDOVS\\n'+'-----'*5, fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.500-shift_text_down, f'CI    = {ci_dovs}', fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.475-shift_text_down, f'CMI = {np.round(cmi_dovs, decimals=2)}', fontsize=ci_info_fontsize)\n",
    "\n",
    "    fig.text(left_text_x, 0.400-shift_text_down, '-----'*5+'\\nUsing AMI\\n'+'-----'*5, fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.375-shift_text_down, f'CI    = {ci_ami}', fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.350-shift_text_down, f'CMI = {np.round(cmi_ami, decimals=2)}', fontsize=ci_info_fontsize)\n",
    "    #-----\n",
    "    fig.text(\n",
    "        left_text_x, 0.325-shift_text_down, \n",
    "        f'$\\Delta$CI    = {ci_dovs-ci_ami} ({np.round(100*(ci_dovs-ci_ami)/ci_dovs, decimals=2)}%)', \n",
    "        fontsize=ci_info_fontsize\n",
    "    )\n",
    "    fig.text(\n",
    "        left_text_x, 0.300-shift_text_down, \n",
    "        f'$\\Delta$CMI = {np.round(cmi_dovs-cmi_ami, decimals=2)} ({np.round(100*(cmi_dovs-cmi_ami)/cmi_dovs, decimals=2)}%)', \n",
    "        fontsize=ci_info_fontsize\n",
    "    )\n",
    "\n",
    "    fig.text(left_text_x, 0.225-shift_text_down, '-----'*5+'\\nAMI w/ DOVS t_beg\\n'+'-----'*5, fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.200-shift_text_down, f'CI    = {ci_ami_dovs_beg}', fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.175-shift_text_down, f'CMI = {np.round(cmi_ami_dovs_beg, decimals=2)}', fontsize=ci_info_fontsize)\n",
    "    #-----\n",
    "    fig.text(\n",
    "        left_text_x, 0.150-shift_text_down, \n",
    "        f'$\\Delta$CI    = {ci_dovs-ci_ami_dovs_beg} ({np.round(100*(ci_dovs-ci_ami_dovs_beg)/ci_dovs, decimals=2)}%)', \n",
    "        fontsize=ci_info_fontsize\n",
    "    )\n",
    "    fig.text(\n",
    "        left_text_x, 0.125-shift_text_down, \n",
    "        f'$\\Delta$CMI = {np.round(cmi_dovs-cmi_ami_dovs_beg, decimals=2)} ({np.round(100*(cmi_dovs-cmi_ami_dovs_beg)/cmi_dovs, decimals=2)}%)', \n",
    "        fontsize=ci_info_fontsize\n",
    "    )\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b88a3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_out_not_NEW(\n",
    "    fig_num, \n",
    "    ami_df_i, \n",
    "    ami_df_i_out, \n",
    "    ami_df_i_not_out, \n",
    "    dovs_outg_t_beg, \n",
    "    dovs_outg_t_end, \n",
    "    cnsrvtv_out_t_beg, \n",
    "    cnsrvtv_out_t_end, \n",
    "    means_df, \n",
    "    outg_rec_nb, \n",
    "    outage_nb, \n",
    "    n_PNs_dovs, \n",
    "    ci_dovs, \n",
    "    cmi_dovs, \n",
    "    ci_ami, \n",
    "    cmi_ami, \n",
    "    only_connect_continuous=True, \n",
    "    data_freq=pd.Timedelta('15T'),  \n",
    "    name='AMI', \n",
    "    results_2_dict=None, \n",
    "    expand_time=pd.Timedelta('1 hour'), \n",
    "    mean_keys_to_include=['winner', 'conservative', 'zero_times'], \n",
    "    removed_due_to_overlap_col=None, \n",
    "    default_subplots_args=None, \n",
    "    **kwargs\n",
    "):\n",
    "    #-------------------------\n",
    "    if removed_due_to_overlap_col is not None:\n",
    "        assert(removed_due_to_overlap_col in ami_df_i.columns.tolist())\n",
    "        assert(removed_due_to_overlap_col in ami_df_i_out.columns.tolist())\n",
    "        assert(removed_due_to_overlap_col in ami_df_i_not_out.columns.tolist())\n",
    "    #-------------------------\n",
    "    dflt_default_subplots_args = dict(\n",
    "        n_x=1,\n",
    "        n_y=3,\n",
    "        fig_num=fig_num,\n",
    "        sharex=False,\n",
    "        sharey=False,\n",
    "        unit_figsize_width=14,\n",
    "        unit_figsize_height=6, \n",
    "        return_flattened_axes=True,\n",
    "        row_major=True\n",
    "    )\n",
    "    if default_subplots_args is None:\n",
    "        default_subplots_args = dflt_default_subplots_args\n",
    "    else:\n",
    "        assert(isinstance(default_subplots_args, dict))\n",
    "        default_subplots_args = Utilities.supplement_dict_with_default_values(\n",
    "            to_supplmnt_dict=default_subplots_args, \n",
    "            default_values_dict=dflt_default_subplots_args, \n",
    "            extend_any_lists=False, \n",
    "            inplace=False\n",
    "        )\n",
    "    fig, axs = Plot_General.default_subplots(**default_subplots_args)\n",
    "    Plot_General.adjust_subplots_args(fig, dict(hspace=0.30))\n",
    "\n",
    "    palette = Plot_General.get_standard_colors_dict(\n",
    "        keys=ami_df_i['serialnumber'].unique().tolist(), \n",
    "        palette='colorblind'\n",
    "    )\n",
    "\n",
    "    #-------------------------\n",
    "    # The subplots share many common arguments.  \n",
    "    # Let's collect them all first to avoid repeating code\n",
    "    shared_plot_kwargs = dict(\n",
    "        x                          = 'starttimeperiod_local', \n",
    "        y                          = 'value', \n",
    "        hue                        = 'serialnumber', \n",
    "        out_t_beg                  = dovs_outg_t_beg, \n",
    "        out_t_end                  = dovs_outg_t_end, \n",
    "        expand_time                = expand_time, \n",
    "        plot_time_beg_end          = [cnsrvtv_out_t_beg, cnsrvtv_out_t_end], \n",
    "        only_connect_continuous    = only_connect_continuous, \n",
    "        data_freq                  = data_freq,     \n",
    "        data_label                 = '', \n",
    "        ax_args                    = None, \n",
    "        xlabel_args                = None, \n",
    "        ylabel_args                = None, \n",
    "        df_mean                    = None, \n",
    "        df_mean_col                = None, \n",
    "        mean_args                  = None, \n",
    "        draw_outage_limits         = True, \n",
    "        draw_outage_limits_kwargs  = dict(alpha=1.0, linewidth=5.0, ymax=0.1), \n",
    "        include_outage_limits_text = dict(\n",
    "            out_t_beg_text  = 'DOVS Beg.', \n",
    "            out_t_beg_ypos  = (0.12, 'ax_coord'), \n",
    "            out_t_beg_va    = 'bottom', \n",
    "            out_t_beg_ha    = 'center', \n",
    "            out_t_beg_color = 'red', \n",
    "            #-----\n",
    "            out_t_end_text  = 'DOVS End', \n",
    "            out_t_end_ypos  = (0.12, 'ax_coord'), \n",
    "            out_t_end_va    = 'bottom', \n",
    "            out_t_end_ha    = 'center', \n",
    "            out_t_end_color = 'green', \n",
    "        ), \n",
    "        draw_without_hue_also      = False, \n",
    "        seg_line_freq              = None, \n",
    "        palette                    = palette\n",
    "    )\n",
    "    \n",
    "    #-------------------------\n",
    "    i_subplot=0\n",
    "    if removed_due_to_overlap_col is None:\n",
    "        fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "            fig=fig, \n",
    "            ax=axs[i_subplot], \n",
    "            data=ami_df_i, \n",
    "            title_args=dict(label=f\"All (#SNs = {ami_df_i['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "            **shared_plot_kwargs\n",
    "        )\n",
    "    else:\n",
    "        fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "            fig=fig, \n",
    "            ax=axs[i_subplot], \n",
    "            data=ami_df_i[ami_df_i[removed_due_to_overlap_col]==False], \n",
    "            title_args=dict(label=f\"All (#SNs = {ami_df_i['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "            **shared_plot_kwargs\n",
    "        )\n",
    "        #-----\n",
    "        if ami_df_i[ami_df_i[removed_due_to_overlap_col]==True].shape[0]>0:\n",
    "            fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "                fig=fig, \n",
    "                ax=axs[i_subplot], \n",
    "                data=ami_df_i[ami_df_i[removed_due_to_overlap_col]==True], \n",
    "                title_args=dict(label=f\"All (#SNs = {ami_df_i['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "                lineplot_kwargs=dict(linestyle='dotted', alpha=0.5), \n",
    "                **shared_plot_kwargs\n",
    "            )\n",
    "    axs[i_subplot].legend().set_visible(False)\n",
    "    Plot_General.set_general_plotting_args(\n",
    "        ax=axs[i_subplot], \n",
    "        tick_args =[\n",
    "            dict(axis='x', labelrotation=0, labelsize=14.0, direction='out'), \n",
    "            dict(axis='y', labelrotation=0, labelsize=14.0, direction='out')\n",
    "        ], \n",
    "        xlabel_args=dict(xlabel=axs[i_subplot].get_xlabel(), fontsize=16), \n",
    "        ylabel_args=dict(ylabel=axs[i_subplot].get_ylabel(), fontsize=16)\n",
    "    )\n",
    "\n",
    "\n",
    "    #-------------------------\n",
    "    i_subplot=1\n",
    "    if ami_df_i_out.shape[0]>0:\n",
    "        if removed_due_to_overlap_col is None:\n",
    "            fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "                fig=fig, \n",
    "                ax=axs[i_subplot], \n",
    "                data=ami_df_i_out, \n",
    "                title_args=dict(label=f\"Out (#SNs = {ami_df_i_out['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "                **shared_plot_kwargs\n",
    "            )\n",
    "        else:\n",
    "            fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "                fig=fig, \n",
    "                ax=axs[i_subplot], \n",
    "                data=ami_df_i_out[ami_df_i_out[removed_due_to_overlap_col]==False], \n",
    "                title_args=dict(label=f\"Out (#SNs = {ami_df_i_out['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "                **shared_plot_kwargs\n",
    "            )\n",
    "            #-----\n",
    "            if ami_df_i_out[ami_df_i_out[removed_due_to_overlap_col]==True].shape[0]>0:\n",
    "                fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "                    fig=fig, \n",
    "                    ax=axs[i_subplot], \n",
    "                    data=ami_df_i_out[ami_df_i_out[removed_due_to_overlap_col]==True], \n",
    "                    title_args=dict(label=f\"Out (#SNs = {ami_df_i_out['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "                    lineplot_kwargs=dict(linestyle='dashdot', alpha=0.5), \n",
    "                    **shared_plot_kwargs\n",
    "                )\n",
    "        axs[i_subplot].legend().set_visible(False)\n",
    "        add_all_best_ests_to_axis(\n",
    "            axs[i_subplot], \n",
    "            means_df, \n",
    "            line_kwargs_by_est_key=dict(\n",
    "                conservative=dict(alpha=0.25, linewidth=5.0, ymax=0.6), \n",
    "                zero_times=dict(alpha=0.25, linewidth=5.0, ymax=0.4) \n",
    "            ), \n",
    "            keys_to_include=mean_keys_to_include\n",
    "        )\n",
    "        Plot_General.set_general_plotting_args(\n",
    "            ax=axs[i_subplot], \n",
    "            tick_args =[\n",
    "                dict(axis='x', labelrotation=0, labelsize=14.0, direction='out'), \n",
    "                dict(axis='y', labelrotation=0, labelsize=14.0, direction='out')\n",
    "            ], \n",
    "            xlabel_args=dict(xlabel=axs[i_subplot].get_xlabel(), fontsize=16), \n",
    "            ylabel_args=dict(ylabel=axs[i_subplot].get_ylabel(), fontsize=16)\n",
    "        )\n",
    "    else:\n",
    "        axs[i_subplot].set_title(\n",
    "            label=f'Out', \n",
    "            fontdict=dict(fontsize=24)\n",
    "        )\n",
    "\n",
    "    #-------------------------\n",
    "    i_subplot=2\n",
    "    if ami_df_i_not_out.shape[0]>0:\n",
    "        if removed_due_to_overlap_col is None:\n",
    "            fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "                fig=fig, \n",
    "                ax=axs[i_subplot], \n",
    "                data=ami_df_i_not_out, \n",
    "                title_args=dict(label=f\"Not Out (#SNs = {ami_df_i_not_out['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "                **shared_plot_kwargs\n",
    "            )\n",
    "        else:\n",
    "            fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "                fig=fig, \n",
    "                ax=axs[i_subplot], \n",
    "                data=ami_df_i_not_out[ami_df_i_not_out[removed_due_to_overlap_col]==False], \n",
    "                title_args=dict(label=f\"Not Out (#SNs = {ami_df_i_not_out['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "                **shared_plot_kwargs\n",
    "            )\n",
    "            #-----\n",
    "            if ami_df_i_not_out[ami_df_i_not_out[removed_due_to_overlap_col]==True].shape[0]>0:\n",
    "                fig, axs[i_subplot] = AMINonVee.plot_usage_around_outage(\n",
    "                    fig=fig, \n",
    "                    ax=axs[i_subplot], \n",
    "                    data=ami_df_i_not_out[ami_df_i_not_out[removed_due_to_overlap_col]==True], \n",
    "                    title_args=dict(label=f\"Not Out (#SNs = {ami_df_i_not_out['serialnumber'].nunique()})\", fontdict=dict(fontsize=24)), \n",
    "                    lineplot_kwargs=dict(linestyle='dashdot'), \n",
    "                    **shared_plot_kwargs\n",
    "                )\n",
    "        axs[i_subplot].legend().set_visible(False)\n",
    "        Plot_General.set_general_plotting_args(\n",
    "            ax=axs[i_subplot], \n",
    "            tick_args =[\n",
    "                dict(axis='x', labelrotation=0, labelsize=14.0, direction='out'), \n",
    "                dict(axis='y', labelrotation=0, labelsize=14.0, direction='out')\n",
    "            ], \n",
    "            xlabel_args=dict(xlabel=axs[i_subplot].get_xlabel(), fontsize=16), \n",
    "            ylabel_args=dict(ylabel=axs[i_subplot].get_ylabel(), fontsize=16)\n",
    "        )\n",
    "    else:\n",
    "        axs[i_subplot].set_title(label='Not Out', fontdict=dict(fontsize=24))\n",
    "\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    # Add legend to first plot\n",
    "    patch_dovs_beg = Line2D(\n",
    "        [0], [0], color='red', \n",
    "        alpha=1.0, linewidth=5.0, linestyle='-', \n",
    "        label='DOVS Beg.'\n",
    "    )\n",
    "    patch_dovs_end = Line2D(\n",
    "        [0], [0], color='green', \n",
    "        alpha=1.0, linewidth=5.0, linestyle='-', \n",
    "        label='DOVS End'\n",
    "    )\n",
    "    #-----\n",
    "    patch_ui_beg =  Line2D(\n",
    "        [0], [0], color='red', \n",
    "        alpha=1.0, linewidth=5.0, linestyle=':', \n",
    "        label='Beg. Uncertainty Interval'\n",
    "    )\n",
    "    patch_ui_end =  Line2D(\n",
    "        [0], [0], color='green', \n",
    "        alpha=1.0, linewidth=5.0, linestyle=':', \n",
    "        label='End Uncertainty Interval'\n",
    "    )\n",
    "    #-----\n",
    "    patch_best_beg =  Line2D(\n",
    "        [0], [0], color='red', \n",
    "        alpha=1.0, linewidth=1.0, linestyle='--', \n",
    "        label='Best Est. Beg.'\n",
    "    )\n",
    "    patch_best_end =  Line2D(\n",
    "        [0], [0], color='green', \n",
    "        alpha=1.0, linewidth=1.0, linestyle='--', \n",
    "        label='Best Est. End'\n",
    "    )\n",
    "    #-------------------------\n",
    "    if len(set(['conservative', 'zero_times']).difference(mean_keys_to_include))==0:\n",
    "        handles=[patch_dovs_beg, patch_dovs_end, patch_ui_beg, patch_ui_end, patch_best_beg, patch_best_end]\n",
    "    else:\n",
    "        handles=[patch_dovs_beg, patch_dovs_end, patch_best_beg, patch_best_end]\n",
    "    #-------------------------\n",
    "    leg_i_plot   = kwargs.get('leg_i_plot', 0)\n",
    "    leg_kwargs   =  kwargs.get('leg_kwargs', None)\n",
    "    dflt_leg_kwargs = dict(\n",
    "        title=None, \n",
    "        handles=handles, \n",
    "        bbox_to_anchor=(1, 1.025), \n",
    "        loc='upper left', \n",
    "        fontsize=15\n",
    "    )\n",
    "    if leg_kwargs is None:\n",
    "        leg_kwargs = dflt_leg_kwargs\n",
    "    else:\n",
    "        assert(isinstance(leg_kwargs, dict))\n",
    "        leg_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "            to_supplmnt_dict=leg_kwargs, \n",
    "            default_values_dict=dflt_leg_kwargs, \n",
    "            extend_any_lists=False, \n",
    "            inplace=False\n",
    "        )\n",
    "    leg_1 = axs[leg_i_plot].legend(**leg_kwargs)\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    #--------------------------------------------------\n",
    "    n_PNs = ami_df_i['aep_premise_nb'].nunique()\n",
    "    n_SNs = ami_df_i['serialnumber'].nunique()\n",
    "    #-------------------------\n",
    "    ci_info_fontsize = kwargs.get('ci_info_fontsize', 18)\n",
    "    left_text_x      = kwargs.get('left_text_x', 0.95)\n",
    "    shift_text_down  = kwargs.get('shift_text_down', 0)\n",
    "\n",
    "    fig.text(left_text_x, 0.745-shift_text_down, f'OUTG_REC_NB: {outg_rec_nb}', fontsize=ci_info_fontsize+4)\n",
    "    fig.text(left_text_x, 0.720-shift_text_down, f\"OUTAGE_NB:     {outage_nb}\", fontsize=ci_info_fontsize+4)\n",
    "\n",
    "    fig.text(left_text_x, 0.685-shift_text_down, f\"#PNs from DOVS = {n_PNs_dovs}\", fontsize=ci_info_fontsize)\n",
    "\n",
    "    fig.text(left_text_x, 0.660-shift_text_down, \"----- Found in AMI -----\", fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.635-shift_text_down, f\"#PNs = {n_PNs}\", fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.610-shift_text_down, f\"#SNs = {n_SNs}\", fontsize=ci_info_fontsize)\n",
    "\n",
    "    fig.text(left_text_x, 0.550-shift_text_down, '-----'*5+'\\nDOVS\\n'+'-----'*5, fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.530-shift_text_down, f'CI    = {ci_dovs}', fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.510-shift_text_down, f'CMI = {np.round(cmi_dovs, decimals=2)}', fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.490-shift_text_down, f'Beg. = {dovs_outg_t_beg.strftime(\"%m/%d %H:%M:%S\")}', fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.470-shift_text_down, f'End  = {dovs_outg_t_end.strftime(\"%m/%d %H:%M:%S\")}', fontsize=ci_info_fontsize)\n",
    "\n",
    "    fig.text(left_text_x, 0.410-shift_text_down, '-----'*5+'\\n{}\\n'.format(name)+'-----'*5, fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.390-shift_text_down, f'CI    = {ci_ami}', fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.370-shift_text_down, f'CMI = {np.round(cmi_ami, decimals=2)}', fontsize=ci_info_fontsize)\n",
    "    #-----\n",
    "    fig.text(\n",
    "        left_text_x, 0.350-shift_text_down, \n",
    "        f'$\\Delta$CI    = {ci_dovs-ci_ami} ({np.round(100*(ci_dovs-ci_ami)/ci_dovs, decimals=2)}%)', \n",
    "        fontsize=ci_info_fontsize\n",
    "    )\n",
    "    fig.text(\n",
    "        left_text_x, 0.330-shift_text_down, \n",
    "        f'$\\Delta$CMI = {np.round(cmi_dovs-cmi_ami, decimals=2)} ({np.round(100*(cmi_dovs-cmi_ami)/cmi_dovs, decimals=2)}%)', \n",
    "        fontsize=ci_info_fontsize\n",
    "    )\n",
    "    fig.text(left_text_x, 0.310-shift_text_down, f'min(Beg.) = {means_df[\"winner_min\"].min().strftime(\"%m/%d %H:%M:%S\")}', fontsize=ci_info_fontsize)\n",
    "    fig.text(left_text_x, 0.290-shift_text_down, f'max(End) = {means_df[\"winner_max\"].max().strftime(\"%m/%d %H:%M:%S\")}', fontsize=ci_info_fontsize)\n",
    "\n",
    "    if results_2_dict is not None:\n",
    "        assert(isinstance(results_2_dict, dict))\n",
    "        assert(set(results_2_dict.keys()).difference(set(['ci_ami', 'cmi_ami', 'means_df', 'name']))==set())\n",
    "        #-----\n",
    "        fig.text(left_text_x, 0.230-shift_text_down, '-----'*5+'\\n{}\\n'.format(results_2_dict['name'])+'-----'*5, fontsize=ci_info_fontsize)\n",
    "        fig.text(left_text_x, 0.210-shift_text_down, f'CI    = {results_2_dict[\"ci_ami\"]}', fontsize=ci_info_fontsize)\n",
    "        fig.text(left_text_x, 0.190-shift_text_down, f'CMI = {np.round(results_2_dict[\"cmi_ami\"], decimals=2)}', fontsize=ci_info_fontsize)\n",
    "        #-----\n",
    "        fig.text(\n",
    "            left_text_x, 0.170-shift_text_down, \n",
    "            f'$\\Delta$CI    = {ci_dovs-results_2_dict[\"ci_ami\"]} ({np.round(100*(ci_dovs-results_2_dict[\"ci_ami\"])/ci_dovs, decimals=2)}%)', \n",
    "            fontsize=ci_info_fontsize\n",
    "        )\n",
    "        fig.text(\n",
    "            left_text_x, 0.150-shift_text_down, \n",
    "            f'$\\Delta$CMI = {np.round(cmi_dovs-results_2_dict[\"cmi_ami\"], decimals=2)} ({np.round(100*(cmi_dovs-results_2_dict[\"cmi_ami\"])/cmi_dovs, decimals=2)}%)', \n",
    "            fontsize=ci_info_fontsize\n",
    "        )\n",
    "        if results_2_dict[\"means_df\"] is not None:\n",
    "            fig.text(left_text_x, 0.130-shift_text_down, f'min(Beg.) = {results_2_dict[\"means_df\"][\"winner_min\"].min().strftime(\"%m/%d %H:%M:%S\")}', fontsize=ci_info_fontsize)\n",
    "            fig.text(left_text_x, 0.110-shift_text_down, f'max(End) = {results_2_dict[\"means_df\"][\"winner_max\"].max().strftime(\"%m/%d %H:%M:%S\")}', fontsize=ci_info_fontsize)\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4b0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903c7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f1d683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_suboutg_endpts(\n",
    "    fig_num, \n",
    "    ami_df_i, \n",
    "    means_df, \n",
    "    best_ests_df_w_db_lbl, \n",
    "    dovs_outg_t_beg, \n",
    "    dovs_outg_t_end, \n",
    "    outg_rec_nb, \n",
    "    expand_time=pd.Timedelta('15 minutes'), \n",
    "    mean_keys_to_include=['winner', 'conservative', 'zero_times']\n",
    "):\n",
    "    #-------------------------\n",
    "    means_df = means_df.sort_values(by=['winner_min', 'winner_max'])\n",
    "    fig, axs = Plot_General.default_subplots(\n",
    "        n_x=2, \n",
    "        n_y=means_df.shape[0], \n",
    "        fig_num=fig_num\n",
    "    )\n",
    "    if means_df.shape[0]==1:\n",
    "        axs = [axs]\n",
    "    Plot_General.adjust_subplots_args(fig, dict(hspace=0.30))\n",
    "\n",
    "    palette = Plot_General.get_standard_colors_dict(\n",
    "        keys=ami_df_i['serialnumber'].unique().tolist(), \n",
    "        palette='colorblind'\n",
    "    )\n",
    "\n",
    "    #-------------------------\n",
    "    for i_row in range(means_df.shape[0]):\n",
    "        db_label = means_df.iloc[i_row].name\n",
    "        ami_df_i_subset = ami_df_i[ami_df_i['aep_premise_nb'].isin(\n",
    "            best_ests_df_w_db_lbl[best_ests_df_w_db_lbl['db_label']==db_label]['PN'].tolist()\n",
    "        )]\n",
    "        n_SNs = ami_df_i_subset['serialnumber'].nunique()\n",
    "        #****************************************\n",
    "        fig, axs[i_row][0] = AMINonVee.plot_usage_around_outage(\n",
    "            fig=fig, \n",
    "            ax=axs[i_row][0], \n",
    "            data=ami_df_i_subset, \n",
    "            x='starttimeperiod_local', \n",
    "            y='value', \n",
    "            hue='serialnumber', \n",
    "            out_t_beg=dovs_outg_t_beg, \n",
    "            out_t_end=dovs_outg_t_end, \n",
    "            expand_time=pd.Timedelta('15 minutes'), \n",
    "            plot_time_beg_end=[means_df.iloc[i_row]['conservative_min'], means_df.iloc[i_row]['zero_times_min']], \n",
    "            data_label='', \n",
    "            title_args=None, \n",
    "            ax_args=None, \n",
    "            xlabel_args=None, \n",
    "            ylabel_args=None, \n",
    "            df_mean=None, \n",
    "            df_mean_col=None, \n",
    "            mean_args=None, \n",
    "            draw_outage_limits=True, \n",
    "            draw_outage_limits_kwargs=dict(alpha=1.0, linewidth=5.0, ymax=0.1), \n",
    "            include_outage_limits_text=dict(\n",
    "                out_t_beg_text='DOVS Beg.', \n",
    "                out_t_beg_ypos=(0.12, 'ax_coord'), \n",
    "                out_t_beg_va='bottom', \n",
    "                out_t_beg_ha='center', \n",
    "                out_t_beg_color='red', \n",
    "                #-----\n",
    "                out_t_end_text='DOVS End', \n",
    "                out_t_end_ypos=(0.12, 'ax_coord'), \n",
    "                out_t_end_va='bottom', \n",
    "                out_t_end_ha='center', \n",
    "                out_t_end_color='green', \n",
    "            ),\n",
    "            draw_without_hue_also=False, \n",
    "            seg_line_freq=None, \n",
    "            palette=palette\n",
    "        )\n",
    "        axs[i_row][0].legend().set_visible(False)\n",
    "        add_all_best_ests_to_axis(\n",
    "            axs[i_row][0], \n",
    "            means_df.iloc[[i_row]], \n",
    "            line_kwargs_by_est_key=dict(\n",
    "                conservative=dict(alpha=0.25, linewidth=5.0, ymax=0.6), \n",
    "                zero_times=dict(alpha=0.25, linewidth=5.0, ymax=0.4) \n",
    "            ), \n",
    "            keys_to_include=mean_keys_to_include, \n",
    "            expand_ax_to_accommodate=False\n",
    "        )\n",
    "        axs[i_row][0].text(0.85, 0.9, f'#SNs = {n_SNs}', ha='center', va='center', transform=axs[i_row][0].transAxes, fontsize='xx-large')\n",
    "        Plot_General.set_general_plotting_args(\n",
    "            ax=axs[i_row][0], \n",
    "            tick_args =[\n",
    "                dict(axis='x', labelrotation=0, labelsize='large', direction='out'), \n",
    "                dict(axis='y', labelrotation=0, labelsize='large', direction='out')\n",
    "            ], \n",
    "            xlabel_args=dict(xlabel=axs[i_row][0].get_xlabel(), fontsize='xx-large'), \n",
    "            ylabel_args=dict(ylabel=axs[i_row][0].get_ylabel(), fontsize='xx-large')\n",
    "        )\n",
    "        #****************************************\n",
    "        fig, axs[i_row][1] = AMINonVee.plot_usage_around_outage(\n",
    "            fig=fig, \n",
    "            ax=axs[i_row][1], \n",
    "            data=ami_df_i_subset, \n",
    "            x='starttimeperiod_local', \n",
    "            y='value', \n",
    "            hue='serialnumber', \n",
    "            out_t_beg=dovs_outg_t_beg, \n",
    "            out_t_end=dovs_outg_t_end, \n",
    "            expand_time=pd.Timedelta('15 minutes'), \n",
    "            plot_time_beg_end=[means_df.iloc[i_row]['zero_times_max'], means_df.iloc[i_row]['conservative_max']], \n",
    "            data_label='', \n",
    "            title_args=None, \n",
    "            ax_args=None, \n",
    "            xlabel_args=None, \n",
    "            ylabel_args=None, \n",
    "            df_mean=None, \n",
    "            df_mean_col=None, \n",
    "            mean_args=None, \n",
    "            draw_outage_limits=True, \n",
    "            draw_outage_limits_kwargs=dict(alpha=1.0, linewidth=5.0, ymax=0.1), \n",
    "            include_outage_limits_text=dict(\n",
    "                out_t_beg_text='DOVS Beg.', \n",
    "                out_t_beg_ypos=(0.12, 'ax_coord'), \n",
    "                out_t_beg_va='bottom', \n",
    "                out_t_beg_ha='center', \n",
    "                out_t_beg_color='red', \n",
    "                #-----\n",
    "                out_t_end_text='DOVS End', \n",
    "                out_t_end_ypos=(0.12, 'ax_coord'), \n",
    "                out_t_end_va='bottom', \n",
    "                out_t_end_ha='center', \n",
    "                out_t_end_color='green', \n",
    "            ),\n",
    "            draw_without_hue_also=False, \n",
    "            seg_line_freq=None, \n",
    "            palette=palette\n",
    "        )\n",
    "        axs[i_row][1].legend().set_visible(False)\n",
    "        add_all_best_ests_to_axis(\n",
    "            axs[i_row][1], \n",
    "            means_df.iloc[[i_row]], \n",
    "            line_kwargs_by_est_key=dict(\n",
    "                conservative=dict(alpha=0.25, linewidth=5.0, ymax=0.6), \n",
    "                zero_times=dict(alpha=0.25, linewidth=5.0, ymax=0.4) \n",
    "            ), \n",
    "            keys_to_include=mean_keys_to_include, \n",
    "            expand_ax_to_accommodate=False\n",
    "        )\n",
    "        axs[i_row][1].text(0.15, 0.9, f'#SNs = {n_SNs}', ha='center', va='center', transform=axs[i_row][1].transAxes, fontsize='xx-large')\n",
    "        Plot_General.set_general_plotting_args(\n",
    "            ax=axs[i_row][1], \n",
    "            tick_args =[\n",
    "                dict(axis='x', labelrotation=0, labelsize='large', direction='out'), \n",
    "                dict(axis='y', labelrotation=0, labelsize='large', direction='out')\n",
    "            ], \n",
    "            xlabel_args=dict(xlabel=axs[i_row][1].get_xlabel(), fontsize='xx-large'), \n",
    "            ylabel_args=dict(ylabel=axs[i_row][1].get_ylabel(), fontsize='xx-large')\n",
    "        )\n",
    "    #-------------------------\n",
    "    #--------------------------------------------------\n",
    "    # Add legend to first row\n",
    "    patch_dovs_beg = Line2D(\n",
    "        [0], [0], color='red', \n",
    "        alpha=1.0, linewidth=5.0, linestyle='-', \n",
    "        label='DOVS Beg.'\n",
    "    )\n",
    "    patch_dovs_end = Line2D(\n",
    "        [0], [0], color='green', \n",
    "        alpha=1.0, linewidth=5.0, linestyle='-', \n",
    "        label='DOVS End'\n",
    "    )\n",
    "    #-----\n",
    "    patch_ui_beg =  Line2D(\n",
    "        [0], [0], color='red', \n",
    "        alpha=1.0, linewidth=5.0, linestyle=':', \n",
    "        label='Beg. Uncertainty Interval'\n",
    "    )\n",
    "    patch_ui_end =  Line2D(\n",
    "        [0], [0], color='green', \n",
    "        alpha=1.0, linewidth=5.0, linestyle=':', \n",
    "        label='End Uncertainty Interval'\n",
    "    )\n",
    "    #-----\n",
    "    patch_best_beg =  Line2D(\n",
    "        [0], [0], color='red', \n",
    "        alpha=1.0, linewidth=1.0, linestyle='--', \n",
    "        label='Best Est. Beg.'\n",
    "    )\n",
    "    patch_best_end =  Line2D(\n",
    "        [0], [0], color='green', \n",
    "        alpha=1.0, linewidth=1.0, linestyle='--', \n",
    "        label='Best Est. End'\n",
    "    )\n",
    "    #-------------------------\n",
    "    if len(set(['conservative', 'zero_times']).difference(mean_keys_to_include))==0:\n",
    "        handles=[patch_dovs_beg, patch_dovs_end, patch_ui_beg, patch_ui_end, patch_best_beg, patch_best_end]\n",
    "    else:\n",
    "        handles=[patch_dovs_beg, patch_dovs_end, patch_best_beg, patch_best_end]\n",
    "    #-------------------------\n",
    "    leg_1 = axs[0][1].legend(\n",
    "        title=None, \n",
    "        handles=handles, \n",
    "        bbox_to_anchor=(1, 1.025), \n",
    "        loc='upper left', \n",
    "        fontsize=15\n",
    "    )                \n",
    "    #-------------------------\n",
    "    fig.suptitle(f\"OUTG_REC_NB: {outg_rec_nb}\", y=0.95, fontsize='xx-large')\n",
    "    #-------------------------\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2d33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58588ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aeacaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_i_outg_helper(\n",
    "    best_ests_df_i, \n",
    "    sort_cols=['winner_min', 'winner_max'], \n",
    "    i_outg_col='i_outg'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Intended only for use within set_i_outg_in_best_ests_df.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    return_df=best_ests_df_i.sort_values(by=sort_cols)\n",
    "    return_df[i_outg_col]=list(range(return_df.shape[0]))\n",
    "    #-------------------------\n",
    "    return return_df\n",
    "\n",
    "def set_i_outg_in_best_ests_df(\n",
    "    best_ests_df, \n",
    "    groupby_cols=['PN'], \n",
    "    sort_cols=['winner_min', 'winner_max'], \n",
    "    i_outg_col='i_outg'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure all the needed columns are contained in best_ests_df\n",
    "    assert(set(groupby_cols+sort_cols).difference(set(best_ests_df.columns.tolist()))==set())\n",
    "    #-----\n",
    "    assert(isinstance(groupby_cols, list))\n",
    "    assert(isinstance(sort_cols, list))\n",
    "    #-------------------------\n",
    "    if i_outg_col not in best_ests_df.columns.tolist():\n",
    "        best_ests_df[i_outg_col] = None\n",
    "    #-------------------------\n",
    "    return_df = best_ests_df.groupby(groupby_cols, as_index=False, group_keys=False).apply(\n",
    "        lambda x: set_i_outg_helper(\n",
    "            best_ests_df_i = x, \n",
    "            sort_cols      = sort_cols, \n",
    "            i_outg_col     = i_outg_col\n",
    "        )\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Make sure all values set\n",
    "    assert(return_df[i_outg_col].isna().sum()==0)\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f42338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_overlapping_dovs(\n",
    "    PNs, \n",
    "    outg_t_beg, \n",
    "    outg_t_end, \n",
    "    dovs_sql_fcn=DOVSOutages_SQL.build_sql_std_outage, \n",
    "    addtnl_dovs_sql_kwargs=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    Find any DOVS events which potentially overlap with the premise numbers in PNs during\n",
    "    the time period defined by outg_t_beg and outg_t_end.\n",
    "    \n",
    "    NOTE: outg_t_beg/_end can come from a DOVS events, or, e.g., from best_ests_df[t_min_col].min() and\n",
    "          best_ests_df[t_max_col].max()\n",
    "    -------------------------      \n",
    "    To find potential DOVS outages overlapping with our findings, for the given PNs, collect all DOVS events whose \n",
    "      ending (dt_on_ts) is greater than (or equal to) our earliest estimate \n",
    "      and beginning (dt_off_ts_full) is less than (or equal to) our latest estimate\n",
    "    It may be easier to think about the logic from the standpoint of which DOVS events to EXCLUDE:\n",
    "      Any DOVS event with ending (dt_on_ts) less than our earliest estimate clearly finished after our window\n",
    "        and therefore does not overlap\n",
    "      Any DOVS event with beginning (dt_off_ts_full) greater than our latest estimate clearly began after our\n",
    "       window and therefore does not overlap.\n",
    "    -----\n",
    "    ==>\n",
    "        A DOVS event is considered overlapping if both of the following are True:\n",
    "            - it begins before the present outage ends\n",
    "                - i.e., outg_t_beg_i <= outg_t_end (or, dt_off_ts_full_i <= outg_t_end)\n",
    "            - it ends after the present outage ends\n",
    "                - i.e., out_t_end_i >= out_t_beg   (or, dt_on_ts >= out_t_beg)\n",
    "    -------------------------            \n",
    "    addtnl_dovs_sql_kwargs:\n",
    "        Any additional arguments to use in build_sql_function_kwargs when building dovs_df\n",
    "        NOTE: premise_nbs, field_to_split, dt_on_ts, and dt_off_ts_full are handled by the function and \n",
    "                should therefore NOT be included in addtnl_dovs_sql_kwargs\n",
    "              These will be set as:\n",
    "                - premise_nbs (from the PNs argument)\n",
    "                - field_to_split (set to 'premise_nbs')\n",
    "                - dt_on_ts (using the outg_t_beg argument)\n",
    "                  -- See above if confused why dt_on_ts paired with outg_t_beg\n",
    "                - dt_off_ts_full (using outg_t_end argument)\n",
    "                  -- See above if confused why dt_off_ts_full paired with outg_t_end\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(isinstance(PNs, list))\n",
    "    #-------------------------\n",
    "    dflt_dovs_sql_kwargs = dict(\n",
    "        premise_nbs=PNs, \n",
    "        field_to_split='premise_nbs', \n",
    "        dt_on_ts=dict(\n",
    "            value=outg_t_beg, \n",
    "            comparison_operator='>='\n",
    "        ), \n",
    "        dt_off_ts_full=dict(\n",
    "            value=outg_t_end, \n",
    "            comparison_operator='<='\n",
    "        )        \n",
    "    )\n",
    "    #-----\n",
    "    if addtnl_dovs_sql_kwargs is None:\n",
    "        dovs_sql_kwargs = dflt_dovs_sql_kwargs\n",
    "    else:\n",
    "        assert(isinstance(addtnl_dovs_sql_kwargs, dict))\n",
    "        #-----\n",
    "        # Make sure none of the dovs_sql_kwargs handled by this function are included in addtnl_dovs_sql_kwargs\n",
    "        assert(set(dflt_dovs_sql_kwargs.keys()).intersection(set(addtnl_dovs_sql_kwargs.keys()))==set())\n",
    "        #-----\n",
    "        dovs_sql_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "            to_supplmnt_dict    = dflt_dovs_sql_kwargs, \n",
    "            default_values_dict = addtnl_dovs_sql_kwargs, \n",
    "            extend_any_lists    = True, \n",
    "            inplace             = False\n",
    "        )\n",
    "    #-------------------------\n",
    "    dovs = DOVSOutages(\n",
    "        df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "        contstruct_df_args=None, \n",
    "        init_df_in_constructor=True,\n",
    "        build_sql_function=dovs_sql_fcn, \n",
    "        build_sql_function_kwargs=dovs_sql_kwargs\n",
    "    )\n",
    "    dovs_df = dovs.df.copy()\n",
    "    #-------------------------\n",
    "    return dovs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a78ff73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outgs_in_dovs_df_overlapping_interval(\n",
    "    t_min, \n",
    "    t_max, \n",
    "    dovs_df, \n",
    "    outg_rec_nb_i, \n",
    "    outg_rec_nb_col='OUTG_REC_NB', \n",
    "    t_min_col='DT_OFF_TS_FULL', \n",
    "    t_max_col='DT_ON_TS'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given t_min, t_max, and dovs_df, find any entries in dovs_df which overlap with the interval [t_min, t_max].\n",
    "    \n",
    "    t_min,t_max:\n",
    "        Define the interval during which to look for overlapping DOVS events\n",
    "    \n",
    "    dovs_df:\n",
    "        pd.DataFrame containing DOVS outages\n",
    "        There MUST ONLY BE one single row per outg_rec_nb\n",
    "        \n",
    "    outg_rec_nb_i:\n",
    "        If supplied, outg_rec_nb_i is excluded from any found overlapping DOVS events.\n",
    "        In this case, it is assumed t_min,t_max and outg_rec_nb_i are related, and therefore one does not\n",
    "          want to include outg_rec_nb_i in the final results, because, of course, it trivially overlaps with\n",
    "          itself!\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure needed columns are contained in dovs_df\n",
    "    assert(set([outg_rec_nb_col, t_min_col, t_max_col]).difference(set(dovs_df.columns.tolist()))==set())\n",
    "    #-----\n",
    "    # There should only be a single row for each outg_rec_nb\n",
    "    assert(dovs_df[outg_rec_nb_col].nunique()==dovs_df.shape[0])\n",
    "    #-------------------------\n",
    "    # Grab the interval\n",
    "    interval_1 = pd.Interval(t_min, t_max)\n",
    "    #-------------------------\n",
    "    # For each row (i.e., for each outage), check whether overlaps with interval_1\n",
    "    # Store that information in tmp_overlap_col\n",
    "    tmp_overlap_col = Utilities.generate_random_string()\n",
    "    dovs_df[tmp_overlap_col] = dovs_df.apply(\n",
    "        lambda x: pd.Interval(x[t_min_col], x[t_max_col]).overlaps(interval_1), \n",
    "        axis=1\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Collect the outg_rec_nbs which overlap with interval_1\n",
    "    overlap_outg_rec_nbs = dovs_df[dovs_df[tmp_overlap_col]==True][outg_rec_nb_col].unique().tolist()\n",
    "    # If outg_rec_nb_i was supplied, then ensure it is excluded from overlap_outg_rec_nbs\n",
    "    if outg_rec_nb_i is not None:\n",
    "        overlap_outg_rec_nbs = [x for x in overlap_outg_rec_nbs if x!=outg_rec_nb_i]\n",
    "    #-------------------------\n",
    "    # Drop tmp_overlap_col\n",
    "    dovs_df = dovs_df.drop(columns=[tmp_overlap_col])    \n",
    "    #-------------------------\n",
    "    return overlap_outg_rec_nbs\n",
    "\n",
    "\n",
    "def get_outgs_in_dovs_df_overlapping_outg_rec_nb_i(\n",
    "    outg_rec_nb_i, \n",
    "    dovs_df, \n",
    "    outg_rec_nb_col='OUTG_REC_NB', \n",
    "    t_min_col='DT_OFF_TS_FULL', \n",
    "    t_max_col='DT_ON_TS'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given outg_rec_nb_i and dovs_df, find any entries in dovs_df which overlap with outg_rec_nb_i.\n",
    "    If outg_rec_nb_i is contained in dovs_df, simply extract on/off times\n",
    "    If outg_rec_nb_i is NOT contained in dovs_df, run SQL query to retrieve info\n",
    "    \n",
    "    dovs_df:\n",
    "        pd.DataFrame containing DOVS outages\n",
    "        There MUST ONLY BE one single row per outg_rec_nb\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure needed columns are contained in dovs_df\n",
    "    assert(set([outg_rec_nb_col, t_min_col, t_max_col]).difference(set(dovs_df.columns.tolist()))==set())\n",
    "    #-----\n",
    "    # There should only be a single row for each outg_rec_nb\n",
    "    assert(dovs_df[outg_rec_nb_col].nunique()==dovs_df.shape[0])\n",
    "    #-------------------------\n",
    "    if outg_rec_nb_i in dovs_df[outg_rec_nb_col].unique().tolist():\n",
    "        dovs_df_i = dovs_df[dovs_df[outg_rec_nb_col]==outg_rec_nb_i].copy()\n",
    "    else:\n",
    "        # Build dovs_df_i\n",
    "        dovs_i = DOVSOutages(\n",
    "            df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "            contstruct_df_args=None, \n",
    "            init_df_in_constructor=True,\n",
    "            build_sql_function=DOVSOutages_SQL.build_sql_outage, \n",
    "            build_sql_function_kwargs=dict(\n",
    "                outg_rec_nbs=[outg_rec_nb_i], \n",
    "            )\n",
    "        )\n",
    "        dovs_df_i = dovs_i.df.copy()\n",
    "        #-------------------------\n",
    "        if outg_rec_nb_col not in dovs_df_i.columns.tolist():\n",
    "            assert('OUTG_REC_NB' in dovs_df_i.columns.tolist())\n",
    "            dovs_df_i = dovs_df_i.rename(columns={'OUTG_REC_NB':outg_rec_nb_col})\n",
    "        #-----\n",
    "        if t_min_col not in dovs_df_i.columns.tolist():\n",
    "            assert('DT_OFF_TS_FULL' in dovs_df_i.columns.tolist())\n",
    "            dovs_df_i = dovs_df_i.rename(columns={'DT_OFF_TS_FULL':t_min_col})\n",
    "        #-----\n",
    "        if t_max_col not in dovs_df_i.columns.tolist():\n",
    "            assert('DT_ON_TS' in dovs_df_i.columns.tolist())\n",
    "            dovs_df_i = dovs_df_i.rename(columns={'DT_ON_TS':t_max_col})\n",
    "        #-------------------------\n",
    "    assert(dovs_df_i.shape[0]==1)\n",
    "    #-------------------------\n",
    "    overlap_outg_rec_nbs = get_outgs_in_dovs_df_overlapping_interval(\n",
    "        t_min = dovs_df_i.iloc[0][t_min_col], \n",
    "        t_max = dovs_df_i.iloc[0][t_max_col], \n",
    "        dovs_df = dovs_df, \n",
    "        outg_rec_nb_i = outg_rec_nb_i, \n",
    "        outg_rec_nb_col = outg_rec_nb_col, \n",
    "        t_min_col = t_min_col, \n",
    "        t_max_col = t_max_col\n",
    "    )   \n",
    "    #-------------------------\n",
    "    return overlap_outg_rec_nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d6c3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_overlap_outgs_for_PNs_df_from_srs(\n",
    "    overlap_outgs_for_PNs, \n",
    "    outg_rec_nb_i=None, \n",
    "    best_ests_df=None, \n",
    "    PN_col_best_ests='PN', \n",
    "    return_outg_rec_nb_in_idx=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Both functions get_outgs_in_dovs_df_overlapping_interval_by_PN and get_outgs_in_dovs_df_overlapping_outg_rec_nb_i_by_PN \n",
    "      use these methods, so makes sense to put in function\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(isinstance(overlap_outgs_for_PNs, pd.Series))\n",
    "    overlap_outgs_for_PNs_df = overlap_outgs_for_PNs.to_frame(name='overlap_outg_rec_nbs')\n",
    "    overlap_outgs_for_PNs_df['n_overlap'] = overlap_outgs_for_PNs_df['overlap_outg_rec_nbs'].apply(len)\n",
    "    #-------------------------\n",
    "    if best_ests_df is not None:\n",
    "        assert(\n",
    "            isinstance(best_ests_df, pd.DataFrame) and \n",
    "            PN_col_best_ests in best_ests_df.columns.tolist()\n",
    "        )\n",
    "        #-----\n",
    "        overlap_outgs_for_PNs_df['lost_power'] = False\n",
    "        overlap_outgs_for_PNs_df.loc[\n",
    "            overlap_outgs_for_PNs_df.index.isin(best_ests_df[PN_col_best_ests].unique().tolist()), \n",
    "            'lost_power'\n",
    "        ] = True\n",
    "        #-----\n",
    "        # Re-order columns\n",
    "        overlap_outgs_for_PNs_df=overlap_outgs_for_PNs_df[['lost_power', 'overlap_outg_rec_nbs', 'n_overlap']]\n",
    "    #-------------------------\n",
    "    if return_outg_rec_nb_in_idx and outg_rec_nb_i is not None:\n",
    "        overlap_outgs_for_PNs_df = Utilities_df.prepend_level_to_MultiIndex(\n",
    "            df=overlap_outgs_for_PNs_df, \n",
    "            level_val=outg_rec_nb_i, \n",
    "            level_name='OUTG_REC_NB', \n",
    "            axis=0\n",
    "        )\n",
    "    #-------------------------\n",
    "    return overlap_outgs_for_PNs_df    \n",
    "\n",
    "def get_outgs_in_dovs_df_overlapping_interval_by_PN(\n",
    "    t_min, \n",
    "    t_max, \n",
    "    dovs_df,\n",
    "    outg_rec_nb_i=None, \n",
    "    best_ests_df=None, \n",
    "    outg_rec_nb_col='OUTG_REC_NB', \n",
    "    PN_col='PREMISE_NB', \n",
    "    t_min_col='DT_OFF_TS_FULL', \n",
    "    t_max_col='DT_ON_TS', \n",
    "    PN_col_best_ests='PN', \n",
    "    return_outg_rec_nb_in_idx=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given t_min, t_max, and dovs_df, find any entries in dovs_df which overlap with the interval [t_min, t_max].\n",
    "    \n",
    "    t_min,t_max:\n",
    "        Define the interval during which to look for overlapping DOVS events\n",
    "    \n",
    "    dovs_df:\n",
    "        pd.DataFrame containing DOVS outages\n",
    "        There MUST ONLY BE one single row per outg_rec_nb\n",
    "        \n",
    "    outg_rec_nb_i:\n",
    "        If supplied, outg_rec_nb_i is excluded from any found overlapping DOVS events.\n",
    "        In this case, it is assumed t_min,t_max and outg_rec_nb_i are related, and therefore one does not\n",
    "          want to include outg_rec_nb_i in the final results, because, of course, it trivially overlaps with\n",
    "          itself!\n",
    "          \n",
    "    best_ests_df:\n",
    "        Premises which actually suffer from an outage are found in best_ests_df.\n",
    "        Thus, if the user supplies best_ests_df, then best_ests_df[PN_col_best_ests] will be used to \n",
    "          fill the 'lost_power' column, which is boolean and denotes whether or not the premise lost power.\n",
    "        If best_ests_df is not supplied, then the 'lost_power' field is not included in the output\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    overlap_outgs_for_PNs = dovs_df.groupby(PN_col, as_index=True).apply(\n",
    "        lambda x: get_outgs_in_dovs_df_overlapping_interval(\n",
    "            t_min=t_min, \n",
    "            t_max=t_max, \n",
    "            dovs_df=x, \n",
    "            outg_rec_nb_i=outg_rec_nb_i, \n",
    "            outg_rec_nb_col=outg_rec_nb_col, \n",
    "            t_min_col=t_min_col, \n",
    "            t_max_col=t_max_col\n",
    "        )\n",
    "    )\n",
    "    #-------------------------\n",
    "    overlap_outgs_for_PNs_df = build_overlap_outgs_for_PNs_df_from_srs(\n",
    "        overlap_outgs_for_PNs=overlap_outgs_for_PNs, \n",
    "        outg_rec_nb_i=outg_rec_nb_i, \n",
    "        best_ests_df=best_ests_df, \n",
    "        PN_col_best_ests=PN_col_best_ests, \n",
    "        return_outg_rec_nb_in_idx=return_outg_rec_nb_in_idx\n",
    "    )\n",
    "    #-------------------------\n",
    "    return overlap_outgs_for_PNs_df\n",
    "\n",
    "def get_outgs_in_dovs_df_overlapping_outg_rec_nb_i_by_PN(\n",
    "    outg_rec_nb_i, \n",
    "    dovs_df, \n",
    "    best_ests_df=None, \n",
    "    outg_rec_nb_col='OUTG_REC_NB', \n",
    "    PN_col='PREMISE_NB', \n",
    "    t_min_col='DT_OFF_TS_FULL', \n",
    "    t_max_col='DT_ON_TS', \n",
    "    PN_col_best_ests='PN', \n",
    "    return_outg_rec_nb_in_idx=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given outg_rec_nb_i and dovs_df, find any entries in dovs_df which overlap with outg_rec_nb_i.\n",
    "    \n",
    "    best_ests_df:\n",
    "        Premises which actually suffer from an outage are found in best_ests_df.\n",
    "        Thus, if the user supplies best_ests_df, then best_ests_df[PN_col_best_ests] will be used to \n",
    "          fill the 'lost_power' column, which is boolean and denotes whether or not the premise lost power.\n",
    "        If best_ests_df is not supplied, then the 'lost_power' field is not included in the output\n",
    "    -----\n",
    "    If outg_rec_nb_i is contained in dovs_df, simply extract on/off times\n",
    "    If outg_rec_nb_i is NOT contained in dovs_df, run SQL query to retrieve info\n",
    "    \n",
    "    dovs_df:\n",
    "        pd.DataFrame containing DOVS outages\n",
    "        There MUST ONLY BE one single row per outg_rec_nb\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    overlap_outgs_for_PNs = dovs_df.groupby(PN_col, as_index=True).apply(\n",
    "        lambda x: get_outgs_in_dovs_df_overlapping_outg_rec_nb_i(\n",
    "            outg_rec_nb_i=outg_rec_nb_i, \n",
    "            dovs_df=x, \n",
    "            outg_rec_nb_col=outg_rec_nb_col, \n",
    "            t_min_col=t_min_col, \n",
    "            t_max_col=t_max_col\n",
    "        )\n",
    "    )\n",
    "    #-------------------------\n",
    "    overlap_outgs_for_PNs_df = build_overlap_outgs_for_PNs_df_from_srs(\n",
    "        overlap_outgs_for_PNs=overlap_outgs_for_PNs, \n",
    "        outg_rec_nb_i=outg_rec_nb_i, \n",
    "        best_ests_df=best_ests_df, \n",
    "        PN_col_best_ests=PN_col_best_ests, \n",
    "        return_outg_rec_nb_in_idx=return_outg_rec_nb_in_idx\n",
    "    )\n",
    "    #-------------------------\n",
    "    return overlap_outgs_for_PNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7193fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_dovs_overlaps_for_PN_i(\n",
    "    best_ests_df_i, \n",
    "    dovs_df, \n",
    "    PN_col='PN', \n",
    "    t_min_col='winner_min', \n",
    "    t_max_col='winner_max', \n",
    "    PN_col_dovs='PREMISE_NB', \n",
    "    t_min_col_dovs='DT_OFF_TS_FULL', \n",
    "    t_max_col_dovs='DT_ON_TS', \n",
    "    outg_rec_nb_col_dovs = 'OUTG_REC_NB', \n",
    "    overlap_outg_col='overlap_DOVS', \n",
    "    overlap_times_col='overlap_times', \n",
    "    keep_col='keep'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Built with intention of being used in a .groupby(...).apply(lambda x: ) setting.\n",
    "    So, be careful if using elsewhere.\n",
    "    NOTE: dovs_df MUST NOT CONTAIN the outg_rec_nb for the data contained in best_ests_df_i.\n",
    "            IF IT DOES, then all entries will be identified, because there will always be an overlap!\n",
    "    -----\n",
    "    This is a bit different from the previous functions (e.g., get_outgs_in_dovs_df_overlapping_interval, \n",
    "       get_outgs_in_dovs_df_overlapping_outg_rec_nb_i_by_PN. etc.)\n",
    "    Those are trying to find any other DOVS events whose posted beginning/ending time overlaps with the given\n",
    "       interval/current outages's beg,end times.\n",
    "\n",
    "    Here, we are dealing with the identified sub-outages found for each premise number.\n",
    "    For each premise number:\n",
    "      Determine if any of the identified sub-outages overlap with any of the potentially overlapping DOVS events.\n",
    "      If no DOVS event overlaps with a given sub-outage, the sub-outage is kept.\n",
    "      Which sub-outages to keep is tracked in the keep_srs_i series, which is updated at each iteration over\n",
    "        the potential conflicting outages through keep_boolean_ij.\n",
    "      The DOVS events overlapping the sub-outages are tracked in the overlap_df_i series\n",
    "    Note:  At the end, keep_srs_i should equal overlap_df_i[overlap_outg_col].apply(lambda x: len(x)==0)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Designed for single PN\n",
    "    assert(best_ests_df_i[PN_col].nunique()==1)\n",
    "    PN_i = best_ests_df_i[PN_col].unique().tolist()[0]\n",
    "    #-------------------------\n",
    "    # Make sure overlap_outg_col/overlap_times_col/keep_col aren't already contained in best_ests_df_i\n",
    "    if overlap_outg_col in best_ests_df_i.columns.tolist():\n",
    "        overlap_outg_col = overlap_outg_col + f'_{Utilities.generate_random_string(str_len=4)}'\n",
    "        assert(overlap_outg_col not in best_ests_df_i.columns.tolist())\n",
    "    #-----\n",
    "    if overlap_times_col in best_ests_df_i.columns.tolist():\n",
    "        overlap_times_col = overlap_times_col + f'_{Utilities.generate_random_string(str_len=4)}'\n",
    "        assert(overlap_times_col not in best_ests_df_i.columns.tolist())\n",
    "    #-----\n",
    "    if keep_col in best_ests_df_i.columns.tolist():\n",
    "        keep_col = keep_col + f'_{Utilities.generate_random_string(str_len=4)}'\n",
    "        assert(keep_col not in best_ests_df_i.columns.tolist())\n",
    "    #-------------------------\n",
    "    # Grab relevant entries from dovs_df\n",
    "    dovs_df_i = dovs_df[dovs_df[PN_col_dovs]==PN_i]\n",
    "    \n",
    "    # If no relevant entires, return\n",
    "    if dovs_df_i.shape[0]==0:\n",
    "        return_df = best_ests_df_i.copy()\n",
    "        return_df[overlap_outg_col]  = [[] for _ in range(return_df.shape[0])]\n",
    "        return_df[overlap_times_col] = [[] for _ in range(return_df.shape[0])]\n",
    "        return_df[keep_col]          = True\n",
    "        return return_df\n",
    "\n",
    "    # Initiate the boolean slicing series\n",
    "    keep_srs_i    = pd.Series(data=True, index=best_ests_df_i.index)\n",
    "    overlap_df_i = pd.DataFrame(\n",
    "        data={\n",
    "            overlap_outg_col  : [[] for _ in range(best_ests_df_i.shape[0])], \n",
    "            overlap_times_col : [[] for _ in range(best_ests_df_i.shape[0])]\n",
    "        }, \n",
    "        index=best_ests_df_i.index\n",
    "    )\n",
    "\n",
    "    # Iterate through (possibly) conflicting outages, finding the slicing boolean (keep_boolean_ij) for each\n",
    "    #   and updating the overall slicing boolean (keep_srs_i)\n",
    "    # In general, slicing booleans contain information regarding which sub-outages should be kept for this\n",
    "    #   specific premise number (i.e., each row represents a sub-outage).\n",
    "    for idx_j, cnflct_outg_j in dovs_df_i.iterrows():\n",
    "        # I guess it's actually easier to essentially find those which overlap and then take the logical opposite.\n",
    "        # To find those which don't overlap directly would need two separate and statements, I believe, whereas\n",
    "        #   this only needs one\n",
    "        keep_boolean_ij = ~(\n",
    "            (best_ests_df_i[t_min_col]<=cnflct_outg_j[t_max_col_dovs]) & \n",
    "            (best_ests_df_i[t_max_col]>=cnflct_outg_j[t_min_col_dovs])\n",
    "        )\n",
    "        assert(all(keep_srs_i.index==keep_boolean_ij.index)) # Sanity, not really necessary...\n",
    "        #----------\n",
    "        # If cnflct_outg_j overlaps with any of the entries of best_ests_df_i, add the overlapping outg_rec_nb\n",
    "        #   to the appropriate list in overlap_df_i\n",
    "        # The overlap information is simply ~keep_boolean_ij (because we only keep if non-overlapping)\n",
    "        overlap_df_i.loc[~keep_boolean_ij, overlap_outg_col] = overlap_df_i.loc[~keep_boolean_ij, overlap_outg_col].apply(\n",
    "            lambda x: x+[cnflct_outg_j[outg_rec_nb_col_dovs]]\n",
    "        )\n",
    "        #-----\n",
    "        overlap_df_i.loc[~keep_boolean_ij, overlap_times_col] = overlap_df_i.loc[~keep_boolean_ij, overlap_times_col].apply(\n",
    "            lambda x: x+[(cnflct_outg_j[t_min_col_dovs], cnflct_outg_j[t_max_col_dovs])]\n",
    "        )\n",
    "        #----------\n",
    "        keep_srs_i = keep_srs_i & keep_boolean_ij\n",
    "        \n",
    "    #-------------------------\n",
    "    # Sanity check\n",
    "    assert(overlap_df_i[overlap_outg_col].apply(lambda x: len(x)==0).equals(keep_srs_i))\n",
    "    assert((overlap_df_i.index==keep_srs_i.index).all())\n",
    "    #-------------------------\n",
    "    df_to_merge = pd.merge(\n",
    "        overlap_df_i, \n",
    "        keep_srs_i.to_frame(name=keep_col), \n",
    "        left_index=True, \n",
    "        right_index=True, \n",
    "        how='inner'\n",
    "    )\n",
    "    #-------------------------\n",
    "    assert(set(best_ests_df_i.index).symmetric_difference(set(df_to_merge.index))==set())\n",
    "    assert(set(best_ests_df_i.columns).intersection(set(df_to_merge.columns))==set())\n",
    "    #-----\n",
    "    return_df = pd.merge(\n",
    "        best_ests_df_i, \n",
    "        df_to_merge, \n",
    "        left_index=True, \n",
    "        right_index=True, \n",
    "        how='inner'\n",
    "    )\n",
    "    #-------------------------        \n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7911fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_dovs_overlaps_from_best_ests(\n",
    "    best_ests_df, \n",
    "    outg_rec_nb, \n",
    "    dovs_df, \n",
    "    get_ptntl_ovrlp_dovs_kwargs=None, \n",
    "    assert_no_overlaps=True, \n",
    "    PN_col='PN', \n",
    "    t_min_col='winner_min', \n",
    "    t_max_col='winner_max', \n",
    "    PN_col_dovs='PREMISE_NB', \n",
    "    t_min_col_dovs='DT_OFF_TS_FULL', \n",
    "    t_max_col_dovs='DT_ON_TS', \n",
    "    outg_rec_nb_col_dovs='OUTG_REC_NB', \n",
    "    overlap_outg_col='overlap_DOVS', \n",
    "    overlap_times_col='overlap_times', \n",
    "    keep_col='keep'\n",
    "):\n",
    "    r\"\"\"\n",
    "    If dovs_df is not supplied by user, it will built.\n",
    "        Default behavior is to query using the premise number and time restrictions from best_ests_df.\n",
    "        However, one could override this behavior by supplying different arguments for get_ptntl_ovrlp_dovs_kwargs.\n",
    "        \n",
    "    If dovs_df is supplied by the user:\n",
    "        - it must include outg_rec_nb\n",
    "        - it must contain a 'PREMISE_NB' column\n",
    "        \n",
    "    get_ptntl_ovrlp_dovs_kwargs:\n",
    "        A dictionary with key/value pairs suitable for input into get_potential_overlapping_dovs.\n",
    "        The keys and default values are:\n",
    "            - PNs: \n",
    "                -- premise numbers to query\n",
    "                -- default = best_ests_df[PN_col].unique().tolist()\n",
    "            - outg_t_beg: \n",
    "                -- beginning time to look for overlaps\n",
    "                -- default = best_ests_df[t_min_col].min()\n",
    "            - outg_t_end: \n",
    "                -- ending time to look for overlaps\n",
    "                -- default = best_ests_df[t_max_col].max()\n",
    "            - dovs_sql_fcn: \n",
    "                -- SQL functin to use for query \n",
    "                -- default = DOVSOutages_SQL.build_sql_std_outage\n",
    "            - addtnl_dovs_sql_kwargs: \n",
    "                -- Any additional kwargs to input into dovs_sql_fcn when running query\n",
    "                -- default = None\n",
    "                -- NOTE: premise_nbs, field_to_split, dt_on_ts, and dt_off_ts_full are handled by \n",
    "                         get_potential_overlapping_dovs and should therefore NOT be included in \n",
    "                         addtnl_dovs_sql_kwargs\n",
    "    \n",
    "    NOTE: Using build_sql_std_outage by default, so typical restrictions on outages types etc.\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    # Build dovs, if needed\n",
    "    #-----\n",
    "    # As mentioned above, the functionality here allows the user to override the default behavior by supplying\n",
    "    #   different arguments for get_ptntl_ovrlp_dovs_kwargs.\n",
    "    #-------------------------\n",
    "    if dovs_df is None:\n",
    "        dflt_get_ptntl_ovrlp_dovs_kwargs = dict(\n",
    "            PNs                    = best_ests_df[PN_col].unique().tolist(), \n",
    "            outg_t_beg             = best_ests_df[t_min_col].min(), \n",
    "            outg_t_end             = best_ests_df[t_max_col].max(), \n",
    "            dovs_sql_fcn           = DOVSOutages_SQL.build_sql_std_outage, \n",
    "            addtnl_dovs_sql_kwargs = dict(include_DOVS_PREMISE_DIM=True)\n",
    "        )\n",
    "        #-----\n",
    "        if get_ptntl_ovrlp_dovs_kwargs is None:\n",
    "            get_ptntl_ovrlp_dovs_kwargs = dflt_get_ptntl_ovrlp_dovs_kwargs\n",
    "        else:\n",
    "            get_ptntl_ovrlp_dovs_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "                to_supplmnt_dict    = get_ptntl_ovrlp_dovs_kwargs, \n",
    "                default_values_dict = dflt_get_ptntl_ovrlp_dovs_kwargs, \n",
    "                extend_any_lists    = False, \n",
    "                inplace             = False\n",
    "            )\n",
    "        #-----\n",
    "        dovs_df = get_potential_overlapping_dovs(**get_ptntl_ovrlp_dovs_kwargs)\n",
    "        #-------------------------\n",
    "        if outg_rec_nb_col_dovs not in dovs_df.columns.tolist():\n",
    "            assert('OUTG_REC_NB' in dovs_df.columns.tolist())\n",
    "            dovs_df = dovs_df.rename(columns={'OUTG_REC_NB':outg_rec_nb_col_dovs})\n",
    "        #-----\n",
    "        if t_min_col_dovs not in dovs_df.columns.tolist():\n",
    "            assert('DT_OFF_TS_FULL' in dovs_df.columns.tolist())\n",
    "            dovs_df = dovs_df.rename(columns={'DT_OFF_TS_FULL':t_min_col_dovs})\n",
    "        #-----\n",
    "        if t_max_col_dovs not in dovs_df.columns.tolist():\n",
    "            assert('DT_ON_TS' in dovs_df.columns.tolist())\n",
    "            dovs_df = dovs_df.rename(columns={'DT_ON_TS':t_max_col_dovs})\n",
    "    #--------------------------------------------------\n",
    "    # Mainly sanity checks....\n",
    "    #-----\n",
    "    nec_dovs_cols = [outg_rec_nb_col_dovs, PN_col_dovs, t_min_col_dovs, t_max_col_dovs]\n",
    "    assert(set(nec_dovs_cols).difference(set(dovs_df.columns.tolist()))==set())\n",
    "    #-----\n",
    "    outg_rec_nbs_and_times = dovs_df[\n",
    "        [outg_rec_nb_col_dovs, t_min_col_dovs, t_max_col_dovs]\n",
    "    ].drop_duplicates().set_index(outg_rec_nb_col_dovs)\n",
    "    #-----\n",
    "    # Current outg_rec_nb should definitely be included!\n",
    "    assert(outg_rec_nb in outg_rec_nbs_and_times.index.tolist())\n",
    "    #-----\n",
    "    # Should only be one entry per outg_rec_nb\n",
    "    assert(outg_rec_nbs_and_times.index.nunique()==outg_rec_nbs_and_times.shape[0])\n",
    "    #-------------------------\n",
    "    # For a given PN, there definitely should not be any overlap between two DOVS OUTG_REC_NBs\n",
    "    #   i.e., a given PN cannot belong to two outages at the same time!\n",
    "    # ==========\n",
    "    # OLD METHOD:\n",
    "    #     Test this by checking whether Utilities.get_overlap_intervals returns an object with\n",
    "    #       the same length as the input for each PN (if there were any overlaps, the returned\n",
    "    #       object would have a shorter length)\n",
    "    # REASON FOR CHANGE:\n",
    "    #     This is actually a little bit too strict.\n",
    "    #     For a given PN, if potentially overlapping DOVS outages overlap with each other but do not\n",
    "    #       overlap with outg_rec_nb, they do not affect the processing of this outage, and therefore\n",
    "    #       are not of issue.\n",
    "    #     The old method was stricter, whereas the new method is more relaxed and matches the description above\n",
    "    #-----\n",
    "    # PNs_w_mult_outgs_smltnsly: multiple outages simultaneously\n",
    "#     PNs_w_mult_outgs_smltnsly = dovs_df.groupby(PN_col_dovs, as_index=True, group_keys=False).apply(\n",
    "#         lambda x: len(Utilities.get_overlap_intervals(x[[t_min_col_dovs, t_max_col_dovs]].values))!=x.shape[0]\n",
    "#     )\n",
    "#     if assert_no_overlaps:\n",
    "#         assert(not PNs_w_mult_outgs_smltnsly.any())\n",
    "    # END: OLD METHOD\n",
    "    # ==========\n",
    "    overlap_outgs_for_PNs_df = get_outgs_in_dovs_df_overlapping_outg_rec_nb_i_by_PN(\n",
    "        outg_rec_nb_i    = outg_rec_nb, \n",
    "        dovs_df          = dovs_df, \n",
    "        best_ests_df     = best_ests_df, \n",
    "        outg_rec_nb_col  = outg_rec_nb_col_dovs, \n",
    "        PN_col           = PN_col_dovs, \n",
    "        t_min_col        = t_min_col_dovs, \n",
    "        t_max_col        = t_max_col_dovs, \n",
    "        PN_col_best_ests = PN_col\n",
    "    )\n",
    "    if assert_no_overlaps:\n",
    "        assert((overlap_outgs_for_PNs_df['n_overlap']>0).sum()==0)\n",
    "    #--------------------------------------------------\n",
    "    # Do not need current outg_rec_nb anymore, only the others\n",
    "    dovs_df = dovs_df[dovs_df[outg_rec_nb_col_dovs] != outg_rec_nb].copy()\n",
    "    #--------------------------------------------------\n",
    "    # Simple-minded way would be to exclude any entries in best_ests_df which overlap \n",
    "    #   with the outage times in outg_rec_nbs_and_times.\n",
    "    #   This would be correct in most all cases\n",
    "    # HOWEVER, to be completely correct, this should be done at the PREMISE_NB level!\n",
    "    #-----\n",
    "    # I don't want to simply merge best_ests_df with dovs_df (by PN) because the former can have multiple sub-outages\n",
    "    #   per PN, and the latter can have multiple (possibly) conflicting outages.\n",
    "    # Merging the two would be sloppy.\n",
    "    # Therefore, I'll use groupby\n",
    "    return_df = best_ests_df.groupby(PN_col, as_index=False, group_keys=False).apply(\n",
    "        lambda x: identify_dovs_overlaps_for_PN_i(\n",
    "            best_ests_df_i       = x, \n",
    "            dovs_df              = dovs_df, \n",
    "            PN_col               = PN_col, \n",
    "            t_min_col            = t_min_col, \n",
    "            t_max_col            = t_max_col, \n",
    "            PN_col_dovs          = PN_col_dovs, \n",
    "            t_min_col_dovs       = t_min_col_dovs, \n",
    "            t_max_col_dovs       = t_max_col_dovs, \n",
    "            outg_rec_nb_col_dovs = outg_rec_nb_col_dovs, \n",
    "            overlap_outg_col     = overlap_outg_col, \n",
    "            overlap_times_col    = overlap_times_col, \n",
    "            keep_col             = keep_col\n",
    "        )\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa693c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_any_dovs_overlaps_from_best_ests(\n",
    "    best_ests_df, \n",
    "    outg_rec_nb, \n",
    "    dovs_df, \n",
    "    get_ptntl_ovrlp_dovs_kwargs=None, \n",
    "    assert_no_overlaps=True, \n",
    "    PN_col='PN', \n",
    "    t_min_col='winner_min', \n",
    "    t_max_col='winner_max', \n",
    "    i_outg_col='i_outg', \n",
    "    PN_col_dovs='PREMISE_NB', \n",
    "    t_min_col_dovs='DT_OFF_TS_FULL', \n",
    "    t_max_col_dovs='DT_ON_TS', \n",
    "    outg_rec_nb_col_dovs='OUTG_REC_NB', \n",
    "    overlap_outg_col='overlap_DOVS', \n",
    "    overlap_times_col='overlap_times', \n",
    "    keep_col='keep'\n",
    "):\n",
    "    r\"\"\"\n",
    "    If dovs_df is not supplied by user, it will built.\n",
    "        Default behavior is to query using the premise number and time restrictions from best_ests_df.\n",
    "        However, one could override this behavior by supplying different arguments for get_ptntl_ovrlp_dovs_kwargs.\n",
    "        \n",
    "    If dovs_df is supplied by the user:\n",
    "        - it must include outg_rec_nb\n",
    "        - it must contain a 'PREMISE_NB' column\n",
    "        \n",
    "    get_ptntl_ovrlp_dovs_kwargs:\n",
    "        A dictionary with key/value pairs suitable for input into get_potential_overlapping_dovs.\n",
    "        The keys and default values are:\n",
    "            - PNs: \n",
    "                -- premise numbers to query\n",
    "                -- default = best_ests_df[PN_col].unique().tolist()\n",
    "            - outg_t_beg: \n",
    "                -- beginning time to look for overlaps\n",
    "                -- default = best_ests_df[t_min_col].min()\n",
    "            - outg_t_end: \n",
    "                -- ending time to look for overlaps\n",
    "                -- default = best_ests_df[t_max_col].max()\n",
    "            - dovs_sql_fcn: \n",
    "                -- SQL functin to use for query \n",
    "                -- default = DOVSOutages_SQL.build_sql_std_outage\n",
    "            - addtnl_dovs_sql_kwargs: \n",
    "                -- Any additional kwargs to input into dovs_sql_fcn when running query\n",
    "                -- default = None\n",
    "                -- NOTE: premise_nbs, field_to_split, dt_on_ts, and dt_off_ts_full are handled by \n",
    "                         get_potential_overlapping_dovs and should therefore NOT be included in \n",
    "                         addtnl_dovs_sql_kwargs\n",
    "    \n",
    "    NOTE: Using build_sql_std_outage by default, so typical restrictions on outages types etc.\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    return_df = identify_dovs_overlaps_from_best_ests(\n",
    "        best_ests_df                = best_ests_df, \n",
    "        outg_rec_nb                 = outg_rec_nb, \n",
    "        dovs_df                     = dovs_df, \n",
    "        get_ptntl_ovrlp_dovs_kwargs = get_ptntl_ovrlp_dovs_kwargs, \n",
    "        assert_no_overlaps          = assert_no_overlaps, \n",
    "        PN_col                      = PN_col, \n",
    "        t_min_col                   = t_min_col, \n",
    "        t_max_col                   = t_max_col, \n",
    "        PN_col_dovs                 = PN_col_dovs, \n",
    "        t_min_col_dovs              = t_min_col_dovs, \n",
    "        t_max_col_dovs              = t_max_col_dovs, \n",
    "        outg_rec_nb_col_dovs        = outg_rec_nb_col_dovs, \n",
    "        overlap_outg_col            = overlap_outg_col, \n",
    "        overlap_times_col           = overlap_times_col, \n",
    "        keep_col                    = keep_col\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    # Locate overlap_outg_col, overlap_times_col, and keep_col in return_df\n",
    "    #-----\n",
    "    found_overlap_outg_col = Utilities.find_in_list_with_regex(\n",
    "        lst=return_df.columns.tolist(), \n",
    "        regex_pattern=r'{}.*'.format(overlap_outg_col)\n",
    "    )\n",
    "    assert(len(found_overlap_outg_col)==1)\n",
    "    overlap_outg_col = found_overlap_outg_col[0]\n",
    "    #-----\n",
    "    found_overlap_times_col = Utilities.find_in_list_with_regex(\n",
    "        lst=return_df.columns.tolist(), \n",
    "        regex_pattern=r'{}.*'.format(overlap_times_col)\n",
    "    )\n",
    "    assert(len(found_overlap_times_col)==1)\n",
    "    overlap_times_col = found_overlap_times_col[0]\n",
    "    #-----\n",
    "    found_keep_col = Utilities.find_in_list_with_regex(\n",
    "        lst=return_df.columns.tolist(), \n",
    "        regex_pattern=r'{}.*'.format(keep_col)\n",
    "    )\n",
    "    assert(len(found_keep_col)==1)\n",
    "    keep_col = found_keep_col[0]\n",
    "    #--------------------------------------------------\n",
    "    # Keep only those with keep_col values equal to True\n",
    "    return_df = return_df[return_df[keep_col]==True].copy()\n",
    "    #-----\n",
    "    # No longer need overlap_outg_col, overlap_times_col, or keep_col, as they are trivially equal to [],[],True for all\n",
    "    return_df = return_df.drop(columns=[overlap_outg_col, overlap_times_col, keep_col])\n",
    "    #--------------------------------------------------\n",
    "    # Re-build i_outg_col to account for any sub-outages removed\n",
    "    return_df = set_i_outg_in_best_ests_df(\n",
    "        best_ests_df = return_df, \n",
    "        groupby_cols = [PN_col], \n",
    "        sort_cols    = [t_min_col, t_max_col], \n",
    "        i_outg_col   = i_outg_col\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d07e7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_removed_srs(\n",
    "    removed_srs, \n",
    "    ami_df_i, \n",
    "    best_ests_df, \n",
    "    PN_col_ami='aep_premise_nb', \n",
    "    time_col_ami='starttimeperiod_local', \n",
    "    PN_col_be='PN', \n",
    "    t_min_col_be='winner_min', \n",
    "    t_max_col_be='winner_max', \n",
    "    keep_col_be='keep', \n",
    "):\n",
    "    r\"\"\"\n",
    "    Expand the bounds of removed_srs in set_removed_due_to_overlap_in_ami_df_i.\n",
    "    -----\n",
    "    The point of building the 'removed_due_to_overlap' in ami_df_i is simply for plotting purposes.\n",
    "    If there is a block in the middle of ami_df_i which is marked 'removed_due_to_overlap'=True, and the data with those\n",
    "      values removed is plotted, the figure will still appear as if there are data points for the removed times.\n",
    "    The reason being that markers are not drawn, so it is not obvious where datapoints are.\n",
    "    The value to the left of the points to the removed will be connected with a straight line to those to the right of \n",
    "      the points to be removed.\n",
    "    -----\n",
    "    The purpose of this function is to make the plot a little clearer.\n",
    "    If a removal period happens before the first sub-outage for a PN, expand the left point of the removal period to be\n",
    "      equal to the minimum time value in ami_df_i for the PN.\n",
    "    If a removal period happens after the last sub-outage for a PN, expand the right point of the removal period to be\n",
    "      equal to the maximum time value in ami_df_i for the PN.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    winner_minmax_by_PN = best_ests_df[best_ests_df[keep_col_be]==True].groupby(PN_col_be).agg(\n",
    "        {t_min_col_be:'min', t_max_col_be:'max'}\n",
    "    )\n",
    "    #-----\n",
    "    ami_minmax_times_by_PN = ami_df_i.groupby(PN_col_ami).agg(\n",
    "        {time_col_ami:['min', 'max']}\n",
    "    )\n",
    "    assert(set(removed_srs.index).difference(set(ami_minmax_times_by_PN.index))==set())\n",
    "    #-------------------------\n",
    "    # Iterate through PNs in removed_srs and build expanded values\n",
    "    return_srs = dict()\n",
    "    for PN_j, overlap_times_j in removed_srs.items():\n",
    "        # If PN_j did not actually lose power, there will be no entry in winner_minmax_by_PN\n",
    "        # In such a case, leave values as they are\n",
    "        if PN_j not in winner_minmax_by_PN.index:\n",
    "            overlap_times_j_exp = overlap_times_j\n",
    "        else:\n",
    "            overlap_times_j_exp = []\n",
    "            for overlap_pd_j_k in overlap_times_j:\n",
    "                # If a removal period happens before the first sub-outage for a PN, expand the left point of \n",
    "                #   the removal period to be equal to the minimum time value in ami_df_i for the PN.\n",
    "                if overlap_pd_j_k[1] < winner_minmax_by_PN.loc[PN_j, t_min_col_be]:\n",
    "                    overlap_pd_j_k_0 = ami_minmax_times_by_PN.loc[PN_j, (time_col_ami, 'min')]\n",
    "                else:\n",
    "                    overlap_pd_j_k_0 = overlap_pd_j_k[0]\n",
    "                #-----\n",
    "                # If a removal period happens after the last sub-outage for a PN, expand the right point of\n",
    "                #   the removal period to be equal to the maximum time value in ami_df_i for the PN.        \n",
    "                if overlap_pd_j_k[0] > winner_minmax_by_PN.loc[PN_j, t_max_col_be]:\n",
    "                    overlap_pd_j_k_1 = ami_minmax_times_by_PN.loc[PN_j, (time_col_ami, 'max')]\n",
    "                else:\n",
    "                    overlap_pd_j_k_1 = overlap_pd_j_k[1]\n",
    "                #-----\n",
    "                overlap_times_j_exp.append((overlap_pd_j_k_0, overlap_pd_j_k_1)) \n",
    "        assert(PN_j not in return_srs.keys())\n",
    "        return_srs[PN_j] = overlap_times_j_exp\n",
    "    #-------------------------\n",
    "    return_srs = pd.Series(return_srs, name=removed_srs.name)\n",
    "    return return_srs\n",
    "\n",
    "\n",
    "def set_removed_due_to_overlap_helper(\n",
    "    ami_df_i, \n",
    "    PN_j, \n",
    "    overlap_times_j, \n",
    "    time_col='starttimeperiod_local', \n",
    "    PN_col='aep_premise_nb', \n",
    "    removed_due_to_overlap_col='removed_due_to_overlap'\n",
    "):\n",
    "    r\"\"\"\n",
    "    ONLY INTENDED FOR USE INSIDE OF set_removed_due_to_overlap_in_ami_df_i\n",
    "    \n",
    "    In order to set removed_due_to_overlap, the logic will be: set if PN is correct and time falls within\n",
    "      any one of the overlap_times taken from best_ests_df\n",
    "    In code, this amounts to:\n",
    "        ami_df_i[\n",
    "            (ami_df_i[PN_col]==PN_j) & \n",
    "            (    \n",
    "                (        \n",
    "                    (ami_df_i[time_col] >= overlap_time_min_j_0) &\n",
    "                    (ami_df_i[time_col] <= overlap_time_max_j_0)\n",
    "                )\n",
    "                ||\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                (        \n",
    "                    (ami_df_i[time_col] >= overlap_time_min_j_n) &\n",
    "                    (ami_df_i[time_col] <= overlap_time_max_j_n)\n",
    "                )    \n",
    "            )\n",
    "        ]\n",
    "    Due to the unknown number of overlap times, this is easiest to accomplish using DFSlicers    \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure all elements in overlap_times_j are lists/tuples of length 2\n",
    "    assert(Utilities.are_all_list_elements_one_of_types(overlap_times_j, [list, tuple]))\n",
    "    assert(Utilities.are_list_elements_lengths_homogeneous(overlap_times_j, 2))\n",
    "    #-------------------------\n",
    "    pn_slicer = slicer = DFSlicer(\n",
    "        single_slicers = [dict(column=PN_col, value=PN_j, comparison_operator='==')]\n",
    "    )\n",
    "    #-----\n",
    "    pn_slcr_bool_srs = pn_slicer.get_slicing_booleans(\n",
    "        df=ami_df_i\n",
    "    )\n",
    "    #-------------------------\n",
    "    time_slicers = []\n",
    "    for overlap_time_min, overlap_time_max in overlap_times_j:\n",
    "        ts_i = DFSlicer(\n",
    "            single_slicers = [\n",
    "                dict(column=time_col, value=overlap_time_min, comparison_operator='>='), \n",
    "                dict(column=time_col, value=overlap_time_max, comparison_operator='<=')\n",
    "            ], \n",
    "            join_single_slicers='and'\n",
    "        )\n",
    "        time_slicers.append(ts_i)\n",
    "    #-----\n",
    "    time_slcr_bool_srs = DFSlicer.combine_slicers_and_get_slicing_booleans(\n",
    "        df=ami_df_i, \n",
    "        slicers=time_slicers, \n",
    "        join_slicers='or', \n",
    "        apply_not=False\n",
    "    )\n",
    "    #-------------------------\n",
    "    assert((time_slcr_bool_srs.index==pn_slcr_bool_srs.index).all())\n",
    "    slcr_bool_srs = time_slcr_bool_srs & pn_slcr_bool_srs\n",
    "    #-------------------------\n",
    "    ami_df_i.loc[slcr_bool_srs, removed_due_to_overlap_col] = True\n",
    "    #-------------------------\n",
    "    return ami_df_i\n",
    "\n",
    "def set_removed_due_to_overlap_in_ami_df_i(\n",
    "    ami_df_i, \n",
    "    best_ests_df, \n",
    "    PN_col='aep_premise_nb', \n",
    "    time_idfr='starttimeperiod_local', \n",
    "    PN_col_be='PN', \n",
    "    t_min_col_be='winner_min', \n",
    "    t_max_col_be='winner_max', \n",
    "    keep_col_be='keep', \n",
    "    overlap_times_col_be='overlap_times', \n",
    "    removed_due_to_overlap_col='removed_due_to_overlap', \n",
    "    expand_removed_times=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    In order to set removed_due_to_overlap, the logic will be: set if PN is correct and time falls within\n",
    "      any one of the overlap_times taken from best_ests_df\n",
    "    In code, this amounts to:\n",
    "        ami_df_i[\n",
    "            (ami_df_i['aep_premise_nb']==PN_ij) & \n",
    "            (    \n",
    "                (        \n",
    "                    (ami_df_i[time_col] >= overlap_time_min_0) &\n",
    "                    (ami_df_i[time_col] <= overlap_time_max_0)\n",
    "                )\n",
    "                ||\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                (        \n",
    "                    (ami_df_i[time_col] >= overlap_time_min_n) &\n",
    "                    (ami_df_i[time_col] <= overlap_time_max_n)\n",
    "                )    \n",
    "            )\n",
    "        ]\n",
    "    Due to the unknown number of overlap times, this is easiest to accomplish using DFSlicers    \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    # In order for method to work, time information must be in column, not index.\n",
    "    # If found in index, call reset_index\n",
    "    time_idfr_loc = Utilities_df.get_idfr_loc(\n",
    "        df=ami_df_i, \n",
    "        idfr=time_idfr\n",
    "    )\n",
    "    #-------------------------\n",
    "    if time_idfr_loc[1]:\n",
    "        # For now, make sure ami_df_i has a single index (if MultiIndex, this functionality can be built out later)\n",
    "        # time information is located in index, so reset_index must be called for methods to work\n",
    "        # Looking back at final code: I'm not sure anything additional needs to be done for MultiIndex case\n",
    "        assert(ami_df_i.index.nlevels==1)\n",
    "        #-----\n",
    "        if ami_df_i.index.names[time_idfr_loc[0]]:\n",
    "            time_col = ami_df_i.index.names[time_idfr_loc[0]]\n",
    "        else:\n",
    "            time_col = 'time_idx_'+Utilities.generate_random_string(str_len=4)\n",
    "            assert(time_col not in ami_df_i.columns.tolist())\n",
    "            assert(time_col not in list(ami_df_i.index.names))\n",
    "            ami_df_i.index = ami_df_i.index.set_names(time_col, level=time_idfr_loc[0])\n",
    "        #-----\n",
    "        idx_cols = list(ami_df_i.index.names)\n",
    "        ami_df_i = ami_df_i.reset_index()\n",
    "    else:\n",
    "        time_col = time_idfr_loc[0]\n",
    "        idx_cols=None\n",
    "    #-------------------------\n",
    "    assert(set([PN_col, time_col]).difference(set(ami_df_i.columns.tolist()))==set())\n",
    "    #--------------------------------------------------\n",
    "    # From best_ests_df, find all sub-outages to be removed by PN\n",
    "    assert(set([PN_col_be, keep_col_be, overlap_times_col_be]).difference(set(best_ests_df.columns.tolist()))==set())\n",
    "    removed_srs = best_ests_df[best_ests_df[keep_col_be]==False].copy()\n",
    "    removed_srs = removed_srs.groupby(PN_col_be)[overlap_times_col_be].sum()\n",
    "    if expand_removed_times:\n",
    "        removed_srs = expand_removed_srs(\n",
    "            removed_srs  = removed_srs, \n",
    "            ami_df_i     = ami_df_i, \n",
    "            best_ests_df = best_ests_df, \n",
    "            PN_col_ami   = PN_col, \n",
    "            time_col_ami = time_col, \n",
    "            PN_col_be    = PN_col_be, \n",
    "            t_min_col_be = t_min_col_be, \n",
    "            t_max_col_be = t_max_col_be, \n",
    "            keep_col_be  = keep_col_be \n",
    "        )\n",
    "    #--------------------------------------------------\n",
    "    # Iterate through each PN in removed_srs and use set_removed_due_to_overlap_helper to set the appropriate\n",
    "    #   values in ami_df_i\n",
    "    ami_df_i[removed_due_to_overlap_col] = False\n",
    "    for PN_j, overlap_times_j in removed_srs.items():\n",
    "        ami_df_i = set_removed_due_to_overlap_helper(\n",
    "            ami_df_i                   = ami_df_i, \n",
    "            PN_j                       = PN_j, \n",
    "            overlap_times_j            = overlap_times_j, \n",
    "            time_col                   = time_col, \n",
    "            PN_col                     = PN_col, \n",
    "            removed_due_to_overlap_col = removed_due_to_overlap_col\n",
    "        )\n",
    "    #--------------------------------------------------\n",
    "    # If .reset_index was called earlier, set index back to original values\n",
    "    if idx_cols is not None:\n",
    "        ami_df_i = ami_df_i.set_index(idx_cols)\n",
    "    #--------------------------------------------------\n",
    "    return ami_df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf992b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb30fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f16da661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_n_PNs_w_power_srs(\n",
    "    best_ests_df, \n",
    "    ami_df_i, \n",
    "    return_pct=True, \n",
    "    PN_col='PN', \n",
    "    t_min_col='winner_min', \n",
    "    t_max_col='winner_max', \n",
    "    i_outg_col='i_outg', \n",
    "    PN_col_ami_df='aep_premise_nb'\n",
    "):\n",
    "    r\"\"\"\n",
    "    ami_df_i\n",
    "        - Only needed for finding n_PNs_tot\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    df = best_ests_df[[PN_col, i_outg_col, t_min_col, t_max_col]].copy()\n",
    "    #-------------------------\n",
    "    all_times_sorted = natsorted(set(df[t_min_col].tolist()+df[t_max_col].tolist()))\n",
    "    all_times_sorted = [all_times_sorted[0]-pd.Timedelta('1T')] + all_times_sorted\n",
    "    #-----\n",
    "    # For each unique time, determine how many PNs without power\n",
    "    n_PNs_tot = ami_df_i[PN_col_ami_df].nunique()\n",
    "    time_and_n_PNs_w_power=dict()\n",
    "    for time_i in all_times_sorted:\n",
    "        df_i = df[\n",
    "            (df[t_min_col]<=time_i) & \n",
    "            (df[t_max_col]>time_i)\n",
    "        ].copy()\n",
    "        # Should find at most one outage for a given PN!\n",
    "        assert(df_i.shape[0]==df_i[PN_col].nunique())\n",
    "        n_PNs_out_i = df_i.shape[0]\n",
    "        assert(time_i not in time_and_n_PNs_w_power.keys())\n",
    "        if return_pct:\n",
    "            time_and_n_PNs_w_power[time_i] = 100*(n_PNs_tot-n_PNs_out_i)/n_PNs_tot\n",
    "        else:\n",
    "            time_and_n_PNs_w_power[time_i] = n_PNs_tot-n_PNs_out_i\n",
    "    #-------------------------\n",
    "    n_PNs_w_power_srs = pd.Series(data=time_and_n_PNs_w_power)\n",
    "    #-------------------------\n",
    "    return n_PNs_w_power_srs\n",
    "\n",
    "def simplify_n_PNs_w_power_srs(\n",
    "    n_PNs_w_power_srs, \n",
    "    freq='1T'\n",
    "):\n",
    "    r\"\"\"\n",
    "    If there are a bunch of entries close together, this will keep only the one with the largest value.\n",
    "    Intention: To be used when including data point information in text on plot for n_PNs_w_power_srs\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(isinstance(n_PNs_w_power_srs, pd.Series))\n",
    "    #-------------------------\n",
    "    if n_PNs_w_power_srs.name:\n",
    "        name = n_PNs_w_power_srs.name\n",
    "    else:\n",
    "        name = 'pct_w_power'\n",
    "    #-------------------------\n",
    "    n_PNs_w_power_srs_simp = n_PNs_w_power_srs.to_frame(name=name).reset_index().groupby(\n",
    "        pd.Grouper(freq=freq, key='index')\n",
    "    ).apply(\n",
    "        lambda x: x.loc[x[name].idxmax()] if x.shape[0]>0 else None\n",
    "    )\n",
    "    n_PNs_w_power_srs_simp = n_PNs_w_power_srs_simp.dropna()\n",
    "    n_PNs_w_power_srs_simp=n_PNs_w_power_srs_simp.set_index('index').squeeze()\n",
    "    #-------------------------\n",
    "    return n_PNs_w_power_srs_simp\n",
    "\n",
    "\n",
    "def get_periods_above_threshold(\n",
    "    n_PNs_w_power_srs, \n",
    "    threshold, \n",
    "    return_indices=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given n_PNs_w_power_srs, returns the beginning and ending of periods during which the number\n",
    "    of premises with power is above threshold\n",
    "    \n",
    "    n_PNs_w_power_srs:\n",
    "        A series with index equal to timestamps and values equal to the percent of premises with power\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(isinstance(n_PNs_w_power_srs, pd.Series))\n",
    "    #-------------------------\n",
    "    n_PNs_w_power_srs = n_PNs_w_power_srs.copy()\n",
    "    if n_PNs_w_power_srs.name:\n",
    "        pct_w_power_col = n_PNs_w_power_srs.name\n",
    "    else:\n",
    "        pct_w_power_col = Utilities.generate_random_string()\n",
    "        n_PNs_w_power_srs.name = pct_w_power_col\n",
    "    #-------------------------\n",
    "    above_thresh_col = 'above_thresh'\n",
    "    above_thresh_df = (n_PNs_w_power_srs>threshold).astype(int).to_frame()\n",
    "    above_thresh_df=above_thresh_df.rename(columns={pct_w_power_col:above_thresh_col})\n",
    "    #-----\n",
    "    tmp_diff_col = Utilities.generate_random_string()\n",
    "    above_thresh_df[tmp_diff_col] = above_thresh_df[above_thresh_col].diff()\n",
    "    #-------------------------\n",
    "    # The .diff() operation always leaves the first element as a NaN\n",
    "    # However, if the first element pct_w_power>threshold it represents the beginning\n",
    "    #   of a black and the diff should be +1\n",
    "    if above_thresh_df.iloc[0][above_thresh_col]==1:\n",
    "        above_thresh_df.loc[above_thresh_df.index[0], tmp_diff_col] = 1\n",
    "    #-------------------------\n",
    "    # Continuous blocks of True (i.e., blocks with pct_w_power > threshold) begin with diff = +1 and\n",
    "    #   end at the element preceding diff = -1\n",
    "    block_beg_idxs = above_thresh_df.reset_index().index[above_thresh_df[tmp_diff_col]==1].tolist()\n",
    "    #-----\n",
    "    block_end_idxs = above_thresh_df.reset_index().index[above_thresh_df[tmp_diff_col]==-1].tolist()\n",
    "    block_end_idxs = [x-1 for x in block_end_idxs]\n",
    "    # If power is ongoing at the end of the data, the procedure above will miss the last end idx\n",
    "    #   If single point above threshold at end of data ==> tmp_diff_col = +1\n",
    "    #   If multiple points above threshold at the end of the data, then tmp_diff_col for the last value will be 0\n",
    "    #     In this case, there does not exist a tmp_diff_col=-1 to signal the end of the block, so must add by hand\n",
    "    if above_thresh_df.iloc[-1][above_thresh_col]==1:\n",
    "        block_end_idxs.append(above_thresh_df.shape[0]-1)\n",
    "    #--------------------------------------------------\n",
    "    # periods of length one should have idx in both block_beg_idxs and block_end_idxs\n",
    "    len_1_pd_idxs = natsorted(set(block_beg_idxs).intersection(set(block_end_idxs)))\n",
    "    #-------------------------\n",
    "    # Remove the len_1 idxs so the remaineders can be matched\n",
    "    # NOTE: The following procedure relies on block_beg(end)_idxs being sorting\n",
    "    block_beg_idxs = natsorted(set(block_beg_idxs).difference(len_1_pd_idxs))\n",
    "    block_end_idxs = natsorted(set(block_end_idxs).difference(len_1_pd_idxs))\n",
    "    assert(len(block_beg_idxs)==len(block_end_idxs))\n",
    "    block_begend_idxs = list(zip(block_beg_idxs, block_end_idxs))\n",
    "    #-------------------------\n",
    "    # Include the length 1 blocks!\n",
    "    block_begend_idxs.extend([(x,x) for x in len_1_pd_idxs])\n",
    "    #-------------------------\n",
    "    # Sort\n",
    "    block_begend_idxs = natsorted(block_begend_idxs, key=lambda x: x[0])\n",
    "    #-------------------------\n",
    "    # Sanity check!\n",
    "    for i in range(len(block_begend_idxs)):\n",
    "        assert(len(block_begend_idxs[i])==2)\n",
    "        assert(block_begend_idxs[i][1]>=block_begend_idxs[i][0])\n",
    "        if i>0:\n",
    "            assert(block_begend_idxs[i][0]>block_begend_idxs[i-1][1])\n",
    "    #-------------------------\n",
    "    # Convert indices to actual values\n",
    "    block_begend = [(above_thresh_df.index[x[0]], above_thresh_df.index[x[1]]) for x in block_begend_idxs]\n",
    "    #-------------------------\n",
    "    if return_indices:\n",
    "        return block_begend_idxs\n",
    "    else:\n",
    "        return block_begend\n",
    "    \n",
    "def get_first_last_above_threshold(\n",
    "    n_PNs_w_power_srs, \n",
    "    threshold\n",
    "):\n",
    "    r\"\"\"\n",
    "    Get the first and last time (after initial power loss) that power was regained.\n",
    "    For simple outages, these should be approximately equal\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    pds_above_thresh = get_periods_above_threshold(\n",
    "        n_PNs_w_power_srs=n_PNs_w_power_srs, \n",
    "        threshold=threshold, \n",
    "        return_indices=False\n",
    "    )\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # len(pds_above_thresh)==2:\n",
    "    #     For simple outages, where all initially have power, power is lost suddenly for all, and power is regained \n",
    "    #       suddenly for all, there should be two periods above threshold (len(pds_above_thresh)==2)\n",
    "    #\n",
    "    # len(pds_above_thresh)==1:\n",
    "    #     This can arise from a few different situations (#2 is the only usable of the 3):\n",
    "    #       1: - 'Power' is never lost (i.e., the percent of premises with power never dips below threshold)\n",
    "    #          -  The single period spans the data\n",
    "    #          -  The period data is of no use\n",
    "    #       2: - 'Power' is already out at the beginning of the data \n",
    "    #          -  The beginning of the single period may be used to signify when power is restored\n",
    "    #       3: - 'Power' is on at the beginning of the data, goes out, but is never restored\n",
    "    #          -  The period data is of no use\n",
    "    #\n",
    "    # len(pds_above_thresh)==0:\n",
    "    #     'Power' is out for the entirety of the data\n",
    "    #\n",
    "    # len(pds_above_thresh)>2:\n",
    "    #     More complicated structure with sub-outages\n",
    "    #\n",
    "    #-------------------------\n",
    "    # How we interpret the data depends on whether there is power at the beginng and end of the data\n",
    "    power_at_beg = n_PNs_w_power_srs.iloc[0]>threshold\n",
    "    power_at_end = n_PNs_w_power_srs.iloc[-1]>threshold\n",
    "    #-------------------------\n",
    "    if len(pds_above_thresh)==0:\n",
    "        assert(not power_at_beg and not power_at_end)\n",
    "        frst_above=None\n",
    "        last_above=None\n",
    "    #-------------------------\n",
    "    elif len(pds_above_thresh)==1:\n",
    "        assert(power_at_beg or power_at_end)\n",
    "        last_above=None\n",
    "        # If power_at_beg and power_at_end, then 'power' is never lost ==> situation #1 (described above)\n",
    "        # If power_at_beg==False and power_at_end==True  ==> situation #2 (described above)\n",
    "        # If power_at_beg==True  and power_at_end==False ==> situation #3 (described above)\n",
    "        if power_at_beg==False and power_at_end==True:\n",
    "            frst_above = pds_above_thresh[0][0]\n",
    "        else:\n",
    "            frst_above = None\n",
    "    #-------------------------\n",
    "    else:\n",
    "        assert(power_at_beg or power_at_end)\n",
    "        #-----\n",
    "        # First time above threshold:\n",
    "        #   If first data point is above threshold, then return beginning of the second period (at index=1)\n",
    "        #     - i.e., if there is 'power' at the beginning of the data, return the first time power was restored\n",
    "        #         after it was lost (where 'power' is defined as a minimum threshold of premises having power)\n",
    "        #   If first data point is not above threshold, the power was initially out, so return beginning of first \n",
    "        #     period (at index=0)\n",
    "        if power_at_beg:\n",
    "            frst_above = pds_above_thresh[1][0]\n",
    "        else:\n",
    "            frst_above = pds_above_thresh[0][0]\n",
    "        #-----\n",
    "        # Last time above threshold:\n",
    "        # If last data point is above threshold, then return beginning of last period (at index=-1)\n",
    "        #     - i.e., if there is 'power' at the end of the data, return the beginning of the last period\n",
    "        # If the last data point is not above threshold, the 'power' is out at the end of the data, so\n",
    "        #   in this case return None\n",
    "        if power_at_end:\n",
    "            last_above = pds_above_thresh[-1][0]\n",
    "        else:\n",
    "            last_above = None\n",
    "    #-------------------------\n",
    "    return frst_above, last_above\n",
    "    \n",
    "def get_n_PNs_w_power_srs_time_to_print(\n",
    "    n_PNs_w_power_srs, \n",
    "    threshold\n",
    "):\n",
    "    r\"\"\"\n",
    "    In figure, entries with n_PNs_w_power>threshold are typically circled and have both their pct and time\n",
    "    printed next to the data point.\n",
    "    However, if there are multiple in a row, this can be messy.\n",
    "    In such cases, we only want to print the time for those at the beginning and end of each grouping.\n",
    "    This function identifies which entries should have their times printed\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(isinstance(n_PNs_w_power_srs, pd.Series))\n",
    "    #-------------------------\n",
    "    pds_above_thresh = get_periods_above_threshold(\n",
    "        n_PNs_w_power_srs=n_PNs_w_power_srs, \n",
    "        threshold=threshold, \n",
    "        return_indices=True\n",
    "    )\n",
    "    # Get unique idxs from pds_above_thresh\n",
    "    print_times = natsorted(set(itertools.chain.from_iterable(pds_above_thresh)))\n",
    "    #-------------------------\n",
    "    # Always print the first and last times!\n",
    "    if 0 not in print_times:\n",
    "        print_times = [0]+print_times\n",
    "    if n_PNs_w_power_srs.shape[0]-1 not in print_times:\n",
    "        print_times = print_times+[n_PNs_w_power_srs.shape[0]-1]\n",
    "    #-------------------------\n",
    "    return_print_times_srs = n_PNs_w_power_srs.iloc[print_times]\n",
    "    #-------------------------\n",
    "    return return_print_times_srs\n",
    "\n",
    "\n",
    "def plot_n_PNs_w_power_srs(\n",
    "    n_PNs_w_power_srs, \n",
    "    simp_freq='1T', \n",
    "    threshold=None, \n",
    "    fig_num=0, \n",
    "    title=None, \n",
    "    adjust_texts=True, \n",
    "    fig_ax=None, \n",
    "    **kwargs\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    threshold_color = kwargs.get('threshold_color', 'red')\n",
    "    #-------------------------\n",
    "    if not n_PNs_w_power_srs.name:\n",
    "        n_PNs_w_power_srs.name = 'pct_w_power'\n",
    "    #-------------------------\n",
    "    if fig_ax is None:\n",
    "        fig,ax = Plot_General.default_subplots(fig_num=fig_num)\n",
    "    else:\n",
    "        fig = fig_ax[0]\n",
    "        ax  = fig_ax[1]\n",
    "    #-------------------------\n",
    "    # Line plot of the data\n",
    "    sns.lineplot(\n",
    "        ax=ax, \n",
    "        data=n_PNs_w_power_srs.to_frame().reset_index(), \n",
    "        x='index', \n",
    "        y=n_PNs_w_power_srs.name, \n",
    "        marker='o'\n",
    "    )\n",
    "    #-----\n",
    "    # Grab txt_srs (depending on simp_freq) to be used when printing\n",
    "    #   points data on plot\n",
    "    if simp_freq is not None:\n",
    "        txt_srs = simplify_n_PNs_w_power_srs(\n",
    "            n_PNs_w_power_srs, \n",
    "            freq=simp_freq\n",
    "        )\n",
    "    else:\n",
    "        txt_srs = n_PNs_w_power_srs\n",
    "\n",
    "    #-------------------------\n",
    "    if threshold is not None:\n",
    "        # Draw red circles around any data above threshold\n",
    "        sns.scatterplot(\n",
    "            ax=ax, \n",
    "            data=n_PNs_w_power_srs[n_PNs_w_power_srs>threshold].to_frame().reset_index(), \n",
    "            x='index', \n",
    "            y=n_PNs_w_power_srs.name, \n",
    "            marker='o', \n",
    "            edgecolor=threshold_color, \n",
    "            facecolor='none', \n",
    "            s=100\n",
    "        )\n",
    "        #-----\n",
    "        # Draw horizontal line indicating threshold value\n",
    "        ax.axhline(threshold, color=threshold_color, linestyle='dashed', alpha=0.5)\n",
    "        #-----\n",
    "        # Grab print_times_srs, used to print full time information next to \n",
    "        #   some of the above-threshold values (printed at beginning and end of blocks)\n",
    "        print_times_srs = get_n_PNs_w_power_srs_time_to_print(\n",
    "            n_PNs_w_power_srs=n_PNs_w_power_srs, \n",
    "            threshold=threshold\n",
    "        )\n",
    "        #-----\n",
    "        # Don't want any repeats between print_times_srs and txt_srs\n",
    "        txt_srs = txt_srs.loc[list(set(txt_srs.index).difference(set(print_times_srs.index)))]           \n",
    "\n",
    "    #----- Printing points data on plot ---------------\n",
    "    texts = []\n",
    "    for time_i, pct_i in txt_srs.items():\n",
    "        texts.append(ax.annotate(np.round(pct_i, decimals=2), (time_i, pct_i)))\n",
    "    #-----\n",
    "    if threshold is not None:\n",
    "        for time_i, pct_i in print_times_srs.items():\n",
    "            texts.append(\n",
    "                ax.annotate(\n",
    "                    f\"{np.round(pct_i, decimals=2)}, {time_i.strftime('%d %H:%M:%S')}\", \n",
    "                (time_i, pct_i), \n",
    "                color=threshold_color\n",
    "                )\n",
    "            )\n",
    "    if adjust_texts:\n",
    "        adjustText.adjust_text(texts, ax=ax)\n",
    "    #--------------------------------------------------\n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontdict=dict(fontsize=24))\n",
    "    #--------------------------------------------------\n",
    "    return fig,ax\n",
    "\n",
    "\n",
    "def build_n_PNs_w_power_srs_and_plot(\n",
    "    best_ests_df, \n",
    "    ami_df_i, \n",
    "    return_pct=True, \n",
    "    simp_freq='1T', \n",
    "    threshold=None, \n",
    "    fig_num=0, \n",
    "    fig_ax=None, \n",
    "    title=None, \n",
    "    adjust_texts=True, \n",
    "    PN_col='PN', \n",
    "    t_min_col='winner_min', \n",
    "    t_max_col='winner_max', \n",
    "    i_outg_col='i_outg', \n",
    "    PN_col_ami_df='aep_premise_nb', \n",
    "    **plot_kwargs\n",
    "):\n",
    "    r\"\"\"\n",
    "    ami_df_i\n",
    "        - Only needed for finding n_PNs_tot\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    n_PNs_w_power_srs = build_n_PNs_w_power_srs(\n",
    "        best_ests_df  = best_ests_df, \n",
    "        ami_df_i      = ami_df_i, \n",
    "        return_pct    = return_pct, \n",
    "        PN_col        = PN_col, \n",
    "        t_min_col     = t_min_col, \n",
    "        t_max_col     = t_max_col, \n",
    "        i_outg_col    = i_outg_col, \n",
    "        PN_col_ami_df = PN_col_ami_df\n",
    "    )\n",
    "    #-----\n",
    "    if not n_PNs_w_power_srs.name:\n",
    "        n_PNs_w_power_srs.name = 'pct_w_power'\n",
    "    #-------------------------    \n",
    "    fig,ax = plot_n_PNs_w_power_srs(\n",
    "        n_PNs_w_power_srs=n_PNs_w_power_srs, \n",
    "        simp_freq=simp_freq, \n",
    "        threshold=threshold, \n",
    "        fig_num=fig_num, \n",
    "        title=title, \n",
    "        adjust_texts=adjust_texts, \n",
    "        fig_ax=fig_ax, \n",
    "        **plot_kwargs\n",
    "    )\n",
    "    #-------------------------\n",
    "    return n_PNs_w_power_srs, fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1adc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c39fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fa71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
