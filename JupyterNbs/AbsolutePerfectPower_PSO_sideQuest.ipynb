{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf88300-dbe5-4de1-837f-0bb393be2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload(Utilities)\n",
    "\n",
    "import sys, os\n",
    "import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "import time\n",
    "from sklearn.cluster import DBSCAN\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import win32com.client as win32\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "import Plot_General\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9601d-ae6e-4821-9577-be247f0b23e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302684e4-87eb-4c0e-b168-0fda6e941497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "def build_pdpu_matcher_sql(\n",
    "    date_0                 , \n",
    "    date_1                 , \n",
    "    opco                   , \n",
    "    td_sec_seqntl_pu_or_pd = 5, \n",
    "    addtnl_group_by        = ['station_nb', 'circuit_nb'], \n",
    "    pd_ids                 = ['3.26.0.47', '3.26.136.47', '3.26.136.66'], \n",
    "    pu_ids                 = ['3.26.0.216', '3.26.136.216'], \n",
    "    include_final_select   = True, \n",
    "):\n",
    "    r\"\"\"\n",
    "    NOTE: In order to be returned, the meter must have a listed phase_val from cds_ds_db.com_ccp_dgis_extract \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    # BELOW LINES ARE TEMPORARY UNTIL FULL cds_ds_db.com_ccp_dgis_extract table set up!!!!!\n",
    "    assert(opco in ['pso', 'im'])\n",
    "    if opco=='pso':\n",
    "        dgis_query = 'SELECT prem_nb, \" phase_val\" AS phase_val FROM cds_ds_db.com_ccp_dgis_extract'\n",
    "    else:\n",
    "        dgis_query = 'SELECT prem_nb, phase_val FROM cds_ds_db.com_ccp_dgis_extract_im'\n",
    "    #--------------------------------------------------\n",
    "    if addtnl_group_by is None:\n",
    "        group_by = []\n",
    "    else:\n",
    "        group_by = addtnl_group_by\n",
    "    #--------------------------------------------------\n",
    "    accptbl_group_by = ['station_nb', 'station_nm', 'circuit_nb', 'circuit_nm', 'trsf_pole_nb']\n",
    "    assert(set(group_by).difference(set(accptbl_group_by))==set())\n",
    "    #--------------------------------------------------\n",
    "    datetime_pattern = r\"([0-9]{4}-[0-9]{2}-[0-9]{2})T([0-9]{2}:[0-9]{2}:[0-9]{2}).*\"\n",
    "    group_by_str = Utilities.join_list(\n",
    "        list_to_join  = group_by, \n",
    "        quotes_needed = False, \n",
    "        join_str      = ', '\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    if len(group_by)>0:\n",
    "        group_by_comma = ', '\n",
    "    else:\n",
    "        group_by_comma = ''\n",
    "    #--------------------------------------------------\n",
    "    sql = \"\"\"\n",
    "    WITH MP AS (\n",
    "        SELECT\n",
    "            MP.mfr_devc_ser_nbr,\n",
    "            MP.prem_nb, \\n{}\n",
    "            COMP.opco_nm, \n",
    "            DGIS.phase_val\n",
    "    \tFROM \n",
    "            default.meter_premise MP\n",
    "    \tINNER JOIN (SELECT opco_nb,opco_nm FROM default.company) COMP \n",
    "            ON  MP.co_cd_ownr=COMP.opco_nb\n",
    "    \tINNER JOIN ({}) DGIS \n",
    "            ON  MP.prem_nb=DGIS.prem_nb\"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            MP.{x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )+group_by_comma, \n",
    "        dgis_query\n",
    "    )\n",
    "    if opco is not None:\n",
    "        sql += \"\"\"\n",
    "        WHERE \n",
    "            COMP.opco_nm = '{}'\"\"\".format(opco)\n",
    "    sql += \"\\n    ),\"\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    filtered_events AS (\n",
    "        SELECT\n",
    "            EDE.serialnumber,\n",
    "            EDE.aep_premise_nb, \n",
    "            CAST(regexp_replace(EDE.valuesinterval, '{}', '$1 $2') AS TIMESTAMP) AS valuesinterval,\n",
    "            CASE \n",
    "                WHEN EDE.enddeviceeventtypeid IN ({}) THEN 1 -- Power-up\n",
    "                WHEN EDE.enddeviceeventtypeid IN ({}) THEN 0 -- Power-down\n",
    "                ELSE NULL\n",
    "            END AS power_status, \n",
    "            MP.*\n",
    "        FROM \n",
    "            meter_events.end_device_event EDE\n",
    "        INNER JOIN MP \n",
    "            ON  EDE.serialnumber=MP.mfr_devc_ser_nbr \n",
    "            AND EDE.aep_premise_nb=MP.prem_nb\n",
    "        WHERE \n",
    "            EDE.aep_event_dt BETWEEN '{}' AND '{}'\n",
    "            AND   EDE.enddeviceeventtypeid IN ({}) \\n{}\"\"\".format(\n",
    "        datetime_pattern, \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = pu_ids, \n",
    "            quotes_needed = True, \n",
    "            join_str      = ', '\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = pd_ids, \n",
    "            quotes_needed = True, \n",
    "            join_str      = ', '\n",
    "        ), \n",
    "        date_0, \n",
    "        date_1, \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = pd_ids+pu_ids, \n",
    "            quotes_needed = True, \n",
    "            join_str      = ', '\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f\"            AND NULLIF(TRIM(MP.{x}), '') IS NOT NULL\" for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ' \\n'\n",
    "        )\n",
    "    )\n",
    "    if opco is not None:\n",
    "        sql += \"\\n            AND   EDE.aep_opco = '{}'\".format(opco)\n",
    "    sql += \"\\n    ),\"\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    marked_events_by_SN AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            CASE\n",
    "                WHEN ROW_NUMBER() OVER (PARTITION BY {}aep_premise_nb, serialnumber ORDER BY valuesinterval, power_status) = 1 AND power_status = 0 THEN -1\n",
    "                WHEN ROW_NUMBER() OVER (PARTITION BY {}aep_premise_nb, serialnumber ORDER BY valuesinterval, power_status) = 1 AND power_status = 1 THEN  0\n",
    "                ELSE power_status - LAG(power_status) OVER (PARTITION BY {}aep_premise_nb, serialnumber ORDER BY valuesinterval, power_status) \n",
    "            END AS diff_power_status, \n",
    "            DATE_DIFF('second', LAG(valuesinterval) OVER (PARTITION BY {}aep_premise_nb, serialnumber ORDER BY valuesinterval, power_status), valuesinterval) AS diff_time\n",
    "        FROM \n",
    "            filtered_events\n",
    "    ), \"\"\".format(\n",
    "        group_by_str+group_by_comma, \n",
    "        group_by_str+group_by_comma, \n",
    "        group_by_str+group_by_comma, \n",
    "        group_by_str+group_by_comma\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    id_outage_groups_by_SN AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            SUM(CASE\n",
    "                    WHEN power_status = 0 THEN -- diff_power_status can only equal 0 or -1 in this case\n",
    "                        CASE\n",
    "                            WHEN diff_power_status = -1 THEN 1                   -- always start new group on transition\n",
    "                            WHEN diff_power_status = 0 AND diff_time > {} THEN 1 -- if previous PD was more than 1 minute prior, define new\n",
    "                            ELSE 0                                               -- if previous PD was within past minute, keep in this group\n",
    "                        END\n",
    "                    WHEN power_status = 1 THEN -- diff_power_status can only equal 0 or +1 in this case\n",
    "                        CASE\n",
    "                            WHEN diff_power_status = 1 THEN 0                    -- If PD immediately preceding, always keep together\n",
    "                            WHEN diff_power_status = 0 AND diff_time > {} THEN 1 -- if previous PU was more than 1 minute prior, define new\n",
    "                            ELSE 0                                               -- if previous PU was within past minute, keep in this group\n",
    "                        END\n",
    "                    ELSE NULL -- Should never happen \n",
    "                END) OVER (PARTITION BY {}aep_premise_nb, serialnumber ORDER BY valuesinterval) AS outage_group\n",
    "        FROM \n",
    "            marked_events_by_SN\n",
    "    ), \"\"\".format(\n",
    "        td_sec_seqntl_pu_or_pd, \n",
    "        td_sec_seqntl_pu_or_pd, \n",
    "        group_by_str+group_by_comma\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    agg_outage_groups_by_SN AS (\n",
    "        SELECT \\n{}\n",
    "            aep_premise_nb, \n",
    "            serialnumber,\n",
    "            phase_val, \n",
    "            CASE \n",
    "                WHEN COUNT(DISTINCT power_status) > 1 THEN MIN(CASE WHEN power_status = 0 THEN valuesinterval END) --could also use MAX, depending on preferences\n",
    "                ELSE\n",
    "                    CASE\n",
    "                        WHEN MIN(power_status) = 0 THEN MIN(valuesinterval)\n",
    "                        ELSE NULL\n",
    "                    END\n",
    "            END AS outage_start, \n",
    "            CASE \n",
    "                WHEN COUNT(DISTINCT power_status) > 1 THEN MAX(CASE WHEN power_status = 1 THEN valuesinterval END) --could also use MIN, depending on preferences\n",
    "                ELSE\n",
    "                    CASE\n",
    "                        WHEN MIN(power_status) = 0 THEN NULL\n",
    "                        ELSE MAX(valuesinterval) \n",
    "                    END\n",
    "            END AS outage_end, \n",
    "            SUM(CASE WHEN power_status = 0 THEN 1 END) AS n_pd, \n",
    "            SUM(CASE WHEN power_status = 1 THEN 1 END) AS n_pu\n",
    "        FROM \n",
    "            id_outage_groups_by_SN\n",
    "        GROUP BY \\n{}\n",
    "            aep_premise_nb, \n",
    "            serialnumber, \n",
    "            phase_val, \n",
    "            outage_group\n",
    "        ORDER BY \\n{}\n",
    "            aep_premise_nb, \n",
    "            serialnumber, \n",
    "            phase_val, \n",
    "            outage_start\n",
    "    )\"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )+group_by_comma, \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )+group_by_comma, \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )+group_by_comma, \n",
    "    )\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    if include_final_select:\n",
    "        sql += \"\"\"\n",
    "        SELECT \n",
    "            *, \n",
    "            DATE_DIFF('second', outage_start, outage_end) as duration\n",
    "        FROM \n",
    "            agg_outage_groups_by_SN\"\"\"\n",
    "    #--------------------------------------------------\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11c5d9-7f3b-4f84-92f8-d26dd85ce408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9c464-a49f-4664-bffc-aad485ec42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "def build_CRED_sql(\n",
    "    date_0                 , \n",
    "    date_1                 , \n",
    "    opco                   , \n",
    "    min_pct_SN             = None, \n",
    "    td_sec_group_daisy     = 5, \n",
    "    td_sec_seqntl_pu_or_pd = 5, \n",
    "    td_sec_final_daisy     = 5, \n",
    "    group_by               = ['station_nb', 'circuit_nb'], \n",
    "    pd_ids                 = ['3.26.0.47', '3.26.136.47', '3.26.136.66'], \n",
    "    pu_ids                 = ['3.26.0.216', '3.26.136.216']\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    # BELOW LINES ARE TEMPORARY UNTIL FULL cds_ds_db.com_ccp_dgis_extract table set up!!!!!\n",
    "    assert(opco in ['pso', 'im'])\n",
    "    if opco=='pso':\n",
    "        dgis_query = 'SELECT prem_nb, \" phase_val\" AS phase_val FROM cds_ds_db.com_ccp_dgis_extract'\n",
    "    else:\n",
    "        dgis_query = 'SELECT prem_nb, phase_val FROM cds_ds_db.com_ccp_dgis_extract_im'\n",
    "    #--------------------------------------------------\n",
    "    accptbl_group_by = ['station_nb', 'station_nm', 'circuit_nb', 'circuit_nm', 'trsf_pole_nb']\n",
    "    assert(set(group_by).difference(set(accptbl_group_by))==set())\n",
    "    #--------------------------------------------------\n",
    "    datetime_pattern = r\"([0-9]{4}-[0-9]{2}-[0-9]{2})T([0-9]{2}:[0-9]{2}:[0-9]{2}).*\"\n",
    "    group_by_str = Utilities.join_list(\n",
    "        list_to_join  = group_by, \n",
    "        quotes_needed = False, \n",
    "        join_str      = ', '\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql = build_pdpu_matcher_sql(\n",
    "        date_0                 = date_0, \n",
    "        date_1                 = date_1, \n",
    "        opco                   = opco, \n",
    "        td_sec_seqntl_pu_or_pd = td_sec_seqntl_pu_or_pd, \n",
    "        addtnl_group_by        = group_by, \n",
    "        pd_ids                 = pd_ids, \n",
    "        pu_ids                 = pu_ids, \n",
    "        include_final_select   = False\n",
    "    )+','\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    id_outage_groups_0 AS (\n",
    "        SELECT \n",
    "            *, \n",
    "            DATE_DIFF('second', outage_start, outage_end) as duration, \n",
    "            -- Below, if first entry, set difference to 0 so always evaluates to True\n",
    "            -- Additional ordering by aep_premise_nb, serialnumber to make sure first entry always consistent/reproducible\n",
    "            CASE\n",
    "                WHEN ROW_NUMBER() OVER (PARTITION BY {} ORDER BY outage_start, aep_premise_nb, serialnumber) = 1 THEN 0\n",
    "                ELSE \n",
    "                    CASE\n",
    "                        WHEN DATE_DIFF('second', LAG(outage_start) OVER (PARTITION BY {} ORDER BY outage_start, aep_premise_nb, serialnumber), outage_start) <= {} THEN 0\n",
    "                        ELSE 1\n",
    "                    END\n",
    "            END AS grp_incrementor\n",
    "        FROM \n",
    "            agg_outage_groups_by_SN\n",
    "        WHERE \n",
    "            outage_start IS NOT NULL\n",
    "    ), \"\"\".format(\n",
    "        group_by_str, \n",
    "        group_by_str, \n",
    "        td_sec_group_daisy\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    id_outage_groups_1 AS (\n",
    "        SELECT \n",
    "            *, \n",
    "            CASE\n",
    "                WHEN duration < 8 THEN 0\n",
    "                WHEN duration >= 8 and duration < 300 THEN 1\n",
    "                WHEN duration >= 300 THEN 2\n",
    "                ELSE 3\n",
    "            END as duration_grp, \n",
    "            SUM(grp_incrementor) OVER (PARTITION BY {} ORDER BY outage_start, aep_premise_nb, serialnumber) as group_by_grp_i\n",
    "        FROM \n",
    "            id_outage_groups_0\n",
    "    ), \"\"\".format(\n",
    "        group_by_str\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    agg_outage_groups_0_total AS (\n",
    "        SELECT \\n{}, \n",
    "            group_by_grp_i,\n",
    "            COUNT(serialnumber)            AS total_SN_events,\n",
    "            COUNT(DISTINCT serialnumber)   AS unique_SNs,\n",
    "            COUNT(aep_premise_nb)          AS total_PN_events,\n",
    "            COUNT(DISTINCT aep_premise_nb) AS unique_PNs, \n",
    "            MIN(outage_start)              AS min_outage_start, \n",
    "            MAX(outage_start)              AS max_outage_start, \n",
    "            MIN(duration)                  AS min_duration, \n",
    "            MAX(duration)                  AS max_duration, \n",
    "            AVG(duration)                  AS avg_duration, \n",
    "            AVG(duration*duration)         AS avg_duration_sq, \n",
    "            STDDEV(duration)               AS std_duration, \n",
    "            STDDEV_POP(duration)           AS std_pop_duration\n",
    "        FROM \n",
    "            id_outage_groups_1\n",
    "        GROUP BY \\n{}, \n",
    "            group_by_grp_i\n",
    "        ORDER BY \\n{}, \n",
    "            group_by_grp_i\n",
    "    ),  \"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    agg_outage_groups_0_phase_dur AS (\n",
    "        SELECT \\n{}, \n",
    "            phase_val, \n",
    "            group_by_grp_i,\n",
    "            duration_grp, \n",
    "            COUNT(serialnumber)            AS total_SN_events,\n",
    "            COUNT(DISTINCT serialnumber)   AS unique_SNs,\n",
    "            COUNT(aep_premise_nb)          AS total_PN_events,\n",
    "            COUNT(DISTINCT aep_premise_nb) AS unique_PNs, \n",
    "            MIN(outage_start)              AS min_outage_start, \n",
    "            MAX(outage_start)              AS max_outage_start, \n",
    "            MIN(duration)                  AS min_duration, \n",
    "            MAX(duration)                  AS max_duration, \n",
    "            AVG(duration)                  AS avg_duration, \n",
    "            AVG(duration*duration)         AS avg_duration_sq, \n",
    "            STDDEV(duration)               AS std_duration, \n",
    "            STDDEV_POP(duration)           AS std_pop_duration\n",
    "        FROM \n",
    "            id_outage_groups_1\n",
    "        GROUP BY \\n{}, \n",
    "            phase_val, \n",
    "            group_by_grp_i, \n",
    "            duration_grp\n",
    "        ORDER BY \\n{}, \n",
    "            phase_val, \n",
    "            group_by_grp_i, \n",
    "            duration_grp\n",
    "    ),  \"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    agg_outage_groups_0_phase_dur_wide AS (\n",
    "        SELECT \\n{}, \n",
    "            group_by_grp_i,\n",
    "            -- Below, there shuold only be one entry for each duration_grp (for each group in GROUP BY clause)\n",
    "            -- Therefore, the SUM/MIN/MAX/w.e. are not really necessary\n",
    "            ----------------------------------------------------------------------------------------------------\n",
    "            -- Net momentaries, requested by client\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN duration_grp = 1 THEN total_SN_events ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_1, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN duration_grp = 1 THEN total_PN_events ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_1, \n",
    "            ----------------------------------------------------------------------------------------------------\n",
    "            -- A_0\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 0 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_A_0, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 0 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_A_0, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 0 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_A_0, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 0 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_A_0, \n",
    "            --------------------------------------------------\n",
    "            -- B_0\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 0 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_B_0, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 0 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_B_0, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 0 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_B_0, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 0 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_B_0, \n",
    "            --------------------------------------------------\n",
    "            -- C_0\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 0 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_C_0, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 0 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_C_0, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 0 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_C_0, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 0 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_C_0, \n",
    "            ----------------------------------------------------------------------------------------------------\n",
    "            -- A_1\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 1 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_A_1, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 1 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_A_1, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 1 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_A_1, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 1 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_A_1, \n",
    "            --------------------------------------------------\n",
    "            -- B_1\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 1 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_B_1, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 1 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_B_1, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 1 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_B_1, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 1 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_B_1, \n",
    "            --------------------------------------------------\n",
    "            -- C_1\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 1 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_C_1, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 1 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_C_1, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 1 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_C_1, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 1 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_C_1, \n",
    "            ----------------------------------------------------------------------------------------------------\n",
    "            -- A_2\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 2 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_A_2, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 2 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_A_2, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 2 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_A_2, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 2 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_A_2, \n",
    "            --------------------------------------------------\n",
    "            -- B_2\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 2 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_B_2, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 2 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_B_2, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 2 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_B_2, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 2 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_B_2, \n",
    "            --------------------------------------------------\n",
    "            -- C_2\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 2 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_C_2, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 2 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_C_2, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 2 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_C_2, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 2 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_C_2, \n",
    "            ----------------------------------------------------------------------------------------------------\n",
    "            -- A_3\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 3 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_A_3, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 3 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_A_3, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 3 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_A_3, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') AND duration_grp = 3 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_A_3, \n",
    "            --------------------------------------------------\n",
    "            -- B_3\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 3 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_B_3, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 3 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_B_3, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 3 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_B_3, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') AND duration_grp = 3 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_B_3, \n",
    "            --------------------------------------------------\n",
    "            -- C_3\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 3 THEN total_SN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_SN_events_C_3, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 3 THEN unique_SNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_SNs_C_3, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 3 THEN total_PN_events\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS total_PN_events_C_3, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') AND duration_grp = 3 THEN unique_PNs\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS unique_PNs_C_3\n",
    "        FROM \n",
    "            agg_outage_groups_0_phase_dur\n",
    "        GROUP BY \\n{}, \n",
    "            group_by_grp_i\n",
    "        ORDER BY \\n{}, \n",
    "            group_by_grp_i\n",
    "    ),  \"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    group_infos AS (\n",
    "        SELECT \\n{},\n",
    "            COUNT(DISTINCT MP.mfr_devc_ser_nbr) AS group_SN_cnt,\n",
    "            COUNT(DISTINCT MP.prem_nb)          AS group_PN_cnt, \n",
    "            AVG(CAST(COALESCE(NULLIF(TRIM(MP.latitude), ''),  NULLIF(TRIM(MP.latitude_nb), '')) AS DOUBLE))  AS avg_latitude, \n",
    "            AVG(CAST(COALESCE(NULLIF(TRIM(MP.longitude), ''), NULLIF(TRIM(MP.longitude_nb), '')) AS DOUBLE)) AS avg_longitude\n",
    "        FROM \n",
    "            default.meter_premise MP\n",
    "        INNER JOIN (SELECT opco_nb,opco_nm FROM default.company) COMP \n",
    "            ON  MP.co_cd_ownr=COMP.opco_nb\n",
    "        INNER JOIN ({}) DGIS\n",
    "            ON  MP.prem_nb=DGIS.prem_nb\n",
    "        WHERE \n",
    "            COMP.opco_nm = '{}'\n",
    "        GROUP BY \\n{}\n",
    "    ),\"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            MP.{x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        dgis_query, \n",
    "        opco, \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            MP.{x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    group_infos_by_phase_0 AS (\n",
    "        SELECT \\n{},\n",
    "            DGIS.phase_val, \n",
    "            COUNT(DISTINCT MP.mfr_devc_ser_nbr) AS group_SN_cnt,\n",
    "            COUNT(DISTINCT MP.prem_nb)          AS group_PN_cnt\n",
    "        FROM \n",
    "            default.meter_premise MP\n",
    "        INNER JOIN (SELECT opco_nb,opco_nm FROM default.company) COMP \n",
    "            ON  MP.co_cd_ownr=COMP.opco_nb\n",
    "        INNER JOIN ({}) DGIS\n",
    "            ON  MP.prem_nb=DGIS.prem_nb\n",
    "        WHERE \n",
    "            COMP.opco_nm = '{}'\n",
    "        GROUP BY \\n{}, \n",
    "            DGIS.phase_val\n",
    "    ),\"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            MP.{x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        dgis_query, \n",
    "        opco, \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            MP.{x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )\n",
    "    ) \n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    group_infos_by_phase AS (\n",
    "        SELECT \\n{},\n",
    "            ----------------------------------------------------------------------------------------------------\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') THEN group_SN_cnt\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS group_SN_cnt_A, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='A' or phase_val='AB' or phase_val='AC' or phase_val='ABC') THEN group_PN_cnt\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS group_PN_cnt_A, \n",
    "            ----------------------------------------------------------------------------------------------------\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') THEN group_SN_cnt\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS group_SN_cnt_B, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='B' or phase_val='AB' or phase_val='BC' or phase_val='ABC') THEN group_PN_cnt\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS group_PN_cnt_B, \n",
    "            ----------------------------------------------------------------------------------------------------\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') THEN group_SN_cnt\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS group_SN_cnt_C, \n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN (phase_val='C' or phase_val='AC' or phase_val='BC' or phase_val='ABC') THEN group_PN_cnt\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ) AS group_PN_cnt_C\n",
    "        FROM \n",
    "            group_infos_by_phase_0\n",
    "        GROUP BY \\n{}\n",
    "    ),\"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    agg_outage_groups AS (\n",
    "        SELECT \n",
    "            aog_0_tot.*, \n",
    "            group_infos.group_SN_cnt, \n",
    "            group_infos.group_PN_cnt, \n",
    "            group_infos.avg_latitude, \n",
    "            group_infos.avg_longitude, \n",
    "            aog_0_tot.total_SN_events*1.0 / group_infos.group_SN_cnt*1.0 AS norm_total_SN_events, -- *1.0 to make double instead of int!\n",
    "            aog_0_tot.unique_SNs*1.0      / group_infos.group_SN_cnt*1.0 AS norm_unique_SNs, \n",
    "            aog_0_tot.total_PN_events*1.0 / group_infos.group_PN_cnt*1.0 AS norm_total_PN_events, \n",
    "            aog_0_tot.unique_PNs*1.0      / group_infos.group_PN_cnt*1.0 AS norm_unique_PNs\n",
    "        FROM \n",
    "            agg_outage_groups_0_total aog_0_tot\n",
    "        LEFT JOIN group_infos \n",
    "            ON  {}   \n",
    "    ), \"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f\"aog_0_tot.{x}=group_infos.{x}\" for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = '\\n            AND '\n",
    "        )\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    id_super_time_grps_0 AS (\n",
    "        SELECT\n",
    "            *, \n",
    "            CASE\n",
    "                WHEN ROW_NUMBER() OVER (ORDER BY min_outage_start) = 1 THEN 0\n",
    "                ELSE \n",
    "                    CASE\n",
    "                        WHEN DATE_DIFF('second', LAG(max_outage_start) OVER (ORDER BY min_outage_start), min_outage_start) <= {} THEN 0\n",
    "                        ELSE 1\n",
    "                    END\n",
    "            END AS super_time_grp_incrmntr\n",
    "        FROM\n",
    "            agg_outage_groups\"\"\".format(\n",
    "        td_sec_final_daisy\n",
    "    )\n",
    "    if min_pct_SN is not None:\n",
    "        sql += \"\"\"\n",
    "        WHERE\n",
    "            norm_unique_SNs > {}\"\"\".format(min_pct_SN)\n",
    "    sql += \"\"\"\n",
    "        ORDER BY \n",
    "            min_outage_start\n",
    "    ), \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    id_super_time_grps_1 AS (\n",
    "        SELECT \n",
    "            *, \n",
    "            SUM(super_time_grp_incrmntr) OVER (ORDER BY min_outage_start) as super_time_grp\n",
    "        FROM \n",
    "            id_super_time_grps_0\n",
    "    ) \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    SELECT \n",
    "        fnl.super_time_grp, \n",
    "        super_times.super_min_outage_start, \\n{}, \n",
    "        fnl.group_by_grp_i, \n",
    "        100.0*fnl.norm_unique_SNs      AS norm_unique_SNs, \n",
    "        100.0*fnl.norm_unique_PNs      AS norm_unique_PNs, \n",
    "        100.0*fnl.norm_total_SN_events AS norm_total_SN_events, \n",
    "        100.0*fnl.norm_total_PN_events AS norm_total_PN_events, \n",
    "        --------------------------------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_A_0*1.0      / group_infos_bp.group_SN_cnt_A*1.0 AS norm_unique_SNs_A_0,\n",
    "        100.0*aog_phs_dur.unique_PNs_A_0*1.0      / group_infos_bp.group_PN_cnt_A*1.0 AS norm_unique_PNs_A_0,\n",
    "        100.0*aog_phs_dur.total_SN_events_A_0*1.0 / group_infos_bp.group_SN_cnt_A*1.0 AS norm_total_SN_events_A_0,\n",
    "        100.0*aog_phs_dur.total_PN_events_A_0*1.0 / group_infos_bp.group_PN_cnt_A*1.0 AS norm_total_PN_events_A_0,\n",
    "        -------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_B_0*1.0      / group_infos_bp.group_SN_cnt_B*1.0 AS norm_unique_SNs_B_0,\n",
    "        100.0*aog_phs_dur.unique_PNs_B_0*1.0      / group_infos_bp.group_PN_cnt_B*1.0 AS norm_unique_PNs_B_0,\n",
    "        100.0*aog_phs_dur.total_SN_events_B_0*1.0 / group_infos_bp.group_SN_cnt_B*1.0 AS norm_total_SN_events_B_0,\n",
    "        100.0*aog_phs_dur.total_PN_events_B_0*1.0 / group_infos_bp.group_PN_cnt_B*1.0 AS norm_total_PN_events_B_0,\n",
    "        -------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_C_0*1.0      / group_infos_bp.group_SN_cnt_C*1.0 AS norm_unique_SNs_C_0,\n",
    "        100.0*aog_phs_dur.unique_PNs_C_0*1.0      / group_infos_bp.group_PN_cnt_C*1.0 AS norm_unique_PNs_C_0,\n",
    "        100.0*aog_phs_dur.total_SN_events_C_0*1.0 / group_infos_bp.group_SN_cnt_C*1.0 AS norm_total_SN_events_C_0,\n",
    "        100.0*aog_phs_dur.total_PN_events_C_0*1.0 / group_infos_bp.group_PN_cnt_C*1.0 AS norm_total_PN_events_C_0,\n",
    "        --------------------------------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_A_1*1.0      / group_infos_bp.group_SN_cnt_A*1.0 AS norm_unique_SNs_A_1,\n",
    "        100.0*aog_phs_dur.unique_PNs_A_1*1.0      / group_infos_bp.group_PN_cnt_A*1.0 AS norm_unique_PNs_A_1,\n",
    "        100.0*aog_phs_dur.total_SN_events_A_1*1.0 / group_infos_bp.group_SN_cnt_A*1.0 AS norm_total_SN_events_A_1,\n",
    "        100.0*aog_phs_dur.total_PN_events_A_1*1.0 / group_infos_bp.group_PN_cnt_A*1.0 AS norm_total_PN_events_A_1,\n",
    "        -------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_B_1*1.0      / group_infos_bp.group_SN_cnt_B*1.0 AS norm_unique_SNs_B_1,\n",
    "        100.0*aog_phs_dur.unique_PNs_B_1*1.0      / group_infos_bp.group_PN_cnt_B*1.0 AS norm_unique_PNs_B_1,\n",
    "        100.0*aog_phs_dur.total_SN_events_B_1*1.0 / group_infos_bp.group_SN_cnt_B*1.0 AS norm_total_SN_events_B_1,\n",
    "        100.0*aog_phs_dur.total_PN_events_B_1*1.0 / group_infos_bp.group_PN_cnt_B*1.0 AS norm_total_PN_events_B_1,\n",
    "        -------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_C_1*1.0      / group_infos_bp.group_SN_cnt_C*1.0 AS norm_unique_SNs_C_1,\n",
    "        100.0*aog_phs_dur.unique_PNs_C_1*1.0      / group_infos_bp.group_PN_cnt_C*1.0 AS norm_unique_PNs_C_1,\n",
    "        100.0*aog_phs_dur.total_SN_events_C_1*1.0 / group_infos_bp.group_SN_cnt_C*1.0 AS norm_total_SN_events_C_1,\n",
    "        100.0*aog_phs_dur.total_PN_events_C_1*1.0 / group_infos_bp.group_PN_cnt_C*1.0 AS norm_total_PN_events_C_1,\n",
    "        --------------------------------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_A_2*1.0      / group_infos_bp.group_SN_cnt_A*1.0 AS norm_unique_SNs_A_2,\n",
    "        100.0*aog_phs_dur.unique_PNs_A_2*1.0      / group_infos_bp.group_PN_cnt_A*1.0 AS norm_unique_PNs_A_2,\n",
    "        100.0*aog_phs_dur.total_SN_events_A_2*1.0 / group_infos_bp.group_SN_cnt_A*1.0 AS norm_total_SN_events_A_2,\n",
    "        100.0*aog_phs_dur.total_PN_events_A_2*1.0 / group_infos_bp.group_PN_cnt_A*1.0 AS norm_total_PN_events_A_2,\n",
    "        -------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_B_2*1.0      / group_infos_bp.group_SN_cnt_B*1.0 AS norm_unique_SNs_B_2,\n",
    "        100.0*aog_phs_dur.unique_PNs_B_2*1.0      / group_infos_bp.group_PN_cnt_B*1.0 AS norm_unique_PNs_B_2,\n",
    "        100.0*aog_phs_dur.total_SN_events_B_2*1.0 / group_infos_bp.group_SN_cnt_B*1.0 AS norm_total_SN_events_B_2,\n",
    "        100.0*aog_phs_dur.total_PN_events_B_2*1.0 / group_infos_bp.group_PN_cnt_B*1.0 AS norm_total_PN_events_B_2,\n",
    "        -------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_C_2*1.0      / group_infos_bp.group_SN_cnt_C*1.0 AS norm_unique_SNs_C_2,\n",
    "        100.0*aog_phs_dur.unique_PNs_C_2*1.0      / group_infos_bp.group_PN_cnt_C*1.0 AS norm_unique_PNs_C_2,\n",
    "        100.0*aog_phs_dur.total_SN_events_C_2*1.0 / group_infos_bp.group_SN_cnt_C*1.0 AS norm_total_SN_events_C_2,\n",
    "        100.0*aog_phs_dur.total_PN_events_C_2*1.0 / group_infos_bp.group_PN_cnt_C*1.0 AS norm_total_PN_events_C_2,\n",
    "        --------------------------------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_A_3*1.0      / group_infos_bp.group_SN_cnt_A*1.0 AS norm_unique_SNs_A_3,\n",
    "        100.0*aog_phs_dur.unique_PNs_A_3*1.0      / group_infos_bp.group_PN_cnt_A*1.0 AS norm_unique_PNs_A_3,\n",
    "        100.0*aog_phs_dur.total_SN_events_A_3*1.0 / group_infos_bp.group_SN_cnt_A*1.0 AS norm_total_SN_events_A_3,\n",
    "        100.0*aog_phs_dur.total_PN_events_A_3*1.0 / group_infos_bp.group_PN_cnt_A*1.0 AS norm_total_PN_events_A_3,\n",
    "        -------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_B_3*1.0      / group_infos_bp.group_SN_cnt_B*1.0 AS norm_unique_SNs_B_3,\n",
    "        100.0*aog_phs_dur.unique_PNs_B_3*1.0      / group_infos_bp.group_PN_cnt_B*1.0 AS norm_unique_PNs_B_3,\n",
    "        100.0*aog_phs_dur.total_SN_events_B_3*1.0 / group_infos_bp.group_SN_cnt_B*1.0 AS norm_total_SN_events_B_3,\n",
    "        100.0*aog_phs_dur.total_PN_events_B_3*1.0 / group_infos_bp.group_PN_cnt_B*1.0 AS norm_total_PN_events_B_3,\n",
    "        -------------------------\n",
    "        100.0*aog_phs_dur.unique_SNs_C_3*1.0      / group_infos_bp.group_SN_cnt_C*1.0 AS norm_unique_SNs_C_3,\n",
    "        100.0*aog_phs_dur.unique_PNs_C_3*1.0      / group_infos_bp.group_PN_cnt_C*1.0 AS norm_unique_PNs_C_3,\n",
    "        100.0*aog_phs_dur.total_SN_events_C_3*1.0 / group_infos_bp.group_SN_cnt_C*1.0 AS norm_total_SN_events_C_3,\n",
    "        100.0*aog_phs_dur.total_PN_events_C_3*1.0 / group_infos_bp.group_PN_cnt_C*1.0 AS norm_total_PN_events_C_3,\n",
    "        --------------------------------------------------\n",
    "        fnl.min_outage_start, \n",
    "        fnl.max_outage_start, \n",
    "        fnl.min_duration, \n",
    "        fnl.max_duration, \n",
    "        fnl.avg_duration, \n",
    "        fnl.avg_duration_sq, \n",
    "        fnl.std_duration, \n",
    "        fnl.std_pop_duration, \n",
    "        fnl.unique_SNs, \n",
    "        fnl.unique_PNs, \n",
    "        fnl.total_SN_events, \n",
    "        fnl.total_PN_events, \n",
    "        --------------------------------------------------\n",
    "        aog_phs_dur.total_SN_events_1, \n",
    "        aog_phs_dur.total_PN_events_1, \n",
    "        --------------------------------------------------\n",
    "        aog_phs_dur.unique_SNs_A_0,\n",
    "        aog_phs_dur.unique_PNs_A_0,\n",
    "        aog_phs_dur.total_SN_events_A_0,\n",
    "        aog_phs_dur.total_PN_events_A_0,\n",
    "        -------------------------\n",
    "        aog_phs_dur.unique_SNs_B_0,\n",
    "        aog_phs_dur.unique_PNs_B_0,\n",
    "        aog_phs_dur.total_SN_events_B_0,\n",
    "        aog_phs_dur.total_PN_events_B_0,\n",
    "        -------------------------\n",
    "        aog_phs_dur.unique_SNs_C_0,\n",
    "        aog_phs_dur.unique_PNs_C_0,\n",
    "        aog_phs_dur.total_SN_events_C_0,\n",
    "        aog_phs_dur.total_PN_events_C_0,\n",
    "        --------------------------------------------------\n",
    "        aog_phs_dur.unique_SNs_A_1,\n",
    "        aog_phs_dur.unique_PNs_A_1,\n",
    "        aog_phs_dur.total_SN_events_A_1,\n",
    "        aog_phs_dur.total_PN_events_A_1,\n",
    "        -------------------------\n",
    "        aog_phs_dur.unique_SNs_B_1,\n",
    "        aog_phs_dur.unique_PNs_B_1,\n",
    "        aog_phs_dur.total_SN_events_B_1,\n",
    "        aog_phs_dur.total_PN_events_B_1,\n",
    "        -------------------------\n",
    "        aog_phs_dur.unique_SNs_C_1,\n",
    "        aog_phs_dur.unique_PNs_C_1,\n",
    "        aog_phs_dur.total_SN_events_C_1,\n",
    "        aog_phs_dur.total_PN_events_C_1,\n",
    "        --------------------------------------------------\n",
    "        aog_phs_dur.unique_SNs_A_2,\n",
    "        aog_phs_dur.unique_PNs_A_2,\n",
    "        aog_phs_dur.total_SN_events_A_2,\n",
    "        aog_phs_dur.total_PN_events_A_2,\n",
    "        -------------------------\n",
    "        aog_phs_dur.unique_SNs_B_2,\n",
    "        aog_phs_dur.unique_PNs_B_2,\n",
    "        aog_phs_dur.total_SN_events_B_2,\n",
    "        aog_phs_dur.total_PN_events_B_2,\n",
    "        -------------------------\n",
    "        aog_phs_dur.unique_SNs_C_2,\n",
    "        aog_phs_dur.unique_PNs_C_2,\n",
    "        aog_phs_dur.total_SN_events_C_2,\n",
    "        aog_phs_dur.total_PN_events_C_2,\n",
    "        --------------------------------------------------\n",
    "        aog_phs_dur.unique_SNs_A_3,\n",
    "        aog_phs_dur.unique_PNs_A_3,\n",
    "        aog_phs_dur.total_SN_events_A_3,\n",
    "        aog_phs_dur.total_PN_events_A_3,\n",
    "        -------------------------\n",
    "        aog_phs_dur.unique_SNs_B_3,\n",
    "        aog_phs_dur.unique_PNs_B_3,\n",
    "        aog_phs_dur.total_SN_events_B_3,\n",
    "        aog_phs_dur.total_PN_events_B_3,\n",
    "        -------------------------\n",
    "        aog_phs_dur.unique_SNs_C_3,\n",
    "        aog_phs_dur.unique_PNs_C_3,\n",
    "        aog_phs_dur.total_SN_events_C_3,\n",
    "        aog_phs_dur.total_PN_events_C_3,\n",
    "        --------------------------------------------------\n",
    "        fnl.group_SN_cnt, \n",
    "        fnl.group_PN_cnt, \n",
    "        group_infos_bp.group_SN_cnt_A, \n",
    "        group_infos_bp.group_PN_cnt_A, \n",
    "        group_infos_bp.group_SN_cnt_B, \n",
    "        group_infos_bp.group_PN_cnt_B, \n",
    "        group_infos_bp.group_SN_cnt_C, \n",
    "        group_infos_bp.group_PN_cnt_C, \n",
    "        fnl.avg_latitude, \n",
    "        fnl.avg_longitude\n",
    "    FROM\n",
    "        id_super_time_grps_1 fnl\n",
    "    LEFT JOIN agg_outage_groups_0_phase_dur_wide aog_phs_dur \n",
    "        ON  {}\n",
    "        AND fnl.group_by_grp_i=aog_phs_dur.group_by_grp_i\n",
    "    LEFT JOIN group_infos_by_phase group_infos_bp\n",
    "        ON  {}\n",
    "    LEFT JOIN (\n",
    "        SELECT \n",
    "            super_time_grp,\n",
    "            MIN(min_outage_start) AS super_min_outage_start \n",
    "        FROM \n",
    "            id_super_time_grps_1\n",
    "        GROUP BY\n",
    "            super_time_grp\n",
    "    ) super_times\n",
    "        ON  fnl.super_time_grp=super_times.super_time_grp\n",
    "    ORDER BY \n",
    "        fnl.super_time_grp, \\n{}, \n",
    "        fnl.group_by_grp_i\"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'        fnl.{x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f\"fnl.{x}=aog_phs_dur.{x}\" for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = '\\n        AND '\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f\"fnl.{x}=group_infos_bp.{x}\" for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = '\\n        AND '\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'        fnl.{x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccd7da-f033-4e7e-b4f9-b8a00a31bb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc059a-cfef-44ef-b519-8c1cab64a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "def build_avg_momentaries_sql(\n",
    "    date_0                 , \n",
    "    date_1                 , \n",
    "    opco                   , \n",
    "    td_sec_seqntl_pu_or_pd = 5, \n",
    "    group_by               = ['station_nb', 'circuit_nb'], \n",
    "    pd_ids                 = ['3.26.0.47', '3.26.136.47', '3.26.136.66'], \n",
    "    pu_ids                 = ['3.26.0.216', '3.26.136.216']\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    accptbl_group_by = ['station_nb', 'station_nm', 'circuit_nb', 'circuit_nm', 'trsf_pole_nb']\n",
    "    assert(set(group_by).difference(set(accptbl_group_by))==set())\n",
    "    #--------------------------------------------------\n",
    "    # NOTE: In SQL queries, the dates in the range are INCLUSIVE, hence to addition of 1 day below\n",
    "    pd_n_days = (pd.to_datetime(date_1)-pd.to_datetime(date_0)+pd.Timedelta('1D')).days\n",
    "    #--------------------------------------------------\n",
    "    sql = build_pdpu_matcher_sql(\n",
    "        date_0                 = date_0, \n",
    "        date_1                 = date_1, \n",
    "        opco                   = opco, \n",
    "        td_sec_seqntl_pu_or_pd = td_sec_seqntl_pu_or_pd, \n",
    "        addtnl_group_by        = group_by, \n",
    "        pd_ids                 = pd_ids, \n",
    "        pu_ids                 = pu_ids, \n",
    "        include_final_select   = False\n",
    "    )+','\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    agg_outage_groups_by_SN_2 AS (\n",
    "        SELECT \n",
    "            *, \n",
    "            DATE_DIFF('second', outage_start, outage_end) as duration\n",
    "        FROM \n",
    "            agg_outage_groups_by_SN\n",
    "        WHERE \n",
    "            outage_start IS NOT NULL\n",
    "            AND DATE_DIFF('second', outage_start, outage_end) >= 8\n",
    "            AND DATE_DIFF('second', outage_start, outage_end) < 300\n",
    "    ), \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    n_momentaries_by_SN AS (\n",
    "        SELECT \\n{}, \n",
    "            aep_premise_nb, \n",
    "            serialnumber,\n",
    "            phase_val, \n",
    "            COUNT(*) AS n_momentaries\n",
    "        FROM\n",
    "            agg_outage_groups_by_SN_2\n",
    "        GROUP BY \\n{},  \n",
    "            aep_premise_nb, \n",
    "            serialnumber, \n",
    "            phase_val\n",
    "        ORDER BY \\n{},  \n",
    "            aep_premise_nb, \n",
    "            serialnumber, \n",
    "            phase_val\n",
    "    ), \"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    group_infos AS (\n",
    "        SELECT \\n{},\n",
    "            COUNT(DISTINCT MP.mfr_devc_ser_nbr) AS group_SN_cnt,\n",
    "            COUNT(DISTINCT MP.prem_nb)          AS group_PN_cnt\n",
    "        FROM \n",
    "            default.meter_premise MP\n",
    "        INNER JOIN (SELECT opco_nb,opco_nm FROM default.company) COMP \n",
    "            ON  MP.co_cd_ownr=COMP.opco_nb\n",
    "        INNER JOIN (SELECT prem_nb, \" phase_val\" AS phase_val FROM cds_ds_db.com_ccp_dgis_extract) DGIS\n",
    "            ON  MP.prem_nb=DGIS.prem_nb\n",
    "        WHERE \n",
    "            COMP.opco_nm = '{}'\n",
    "        GROUP BY \\n{}\n",
    "    ),\"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            MP.{x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        opco, \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            MP.{x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        )\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    sql += \"\"\"\n",
    "    agg_momentaries AS (\n",
    "        SELECT \\n{}, \n",
    "            COUNT(DISTINCT CASE WHEN n_momentaries > 13 THEN serialnumber END)   AS nSNs_w_gt13_mom,\n",
    "            COUNT(DISTINCT CASE WHEN n_momentaries > 13 THEN aep_premise_nb END) AS nPNs_w_gt13_mom,\n",
    "            SUM(n_momentaries) AS sum_n_momentaries\n",
    "        FROM\n",
    "            n_momentaries_by_SN\n",
    "        GROUP BY \\n{} \n",
    "    ) \"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'            {x}' for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    # NOTE: Below, for average, cannot simply call AVG, as this would be the average of meters registering an outage\n",
    "    #       We want average of all meters in group, so we must normalize using group_infos.group_SN_cnt\n",
    "    #-------------------------\n",
    "    # Include the nSNs_w_gt13_mom/nPNs_w_gt13_mom columns ONLY if pd_n_days==365\n",
    "    n_w_gt13_mom_cols = []\n",
    "    if pd_n_days==365:\n",
    "        n_w_gt13_mom_cols = ['nSNs_w_gt13_mom', 'nPNs_w_gt13_mom']\n",
    "    #-------------------------\n",
    "    sql += \"\"\"\n",
    "    SELECT \\n{}, \n",
    "        agg_mom.sum_n_momentaries*1.0/(group_infos.group_SN_cnt*1.0*{}) AS avg_daily_n_mom_per_SN, \n",
    "        agg_mom.sum_n_momentaries*1.0/(group_infos.group_PN_cnt*1.0*{}) AS avg_daily_n_mom_per_PN\n",
    "    FROM\n",
    "        agg_momentaries agg_mom\n",
    "    LEFT JOIN group_infos \n",
    "        ON  {} \"\"\".format(\n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f'        agg_mom.{x}' for x in group_by+n_w_gt13_mom_cols], \n",
    "            quotes_needed = False, \n",
    "            join_str      = ', \\n'\n",
    "        ), \n",
    "        pd_n_days, \n",
    "        pd_n_days, \n",
    "        Utilities.join_list(\n",
    "            list_to_join  = [f\"agg_mom.{x}=group_infos.{x}\" for x in group_by], \n",
    "            quotes_needed = False, \n",
    "            join_str      = '\\n        AND '\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4cef4-9ac8-43d3-a593-41f2c2c072aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3c606-306d-4257-8800-f2116e1cca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "def set_spatial_groups_for_time_group_i(\n",
    "    df_i           , \n",
    "    eps_km         = 25, \n",
    "    min_samples    = 1, \n",
    "    lat_col        = 'avg_latitude', \n",
    "    lon_col        = 'avg_longitude', \n",
    "    spat_group_col = 'spatial_grp'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Intended for use with df_i containing only a single time group!\n",
    "    Also, lat_col/lon_col must NOT contain any NaNs (i.e., parent df should be cleaned before using this)\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    kms_per_radian = 6371.0088\n",
    "    eps            = eps_km/kms_per_radian\n",
    "    #-------------------------\n",
    "    db = DBSCAN(\n",
    "        eps         = eps, \n",
    "        min_samples = min_samples, \n",
    "        algorithm   = 'ball_tree', \n",
    "        metric      = 'haversine'\n",
    "    ).fit(np.radians(df_i[[lat_col, lon_col]]))\n",
    "    #-------------------------\n",
    "    df_i[spat_group_col] = db.labels_\n",
    "    #-------------------------\n",
    "    return df_i\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "def set_spatial_groups(\n",
    "    df             , \n",
    "    time_group_col = 'super_time_grp', \n",
    "    eps_km         = 25, \n",
    "    min_samples    = 1, \n",
    "    lat_col        = 'avg_latitude', \n",
    "    lon_col        = 'avg_longitude', \n",
    "    spat_group_col = 'spatial_grp', \n",
    "    fnl_group_col  = 'super_time_spatial_grp'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Intended for use with df_i containing only a single time group!\n",
    "    Also, lat_col/lon_col must NOT contain any NaNs (i.e., parent df should be cleaned before using this)\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    df[spat_group_col] = -1\n",
    "    #-------------------------\n",
    "    # DBSCAN doesn't like NaN values, so drop those\n",
    "    # We will add back at end\n",
    "    df_na     = df[(df[lat_col].isna()) | (df[lon_col].isna())].copy()\n",
    "    df_non_na = df.dropna(subset=[lat_col, lon_col]).copy()\n",
    "    #-------------------------\n",
    "    df_non_na = df_non_na.groupby(time_group_col, as_index=False, group_keys=False)[df_non_na.columns].apply(\n",
    "        lambda x: set_spatial_groups_for_time_group_i(\n",
    "            df_i           = x, \n",
    "            eps_km         = eps_km, \n",
    "            min_samples    = min_samples, \n",
    "            lat_col        = lat_col, \n",
    "            lon_col        = lon_col, \n",
    "            spat_group_col = spat_group_col\n",
    "        )\n",
    "    )\n",
    "    #-------------------------\n",
    "    df_non_na[fnl_group_col] = df_non_na[time_group_col].astype(str) + '_' + df_non_na[spat_group_col].astype(str)\n",
    "    df_na[fnl_group_col]     = df_na[time_group_col].astype(str)     + '_NaN'\n",
    "    #-------------------------\n",
    "    df = pd.concat([df_non_na, df_na], axis=0)\n",
    "    #-------------------------\n",
    "    # Move [fnl_group_col, time_group_col, spat_group_col] to front\n",
    "    front_cols     = [fnl_group_col, time_group_col, spat_group_col]\n",
    "    remaining_cols = [x for x in df.columns if x not in front_cols]\n",
    "    df             =  df[front_cols + remaining_cols]\n",
    "    #-------------------------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075362e-3090-4d9e-bced-a2cc3ebed27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863bd0f1-09a0-43f6-89aa-e187e9c16430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "def plot_spatial_groups_for_time_group_i(\n",
    "    df_i, \n",
    "    us_ok, \n",
    "    spat_group_col = 'spatial_grp', \n",
    "    lat_col        = 'avg_latitude', \n",
    "    lon_col        = 'avg_longitude',     \n",
    "):\n",
    "    r\"\"\"\n",
    "    Intended for use with df_i containing only a single time group!\n",
    "    Also, lat_col/lon_col must NOT contain any NaNs (i.e., parent df should be cleaned before using this)\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    if us_ok is None:\n",
    "        us_shp_path = r'C:\\Users\\s346557\\Downloads\\tl_2023_us_state\\tl_2023_us_state.shp'\n",
    "        us_df = gpd.read_file(us_shp_path)\n",
    "        us_df = us_df.to_crs(\"EPSG:4326\")\n",
    "        #-----\n",
    "        us_ok = us_df[us_df['STUSPS'].isin(['OK'])].copy()\n",
    "    #--------------------------------------------------\n",
    "    geometry = [Point(xy) for xy in zip(df_i[lon_col], df_i[lat_col])]\n",
    "    geo_df_i = gpd.GeoDataFrame(\n",
    "        df_i, \n",
    "        crs = 'EPSG:4326', \n",
    "        geometry = geometry\n",
    "    )\n",
    "    geo_df_i = geo_df_i.dropna(subset=[lon_col, lat_col])\n",
    "    #--------------------------------------------------\n",
    "    colors      = Plot_General.get_standard_colors(n_colors = geo_df_i[spat_group_col].nunique())\n",
    "    edge_colors = Plot_General.get_standard_colors(n_colors = 10, palette='colorblind')\n",
    "    #--------------------------------------------------\n",
    "    tmp_size_col           = Utilities.generate_random_string(letters='letters_only')\n",
    "    geo_df_i[tmp_size_col] = MinMaxScaler(feature_range=(10, 100)).fit_transform(geo_df_i['norm_unique_SNs'].values.reshape(-1,1))    \n",
    "    #-------------------------\n",
    "    fig,ax = Plot_General.default_subplots(\n",
    "        n_x                 = 1,\n",
    "        n_y                 = 1,\n",
    "        fig_num             = 0,\n",
    "        unit_figsize_width  = 14,\n",
    "        unit_figsize_height = 6\n",
    "    )\n",
    "    #-------------------------\n",
    "    us_ok.plot(ax=ax, color='lightgrey')\n",
    "    #-------------------------\n",
    "    for j,spat_group_j in enumerate(natsorted(geo_df_i[spat_group_col].unique())):\n",
    "        df_ij    = geo_df_i[geo_df_i[spat_group_col]==spat_group_j]\n",
    "        label_ij = \"{} - {}\".format(\n",
    "            df_ij['min_outage_start'].min().strftime('%H:%M:%S'), \n",
    "            df_ij['min_outage_start'].max().strftime('%H:%M:%S')\n",
    "        )\n",
    "        df_ij.plot(\n",
    "            ax=ax, \n",
    "            color=colors[j], \n",
    "            alpha=0.5, \n",
    "            edgecolors=edge_colors[j%10], \n",
    "            label=label_ij, \n",
    "            markersize = tmp_size_col\n",
    "        )\n",
    "    #-------------------------\n",
    "    label = \"{} - {}\".format(\n",
    "        df_i['min_outage_start'].min().strftime('%Y-%m-%d %H:%M:%S'), \n",
    "        df_i['min_outage_start'].max().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    )\n",
    "    ax.set_title(\n",
    "        label = label,\n",
    "        fontdict = dict(\n",
    "            fontsize = 'x-large'\n",
    "        )\n",
    "    )\n",
    "    ax.legend()\n",
    "    #--------------------------------------------------\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359861de-afa5-42b5-a22f-2c7532c0d93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ca39d-6b92-4618-92cf-849a0fc599fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "def write_CRED_worksheet(\n",
    "    writer          , \n",
    "    df_fnl          , \n",
    "    reset_index     , \n",
    "    format1         , \n",
    "    format2         , \n",
    "    format3         , \n",
    "    format4         , \n",
    "    idx_format      , \n",
    "    normalize       = True, \n",
    "    n_avg_moms_cols = 0, \n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    if reset_index:\n",
    "        addtnl_col_offset = df_fnl.index.nlevels #should be 1\n",
    "        #-----\n",
    "        # df_fnl.columns = [','.join(x) for x in df_fnl.columns]\n",
    "        df_fnl.columns = [x[1] for x in df_fnl.columns] \n",
    "        df_fnl         = df_fnl.reset_index().copy()\n",
    "        #-----\n",
    "        df_fnl.to_excel(writer, sheet_name='noIdxGroups', header=True, index=True)\n",
    "        worksheet = writer.sheets['noIdxGroups']\n",
    "    else:\n",
    "        addtnl_col_offset = 0\n",
    "        #-----\n",
    "        df_fnl.to_excel(writer, sheet_name='IdxGroups', header=True, index=True)\n",
    "        worksheet = writer.sheets['IdxGroups']\n",
    "    #-------------------------\n",
    "    if normalize:\n",
    "        addtnl_col_offset += 1\n",
    "    #-------------------------\n",
    "    worksheet.autofit()\n",
    "    #-------------------------\n",
    "    color_mapping = {\n",
    "        2+addtnl_col_offset+df_fnl.index.nlevels  : format1, \n",
    "        3+addtnl_col_offset+df_fnl.index.nlevels  : format1, \n",
    "        4+addtnl_col_offset+df_fnl.index.nlevels  : format1, \n",
    "        #-----\n",
    "        5+addtnl_col_offset+df_fnl.index.nlevels  : format2, \n",
    "        6+addtnl_col_offset+df_fnl.index.nlevels  : format2, \n",
    "        7+addtnl_col_offset+df_fnl.index.nlevels  : format2, \n",
    "        #-----\n",
    "        8+addtnl_col_offset+df_fnl.index.nlevels  : format3, \n",
    "        9+addtnl_col_offset+df_fnl.index.nlevels  : format3, \n",
    "        10+addtnl_col_offset+df_fnl.index.nlevels : format3, \n",
    "        #-----\n",
    "        11+addtnl_col_offset+df_fnl.index.nlevels : format4, \n",
    "        12+addtnl_col_offset+df_fnl.index.nlevels : format4, \n",
    "        13+addtnl_col_offset+df_fnl.index.nlevels : format4, \n",
    "        #-----\n",
    "    }\n",
    "    #-----\n",
    "    width = 9\n",
    "    for col,fmt in color_mapping.items():\n",
    "        # worksheet.set_column(col, col, width, cell_format=fmt)\n",
    "        worksheet.set_column(col, col, width)\n",
    "        worksheet.conditional_format(0, col, df_fnl.shape[0]+df_fnl.columns.nlevels, col, options={'type':'no_blanks', 'format':fmt})\n",
    "        worksheet.conditional_format(0, col, df_fnl.shape[0]+df_fnl.columns.nlevels, col, options={'type':'blanks', 'format':fmt})\n",
    "    #-------------------------\n",
    "    # For whatever reason, autofit slightly undersizes min(max)_outage_start columns\n",
    "    worksheet.set_column(\n",
    "        14 + df_fnl.index.nlevels + addtnl_col_offset + n_avg_moms_cols, \n",
    "        15 + df_fnl.index.nlevels + addtnl_col_offset + n_avg_moms_cols, \n",
    "        18\n",
    "    )\n",
    "    #-------------------------\n",
    "    if not reset_index:\n",
    "        for idx_i in range(df_fnl.index.nlevels):\n",
    "            column_i = list(string.ascii_uppercase)[idx_i]\n",
    "            worksheet.merge_range(f'{column_i}1:{column_i}{df_fnl.columns.nlevels+1}', df_fnl.index.names[idx_i], idx_format)\n",
    "    #-------------------------\n",
    "    worksheet.write(df_fnl.shape[0]+df_fnl.columns.nlevels+5, 0, '*Note: % values between 0 and 1 set to 1')\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "def save_CRED_xlsx(\n",
    "    df_fnl          , \n",
    "    save_dir        , \n",
    "    opco            , \n",
    "    date_0          , \n",
    "    date_1          , \n",
    "    normalize       = True, \n",
    "    n_avg_moms_cols = 0, \n",
    "    reset_index     = 'both'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    assert(reset_index=='both' or isinstance(reset_index, bool))\n",
    "    #-------------------------\n",
    "    assert(isinstance(df_fnl, pd.DataFrame) or isinstance(df_fnl, list))\n",
    "    #-------------------------\n",
    "    if date_0==date_1:\n",
    "        save_name = \"CRED_{}_{}\".format(\n",
    "            opco, \n",
    "            pd.to_datetime(date_0).strftime('%Y%m%d')\n",
    "        )\n",
    "    else:\n",
    "        save_name = \"CRED_{}_{}_{}\".format(\n",
    "            opco, \n",
    "            pd.to_datetime(date_0).strftime('%Y%m%d'), \n",
    "            pd.to_datetime(date_1).strftime('%Y%m%d')\n",
    "        )\n",
    "    #-----\n",
    "    if reset_index==False:\n",
    "        save_name += '_noIdxGroups'\n",
    "    save_name += '.xlsx'\n",
    "    #-----\n",
    "    save_path = os.path.join(save_dir, save_name)\n",
    "    #-------------------------\n",
    "    writer = pd.ExcelWriter(\n",
    "        path   = save_path, \n",
    "        engine = 'xlsxwriter'\n",
    "    )\n",
    "    #-------------------------\n",
    "    workbook  = writer.book\n",
    "    #-------------------------\n",
    "    format1 = workbook.add_format({'bg_color': '#a1c9f4', 'border' : 1, 'border_color' : '#000000'})\n",
    "    format2 = workbook.add_format({'bg_color': '#ffb482', 'border' : 1, 'border_color' : '#000000'})\n",
    "    format3 = workbook.add_format({'bg_color': '#8de5a1', 'border' : 1, 'border_color' : '#000000'})\n",
    "    format4 = workbook.add_format({'bg_color': '#ff9f9b', 'border' : 1, 'border_color' : '#000000'})\n",
    "    #-------------------------\n",
    "    bold_centered_format = workbook.add_format({'bold': True, 'align': 'center', 'valign': 'vcenter'})\n",
    "    #-------------------------\n",
    "    if reset_index=='both':\n",
    "        if isinstance(df_fnl, list):\n",
    "            assert(len(df_fnl)==2)\n",
    "            df_1 = df_fnl[0]\n",
    "            df_2 = df_fnl[1]\n",
    "        else:\n",
    "            df_1 = df_fnl\n",
    "            df_2 = df_fnl\n",
    "        #-----\n",
    "        write_CRED_worksheet(\n",
    "            writer          = writer, \n",
    "            df_fnl          = df_1, \n",
    "            reset_index     = False, \n",
    "            format1         = format1, \n",
    "            format2         = format2, \n",
    "            format3         = format3, \n",
    "            format4         = format4, \n",
    "            idx_format      = bold_centered_format, \n",
    "            normalize       = normalize, \n",
    "            n_avg_moms_cols = n_avg_moms_cols, \n",
    "        )\n",
    "        #-----\n",
    "        write_CRED_worksheet(\n",
    "            writer          = writer, \n",
    "            df_fnl          = df_2, \n",
    "            reset_index     = True, \n",
    "            format1         = format1, \n",
    "            format2         = format2, \n",
    "            format3         = format3, \n",
    "            format4         = format4, \n",
    "            idx_format      = bold_centered_format, \n",
    "            normalize       = normalize, \n",
    "            n_avg_moms_cols = n_avg_moms_cols, \n",
    "        )\n",
    "    else:\n",
    "        assert(isinstance(df_fnl, pd.DataFrame))\n",
    "        write_CRED_worksheet(\n",
    "            writer          = writer, \n",
    "            df_fnl          = df_fnl, \n",
    "            reset_index     = reset_index, \n",
    "            format1         = format1, \n",
    "            format2         = format2, \n",
    "            format3         = format3, \n",
    "            format4         = format4, \n",
    "            idx_format      = bold_centered_format, \n",
    "            normalize       = normalize, \n",
    "            n_avg_moms_cols = n_avg_moms_cols, \n",
    "        )\n",
    "    writer.close()\n",
    "    return save_name, save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9896266-583b-40bb-864b-68eb9b3ffd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571218a-771e-4f59-99ca-129b0575a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "def convert_xlsx_to_xlsm(xlsx_file):\n",
    "    # Initialize the Excel application\n",
    "    excel = win32.Dispatch(\"Excel.Application\")\n",
    "    excel.Visible = False  # Set to True if you want to see the process\n",
    "\n",
    "    # Open the .xlsx file\n",
    "    workbook = excel.Workbooks.Open(xlsx_file)\n",
    "\n",
    "    # Save as .xlsm\n",
    "    xlsm_file = xlsx_file.replace('.xlsx', '.xlsm')\n",
    "    workbook.SaveAs(xlsm_file, FileFormat=52)  # 52 is the file format for .xlsm\n",
    "\n",
    "    # Close the workbook and quit Excel\n",
    "    workbook.Close(SaveChanges=True)\n",
    "    excel.Quit()\n",
    "\n",
    "    return xlsm_file\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "def convert_xlsm_to_xlsx(xlsm_file):\n",
    "    # Initialize the Excel application\n",
    "    excel = win32.Dispatch(\"Excel.Application\")\n",
    "    excel.Visible = False  # Set to True if you want to see the process\n",
    "\n",
    "    # Open the .xlsm file\n",
    "    workbook = excel.Workbooks.Open(xlsm_file)\n",
    "\n",
    "    # Suppress alerts\n",
    "    # Specifically because if I don't, a window pops up saying some things can't be saved in macro-free\n",
    "    #   workbook and asks if I want to continue (thankfully, default option is Yes, continue).\n",
    "    excel.DisplayAlerts = False\n",
    "\n",
    "    # Save as .xlsx\n",
    "    xlsx_file = xlsm_file.replace('.xlsm', '.xlsx')\n",
    "    # If path exists, window pops up and asks if I want to replace.  Sadly, default is no, so must remove myself.\n",
    "    if os.path.exists(xlsx_file):\n",
    "        os.remove(xlsx_file)\n",
    "    workbook.SaveAs(xlsx_file, FileFormat=51)  # 51 is the file format for .xlsx\n",
    "\n",
    "    # Close the workbook and quit Excel\n",
    "    workbook.Close(SaveChanges=True)\n",
    "    excel.Quit()\n",
    "\n",
    "    return xlsx_file\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "def import_and_run_vba_macro(\n",
    "    excel_file_path , \n",
    "    vba_file_path   , \n",
    "    vba_macro_name  ,\n",
    "    pretty_group_by , \n",
    "    addtnl_cols     = None\n",
    "):\n",
    "    r\"\"\"\n",
    "    So annoying.  As long as the VBA macro does not accept any parameters, a .xlsx file is fine.\n",
    "    BUT, if it does accept parameters, then .xlsm must be used!\n",
    "\n",
    "    It seems like the multiple win32.Dispatch objects that are utilized here (one explicitly, two behind the\n",
    "      scenes in convert_xlsx_to_xlsm/convert_xlsm_to_xlsx) can overload Excel, I guess.\n",
    "    Inserting a second of wait time seems to fix things....\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Convert .xlsx to .xlsm\n",
    "    xlsm_file = convert_xlsx_to_xlsm(excel_file_path)\n",
    "    time.sleep(1)\n",
    "\n",
    "    #-------------------------\n",
    "    # Create an instance of Excel\n",
    "    excel = win32.Dispatch(\"Excel.Application\")\n",
    "    excel.Visible = False  # Optional: Set to True to see Excel\n",
    "\n",
    "    # Open the workbook\n",
    "    workbook = excel.Workbooks.Open(xlsm_file)\n",
    "\n",
    "    try:\n",
    "        # Import the VBA module\n",
    "        vb_component = workbook.VBProject.VBComponents.Add(1)  # 1 corresponds to a module\n",
    "        with open(vba_file_path, 'r') as f:\n",
    "            vba_code = f.read()\n",
    "        vb_component.CodeModule.AddFromString(vba_code)\n",
    "    \n",
    "        # Run the macro\n",
    "        # It was a headache to try to feed a list to the macro, so convert list to CSV\n",
    "        # But, to avoid confusion, the elements of pretty_group_by should therefore not contain commas!\n",
    "        #-------------------------\n",
    "        assert(np.all([x.find(',')==-1 for x in group_by]))\n",
    "        group_by_string = ','.join(pretty_group_by)\n",
    "        #-------------------------\n",
    "        if addtnl_cols is None:\n",
    "            addtnl_cols_string = ''\n",
    "        else:\n",
    "            assert(np.all([x.find(',')==-1 for x in addtnl_cols]))\n",
    "            addtnl_cols_string = ','.join(addtnl_cols)            \n",
    "        excel.Application.Run(vba_macro_name, group_by_string, addtnl_cols_string)\n",
    "    \n",
    "        # Optional: Save and close the workbook\n",
    "        workbook.Save()\n",
    "        workbook.Close()\n",
    "    \n",
    "        # Quit Excel application\n",
    "        excel.Quit()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Convert back to .xlsx\n",
    "        output_xlsx = convert_xlsm_to_xlsx(xlsm_file)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Clean up\n",
    "        os.remove(xlsm_file)\n",
    "    except:\n",
    "        workbook.Close()\n",
    "        excel.Quit()\n",
    "        os.remove(xlsm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1023d73-1abd-4b07-8d10-d0588b9d0be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2a01e-870c-4ef2-8feb-e654d9192dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "def get_CRED_sort_order(\n",
    "    df            , \n",
    "    group_by      , \n",
    "    date_idx      = 'date', \n",
    "    event_tag_idx = 'super_time_spatial_grp_fnl', \n",
    "    n_moms_col    = 'total_PN_events_1', \n",
    "):\n",
    "    r\"\"\"\n",
    "    Sort by\n",
    "    1. Date\n",
    "    2. Number of momentaries in overall event tag\n",
    "    3. Number of momentaries in group_by columns\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    assert(set(df.index.names).symmetric_difference(set([date_idx, event_tag_idx]+group_by))==set())\n",
    "    #-------------------------\n",
    "    sort_df = df.groupby([date_idx, event_tag_idx])[n_moms_col].sum().sort_values(ascending=False).reset_index(name='sort_lvl_0')\n",
    "    #-------------------------\n",
    "    for i in range(len(group_by)):\n",
    "        sort_df_i = df.groupby([date_idx, event_tag_idx]+group_by[:i+1])[n_moms_col].sum().sort_values(ascending=False).reset_index(name=f'sort_lvl_{i+1}')\n",
    "        #-----\n",
    "        sort_df = pd.merge(\n",
    "            sort_df, \n",
    "            sort_df_i, \n",
    "            how      = 'inner', \n",
    "            left_on  = [date_idx, event_tag_idx]+group_by[:i], \n",
    "            right_on = [date_idx, event_tag_idx]+group_by[:i], \n",
    "        )\n",
    "    #-------------------------\n",
    "    srt_by = [date_idx] + [f'sort_lvl_{i}' for i in range(len(group_by)+1)]\n",
    "    asc    = [True]     + [False for _ in range(len(group_by)+1)]\n",
    "    #-------------------------\n",
    "    idx_order = sort_df.sort_values(by=srt_by, ascending=asc).set_index([date_idx, event_tag_idx]+group_by).index\n",
    "    assert(idx_order.nunique()==len(idx_order))\n",
    "    #-------------------------\n",
    "    return idx_order\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "def sort_CRED_df(\n",
    "    df            , \n",
    "    group_by      , \n",
    "    date_idx      = 'date', \n",
    "    event_tag_idx = 'super_time_spatial_grp_fnl', \n",
    "    n_moms_col    = 'total_PN_events_1', \n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    idx_order = get_CRED_sort_order(\n",
    "        df            = df, \n",
    "        group_by      = group_by, \n",
    "        date_idx      = date_idx, \n",
    "        event_tag_idx = event_tag_idx, \n",
    "        n_moms_col    = n_moms_col, \n",
    "    )\n",
    "    #-------------------------\n",
    "    df = df.loc[idx_order]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c57d8-5699-481a-a005-159b3f41ff05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e4f99-59f6-4081-a02d-f57c333dab88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041a424-90d9-4227-a750-cd1af9b27a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4df0fd-bb29-48f9-85e7-faeb52259493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "def run_cred(\n",
    "    date_0                 , \n",
    "    date_1                 , \n",
    "    opco                   , \n",
    "    min_pct_SN             = 0.10, \n",
    "    by_premise             = True, \n",
    "    normalize              = True, \n",
    "    td_sec_group_daisy     = 5, \n",
    "    td_sec_seqntl_pu_or_pd = 5, \n",
    "    td_sec_final_daisy     = 5, \n",
    "    eps_km                 = 30, \n",
    "    group_by               = ['station_nb', 'circuit_nb'], \n",
    "    pd_ids                 = ['3.26.0.47'], \n",
    "    pu_ids                 = ['3.26.0.216'], \n",
    "    conn_aws               = None\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    accptbl_group_by = ['station_nb', 'station_nm', 'circuit_nb', 'circuit_nm', 'trsf_pole_nb']\n",
    "    assert(set(group_by).difference(set(accptbl_group_by))==set())\n",
    "    #--------------------------------------------------\n",
    "    # BELOW LINES ARE TEMPORARY UNTIL FULL cds_ds_db.com_ccp_dgis_extract table set up!!!!!\n",
    "    assert(opco in ['pso', 'im'])\n",
    "    if opco=='pso':\n",
    "        dgis_query = 'SELECT prem_nb, \" phase_val\" AS phase_val FROM cds_ds_db.com_ccp_dgis_extract'\n",
    "    else:\n",
    "        dgis_query = 'SELECT prem_nb, phase_val FROM cds_ds_db.com_ccp_dgis_extract_im'\n",
    "    #--------------------------------------------------\n",
    "    if conn_aws is None:\n",
    "        conn_aws = Utilities.get_athena_prod_aws_connection()\n",
    "    #-------------------------\n",
    "    CRED_sql = build_CRED_sql(\n",
    "        date_0                 = date_0, \n",
    "        date_1                 = date_1, \n",
    "        opco                   = opco, \n",
    "        min_pct_SN             = min_pct_SN, \n",
    "        td_sec_group_daisy     = td_sec_group_daisy, \n",
    "        td_sec_seqntl_pu_or_pd = td_sec_seqntl_pu_or_pd, \n",
    "        td_sec_final_daisy     = td_sec_final_daisy, \n",
    "        group_by               = group_by, \n",
    "        pd_ids                 = pd_ids, \n",
    "        pu_ids                 = pu_ids\n",
    "    )\n",
    "    #-------------------------\n",
    "    df = pd.read_sql_query(\n",
    "        sql = CRED_sql, \n",
    "        con = conn_aws\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    time_group_col = 'super_time_grp'\n",
    "    min_samples    = 1\n",
    "    lat_col        = 'avg_latitude'\n",
    "    lon_col        = 'avg_longitude'\n",
    "    spat_group_col = 'spatial_grp'\n",
    "    fnl_group_col  = 'super_time_spatial_grp'\n",
    "    \n",
    "    df = set_spatial_groups(\n",
    "        df             = df, \n",
    "        time_group_col = time_group_col, \n",
    "        eps_km         = eps_km, \n",
    "        min_samples    = min_samples, \n",
    "        lat_col        = lat_col, \n",
    "        lon_col        = lon_col, \n",
    "        spat_group_col = spat_group_col, \n",
    "        fnl_group_col  = fnl_group_col\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    # Current process only really makes sense if date_0==date_1\n",
    "    #--------------------------------------------------\n",
    "    # NOTE: In SQL queries, the dates in the range are INCLUSIVE\n",
    "    #       ==> for 48 hours, use pd.Timedelta('1D')\n",
    "    #       ==> for 1 week,   use pd.Timedelta('6D')\n",
    "    #       ==> for 30 days,  use pd.Timedelta('29D')\n",
    "    #       ==> for 180 days, use pd.Timedelta('179D')\n",
    "    #       ==> for 1 year,   use pd.Timedelta('364D')\n",
    "    #--------------------------------------------------\n",
    "    avg_moms_cols    = None\n",
    "    include_avg_moms = date_0==date_1\n",
    "    if include_avg_moms:\n",
    "        avg_moms_cols = []\n",
    "        time_deltas = ['1D', '6D', '29D', '179D', '364D']\n",
    "        #-------------------------\n",
    "        avg_moms_date_1 = date_1\n",
    "        for td_i in time_deltas:\n",
    "            avg_moms_date_0 = (pd.to_datetime(date_1)-pd.Timedelta(td_i)).strftime('%Y-%m-%d')\n",
    "            #-------------------------\n",
    "            sql_avg_moms_i = build_avg_momentaries_sql(\n",
    "                date_0                 = avg_moms_date_0, \n",
    "                date_1                 = avg_moms_date_1, \n",
    "                opco                   = opco, \n",
    "                td_sec_seqntl_pu_or_pd = td_sec_seqntl_pu_or_pd, \n",
    "                group_by               = group_by, \n",
    "                pd_ids                 = pd_ids, \n",
    "                pu_ids                 = pu_ids\n",
    "            )\n",
    "            #-----\n",
    "            df_avg_moms_i = pd.read_sql_query(sql_avg_moms_i, conn_aws)\n",
    "            #-------------------------\n",
    "            pd_n_days_i   = (pd.to_datetime(avg_moms_date_1)-pd.to_datetime(avg_moms_date_0)+pd.Timedelta('1D')).days\n",
    "            avg_PN_col_i  = f'{pd_n_days_i}d'\n",
    "            if pd_n_days_i==365:\n",
    "                df_avg_moms_i = df_avg_moms_i.drop(columns=['avg_daily_n_mom_per_SN', 'nSNs_w_gt13_mom']).rename(columns={'avg_daily_n_mom_per_PN':avg_PN_col_i})\n",
    "                df_avg_moms_i = df_avg_moms_i[group_by + [avg_PN_col_i, 'nPNs_w_gt13_mom']]\n",
    "                #-----\n",
    "                avg_moms_cols.extend([avg_PN_col_i, 'nPNs_w_gt13_mom'])\n",
    "            else:\n",
    "                df_avg_moms_i = df_avg_moms_i.drop(columns=['avg_daily_n_mom_per_SN']).rename(columns={'avg_daily_n_mom_per_PN':avg_PN_col_i})\n",
    "                #-----\n",
    "                avg_moms_cols.append(avg_PN_col_i)\n",
    "            #-------------------------\n",
    "            df = pd.merge(\n",
    "                df, \n",
    "                df_avg_moms_i, \n",
    "                how      = 'left', \n",
    "                left_on  = group_by, \n",
    "                right_on = group_by\n",
    "            )\n",
    "            df[avg_PN_col_i] = df[avg_PN_col_i].fillna(0).round(4)\n",
    "    #--------------------------------------------------\n",
    "    df['date'] = df['min_outage_start'].dt.date\n",
    "    df['super_time_spatial_grp_fnl'] = df['super_min_outage_start'].dt.strftime(\"%Y%m%d_%H:%M:%S\") + '_' + df['spatial_grp'].astype(str)\n",
    "    df = df.set_index(['date', 'super_time_spatial_grp_fnl']+group_by)\n",
    "    #-------------------------\n",
    "    if by_premise:\n",
    "        fnl_cols_general = 'unique_PNs'\n",
    "    else:\n",
    "        fnl_cols_general = 'unique_SNs'\n",
    "    #-------------------------\n",
    "    if normalize:\n",
    "        fnl_cols_general = 'norm_'+fnl_cols_general\n",
    "    #-------------------------\n",
    "    pct_title_appdx  = f\"({'%' if normalize else '#'} {'Premises' if by_premise else 'Serial Numbers'})\"\n",
    "    #-------------------------\n",
    "    fnl_cols = [\n",
    "        '', \n",
    "        #-----\n",
    "        '_A_1', \n",
    "        '_B_1', \n",
    "        '_C_1', \n",
    "        #-----\n",
    "        '_A_2', \n",
    "        '_B_2', \n",
    "        '_C_2', \n",
    "        #-----\n",
    "        '_A_0', \n",
    "        '_B_0', \n",
    "        '_C_0', \n",
    "        #-----\n",
    "        '_A_3', \n",
    "        '_B_3', \n",
    "        '_C_3', \n",
    "    ]\n",
    "    #-------------------------\n",
    "    fnl_cols = [fnl_cols_general+x for x in fnl_cols]\n",
    "    #-------------------------\n",
    "    # Include total number of momentary events\n",
    "    fnl_cols = [fnl_cols[0]] + ['total_PN_events_1'] + fnl_cols[1:]\n",
    "    #-------------------------\n",
    "    if include_avg_moms:\n",
    "        fnl_cols.extend(avg_moms_cols)\n",
    "    #-------------------------\n",
    "    fnl_cols.extend([\n",
    "        'min_outage_start',\n",
    "        'max_outage_start',\n",
    "        'min_duration',\n",
    "        'max_duration',\n",
    "        'avg_duration',\n",
    "        'std_duration',\n",
    "    ])\n",
    "    #-------------------------\n",
    "    if normalize:\n",
    "        fnl_cols = [f\"group_{'PN' if by_premise else 'SN'}_cnt\"] + fnl_cols\n",
    "    #-------------------------\n",
    "    df_fnl = df[fnl_cols].copy()\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    # Round percentage columns to nearest int\n",
    "    # NOTE: We want anything between 0 and 1 to be set as 1 (to avoid confusion where large max_duration but seemingly no sustained events)\n",
    "    non_round_cols = [\n",
    "        f\"group_{'PN' if by_premise else 'SN'}_cnt\", \n",
    "        'total_PN_events_1', \n",
    "        'min_outage_start',\n",
    "        'max_outage_start',\n",
    "        'min_duration',\n",
    "        'max_duration',\n",
    "        'avg_duration',\n",
    "        'std_duration',    \n",
    "    ]\n",
    "    if include_avg_moms:\n",
    "        non_round_cols.extend(avg_moms_cols)\n",
    "    round_cols = list(set(df_fnl.columns).difference(non_round_cols))\n",
    "    # df_fnl[round_cols] = df_fnl[round_cols].fillna(0).round(0).astype(int)\n",
    "    #-------------------------\n",
    "    # We want anything between 0 and 1 to be set as 1 (to avoid confusion where large max_duration but seemingly no sustained events)\n",
    "    # 1. Identify values to be set as 1 by setting to -1 (everything else should be >=0)\n",
    "    for col_i in round_cols:\n",
    "        df_fnl.loc[(df_fnl[col_i]>0) & (df_fnl[col_i]<1), col_i] = -1\n",
    "    # 2. Round like usual\n",
    "    df_fnl[round_cols] = df_fnl[round_cols].fillna(0).round(0).astype(int)\n",
    "    # 3. Set values for identified to 1\n",
    "    for col_i in round_cols:\n",
    "        df_fnl.loc[df_fnl[col_i]==-1, col_i] = 1\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    # Sort by date and total_PN_events_1 in groups\n",
    "    df_fnl = sort_CRED_df(\n",
    "        df            = df_fnl, \n",
    "        group_by      = group_by, \n",
    "        date_idx      = 'date', \n",
    "        event_tag_idx = 'super_time_spatial_grp_fnl', \n",
    "        n_moms_col    = 'total_PN_events_1', \n",
    "    )\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    df_fnl_1 = df_fnl.copy()\n",
    "    #-----\n",
    "    fnl_cols_MI_1 = [\n",
    "        (f\"{'Premises' if by_premise else 'Serial Numbers'}\", f\"Effected ({'%' if normalize else '#'})\"), \n",
    "        #-----\n",
    "        ('Mom.', 'Events'), \n",
    "        #-----\n",
    "        ('Mom. By Phase ' + pct_title_appdx, 'Am'), \n",
    "        ('Mom. By Phase ' + pct_title_appdx, 'Bm'), \n",
    "        ('Mom. By Phase ' + pct_title_appdx, 'Cm'), \n",
    "        #-----\n",
    "        ('Sust. By Phase ' + pct_title_appdx, 'As'), \n",
    "        ('Sust. By Phase ' + pct_title_appdx, 'Bs'), \n",
    "        ('Sust. By Phase ' + pct_title_appdx, 'Cs'), \n",
    "        #-----\n",
    "        ('<8s By Phase ' + pct_title_appdx, 'A-'), \n",
    "        ('<8s By Phase ' + pct_title_appdx, 'B-'), \n",
    "        ('<8s By Phase ' + pct_title_appdx, 'C-'), \n",
    "        #-----\n",
    "        ('Unrslvd By Phase ' + pct_title_appdx, 'Ax'), \n",
    "        ('Unrslvd By Phase ' + pct_title_appdx, 'Bx'), \n",
    "        ('Unrslvd By Phase ' + pct_title_appdx, 'Cx')\n",
    "    ]\n",
    "    #-----\n",
    "    if include_avg_moms:\n",
    "        fnl_cols_MI_1.extend([('Past Momentaries', x) for x in avg_moms_cols])\n",
    "    #-----\n",
    "    fnl_cols_MI_1.extend([\n",
    "        (' ', 'min_outage_start'),\n",
    "        (' ', 'max_outage_start'),\n",
    "        (' ', 'min_duration'),\n",
    "        (' ', 'max_duration'),\n",
    "        (' ', 'avg_duration'),\n",
    "        (' ', 'std_duration')\n",
    "    ])\n",
    "    #-----\n",
    "    if normalize:\n",
    "        fnl_cols_MI_1 = [(f\"{'Premises' if by_premise else 'Serial Numbers'}\", 'Total')] + fnl_cols_MI_1\n",
    "    #-----\n",
    "    df_fnl_1.columns = pd.MultiIndex.from_tuples(fnl_cols_MI_1)\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    df_fnl_2 = df_fnl.copy()\n",
    "    #-----\n",
    "    fnl_cols_MI_2 = [\n",
    "        (' ', f\"{'PNs ' if by_premise else 'SNs '}{'(%)' if normalize else '(#)'}\"), \n",
    "        #-----\n",
    "        (' ', 'Mom. Evs'), \n",
    "        #-----\n",
    "        ('Mom. By Phase ' + pct_title_appdx, 'Am'), \n",
    "        ('Mom. By Phase ' + pct_title_appdx, 'Bm'), \n",
    "        ('Mom. By Phase ' + pct_title_appdx, 'Cm'), \n",
    "        #-----\n",
    "        ('Sust. By Phase ' + pct_title_appdx, 'As'), \n",
    "        ('Sust. By Phase ' + pct_title_appdx, 'Bs'), \n",
    "        ('Sust. By Phase ' + pct_title_appdx, 'Cs'), \n",
    "        #-----\n",
    "        ('<8s By Phase ' + pct_title_appdx, 'A-'), \n",
    "        ('<8s By Phase ' + pct_title_appdx, 'B-'), \n",
    "        ('<8s By Phase ' + pct_title_appdx, 'C-'), \n",
    "        #-----\n",
    "        ('Unrslvd By Phase ' + pct_title_appdx, 'Ax'), \n",
    "        ('Unrslvd By Phase ' + pct_title_appdx, 'Bx'), \n",
    "        ('Unrslvd By Phase ' + pct_title_appdx, 'Cx')\n",
    "    ]\n",
    "    #-----\n",
    "    if include_avg_moms:\n",
    "        fnl_cols_MI_2.extend([(' ', x) for x in avg_moms_cols])\n",
    "    #-----\n",
    "    fnl_cols_MI_2.extend([\n",
    "        (' ', 'min_outage_start'),\n",
    "        (' ', 'max_outage_start'),\n",
    "        (' ', 'min_duration'),\n",
    "        (' ', 'max_duration'),\n",
    "        (' ', 'avg_duration'),\n",
    "        (' ', 'std_duration')\n",
    "    ])\n",
    "    #-----\n",
    "    if normalize:\n",
    "        fnl_cols_MI_2 = [(' ', f\"{'PNs (#)' if by_premise else 'SNs (#)'}\")] + fnl_cols_MI_2\n",
    "    #-----\n",
    "    df_fnl_2.columns = pd.MultiIndex.from_tuples(fnl_cols_MI_2)\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    pretty_idxs = {\n",
    "        'date'                       : 'Date', \n",
    "        'super_time_spatial_grp_fnl' : 'Event Tag', \n",
    "        'station_nb'                 : 'Station Number', \n",
    "        'station_nm'                 : 'Station Name', \n",
    "        'circuit_nb'                 : 'Circuit Number', \n",
    "        'circuit_nm'                 : 'Circuit Name', \n",
    "        'trsf_pole_nb'               : 'Transformer Pole'\n",
    "    }\n",
    "    #-----\n",
    "    assert(set(df_fnl_1.index.names).difference(set(pretty_idxs.keys()))==set())\n",
    "    assert(set(df_fnl_2.index.names).difference(set(pretty_idxs.keys()))==set())\n",
    "    pretty_idx_names = pd.Series(df_fnl_1.index.names).map(pretty_idxs).values.tolist()\n",
    "    assert(df_fnl_1.index.nlevels==df_fnl_2.index.nlevels==len(pretty_idx_names))\n",
    "    #-----\n",
    "    df_fnl_1.index.names = pretty_idx_names\n",
    "    df_fnl_2.index.names = pretty_idx_names\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    return df_fnl, df_fnl_1, df_fnl_2, avg_moms_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a099a2-8b3f-4e9b-9675-8e53070b90a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e61edf-c996-4ee5-8c7a-7dca63b49106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "def save_cred(\n",
    "    cred_df_v1     , \n",
    "    cred_df_v2     , \n",
    "    opco           , \n",
    "    date_0         , \n",
    "    date_1         , \n",
    "    avg_moms_cols  , \n",
    "    normalize      = True, \n",
    "    save_dir_base  = r'C:\\Users\\s346557\\Documents\\LocalData\\CRED', \n",
    "    vba_file_path  = r'C:\\Users\\s346557\\Documents\\Analysis\\CRED_PrettyPivot.bas', \n",
    "    vba_macro_name = \"RunAll\",     \n",
    "    verbose        = True\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #--------------------------------------------------\n",
    "    if save_dir_base is None:\n",
    "        print(\"No save_dir_base supplied!  Failure immenent!\")\n",
    "        assert(0)\n",
    "    #-----\n",
    "    if not os.path.exists(save_dir_base):\n",
    "        if verbose:\n",
    "            print(f'save_dir_base={save_dir_base} does not exists, so creating')\n",
    "        os.makedirs(save_dir_base)\n",
    "    #-----\n",
    "    save_dir = os.path.join(save_dir_base, opco)\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    #--------------------------------------------------\n",
    "    # If vba_file_path is supplied, make sure the file exists\n",
    "    # If it doesn't exist, ignore it by setting equal to None\n",
    "    if vba_file_path is not None:\n",
    "        if not os.path.exists(vba_file_path):\n",
    "            if verbose:\n",
    "                print(f'vba_file_path={vba_file_path} supplied but does not exists, so ignoring')\n",
    "            vba_file_path = None\n",
    "    #-------------------------\n",
    "    # If vba_file_path is not None at this point, the file exists\n",
    "    # Check if vba_macro_name found in file.\n",
    "    # If it's not found, set vba_macro_name equal to None\n",
    "    if vba_file_path is not None:\n",
    "        macro_name_found = False\n",
    "        with open(vba_file_path) as f:\n",
    "            if f'Sub {vba_macro_name}' in f.read():\n",
    "                macro_name_found = True\n",
    "        if not macro_name_found:\n",
    "            if verbose:\n",
    "                print(f'vba_macro_name={vba_macro_name} not found in vba_file_path={vba_file_path}, so ignoring')\n",
    "            vba_macro_name = None\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    save_name, save_path = save_CRED_xlsx(\n",
    "        df_fnl          = [cred_df_v1, cred_df_v2], \n",
    "        save_dir        = save_dir, \n",
    "        opco            = opco, \n",
    "        date_0          = date_0, \n",
    "        date_1          = date_1, \n",
    "        normalize       = normalize, \n",
    "        n_avg_moms_cols = 0 if avg_moms_cols is None else len(avg_moms_cols), \n",
    "        reset_index     = 'both'\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    if (\n",
    "        vba_file_path  is not None and \n",
    "        vba_macro_name is not None\n",
    "    ):\n",
    "        assert(cred_df_v1.index.names==cred_df_v2.index.names)\n",
    "        #-----\n",
    "        start = time.time()\n",
    "        #-----\n",
    "        import_and_run_vba_macro(\n",
    "            excel_file_path = save_path, \n",
    "            vba_file_path   = vba_file_path, \n",
    "            vba_macro_name  = vba_macro_name, \n",
    "            pretty_group_by = list(cred_df_v1.index.names)[2:], \n",
    "            addtnl_cols     = avg_moms_cols\n",
    "        )\n",
    "        #-----\n",
    "        vba_time = time.time()-start\n",
    "        if verbose:\n",
    "            print(f'vba_time = {vba_time}')\n",
    "    #--------------------------------------------------\n",
    "    return save_name, save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ad298-dd1e-4856-bb16-1c1a99c8d530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1bd8c-c7b4-4da8-bf7a-904257e36b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results  = True\n",
    "save_dir_base = r'C:\\Users\\s346557\\Documents\\LocalData\\CRED'\n",
    "#-----\n",
    "by_premise = True\n",
    "normalize  = True\n",
    "#-----\n",
    "vba_file_path = r'C:\\Users\\s346557\\Documents\\Analysis\\CRED_PrettyPivot.bas'\n",
    "# vba_file_path = None\n",
    "vba_macro_name    = \"RunAll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993bbf41-5dd6-4ffe-9eb8-3647c7650a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa8867-df97-4e11-89c9-2bb6b98059b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "date_0                 = '2025-06-04'\n",
    "date_1                 = '2025-06-04'\n",
    "opco                   = 'pso'\n",
    "min_pct_SN             = 0.10\n",
    "td_sec_group_daisy     = 5\n",
    "td_sec_seqntl_pu_or_pd = 5\n",
    "td_sec_final_daisy     = 5\n",
    "eps_km                 = 30\n",
    "group_by               = ['station_nm', 'circuit_nm']\n",
    "# pd_ids                 = ['3.26.0.47', '3.26.136.47', '3.26.136.66']\n",
    "# pu_ids                 = ['3.26.0.216', '3.26.136.216']\n",
    "pd_ids                 = ['3.26.0.47']\n",
    "pu_ids                 = ['3.26.0.216']\n",
    "\n",
    "assert(opco in ['pso', 'im'])\n",
    "#--------------------------------------------------\n",
    "save_dir = os.path.join(save_dir_base, opco)\n",
    "if save_results and not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4c449-2a9f-46da-b38b-e20b1d621534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23a32e-a050-4c06-a7fb-ea411f76f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cred_df_raw, cred_df_v1, cred_df_v2, avg_moms_cols = run_cred(\n",
    "#     date_0                 = date_0, \n",
    "#     date_1                 = date_1, \n",
    "#     opco                   = opco, \n",
    "#     min_pct_SN             = min_pct_SN, \n",
    "#     by_premise             = by_premise, \n",
    "#     normalize              = normalize, \n",
    "#     td_sec_group_daisy     = td_sec_group_daisy, \n",
    "#     td_sec_seqntl_pu_or_pd = td_sec_seqntl_pu_or_pd, \n",
    "#     td_sec_final_daisy     = td_sec_final_daisy, \n",
    "#     eps_km                 = eps_km, \n",
    "#     group_by               = group_by, \n",
    "#     pd_ids                 = pd_ids, \n",
    "#     pu_ids                 = pu_ids, \n",
    "#     conn_aws               = None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165c886-bbaa-4baa-85a6-3de25b8ae11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "CRED_sql = build_CRED_sql(\n",
    "    date_0                 = date_0, \n",
    "    date_1                 = date_1, \n",
    "    opco                   = opco, \n",
    "    min_pct_SN             = min_pct_SN, \n",
    "    td_sec_group_daisy     = td_sec_group_daisy, \n",
    "    td_sec_seqntl_pu_or_pd = td_sec_seqntl_pu_or_pd, \n",
    "    td_sec_final_daisy     = td_sec_final_daisy, \n",
    "    group_by               = group_by, \n",
    "    pd_ids                 = pd_ids, \n",
    "    pu_ids                 = pu_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd86af-9c10-4a01-8fd1-d47a6b3fd30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(CRED_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c036e4-7040-46f7-864d-abf7a77bd82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_aws = Utilities.get_athena_prod_aws_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84282f9-c8ff-4f5c-a216-83deeaab42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(CRED_sql, conn_aws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da451e5c-525a-4e5f-b355-920cca934ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0caeb86-e010-4b03-bfb8-af7c07fec0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_group_col = 'super_time_grp'\n",
    "min_samples    = 1\n",
    "lat_col        = 'avg_latitude'\n",
    "lon_col        = 'avg_longitude'\n",
    "spat_group_col = 'spatial_grp'\n",
    "fnl_group_col  = 'super_time_spatial_grp'\n",
    "\n",
    "df = set_spatial_groups(\n",
    "    df             = df, \n",
    "    time_group_col = time_group_col, \n",
    "    eps_km         = eps_km, \n",
    "    min_samples    = min_samples, \n",
    "    lat_col        = lat_col, \n",
    "    lon_col        = lon_col, \n",
    "    spat_group_col = spat_group_col, \n",
    "    fnl_group_col  = fnl_group_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823af0a-71db-461b-a76a-50e95146d3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d90fe-4f3d-43c1-93ec-ac52c983a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# Current process only really makes sense if date_0==date_1\n",
    "#--------------------------------------------------\n",
    "# NOTE: In SQL queries, the dates in the range are INCLUSIVE\n",
    "#       ==> for 48 hours, use pd.Timedelta('1D')\n",
    "#       ==> for 1 week,   use pd.Timedelta('6D')\n",
    "#       ==> for 30 days,  use pd.Timedelta('29D')\n",
    "#       ==> for 180 days, use pd.Timedelta('179D')\n",
    "#       ==> for 1 year,   use pd.Timedelta('364D')\n",
    "#--------------------------------------------------\n",
    "avg_moms_cols    = None\n",
    "include_avg_moms = date_0==date_1\n",
    "if include_avg_moms:\n",
    "    avg_moms_cols = []\n",
    "    time_deltas = ['1D', '6D', '29D', '179D', '364D']\n",
    "    #-------------------------\n",
    "    avg_moms_date_1 = date_1\n",
    "    for td_i in time_deltas:\n",
    "        avg_moms_date_0 = (pd.to_datetime(date_1)-pd.Timedelta(td_i)).strftime('%Y-%m-%d')\n",
    "        #-------------------------\n",
    "        sql_avg_moms_i = build_avg_momentaries_sql(\n",
    "            date_0                 = avg_moms_date_0, \n",
    "            date_1                 = avg_moms_date_1, \n",
    "            opco                   = opco, \n",
    "            td_sec_seqntl_pu_or_pd = td_sec_seqntl_pu_or_pd, \n",
    "            group_by               = group_by, \n",
    "            pd_ids                 = pd_ids, \n",
    "            pu_ids                 = pu_ids\n",
    "        )\n",
    "        #-----\n",
    "        df_avg_moms_i = pd.read_sql_query(sql_avg_moms_i, conn_aws)\n",
    "        #-------------------------\n",
    "        pd_n_days_i   = (pd.to_datetime(avg_moms_date_1)-pd.to_datetime(avg_moms_date_0)+pd.Timedelta('1D')).days\n",
    "        avg_PN_col_i  = f'{pd_n_days_i}d'\n",
    "        if pd_n_days_i==365:\n",
    "            df_avg_moms_i = df_avg_moms_i.drop(columns=['avg_daily_n_mom_per_SN', 'nSNs_w_gt13_mom']).rename(columns={'avg_daily_n_mom_per_PN':avg_PN_col_i})\n",
    "            df_avg_moms_i = df_avg_moms_i[group_by + [avg_PN_col_i, 'nPNs_w_gt13_mom']]\n",
    "            #-----\n",
    "            avg_moms_cols.extend([avg_PN_col_i, 'nPNs_w_gt13_mom'])\n",
    "        else:\n",
    "            df_avg_moms_i = df_avg_moms_i.drop(columns=['avg_daily_n_mom_per_SN']).rename(columns={'avg_daily_n_mom_per_PN':avg_PN_col_i})\n",
    "            #-----\n",
    "            avg_moms_cols.append(avg_PN_col_i)\n",
    "        #-------------------------\n",
    "        df = pd.merge(\n",
    "            df, \n",
    "            df_avg_moms_i, \n",
    "            how      = 'left', \n",
    "            left_on  = group_by, \n",
    "            right_on = group_by\n",
    "        )\n",
    "        df[avg_PN_col_i] = df[avg_PN_col_i].fillna(0).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbd123-6f4e-4af6-bb95-9edd107d6d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf96a68-6c67-42a4-a2cc-6b98bc34dd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab616dfc-a9ff-4d40-8541-09220695e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "df['date'] = df['min_outage_start'].dt.date\n",
    "df['super_time_spatial_grp_fnl'] = df['super_min_outage_start'].dt.strftime(\"%Y%m%d_%H:%M:%S\") + '_' + df['spatial_grp'].astype(str)\n",
    "df = df.set_index(['date', 'super_time_spatial_grp_fnl']+group_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1019223-8d8e-49f9-8cc3-8f2c00ff842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "if by_premise:\n",
    "    fnl_cols_general = 'unique_PNs'\n",
    "else:\n",
    "    fnl_cols_general = 'unique_SNs'\n",
    "#-------------------------\n",
    "if normalize:\n",
    "    fnl_cols_general = 'norm_'+fnl_cols_general\n",
    "#-------------------------\n",
    "pct_title_appdx  = f\"({'%' if normalize else '#'} {'Premises' if by_premise else 'Serial Numbers'})\"\n",
    "#-------------------------\n",
    "fnl_cols = [\n",
    "    '', \n",
    "    #-----\n",
    "    '_A_1', \n",
    "    '_B_1', \n",
    "    '_C_1', \n",
    "    #-----\n",
    "    '_A_2', \n",
    "    '_B_2', \n",
    "    '_C_2', \n",
    "    #-----\n",
    "    '_A_0', \n",
    "    '_B_0', \n",
    "    '_C_0', \n",
    "    #-----\n",
    "    '_A_3', \n",
    "    '_B_3', \n",
    "    '_C_3', \n",
    "]\n",
    "#-------------------------\n",
    "fnl_cols = [fnl_cols_general+x for x in fnl_cols]\n",
    "#-------------------------\n",
    "# Include total number of momentary events\n",
    "fnl_cols = [fnl_cols[0]] + ['total_PN_events_1'] + fnl_cols[1:]\n",
    "#-------------------------\n",
    "if include_avg_moms:\n",
    "    fnl_cols.extend(avg_moms_cols)\n",
    "#-------------------------\n",
    "fnl_cols.extend([\n",
    "    'min_outage_start',\n",
    "    'max_outage_start',\n",
    "    'min_duration',\n",
    "    'max_duration',\n",
    "    'avg_duration',\n",
    "    'std_duration',\n",
    "])\n",
    "#-------------------------\n",
    "if normalize:\n",
    "    fnl_cols = [f\"group_{'PN' if by_premise else 'SN'}_cnt\"] + fnl_cols\n",
    "#-------------------------\n",
    "df_fnl = df[fnl_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b352aa6-9b48-4cc3-9c63-dce35e3baec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29281efa-bb4c-42e3-bf4f-de5f170883b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3fcd0-46e7-4866-8cb0-6e6cb3d6b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# Round percentage columns to nearest int\n",
    "# NOTE: We want anything between 0 and 1 to be set as 1 (to avoid confusion where large max_duration but seemingly no sustained events)\n",
    "non_round_cols = [\n",
    "    f\"group_{'PN' if by_premise else 'SN'}_cnt\", \n",
    "    'total_PN_events_1', \n",
    "    'min_outage_start',\n",
    "    'max_outage_start',\n",
    "    'min_duration',\n",
    "    'max_duration',\n",
    "    'avg_duration',\n",
    "    'std_duration',    \n",
    "]\n",
    "if include_avg_moms:\n",
    "    non_round_cols.extend(avg_moms_cols)\n",
    "round_cols = list(set(df_fnl.columns).difference(non_round_cols))\n",
    "# df_fnl[round_cols] = df_fnl[round_cols].fillna(0).round(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b94f2-f782-44c9-a70c-068f1fdd87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# We want anything between 0 and 1 to be set as 1 (to avoid confusion where large max_duration but seemingly no sustained events)\n",
    "# 1. Identify values to be set as 1 by setting to -1 (everything else should be >=0)\n",
    "for col_i in round_cols:\n",
    "    df_fnl.loc[(df_fnl[col_i]>0) & (df_fnl[col_i]<1), col_i] = -1\n",
    "# 2. Round like usual\n",
    "df_fnl[round_cols] = df_fnl[round_cols].fillna(0).round(0).astype(int)\n",
    "# 3. Set values for identified to 1\n",
    "for col_i in round_cols:\n",
    "    df_fnl.loc[df_fnl[col_i]==-1, col_i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767cb2d-28cd-43b7-8305-6fbebfd3a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# Sort by date and total_PN_events_1 in groups\n",
    "df_fnl = sort_CRED_df(\n",
    "    df            = df_fnl, \n",
    "    group_by      = group_by, \n",
    "    date_idx      = 'date', \n",
    "    event_tag_idx = 'super_time_spatial_grp_fnl', \n",
    "    n_moms_col    = 'total_PN_events_1', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fde79dd-6752-43b3-960c-47a743e6c90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37781b-1837-41c2-9b92-e4876d6af388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "df_fnl_1 = df_fnl.copy()\n",
    "#-----\n",
    "fnl_cols_MI_1 = [\n",
    "    (f\"{'Premises' if by_premise else 'Serial Numbers'}\", f\"Effected ({'%' if normalize else '#'})\"), \n",
    "    #-----\n",
    "    ('Mom.', 'Events'), \n",
    "    #-----\n",
    "    ('Mom. By Phase ' + pct_title_appdx, 'Am'), \n",
    "    ('Mom. By Phase ' + pct_title_appdx, 'Bm'), \n",
    "    ('Mom. By Phase ' + pct_title_appdx, 'Cm'), \n",
    "    #-----\n",
    "    ('Sust. By Phase ' + pct_title_appdx, 'As'), \n",
    "    ('Sust. By Phase ' + pct_title_appdx, 'Bs'), \n",
    "    ('Sust. By Phase ' + pct_title_appdx, 'Cs'), \n",
    "    #-----\n",
    "    ('<8s By Phase ' + pct_title_appdx, 'A-'), \n",
    "    ('<8s By Phase ' + pct_title_appdx, 'B-'), \n",
    "    ('<8s By Phase ' + pct_title_appdx, 'C-'), \n",
    "    #-----\n",
    "    ('Unrslvd By Phase ' + pct_title_appdx, 'Ax'), \n",
    "    ('Unrslvd By Phase ' + pct_title_appdx, 'Bx'), \n",
    "    ('Unrslvd By Phase ' + pct_title_appdx, 'Cx')\n",
    "]\n",
    "#-----\n",
    "if include_avg_moms:\n",
    "    fnl_cols_MI_1.extend([('Past Momentaries', x) for x in avg_moms_cols])\n",
    "#-----\n",
    "fnl_cols_MI_1.extend([\n",
    "    (' ', 'min_outage_start'),\n",
    "    (' ', 'max_outage_start'),\n",
    "    (' ', 'min_duration'),\n",
    "    (' ', 'max_duration'),\n",
    "    (' ', 'avg_duration'),\n",
    "    (' ', 'std_duration')\n",
    "])\n",
    "#-----\n",
    "if normalize:\n",
    "    fnl_cols_MI_1 = [(f\"{'Premises' if by_premise else 'Serial Numbers'}\", 'Total')] + fnl_cols_MI_1\n",
    "#-----\n",
    "df_fnl_1.columns = pd.MultiIndex.from_tuples(fnl_cols_MI_1)\n",
    "\n",
    "#--------------------------------------------------\n",
    "df_fnl_2 = df_fnl.copy()\n",
    "#-----\n",
    "fnl_cols_MI_2 = [\n",
    "    (' ', f\"{'PNs ' if by_premise else 'SNs '}{'(%)' if normalize else '(#)'}\"), \n",
    "    #-----\n",
    "    (' ', 'Mom. Evs'), \n",
    "    #-----\n",
    "    ('Mom. By Phase ' + pct_title_appdx, 'Am'), \n",
    "    ('Mom. By Phase ' + pct_title_appdx, 'Bm'), \n",
    "    ('Mom. By Phase ' + pct_title_appdx, 'Cm'), \n",
    "    #-----\n",
    "    ('Sust. By Phase ' + pct_title_appdx, 'As'), \n",
    "    ('Sust. By Phase ' + pct_title_appdx, 'Bs'), \n",
    "    ('Sust. By Phase ' + pct_title_appdx, 'Cs'), \n",
    "    #-----\n",
    "    ('<8s By Phase ' + pct_title_appdx, 'A-'), \n",
    "    ('<8s By Phase ' + pct_title_appdx, 'B-'), \n",
    "    ('<8s By Phase ' + pct_title_appdx, 'C-'), \n",
    "    #-----\n",
    "    ('Unrslvd By Phase ' + pct_title_appdx, 'Ax'), \n",
    "    ('Unrslvd By Phase ' + pct_title_appdx, 'Bx'), \n",
    "    ('Unrslvd By Phase ' + pct_title_appdx, 'Cx')\n",
    "]\n",
    "#-----\n",
    "if include_avg_moms:\n",
    "    fnl_cols_MI_2.extend([(' ', x) for x in avg_moms_cols])\n",
    "#-----\n",
    "fnl_cols_MI_2.extend([\n",
    "    (' ', 'min_outage_start'),\n",
    "    (' ', 'max_outage_start'),\n",
    "    (' ', 'min_duration'),\n",
    "    (' ', 'max_duration'),\n",
    "    (' ', 'avg_duration'),\n",
    "    (' ', 'std_duration')\n",
    "])\n",
    "#-----\n",
    "if normalize:\n",
    "    fnl_cols_MI_2 = [(' ', f\"{'PNs (#)' if by_premise else 'SNs (#)'}\")] + fnl_cols_MI_2\n",
    "#-----\n",
    "df_fnl_2.columns = pd.MultiIndex.from_tuples(fnl_cols_MI_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cedea07-faee-4c9e-a8eb-ea495d550d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3aa6bd-52a1-488d-906b-1aa7ec1af728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26aa102-92ff-4251-924f-418964dceb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "pretty_idxs = {\n",
    "    'date'                       : 'Date', \n",
    "    'super_time_spatial_grp_fnl' : 'Event Tag', \n",
    "    'station_nb'                 : 'Station Number', \n",
    "    'station_nm'                 : 'Station Name', \n",
    "    'circuit_nb'                 : 'Circuit Number', \n",
    "    'circuit_nm'                 : 'Circuit Name', \n",
    "    'trsf_pole_nb'               : 'Transformer Pole'\n",
    "}\n",
    "#-----\n",
    "assert(set(df_fnl_1.index.names).difference(set(pretty_idxs.keys()))==set())\n",
    "assert(set(df_fnl_2.index.names).difference(set(pretty_idxs.keys()))==set())\n",
    "pretty_idx_names = pd.Series(df_fnl_1.index.names).map(pretty_idxs).values.tolist()\n",
    "assert(df_fnl_1.index.nlevels==df_fnl_2.index.nlevels==len(pretty_idx_names))\n",
    "#-----\n",
    "df_fnl_1.index.names = pretty_idx_names\n",
    "df_fnl_2.index.names = pretty_idx_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14345684-4749-4a17-ac20-cdfc6c0b6ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90fa514-fa5d-43d4-b9ca-033bab9c158c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633f23a-7d2a-4ac1-91a6-447957320564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if save_results:\n",
    "#     save_name, save_path = save_CRED_xlsx(\n",
    "#         df_fnl          = [df_fnl_1, df_fnl_2], \n",
    "#         save_dir        = save_dir, \n",
    "#         opco            = opco, \n",
    "#         date_0          = date_0, \n",
    "#         date_1          = date_1, \n",
    "#         normalize       = normalize, \n",
    "#         n_avg_moms_cols = 0 if avg_moms_cols is None else len(avg_moms_cols), \n",
    "#         reset_index     = 'both'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ffcd2-5aee-41fd-8194-53b8fce8e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# if (\n",
    "#     save_results and \n",
    "#     vba_file_path is not None and \n",
    "#     os.path.exists(vba_file_path) and\n",
    "#     vba_macro_name is not None\n",
    "# ):\n",
    "#     import_and_run_vba_macro(\n",
    "#         excel_file_path = save_path, \n",
    "#         vba_file_path   = vba_file_path, \n",
    "#         vba_macro_name  = vba_macro_name, \n",
    "#         pretty_group_by = pretty_idx_names[2:], \n",
    "#         addtnl_cols     = avg_moms_cols\n",
    "#     )\n",
    "# print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902cde8-d5f4-4829-98cf-52ad798c2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    save_name, save_path = save_cred(\n",
    "        cred_df_v1     = df_fnl_1, \n",
    "        cred_df_v2     = df_fnl_2, \n",
    "        opco           = opco, \n",
    "        date_0         = date_0, \n",
    "        date_1         = date_1, \n",
    "        avg_moms_cols  = avg_moms_cols, \n",
    "        normalize      = normalize, \n",
    "        save_dir_base  = save_dir_base, \n",
    "        vba_file_path  = vba_file_path, \n",
    "        vba_macro_name = vba_macro_name,     \n",
    "        verbose        = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6600599-c27f-4107-8472-91127fc76d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdbc96e-5d1d-45e9-b71b-89715fa6b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced6b4d-e0c9-45c7-9e05-0a78fa1557a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bcbf09-8235-48d1-aae0-1be0b9202722",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc0bb4-8383-442b-81fb-50553bbcce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_shp_path = r'C:\\Users\\s346557\\Downloads\\tl_2023_us_state\\tl_2023_us_state.shp'\n",
    "us_df = gpd.read_file(us_shp_path)\n",
    "us_df = us_df.to_crs(\"EPSG:4326\")\n",
    "#-----\n",
    "non_continental = ['HI','VI','MP','GU','AK','AS','PR']\n",
    "us_ok = us_df[us_df['STUSPS'].isin(['OK'])].copy()\n",
    "# us_ok.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691720b3-f100-4ce6-ac04-280c818f1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sustnd[df_sustnd['duration_and_super_time_grp_size']==df_sustnd['duration_and_super_time_grp_size'].max()]['duration_and_super_time_grp'].unique()\n",
    "\n",
    "# fig,ax = plot_spatial_groups_for_time_group_i(\n",
    "#     df_i           = df_sustnd[df_sustnd['duration_and_super_time_grp']=='2_10'], \n",
    "#     us_ok          = us_ok, \n",
    "#     spat_group_col = 'spatial_grp', \n",
    "#     lat_col        = 'avg_latitude', \n",
    "#     lon_col        = 'avg_longitude',     \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baaa186-e124-4916-acd2-8478d07f34c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b93150-a5a0-476d-9035-7a98bf84a3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5c6c2-e726-4a0f-a9fb-f217444b7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlook = win32.Dispatch('outlook.application')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362ed03-3f99-4276-a9f8-ed54772b92cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55403a-e72c-4e61-8aff-b7924f33894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlook = win32.Dispatch('outlook.application')\n",
    "\n",
    "mail = outlook.CreateItem(0)\n",
    "mail.To = 'jbuxton@aep.com'\n",
    "mail.CC = 'buxton.45.jb@gmail.com'\n",
    "mail.Subject = 'Message subject 4'\n",
    "mail.Body = 'Testing testing testing'\n",
    "# mail.HTMLBody = '<h2>HTML Message body</h2>' #this field is optional\n",
    "\n",
    "# To attach a file to the email (optional):\n",
    "attachment  = r'C:\\Users\\s346557\\Downloads\\CRED_20240507_20250507.xlsx'\n",
    "mail.Attachments.Add(attachment)\n",
    "\n",
    "mail.Send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bdf41-0464-4b85-a173-a26659215884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7bbe0-c4e4-4a1f-83c6-3fea1263958d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cd6c4-caa5-4be7-b24c-f5c041670025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
