{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4c6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "# NOTE: To reload a class imported as, e.g., \n",
    "# from module import class\n",
    "# One must call:\n",
    "#   1. import module\n",
    "#   2. reload module\n",
    "#   3. from module import class\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "import string\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns, natsort_keygen\n",
    "from packaging import version\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL, DfToSqlMap\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "from OutageDAQ import OutageDAQ, OutageDAQPrBL\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "#sys.path.insert(0, os.path.join(os.path.realpath('..'), 'Utilities'))\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "from Utilities_df import DFConstructType\n",
    "import Utilities_dt\n",
    "import Plot_Box_sns\n",
    "import GrubbsTest\n",
    "from CustomJSON import CustomEncoder, CustomWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a391c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0a051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08d57880",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b4437c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# VARIABLES TO BE SET BY USER!\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "save_dfs_to_file   = True\n",
    "read_dfs_from_file = False\n",
    "save_end_events    = True\n",
    "\n",
    "#-------------------------\n",
    "# run_date is used to collect all results from a given acquisiton run together.\n",
    "# As such, run_date should be set to the first date of the acquisition run, and\n",
    "#   SHOULD NOT be changed for each individual date in a run (which typically lasts\n",
    "#   over the course of days/weeks)\n",
    "run_date = '20240621' # Date of data acquisition\n",
    "\n",
    "#-------------------------\n",
    "# date_0 = '2020-01-01'\n",
    "# date_1 = '2020-12-31'\n",
    "\n",
    "# date_0 = '2022-01-01'\n",
    "# date_1 = '2022-01-07'\n",
    "# window_width = datetime.timedelta(days=1)\n",
    "\n",
    "date_0 = '2023-04-01' # Lower limit for end events\n",
    "date_1 = '2024-03-31' # Upper limit for end events\n",
    "window_width = datetime.timedelta(days=31)\n",
    "\n",
    "#-------------------------\n",
    "run_using_slim = True\n",
    "\n",
    "#--------------------------------------------------\n",
    "# NOTE: below, states and opcos should be consistent!\n",
    "#       i.e., e.g., if states='OH', then opcos should be 'oh' (or None, I suppose)\n",
    "#-------------------------\n",
    "# states used to \n",
    "#   (1) find transformers which suffered at least one outage from DOVS\n",
    "#   (2) find all transformers from MeterPremise\n",
    "# states can be:\n",
    "#   - a single string, e.g. 'OH'\n",
    "#   - a list of strings, e.g., ['OH', 'WV']\n",
    "#   - None\n",
    "# NOTE: states tend to be upper-case!\n",
    "states=['OH']\n",
    "\n",
    "#-------------------------\n",
    "# opcos used with AMIEndEvents to\n",
    "#  (1) find the premise numbers which recorded an event between date_0 and date_1.\n",
    "#  (2) selection/acquisiton of end_device_events\n",
    "# opcos can be:\n",
    "#   - a single string, e.g. 'oh'\n",
    "#   - a list of strings, e.g., ['oh', 'tx']\n",
    "#   - None\n",
    "# NOTE: opcos tend to be lower-case!\n",
    "# NOTE: Acceptable opcos appear to be: ['ap', 'im', 'oh', 'pso', 'swp', 'tx']\n",
    "opcos='oh'\n",
    "\n",
    "#-------------------------\n",
    "# cities = None\n",
    "cities = ['COLUMBUS']\n",
    "\n",
    "#--------------------------------------------------\n",
    "trsf_pole_nbs_to_ignore = [' ', 'TRANSMISSION', 'PRIMARY', 'NETWORK']\n",
    "\n",
    "# TODO!!!!!!!!!!!!!!!!!!\n",
    "single_zip_xfmrs_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b5e9b-9d25-4eef-bd0a-b0f619f5cb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a3d10-bf7a-4488-bd3a-64a04048c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT AFTER DONE CHECKING EVS_SUM\n",
    "# ############### NEW\n",
    "# outg_daq = OutageDAQPrBL(\n",
    "#     run_date                = run_date, \n",
    "#     date_0                  = date_0, \n",
    "#     date_1                  = date_1, \n",
    "#     collect_evs_sum_vw      = False,  \n",
    "#     save_sub_dir            = 'NoOutgs_Pristine', \n",
    "#     window_width            = window_width, \n",
    "#     states                  = states, \n",
    "#     opcos                   = opcos, \n",
    "#     cities                  = cities, \n",
    "#     single_zip_xfmrs_only   = single_zip_xfmrs_only, \n",
    "#     trsf_pole_nbs_to_ignore = trsf_pole_nbs_to_ignore, \n",
    "#     save_end_events         = save_end_events, \n",
    "#     save_dfs_to_file        = save_dfs_to_file, \n",
    "#     read_dfs_from_file      = read_dfs_from_file, \n",
    "#     base_dir                = os.path.join(\n",
    "#         Utilities.get_local_data_dir(), \n",
    "#         r'dovs_and_end_events_data'\n",
    "#     ), \n",
    "#     run_using_slim          = run_using_slim\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0885231-10b9-4337-b40e-8107d17f4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT AFTER DONE CHECKING EVS_SUM\n",
    "# outg_daq.build_or_load_df_outage_location_ids(verbose=True)\n",
    "# outg_daq.build_or_load_df_xfmrs_all_outg(verbose=True)\n",
    "# outg_daq.build_or_load_trsf_pole_zips_info(verbose=True)\n",
    "# outg_daq.build_or_load_df_mp_no_outg(verbose=True)\n",
    "# outg_daq.build_or_load_pns_with_end_events(verbose=True)\n",
    "# outg_daq.build_or_load_df_mp_no_outg_w_events_slim(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184e49d-c4d0-4106-8891-976f90c0d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT AFTER DONE CHECKING EVS_SUM\n",
    "# outg_daq.collect_events(\n",
    "#     batch_size=None, \n",
    "#     verbose=True, \n",
    "#     n_update=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7ff0d-9572-4b3c-ae2e-32f69167b918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89bfd068-615f-40c8-9af0-71cf0f3cc07a",
   "metadata": {},
   "source": [
    "# CHECKING EVS_SUM METHOD VS ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcf9fa-dbaf-4df2-9cbe-252fe1c3e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### NEW\n",
    "outg_daq = OutageDAQPrBL(\n",
    "    run_date                = run_date, \n",
    "    date_0                  = date_0, \n",
    "    date_1                  = date_1, \n",
    "    collect_evs_sum_vw      = False,  \n",
    "    save_sub_dir            = 'PristineBaseline', \n",
    "    window_width            = window_width, \n",
    "    states                  = states, \n",
    "    opcos                   = opcos, \n",
    "    cities                  = cities, \n",
    "    single_zip_xfmrs_only   = single_zip_xfmrs_only, \n",
    "    trsf_pole_nbs_to_ignore = trsf_pole_nbs_to_ignore, \n",
    "    save_end_events         = save_end_events, \n",
    "    save_dfs_to_file        = save_dfs_to_file, \n",
    "    read_dfs_from_file      = read_dfs_from_file, \n",
    "    base_dir                = os.path.join(\n",
    "        Utilities.get_local_data_dir(), \n",
    "        r'dovs_and_end_events_data'\n",
    "    ), \n",
    "    run_using_slim          = run_using_slim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db829e08-abaa-42a7-842f-d3c717ce8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_daq.build_or_load_df_outage_location_ids(verbose=True)\n",
    "outg_daq.build_or_load_df_xfmrs_all_outg(verbose=True)\n",
    "outg_daq.build_or_load_trsf_pole_zips_info(verbose=True)\n",
    "outg_daq.build_or_load_df_mp_no_outg(verbose=True)\n",
    "outg_daq.build_or_load_pns_with_end_events(verbose=True)\n",
    "outg_daq.build_or_load_df_mp_no_outg_w_events_slim(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9d6b6-07d7-4b45-834a-e219ed097f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop down DF size for quick run comp\n",
    "if run_using_slim:\n",
    "    outg_daq.df_mp_no_outg_w_events_slim = outg_daq.df_mp_no_outg_w_events_slim.iloc[:1000]\n",
    "else:\n",
    "    outg_daq.df_mp_no_outg_w_events = outg_daq.df_mp_no_outg_w_events.sort_values(by=['no_outg_rec_nb', 'trsf_pole_nb', 'prem_nb', 'start_date'], ignore_index=True).iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b3d7c-691a-44fe-857b-30f0a9a4b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_daq.collect_events(\n",
    "    batch_size=None, \n",
    "    verbose=True, \n",
    "    n_update=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a31580-617a-46f1-ad42-18b4dc1939c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02271b-db4e-432c-a389-ef0ff3d05abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d1e00-7b70-4467-ab07-34f1850c1193",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### NEWEST\n",
    "outg_daq = OutageDAQPrBL(\n",
    "    run_date                = run_date, \n",
    "    date_0                  = date_0, \n",
    "    date_1                  = date_1, \n",
    "    collect_evs_sum_vw      = True,  \n",
    "    save_sub_dir            = 'NoOutgs_Pristine', \n",
    "    window_width            = window_width, \n",
    "    states                  = states, \n",
    "    opcos                   = opcos, \n",
    "    cities                  = cities, \n",
    "    single_zip_xfmrs_only   = single_zip_xfmrs_only, \n",
    "    trsf_pole_nbs_to_ignore = trsf_pole_nbs_to_ignore, \n",
    "    save_end_events         = save_end_events, \n",
    "    save_dfs_to_file        = save_dfs_to_file, \n",
    "    read_dfs_from_file      = read_dfs_from_file, \n",
    "    base_dir                = os.path.join(\n",
    "        Utilities.get_local_data_dir(), \n",
    "        r'dovs_and_end_events_data'\n",
    "    ), \n",
    "    run_using_slim          = run_using_slim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875e929-5871-4569-916b-aa4e94cabb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_daq.build_or_load_df_outage_location_ids(verbose=True)\n",
    "outg_daq.build_or_load_df_xfmrs_all_outg(verbose=True)\n",
    "outg_daq.build_or_load_trsf_pole_zips_info(verbose=True)\n",
    "outg_daq.build_or_load_df_mp_no_outg(verbose=True)\n",
    "outg_daq.build_or_load_pns_with_end_events(verbose=True)\n",
    "outg_daq.build_or_load_df_mp_no_outg_w_events_slim(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe595d-bd9c-4e5d-8855-ddda86d48bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop down DF size for quick run comp\n",
    "if run_using_slim:\n",
    "    outg_daq.df_mp_no_outg_w_events_slim = outg_daq.df_mp_no_outg_w_events_slim.iloc[:1000]\n",
    "else:\n",
    "    outg_daq.df_mp_no_outg_w_events = outg_daq.df_mp_no_outg_w_events.sort_values(by=['no_outg_rec_nb', 'trsf_pole_nb', 'prem_nb', 'start_date'], ignore_index=True).iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb010a35-124a-4fca-94b1-fb2a8e9443ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_daq.collect_events(\n",
    "    batch_size=None, \n",
    "    verbose=True, \n",
    "    n_update=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33caf95b-7bf7-4c13-b026-ef6e3594e838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ec125-309a-4a6e-bfbb-22a43fb34312",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3b8dc-511b-40ce-b51b-eeb23b5b44b5",
   "metadata": {},
   "source": [
    "# END CHECKING EVS_SUM METHOD VS ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3b487-ee75-43a9-92e8-a4f549f6429a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223dbbe-17dc-4935-bdda-f8827a8b1ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40ac3c-c422-4e9a-83dd-df782e251801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c301579-b505-47db-9c93-aa0cc93324fd",
   "metadata": {},
   "source": [
    "# OLD DEV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd981b-6f83-4373-808f-50d70dec41bb",
   "metadata": {},
   "source": [
    "### OutagesDAQ constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1824b8ba-b330-43b3-97ab-6f579f1870e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COLUMBUS']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecd85b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir_base = C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20240621\\20230401_20240331\\PristineBaseline_OLD\n",
      "end_events_save_args\n",
      "\tsave_to_file : True\n",
      "\tsave_dir : C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20240621\\20230401_20240331\\PristineBaseline_OLD\\EndEvents\n",
      "\tsave_name : end_events.csv\n",
      "\tindex : True\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# DFs will be saved in save_dir_base\n",
    "# Collection of end events files will be saved in os.path.join(save_dir_base, 'EndEvents')\n",
    "save_dir_base = os.path.join(\n",
    "    Utilities.get_local_data_dir(), \n",
    "    r'dovs_and_end_events_data', \n",
    "    run_date, \n",
    "    f\"{date_0.replace('-','')}_{date_1.replace('-','')}\", \n",
    "    'PristineBaseline_OLD'\n",
    ")\n",
    "#-------------------------\n",
    "end_events_save_args = dict(\n",
    "    save_to_file=save_end_events, \n",
    "    save_dir = os.path.join(save_dir_base, 'EndEvents'), \n",
    "    save_name=r'end_events.csv', \n",
    "    index=True\n",
    ")\n",
    "#-------------------------\n",
    "print(f\"save_dir_base = {save_dir_base}\")\n",
    "print('end_events_save_args')\n",
    "for k,v in end_events_save_args.items():\n",
    "    print(f\"\\t{k} : {v}\")\n",
    "#-------------------------\n",
    "if save_dfs_to_file or save_end_events:\n",
    "    if not os.path.exists(save_dir_base):\n",
    "        os.makedirs(save_dir_base)\n",
    "    #-----\n",
    "    if save_end_events and not os.path.exists(end_events_save_args['save_dir']):\n",
    "        os.makedirs(end_events_save_args['save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42db79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "assert(save_dfs_to_file+read_dfs_from_file <=1) # Should never both read and write!\n",
    "assert(pd.to_datetime(date_1)-pd.to_datetime(date_0) > window_width)\n",
    "#--------------------------------------------------\n",
    "if not read_dfs_from_file:\n",
    "    conn_outages = Utilities.get_utldb01p_oracle_connection()\n",
    "    conn_aws = Utilities.get_athena_prod_aws_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7782202",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afef364-d97f-48ab-979a-9b298fc602f3",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# A. Find transformers which didn't experienced an outage between date_0 and date_1\n",
    "# ----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef546d-121d-498e-ac67-a02235f04691",
   "metadata": {},
   "source": [
    "### build_or_load_df_outage_location_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d868c00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DOVSOutages_SQL: Found opco in kwargs\n",
      "Converting to OPERATING_UNIT_ID\n",
      "\n",
      "Input opcos = ['oh']\n",
      "OPERATING_UNIT_IDs used = [3]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Finding all transformers which HAVE experienced an outage\n",
      "--------------------------------------------------\n",
      "sql_stmnt_outage_location_ids:\n",
      "SELECT\n",
      "\tDISTINCT(DOV.LOCATION_ID)\n",
      "FROM DOVSADM.DOVS_OUTAGE_FACT DOV\n",
      "\tLEFT OUTER JOIN DOVSADM.DOVS_MASTER_GEO_DIM DOV1 ON DOV.OPERATING_UNIT_ID=DOV1.OPRTG_UNT_ID AND DOV.STATE_ABBR_TX=DOV1.STATE_ID AND DOV.OPCO_NBR=DOV1.OPCO_ID AND DOV.DISTRICT_NB=DOV1.DISTRICT_ID AND DOV.SRVC_CNTR_NB=DOV1.AREA_ID AND DOV.GIS_CRCT_NB=DOV1.GIS_CIRCUIT_ID\n",
      "\tLEFT OUTER JOIN DOVSADM.DOVS_OUTAGE_ATTRIBUTES_DIM DOV2 ON DOV.OUTG_REC_NB=DOV2.OUTG_REC_NB\n",
      "\tLEFT OUTER JOIN DOVSADM.DOVS_CLEARING_DEVICE_DIM DOV3 ON DOV.DEVICE_CD=DOV3.DEVICE_CD\n",
      "\tLEFT OUTER JOIN DOVSADM.DOVS_EQUIPMENT_TYPES_DIM DOV4 ON DOV.EQUIPMENT_CD=DOV4.EQUIPMENT_CD\n",
      "\tLEFT OUTER JOIN DOVSADM.DOVS_OUTAGE_CAUSE_TYPES_DIM DOV5 ON DOV.MJR_CAUSE_CD=DOV5.MJR_CAUSE_CD AND DOV.MNR_CAUSE_CD=DOV5.MNR_CAUSE_CD\n",
      "\tLEFT OUTER JOIN DOVSADM.DOVS_PREMISE_DIM PRIM ON DOV.OUTG_REC_NB=PRIM.OUTG_REC_NB\n",
      "WHERE DOV.DT_OFF_TS BETWEEN '2023-04-01' AND '2024-03-31'\n",
      "AND   DOV.STATE_ABBR_TX = 'OH'\n",
      "AND   DOV.OPERATING_UNIT_ID = 3\n",
      "AND   PRIM.CITY_NM = 'COLUMBUS'\n",
      "AND   DOV.MJR_CAUSE_CD <> 'NI'\n",
      "AND   DOV.DEVICE_CD <> 85\n",
      "AND   DOV2.INTRPTN_TYP_CD = 'S'\n",
      "AND   DOV2.CURR_REC_STAT_CD = 'A'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_31748\\2503891294.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_outage_location_ids = pd.read_sql_query(sql_stmnt_outage_location_ids, conn_outages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 7.103327751159668\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if read_dfs_from_file:\n",
    "    df_outage_location_ids = pd.read_pickle(os.path.join(save_dir_base, 'df_outage_location_ids.pkl'))\n",
    "else:\n",
    "    #--------------------------------------------------\n",
    "    # First, find all transformers which HAVE experienced an outage\n",
    "    start=time.time()\n",
    "    sql_outage_location_ids = DOVSOutages_SQL.build_sql_find_outage_xfmrs(\n",
    "        mjr_mnr_cause=None, \n",
    "        include_premise=False, \n",
    "        date_range=[date_0, date_1], \n",
    "        states=states, \n",
    "        opcos=opcos, \n",
    "        cities=cities\n",
    "    )\n",
    "    sql_stmnt_outage_location_ids = sql_outage_location_ids.get_sql_statement()\n",
    "    #-------------------------\n",
    "    print('-----'*20+'\\nFinding all transformers which HAVE experienced an outage\\n'+'-----'*10)\n",
    "    print(f'sql_stmnt_outage_location_ids:\\n{sql_stmnt_outage_location_ids}\\n\\n')\n",
    "    #-----\n",
    "    df_outage_location_ids = pd.read_sql_query(sql_stmnt_outage_location_ids, conn_outages)\n",
    "    #-----\n",
    "    print(f'time = {time.time()-start}\\n'+'-----'*20)\n",
    "    #-------------------------\n",
    "    if save_dfs_to_file:\n",
    "        df_outage_location_ids.to_pickle(os.path.join(save_dir_base, 'df_outage_location_ids.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c0a80-9b38-4d8d-98b6-4f12f8688193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0b0fd65-344f-4e28-8939-2fc5796b0b51",
   "metadata": {},
   "source": [
    "### build_or_load_df_xfmrs_all_outg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0333f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Finding set of all transformers\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_31748\\474070031.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_xfmrs_all = pd.read_sql(sql_xfmrs_all, conn_aws)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql_xfmrs_all:\n",
      "WITH COMP AS (\n",
      "\tSELECT opco_nm,opco_nb\n",
      "\tFROM default.company\n",
      ")\n",
      "SELECT\n",
      "\tDISTINCT(trsf_pole_nb),\n",
      "\tCOMP.opco_nm\n",
      "FROM default.meter_premise ereoo\n",
      "\tLEFT JOIN COMP ON ereoo.co_cd_ownr=COMP.opco_nb\n",
      "WHERE state_cd = 'OH'\n",
      "AND   serv_city_ad = 'COLUMBUS'\n",
      "AND   COMP.opco_nm = 'oh'\n",
      "\n",
      "\n",
      "Determining no outage collection as set difference between all transformers and those with outages\n",
      "--------------------------------------------------\n",
      "# trsf_pole_nbs in no-outage collection: 38369\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Removing unwanted trsf_pole_nbs from no-outage collection\n",
      "\n",
      "BEFORE: (38369, 2)\n",
      "AFTER : (38365, 2)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "time = 10.774478435516357\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Removing unwanted trsf_pole_nbs from no-outage collection\n",
      "\n",
      "BEFORE: (38365, 2)\n",
      "AFTER : (38365, 2)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if read_dfs_from_file:\n",
    "    df_xfmrs_all     = pd.read_pickle(os.path.join(save_dir_base, 'df_xfmrs_all.pkl'))\n",
    "    df_xfmrs_outg    = pd.read_pickle(os.path.join(save_dir_base, 'df_xfmrs_outg.pkl'))\n",
    "    df_xfmrs_no_outg = pd.read_pickle(os.path.join(save_dir_base, 'df_xfmrs_no_outg.pkl'))\n",
    "else:\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # Next, get the set of all transformers.\n",
    "    # Then, we will be able to find the set which did not suffer an outage \n",
    "    #   as set(df_xfmrs_all).difference(set(df_outage_location_ids))\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    print('-----'*20+'\\nFinding set of all transformers\\n'+'-----'*10)\n",
    "    start=time.time()\n",
    "    #-----\n",
    "    sql_xfmrs_all = MeterPremise.build_sql_meter_premise(\n",
    "        cols_of_interest = ['DISTINCT(trsf_pole_nb)'], \n",
    "        states           = states, \n",
    "        opcos            = opcos, \n",
    "        cities           = cities, \n",
    "        from_table_alias = None\n",
    "    )\n",
    "    # If OPCOs is None/not supplied, then SQLQuery object will be returned (original, default behavior)\n",
    "    # If OPCOs is supplied, then string will be returned.\n",
    "    # In either case, we need sql_xfmrs_all to be a str before inputting to pd.read_sql\n",
    "    assert(Utilities.is_object_one_of_types(sql_xfmrs_all, [SQLQuery, str]))\n",
    "    if isinstance(sql_xfmrs_all, SQLQuery):\n",
    "        sql_xfmrs_all = sql_xfmrs_all.get_sql_statement()\n",
    "    #-----\n",
    "    df_xfmrs_all = pd.read_sql(sql_xfmrs_all, conn_aws) \n",
    "    #-----\n",
    "    print(f'sql_xfmrs_all:\\n{sql_xfmrs_all}\\n\\n')\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    # Get no outage collection as those in df_xfmrs_all without trsf_pole_nb matching those in df_outage_location_ids\n",
    "    #   i.e., set(df_xfmrs_all).difference(set(df_outage_location_ids))\n",
    "    print(f\"Determining no outage collection as set difference between all transformers and those with outages\\n\"+'-----'*10)\n",
    "    df_xfmrs_no_outg = df_xfmrs_all[~df_xfmrs_all['trsf_pole_nb'].isin(df_outage_location_ids['LOCATION_ID'].tolist())]\n",
    "    assert(df_xfmrs_no_outg['trsf_pole_nb'].shape[0]==df_xfmrs_no_outg['trsf_pole_nb'].nunique())\n",
    "    print(f\"# trsf_pole_nbs in no-outage collection: {df_xfmrs_no_outg['trsf_pole_nb'].nunique()}\")\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    if trsf_pole_nbs_to_ignore is not None:\n",
    "        print('-----'*20+'\\nRemoving unwanted trsf_pole_nbs from no-outage collection\\n')\n",
    "        print(f\"BEFORE: {df_xfmrs_no_outg.shape}\")\n",
    "        df_xfmrs_no_outg = df_xfmrs_no_outg[~df_xfmrs_no_outg['trsf_pole_nb'].isin(trsf_pole_nbs_to_ignore)]\n",
    "        print(f\"AFTER : {df_xfmrs_no_outg.shape}\")\n",
    "        print('-----'*20)\n",
    "        \n",
    "    #--------------------------------------------------\n",
    "    # Get outage collection as those in df_xfmrs_all with trsf_pole_nb matching those in df_outage_location_ids\n",
    "    df_xfmrs_outg    = df_xfmrs_all[df_xfmrs_all['trsf_pole_nb'].isin(df_outage_location_ids['LOCATION_ID'].tolist())]\n",
    "    #--------------------------------------------------\n",
    "    print(f'\\n\\ntime = {time.time()-start}\\n'+'-----'*20)\n",
    "    #--------------------------------------------------\n",
    "    if save_dfs_to_file:\n",
    "        df_xfmrs_all.to_pickle(os.path.join(save_dir_base, 'df_xfmrs_all.pkl'))\n",
    "        df_xfmrs_outg.to_pickle(os.path.join(save_dir_base, 'df_xfmrs_outg.pkl'))\n",
    "        df_xfmrs_no_outg.to_pickle(os.path.join(save_dir_base, 'df_xfmrs_no_outg.pkl'))\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------\n",
    "if trsf_pole_nbs_to_ignore is not None:\n",
    "    print('-----'*20+'\\nRemoving unwanted trsf_pole_nbs from no-outage collection\\n')\n",
    "    print(f\"BEFORE: {df_xfmrs_no_outg.shape}\")\n",
    "    df_xfmrs_no_outg = df_xfmrs_no_outg[~df_xfmrs_no_outg['trsf_pole_nb'].isin(trsf_pole_nbs_to_ignore)]\n",
    "    print(f\"AFTER : {df_xfmrs_no_outg.shape}\")\n",
    "    print('-----'*20)\n",
    "    \n",
    "#--------------------------------------------------\n",
    "trsf_pole_nbs = df_xfmrs_no_outg['trsf_pole_nb'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6020df-743a-4449-8ed2-3130e050700d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79225e4",
   "metadata": {},
   "source": [
    "# Build zip code information for transformers, and possibly restrict data to single zipcode transformers only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a7b6a-8da1-4aef-a224-212e0ffeee32",
   "metadata": {},
   "source": [
    "### build_or_load_trsf_pole_zips_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c98c2496-e083-4fd8-84d3-8dee64ed127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_coll = 38365\n",
      "batch_size = 1000\n",
      "n_batches = 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s346557\\Documents\\Analysis\\GenAn.py:663: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql, conn_db, **read_sql_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/39\n",
      "20/39\n",
      "30/39\n",
      "trsf_pole_zips_df build time: 14.64705514907837\n"
     ]
    }
   ],
   "source": [
    "if read_dfs_from_file:\n",
    "    mp_for_zips_df    = pd.read_pickle(os.path.join(save_dir_base, 'mp_for_zips_df.pkl'))\n",
    "    trsf_pole_df_full = pd.read_pickle(os.path.join(save_dir_base, 'trsf_location_info_df.pkl'))\n",
    "    trsf_pole_zips_df = pd.read_pickle(os.path.join(save_dir_base, 'trsf_pole_zips_df.pkl'))\n",
    "else:\n",
    "    zips_dict = OutageDAQ.build_trsf_pole_zips_df(\n",
    "        field_to_split_and_val = ('trsf_pole_nbs', trsf_pole_nbs), \n",
    "        states                 = states, \n",
    "        opcos                  = opcos, \n",
    "        cities                 = cities\n",
    "    )\n",
    "    #----------\n",
    "    trsf_pole_zips_df = zips_dict['trsf_pole_zips_df']\n",
    "    trsf_pole_df_full = zips_dict['trsf_pole_df_full']\n",
    "    mp_for_zips_df    = zips_dict['mp_for_zips_df']\n",
    "    #--------------------------------------------------\n",
    "    if save_dfs_to_file:\n",
    "        mp_for_zips_df.to_pickle(   os.path.join(save_dir_base, 'mp_for_zips_df.pkl'))\n",
    "        trsf_pole_df_full.to_pickle(os.path.join(save_dir_base, 'trsf_location_info_df.pkl'))\n",
    "        trsf_pole_zips_df.to_pickle(os.path.join(save_dir_base, 'trsf_pole_zips_df.pkl'))\n",
    "#--------------------------------------------------\n",
    "if single_zip_xfmrs_only:\n",
    "    # For our purposes here, we only want entries for which there is a single zip\n",
    "    trsf_pole_nzips   = trsf_pole_zips_df.drop(columns=['zip+4']).drop_duplicates()['trsf_pole_nb'].value_counts()\n",
    "    single_zip_poles  = trsf_pole_nzips[trsf_pole_nzips==1].index.tolist()\n",
    "    #-----\n",
    "    trsf_pole_zips_df = trsf_pole_zips_df[trsf_pole_zips_df['trsf_pole_nb'].isin(single_zip_poles)]\n",
    "    #-----\n",
    "    trsf_pole_nbs = mp_for_zips_df[mp_for_zips_df['trsf_pole_nb'].isin(single_zip_poles)]['trsf_pole_nb'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03b09e-1747-477a-abe1-9cb238cc975e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08350cc9-d4d8-4a6c-ba0c-c5130fb1f53e",
   "metadata": {},
   "source": [
    "### build_or_load_df_mp_no_outg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82862d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# Build joined current and historical MeterPremise collections for all transformers\n",
    "#   found in df_xfmrs_no_outg.\n",
    "# Need to use both current and historical because each transformer will be assigned a random\n",
    "#   datetime (between some date_0 and date_1 limites) below around which to collect events.\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "print(\n",
    "    '-----'*20+'\\nBuilding joined current and historical MeterPremise collections '+\n",
    "    'for all transformers in no-outage collection\\n'+'-----'*10\n",
    ")\n",
    "start = time.time()\n",
    "#-----\n",
    "addtnl_mp_df_cols = ['state_cd', 'circuit_nb', 'circuit_nm', 'station_nb', 'station_nm']\n",
    "df_mp_no_outg_OG = MeterPremise.build_mp_df_curr_hist_for_xfmrs(\n",
    "    trsf_pole_nbs          = trsf_pole_nbs, \n",
    "    join_curr_hist         = True, \n",
    "    drop_approx_duplicates = False, \n",
    "    addtnl_mp_df_curr_cols = addtnl_mp_df_cols, \n",
    "    addtnl_mp_df_hist_cols = addtnl_mp_df_cols\n",
    ")\n",
    "print(f\"\\nFound MeterPremise data for {df_mp_no_outg_OG['trsf_pole_nb'].nunique()} transformers\")\n",
    "print(f'\\ntime = {time.time()-start}\\n'+'-----'*20)\n",
    "#**************************************************\n",
    "#**************************************************\n",
    "# Since df_mp_no_outg_OG takes such a significant time to build, save at this point, so all\n",
    "#   is not lost in case of crash\n",
    "if save_dfs_to_file:\n",
    "    df_mp_no_outg_OG.to_pickle(os.path.join(save_dir_base, 'df_mp_no_outg_curr_hist.pkl'))\n",
    "#**************************************************\n",
    "#**************************************************\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# For each transformer, set a random date interval between date_0 and date_1 around which\n",
    "#   results will be acquired.\n",
    "# This is analogous to the outage event for that dataset.\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "print('-----'*20+f'\\nSetting random date interval of width={window_width} between date_0={date_0} and date_1={date_1}'+\n",
    "      '\\nfor all remaining transformers in no-outage collection\\n')\n",
    "start = time.time()\n",
    "df_mp_no_outg_OG = OutageDAQ.set_random_date_interval_in_df_by_xfmr(\n",
    "    df             = df_mp_no_outg_OG, \n",
    "    date_0         = date_0, \n",
    "    date_1         = date_1, \n",
    "    window_width   = window_width, \n",
    "    xfmr_col       = 'trsf_pole_nb', \n",
    "    rand_seed      = None, \n",
    "    placement_cols = ['start_date', 'end_date'], \n",
    "    inplace        = True    \n",
    ")\n",
    "# If I'm looking at a single year, with a window_width of 30 days, there are only\n",
    "# 365-30=335 possible unique start dates, regardless of number of groups.\n",
    "# So, there are bound to be repeats!\n",
    "# HOWEVER, I am now using also a random time for each day, so the likeliness of repeats\n",
    "#   is significantly reduced!\n",
    "print(f\"# trsf_pole_nbs: {df_mp_no_outg_OG['trsf_pole_nb'].nunique()}\")\n",
    "print(f\"# start_dates  : {df_mp_no_outg_OG['start_date'].nunique()}\")\n",
    "print(f\"# end_dates    : {df_mp_no_outg_OG['end_date'].nunique()}\")\n",
    "print(f'\\ntime = {time.time()-start}\\n'+'-----'*20)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Currently, df_mp_no_outg_OG contains all current and historical MeterPremise data.\n",
    "# Keep only the data for the meters active during the relevant time periods, as\n",
    "#   dictated by the start_date and end_date fields.\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "print('-----'*20+f'\\nKeep only meters active for the relevant times\\n')\n",
    "print(f\"BEFORE: df_mp_no_outg_OG.shape = {df_mp_no_outg_OG.shape}\")\n",
    "df_mp_no_outg_OG = df_mp_no_outg_OG[(df_mp_no_outg_OG['inst_ts']<=df_mp_no_outg_OG['start_date']) & \n",
    "                                    (df_mp_no_outg_OG['rmvl_ts'].fillna(pd.Timestamp.max)>df_mp_no_outg_OG['end_date'])]\n",
    "print(f\"AFTER : df_mp_no_outg_OG.shape = {df_mp_no_outg_OG.shape}\")\n",
    "print('-----'*20)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Drop approximate duplicates from df_mp_no_outg_OG so there is a single entry for each \n",
    "#   serial number/premise\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# PROBLEM:\n",
    "# drop_approx_mp_duplicates will turn some column elements into lists, which is an issue \n",
    "#   when this is plugged into consolidate_df.\n",
    "# The current solution is to have the addtnl_groupby_cols below, but a more general solution \n",
    "#   should be implemented, e.g., if lists, then join the lists or whatever\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "print('-----'*20+f'\\nDrop approximate duplicates\\n')\n",
    "print(f\"BEFORE: df_mp_no_outg_OG.shape = {df_mp_no_outg_OG.shape}\")\n",
    "df_mp_no_outg_OG = MeterPremise.drop_approx_mp_duplicates(\n",
    "    mp_df               = df_mp_no_outg_OG, \n",
    "    fuzziness           = pd.Timedelta('1 hour'), \n",
    "    addtnl_groupby_cols = addtnl_mp_df_cols+['start_date', 'end_date'], \n",
    ")\n",
    "print(f\"AFTER : df_mp_no_outg_OG.shape = {df_mp_no_outg_OG.shape}\")\n",
    "print(f'\\ntime = {time.time()-start}\\n'+'-----'*20)\n",
    "\n",
    "# Make sure no list entries! (PROBABLY NOT NEEDED ANYMORE AFTER DEV STAGE!)\n",
    "for col in df_mp_no_outg_OG.columns:\n",
    "    for i in range(df_mp_no_outg_OG.shape[0]):\n",
    "        if isinstance(df_mp_no_outg_OG.iloc[i][col], list):\n",
    "            print(col, i)\n",
    "            assert(0)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Add no_outg_rec_nb column to allow easier grouping when building rcpo_dfs, and for identification later\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "print('-----'*20+f'\\nCreating no_outg_rec_nb column\\n')\n",
    "start=time.time()\n",
    "rand_pfx = Utilities.generate_random_string(str_len=5, letters=string.ascii_letters + string.digits)\n",
    "df_mp_no_outg_OG['no_outg_rec_nb'] = df_mp_no_outg_OG.groupby(['trsf_pole_nb', 'start_date', 'end_date']).ngroup()\n",
    "df_mp_no_outg_OG['no_outg_rec_nb'] = rand_pfx + df_mp_no_outg_OG['no_outg_rec_nb'].astype(str)\n",
    "print(f'\\ntime = {time.time()-start}\\n'+'-----'*20)\n",
    "\n",
    "#**************************************************\n",
    "#**************************************************\n",
    "if save_dfs_to_file:\n",
    "    df_mp_no_outg_OG.to_pickle(os.path.join(save_dir_base, 'df_mp_no_outg_full.pkl'))\n",
    "#**************************************************\n",
    "#**************************************************\n",
    "\n",
    "# Install and removal timestamps no longer needed, so remove.\n",
    "df_mp_no_outg_OG = df_mp_no_outg_OG.drop(columns=['inst_ts', 'rmvl_ts'])\n",
    "\n",
    "# Sort by no_outg_rec_nb, so all like 'no outage events' are sequential in DF\n",
    "df_mp_no_outg_OG = df_mp_no_outg_OG.sort_values(by=['no_outg_rec_nb', 'trsf_pole_nb', 'prem_nb', 'mfr_devc_ser_nbr'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c3c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ace07f7-1edd-448b-bff4-bd904a13c6c9",
   "metadata": {},
   "source": [
    "### build_or_load_pns_with_end_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8cdd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# Find the premise numbers which recorded an event between date_0 and date_1.\n",
    "# Premises without any events during the period will be removed from df_mp_no_outg_OG.\n",
    "# This is relatively quick, and saves much time in the data acquisition step as we do not\n",
    "#   have to waste time searching for empty results (which is suprisingly sluggish)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "print('-----'*20+f'\\nFind PNs with recorded events between date_0={date_0} and date_1={date_1}\\n'+'-----'*10)\n",
    "start=time.time()\n",
    "pns_with_end_events, sql_pns_with_end_events = AMIEndEvents.get_end_event_distinct_fields(\n",
    "    date_0=date_0, \n",
    "    date_1=date_1, \n",
    "    fields=['aep_premise_nb'], \n",
    "    are_datetime=False, \n",
    "    addtnl_build_sql_function_kwargs = dict(\n",
    "        states = states, \n",
    "        opcos  = opcos, \n",
    "        cities = cities\n",
    "    ), \n",
    "    return_sql=True\n",
    ")\n",
    "#----------\n",
    "print('Removing PNs without recorded events to speed up data acquisition\\n'+'-----'*10)\n",
    "print(f'BEFORE: df_mp_no_outg_OG.shape = {df_mp_no_outg_OG.shape}')\n",
    "print(f\"        df_mp_no_outg_OG['trsf_pole_nb'].nunique() = {df_mp_no_outg_OG['trsf_pole_nb'].nunique()}\")\n",
    "#----------\n",
    "df_mp_no_outg_OG = df_mp_no_outg_OG[df_mp_no_outg_OG['prem_nb'].isin(pns_with_end_events['aep_premise_nb'].tolist())]\n",
    "#----------\n",
    "print(f'AFTER:  df_mp_no_outg_OG.shape = {df_mp_no_outg_OG.shape}')\n",
    "print(f\"        df_mp_no_outg_OG['trsf_pole_nb'].nunique() = {df_mp_no_outg_OG['trsf_pole_nb'].nunique()}\")\n",
    "#----------\n",
    "print(f'\\n\\ntime = {time.time()-start}\\n'+'-----'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567e2e6-56b3-4722-93f3-529decd5409e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3550a32f-51c6-4a0a-a433-8f935daffb16",
   "metadata": {},
   "source": [
    "### build_or_load_df_mp_no_outg_w_events_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a973a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# Build consolidated (slim) version of df_mp_no_outage\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "cols_shared_by_group = addtnl_mp_df_cols\n",
    "cols_to_collect_in_lists = ['mfr_devc_ser_nbr', 'prem_nb']\n",
    "rename_cols = {'mfr_devc_ser_nbr':'serial_numbers', 'prem_nb':'premise_nbs'}\n",
    "\n",
    "print('-----'*20+f'\\nBuilding consolidated version of df_mp_no_outg\\n'+'-----'*10)\n",
    "start=time.time()\n",
    "df_mp_no_outg_slim_OG = Utilities_df.consolidate_df(\n",
    "    df                                  = df_mp_no_outg_OG, \n",
    "    groupby_cols                        = ['no_outg_rec_nb', 'trsf_pole_nb', 'start_date', 'end_date'], \n",
    "    cols_shared_by_group                = cols_shared_by_group, \n",
    "    cols_to_collect_in_lists            = cols_to_collect_in_lists, \n",
    "    include_groupby_cols_in_output_cols = False, \n",
    "    allow_duplicates_in_lists           = False, \n",
    "    recover_uniqueness_violators        = True, \n",
    "    rename_cols                         = rename_cols, \n",
    "    verbose                             = True\n",
    ")\n",
    "#-------------------------\n",
    "df_mp_no_outg_slim_OG=df_mp_no_outg_slim_OG.reset_index().set_index(['no_outg_rec_nb', 'trsf_pole_nb'], drop=False)\n",
    "df_mp_no_outg_slim_OG.index.names=['idx_no_outg_rec_nb', 'idx_trsf_pole_nb']\n",
    "#-----\n",
    "df_mp_no_outg_slim_OG['premise_nbs']    = df_mp_no_outg_slim_OG['premise_nbs'].apply(sorted)\n",
    "df_mp_no_outg_slim_OG['serial_numbers'] = df_mp_no_outg_slim_OG['serial_numbers'].apply(sorted)\n",
    "#-------------------------\n",
    "print(f'\\ntime = {time.time()-start}\\n'+'-----'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198bc28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4fa3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************************************************************\n",
    "# Save DFs\n",
    "#****************************************************************************************************\n",
    "if save_dfs_to_file:\n",
    "    # df_outage_location_ids.to_pickle(os.path.join(save_dir_base, 'df_outage_location_ids.pkl'))\n",
    "    # df_xfmrs_all.to_pickle(os.path.join(save_dir_base, 'df_xfmrs_all.pkl'))\n",
    "    # df_xfmrs_outg.to_pickle(os.path.join(save_dir_base, 'df_xfmrs_outg.pkl'))\n",
    "    # df_xfmrs_no_outg.to_pickle(os.path.join(save_dir_base, 'df_xfmrs_no_outg.pkl'))\n",
    "    pns_with_end_events.to_pickle(os.path.join(save_dir_base, 'pns_with_end_events.pkl'))\n",
    "    #-----\n",
    "    df_mp_no_outg_OG.to_pickle(os.path.join(save_dir_base, 'df_mp_no_outg_PNs_w_events.pkl'))\n",
    "    df_mp_no_outg_slim_OG.to_pickle(os.path.join(save_dir_base, 'df_mp_no_outg_PNs_w_events_slim.pkl'))\n",
    "    \n",
    "    #-------------------------\n",
    "    # Below, date_0 and date_1 should be string, but use str()\n",
    "    #   in case they were converted to pd.Datetime or whatever\n",
    "    run_info = dict(\n",
    "        date_0                  = str(date_0), \n",
    "        date_1                  = str(date_1), \n",
    "        window_width            = str(window_width), \n",
    "        run_using_slim          = run_using_slim,\n",
    "        states                  = states, \n",
    "        opcos                   = opcos, \n",
    "        cities                  = cities, \n",
    "        trsf_pole_nbs_to_ignore = trsf_pole_nbs_to_ignore\n",
    "    )\n",
    "    CustomWriter.output_dict_to_json(os.path.join(save_dir_base, 'run_info.json'), run_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4058f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mp_no_outg_OG = pd.read_pickle(os.path.join(save_dir_base, 'df_mp_no_outg_PNs_w_events.pkl'))\n",
    "# df_mp_no_outg_slim_OG = pd.read_pickle(os.path.join(save_dir_base, 'df_mp_no_outg_PNs_w_events_slim.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e3bdc",
   "metadata": {},
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Collect events\n",
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a596f1b1-e706-4f05-b15a-b168313bae2a",
   "metadata": {},
   "source": [
    "### collect_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8803e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp_no_outg_slim = df_mp_no_outg_slim_OG.copy()\n",
    "df_mp_no_outg      = df_mp_no_outg_OG.copy()\n",
    "#----------\n",
    "df_construct_type              = DFConstructType.kRunSqlQuery\n",
    "contstruct_df_args_end_events  = None\n",
    "addtnl_groupby_cols            = ['trsf_pole_nb', 'no_outg_rec_nb']\n",
    "\n",
    "cols_of_interest_end_dev_event = TableInfos.AMIEndEvents_TI.std_columns_of_interest\n",
    "verbose                        = True\n",
    "n_update                       = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572aa367-802a-4360-b394-1a829b017a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "end_events_sql_function_kwargs = dict(\n",
    "    cols_of_interest                  = cols_of_interest_end_dev_event, \n",
    "    join_mp_args                      = False, \n",
    "    df_args                           = dict(\n",
    "        addtnl_groupby_cols = addtnl_groupby_cols, \n",
    "        t_search_min_col    = 'start_date', \n",
    "        t_search_max_col    = 'end_date'\n",
    "    ), \n",
    "    field_to_split                    = 'df_mp_no_outg', \n",
    "    field_to_split_location_in_kwargs = ['df_mp_no_outg'], \n",
    "    sort_coll_to_split                = False,\n",
    "    verbose                           = verbose, \n",
    "    n_update                          = n_update\n",
    ")\n",
    "#----------\n",
    "# if opcos is not None:\n",
    "addtnl_end_events_sql_function_kwargs = dict(\n",
    "    build_sql_function_kwargs = dict(\n",
    "        states = states, \n",
    "        opcos  = opcos, \n",
    "        cities = cities\n",
    "    )\n",
    ")\n",
    "end_events_sql_function_kwargs = {\n",
    "    **end_events_sql_function_kwargs, \n",
    "    **addtnl_end_events_sql_function_kwargs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae49ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "if run_using_slim:\n",
    "    batch_size = 10\n",
    "    #----------\n",
    "    end_events_sql_function_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "        to_supplmnt_dict    = end_events_sql_function_kwargs, \n",
    "        default_values_dict = dict(\n",
    "            df_mp_no_outg   = df_mp_no_outg_slim, \n",
    "            batch_size      = batch_size, \n",
    "            df_args         = dict(\n",
    "                mapping_to_ami     = DfToSqlMap(df_col='premise_nbs', kwarg='premise_nbs', sql_col='aep_premise_nb'), \n",
    "                is_df_consolidated = True\n",
    "            )\n",
    "        ), \n",
    "        extend_any_lists    = True,\n",
    "        inplace             = True\n",
    "    )\n",
    "#-------------------------\n",
    "else:\n",
    "    batch_size=200\n",
    "    #----------\n",
    "    end_events_sql_function_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "        to_supplmnt_dict = end_events_sql_function_kwargs, \n",
    "        default_values_dict = dict(\n",
    "            df_mp_no_outg   = df_mp_no_outg, \n",
    "            batch_size      = batch_size, \n",
    "            df_args         = dict(\n",
    "                mapping_to_ami     = DfToSqlMap(df_col='prem_nb', kwarg='premise_nbs', sql_col='aep_premise_nb'), \n",
    "                is_df_consolidated = False\n",
    "            )\n",
    "        ), \n",
    "        extend_any_lists    = True,\n",
    "        inplace             = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31134ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "exit_status = Utilities.run_tryexceptwhile_process(\n",
    "    func                = AMIEndEvents,\n",
    "    func_args_dict      = dict(\n",
    "        df_construct_type         = df_construct_type, \n",
    "        contstruct_df_args        = contstruct_df_args_end_events, \n",
    "        build_sql_function        = AMIEndEvents_SQL.build_sql_end_events_for_no_outages, \n",
    "        build_sql_function_kwargs = end_events_sql_function_kwargs, \n",
    "        init_df_in_constructor    = True, \n",
    "        save_args                 = end_events_save_args\n",
    "    ), \n",
    "    max_calls_per_min   = 1, \n",
    "    lookback_period_min = 15, \n",
    "    max_calls_absolute  = 1000, \n",
    "    verbose             = True\n",
    ")\n",
    "print(f'exit_status = {exit_status}')\n",
    "\n",
    "build_time = time.time()-start\n",
    "print(build_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0e48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d974a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7a0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
