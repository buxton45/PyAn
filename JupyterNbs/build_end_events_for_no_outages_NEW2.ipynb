{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505cfdef",
   "metadata": {},
   "source": [
    "# IDEA HERE:\n",
    "Save time in data acquisiton by not grouping by anything (in reality, the data are already grouped by search times in AMI_SQL).\n",
    "After the data acquistion, use the final df_no_outage to put the desired information back in (e.g., if we want no_outg_rec_nb, is_first_after_outg, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "# NOTE: To reload a class imported as, e.g., \n",
    "# from module import class\n",
    "# One must call:\n",
    "#   1. import module\n",
    "#   2. reload module\n",
    "#   3. from module import class\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "import string\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns, natsort_keygen\n",
    "from packaging import version\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "#sys.path.insert(0, os.path.join(os.path.realpath('..'), 'Utilities'))\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "from Utilities_df import DFConstructType\n",
    "import Utilities_dt\n",
    "import Plot_Box_sns\n",
    "import GrubbsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9444aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_active_MP_for_outages_df(\n",
    "    df_outage, \n",
    "    prem_nb_col, \n",
    "    df_mp_curr=None, \n",
    "    df_mp_hist=None, \n",
    "    assert_all_PNs_found=True, \n",
    "    drop_inst_rmvl_cols=False, \n",
    "    outg_rec_nb_col='OUTG_REC_NB',  #TODO!!!!!!!!!!!!!!!!!!!!!!! what if index?!\n",
    "    is_slim=False, \n",
    "    dt_on_ts_col='DT_ON_TS', \n",
    "    df_off_ts_full_col='DT_OFF_TS_FULL', \n",
    "    consolidate_PNs_batch_size=1000, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Similar to build_active_MP_for_outages\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(prem_nb_col in df_outage.columns and \n",
    "           dt_on_ts_col in df_outage.columns and \n",
    "           df_off_ts_full_col in df_outage.columns)\n",
    "    #-------------------------\n",
    "    if not is_slim:\n",
    "        PNs = df_outage[prem_nb_col].unique().tolist()\n",
    "    else:\n",
    "        PNs = Utilities_df.consolidate_column_of_lists(\n",
    "            df=df_outage, \n",
    "            col=prem_nb_col, \n",
    "            sort=True,\n",
    "            include_None=False,\n",
    "            batch_size=consolidate_PNs_batch_size, \n",
    "            verbose=False\n",
    "        )\n",
    "    #-----\n",
    "    PNs = [x for x in PNs if pd.notna(x)]\n",
    "    #-------------------------\n",
    "    mp_df_curr_hist_dict = MeterPremise.build_mp_df_curr_hist_for_PNs(\n",
    "        PNs=PNs, \n",
    "        mp_df_curr=df_mp_curr,\n",
    "        mp_df_hist=df_mp_hist, \n",
    "        join_curr_hist=False, \n",
    "        addtnl_mp_df_curr_cols=None, \n",
    "        addtnl_mp_df_hist_cols=None, \n",
    "        assert_all_PNs_found=assert_all_PNs_found, \n",
    "        assume_one_xfmr_per_PN=True, \n",
    "        drop_approx_duplicates=True\n",
    "    )\n",
    "    df_mp_curr = mp_df_curr_hist_dict['mp_df_curr']\n",
    "    df_mp_hist = mp_df_curr_hist_dict['mp_df_hist']\n",
    "    #-------------------------\n",
    "    # Only reason for making dict is to ensure outg_rec_nbs are not repeated \n",
    "    active_SNs_in_outgs_dfs_dict = {}\n",
    "\n",
    "    if not is_slim:\n",
    "        for outg_rec_nb_i, df_i in df_outage.groupby(outg_rec_nb_col):\n",
    "            # Don't want to include outg_rec_nb_i=-2147483648\n",
    "            if int(outg_rec_nb_i) < 0:\n",
    "                continue\n",
    "            # There should only be a single unique dt_on_ts and dt_off_ts_full for each outage\n",
    "            if(df_i[dt_on_ts_col].nunique()!=1 or \n",
    "               df_i[df_off_ts_full_col].nunique()!=1):\n",
    "                print(f'outg_rec_nb_i = {outg_rec_nb_i}')\n",
    "                print(f'df_i[dt_on_ts_col].nunique()       = {df_i[dt_on_ts_col].nunique()}')\n",
    "                print(f'df_i[df_off_ts_full_col].nunique() = {df_i[df_off_ts_full_col].nunique()}')\n",
    "                print('CRASH IMMINENT!')\n",
    "                assert(0)\n",
    "            # Grab power out/on time and PNs from df_i\n",
    "            dt_on_ts_i       = df_i[dt_on_ts_col].unique()[0]\n",
    "            df_off_ts_full_i = df_i[df_off_ts_full_col].unique()[0]\n",
    "            PNs_i            = df_i[prem_nb_col].unique().tolist()\n",
    "\n",
    "            # Just as was done above for PNs, NaN values must be removed from PNs_i\n",
    "            #   The main purpose here is to remove instances where PNs_i = [nan]\n",
    "            #   NOTE: For case of slim df, the NaNs should already be removed\n",
    "            # After removal, if len(PNs_i)==0, contine\n",
    "            PNs_i = [x for x in PNs_i if pd.notna(x)]\n",
    "            if len(PNs_i)==0:\n",
    "                continue\n",
    "            \n",
    "            # Build active_SNs_df_i and add it to active_SNs_in_outgs_dfs_dict\n",
    "            # NOTE: assume_one_xfmr_per_PN=True above in MeterPremise.build_mp_df_curr_hist_for_PNs,\n",
    "            #       so does not need to be set again (i.e., assume_one_xfmr_per_PN=False below)\n",
    "            active_SNs_df_i = MeterPremise.get_active_SNs_for_PNs_at_datetime_interval(\n",
    "                PNs=PNs_i,\n",
    "                df_mp_curr=df_mp_curr, \n",
    "                df_mp_hist=df_mp_hist, \n",
    "                dt_0=df_off_ts_full_i,\n",
    "                dt_1=dt_on_ts_i,\n",
    "                assume_one_xfmr_per_PN=False, \n",
    "                output_index=None,\n",
    "                output_groupby=None, \n",
    "                assert_all_PNs_found=False\n",
    "            )\n",
    "            active_SNs_df_i[outg_rec_nb_col] = outg_rec_nb_i\n",
    "            assert(outg_rec_nb_i not in active_SNs_in_outgs_dfs_dict)\n",
    "            active_SNs_in_outgs_dfs_dict[outg_rec_nb_i] = active_SNs_df_i\n",
    "    else:\n",
    "        for outg_rec_nb_i, row_i in df_outage.iterrows():\n",
    "            # NOTE: assume_one_xfmr_per_PN=True above in MeterPremise.build_mp_df_curr_hist_for_PNs,\n",
    "            #       so does not need to be set again (i.e., assume_one_xfmr_per_PN=False below)\n",
    "            active_SNs_df_i = MeterPremise.get_active_SNs_for_PNs_at_datetime_interval(\n",
    "                PNs=row_i[prem_nb_col],\n",
    "                df_mp_curr=df_mp_curr, \n",
    "                df_mp_hist=df_mp_hist, \n",
    "                dt_0=row_i[df_off_ts_full_col],\n",
    "                dt_1=row_i[dt_on_ts_col],\n",
    "                assume_one_xfmr_per_PN=False, \n",
    "                output_index=None,\n",
    "                output_groupby=None, \n",
    "                assert_all_PNs_found=False\n",
    "            )\n",
    "            active_SNs_df_i[outg_rec_nb_col] = outg_rec_nb_i\n",
    "            assert(outg_rec_nb_i not in active_SNs_in_outgs_dfs_dict)\n",
    "            active_SNs_in_outgs_dfs_dict[outg_rec_nb_i] = active_SNs_df_i\n",
    "    #-------------------------\n",
    "    active_SNs_df = pd.concat(list(active_SNs_in_outgs_dfs_dict.values()))\n",
    "    #-------------------------\n",
    "    if drop_inst_rmvl_cols:\n",
    "        active_SNs_df = active_SNs_df.drop(columns=[df_mp_install_time_col, df_mp_removal_time_col])\n",
    "    #-------------------------\n",
    "    return active_SNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bfd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_outages_for_pns(\n",
    "    PNs, \n",
    "    date_0, \n",
    "    date_1, \n",
    "    cols_of_interest=None, \n",
    "    mjr_mnr_cause=None, \n",
    "    method='decide_at_runtime', \n",
    "    addtnl_build_sql_std_outage_kwargs=None, \n",
    "    verbose=True, \n",
    "    n_update=10, \n",
    "    batch_size=1000\n",
    "):\n",
    "    r\"\"\"\n",
    "    By default, the returned columns are [DT_ON_TS, DT_OFF_TS_FULL, PREMISE_NB].\n",
    "        The first two are explicitly added below under 'if cols_of_interest is None:'\n",
    "        The last is more subtly added via the 'select_cols_DOVS_PREMISE_DIM=['PREMISE_NB']' input parameter\n",
    "          to DOVSOutages_SQL.build_sql_std_outage\n",
    "      \n",
    "    method:\n",
    "        Possible values: 'query_pns_only', 'query_all', 'decide_at_runtime'\n",
    "        'query_pns_only':  Build the SQL queries using the premise numbers in PNs.\n",
    "                           NOTE: With this method, likely the premise numbers will need to be split into multiple queries, \n",
    "                                 hence the need for full-blown DOVSOutages/GenAn.build_df_general as opposed to use of \n",
    "                                 DOVSOutages.build_sql_std_outage in 'query_all' method\n",
    "                           Pro: Less memory, as only PNs we're interested in are grabbed in SQL queries\n",
    "                           Con: Takes significantly more time to run when number of PNs is large.\n",
    "                           NOTE: If len(PNs) > 350,000, then method 'query_all' will typically be faster.\n",
    "                                 Take this with a grain on salt, as the number 350,000 was found for just one particular\n",
    "                                 collection of PNs for specific date_0 and date_1, so others may differ.\n",
    "\n",
    "        'query_all':       Build the SQL query using only date_0 and date_1 (together with mjr_mnr_cause, etc.), \n",
    "                             i.e., data for ALL premise numbers are grabbed\n",
    "                           After the SQL query returns, slim the data down to include only the PNs of interest.\n",
    "                           Pro: Takes significantly less time to run when number of PNs is large\n",
    "                           Con: Consumes more memory, as we're grabbing everything then slimming down\n",
    "                           \n",
    "        'decide_at_runtime': Decide between methods 'query_pns_only' and 'query_all' at runtime.\n",
    "                             If len(PNs) > 350000, use 'query_all', else use 'query_pns_all'\n",
    "                             As mentioned above, the number 350,000 was found for just one particular\n",
    "                             collection of PNs for specific date_0 and date_1, so others may differ.\n",
    "    \n",
    "    NOTE: This uses DOVSOutages_SQL.build_sql_std_outage, so the standard DOVS cuts (listed below) are included:\n",
    "            DOV.MJR_CAUSE_CD <> 'NI'\n",
    "            DOV.DEVICE_CD <> 85\n",
    "            DOV2.INTRPTN_TYP_CD = 'S'\n",
    "            DOV2.CURR_REC_STAT_CD = 'A'\n",
    "    \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    conn_outages = Utilities.get_utldb01p_oracle_connection()\n",
    "    #-------------------------\n",
    "    if cols_of_interest is None:\n",
    "        cols_of_interest = [\n",
    "            'DT_ON_TS', \n",
    "            {'field_desc': 'DOV.DT_ON_TS - DOV.STEP_DRTN_NB/(60*24)',\n",
    "             'alias': 'DT_OFF_TS_FULL',\n",
    "             'table_alias_prefix': None}\n",
    "        ]\n",
    "    #-------------------------\n",
    "    # Make sure only unique values in PNs\n",
    "    PNs = list(set(PNs))\n",
    "    #-------------------------\n",
    "    assert(method in ['query_pns_only', 'query_all', 'decide_at_runtime'])\n",
    "    if method=='decide_at_runtime':\n",
    "        if len(PNs) > 350000:\n",
    "            method='query_all'\n",
    "        else:\n",
    "            method='query_pns_only'\n",
    "    #-------------------------\n",
    "    if method=='query_pns_only':\n",
    "        build_sql_std_outage_kwargs = dict(\n",
    "            mjr_mnr_cause=mjr_mnr_cause, \n",
    "            include_premise=True, \n",
    "            cols_of_interest=cols_of_interest, \n",
    "            select_cols_DOVS_PREMISE_DIM=['PREMISE_NB'], \n",
    "            alias_DOVS_PREMISE_DIM='PRIM', \n",
    "            date_range=[date_0, date_1], \n",
    "            premise_nbs=PNs, \n",
    "            include_DOVS_MASTER_GEO_DIM=False, \n",
    "            include_DOVS_OUTAGE_ATTRIBUTES_DIM=False, \n",
    "            include_DOVS_CLEARING_DEVICE_DIM=False, \n",
    "            include_DOVS_EQUIPMENT_TYPES_DIM=False, \n",
    "            include_DOVS_OUTAGE_CAUSE_TYPES_DIM=False, \n",
    "            field_to_split='premise_nbs', \n",
    "            batch_size=batch_size, \n",
    "            n_update=n_update, \n",
    "            verbose=verbose\n",
    "        )\n",
    "        if addtnl_build_sql_std_outage_kwargs is not None:\n",
    "            build_sql_std_outage_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "                to_supplmnt_dict=build_sql_std_outage_kwargs, \n",
    "                default_values_dict=addtnl_build_sql_std_outage_kwargs, \n",
    "                extend_any_lists=True, \n",
    "                inplace=True\n",
    "            )\n",
    "        return_df = GenAn.build_df_general(\n",
    "            conn_db=conn_outages, \n",
    "            build_sql_function=DOVSOutages_SQL.build_sql_std_outage, \n",
    "            build_sql_function_kwargs=build_sql_std_outage_kwargs\n",
    "        )\n",
    "    elif method=='query_all':\n",
    "        build_sql_std_outage_kwargs = dict(\n",
    "            mjr_mnr_cause=mjr_mnr_cause, \n",
    "            include_premise=True, \n",
    "            cols_of_interest=cols_of_interest, \n",
    "            select_cols_DOVS_PREMISE_DIM=['PREMISE_NB'], \n",
    "            alias_DOVS_PREMISE_DIM='PRIM', \n",
    "            date_range=[date_0, date_1], \n",
    "            include_DOVS_MASTER_GEO_DIM=False, \n",
    "            include_DOVS_OUTAGE_ATTRIBUTES_DIM=False, \n",
    "            include_DOVS_CLEARING_DEVICE_DIM=False, \n",
    "            include_DOVS_EQUIPMENT_TYPES_DIM=False, \n",
    "            include_DOVS_OUTAGE_CAUSE_TYPES_DIM=False\n",
    "        )\n",
    "        if addtnl_build_sql_std_outage_kwargs is not None:\n",
    "            build_sql_std_outage_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "                to_supplmnt_dict=build_sql_std_outage_kwargs, \n",
    "                default_values_dict=addtnl_build_sql_std_outage_kwargs, \n",
    "                extend_any_lists=True, \n",
    "                inplace=True\n",
    "            )\n",
    "        sql_outages_for_PNs = DOVSOutages_SQL.build_sql_std_outage(**build_sql_std_outage_kwargs)\n",
    "        sql_outages_for_PNs = sql_outages_for_PNs.get_sql_statement()\n",
    "        return_df = pd.read_sql_query(sql_outages_for_PNs, conn_outages)\n",
    "        return_df = return_df[return_df['PREMISE_NB'].isin(PNs)]\n",
    "    else:\n",
    "        assert(0)\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clean_subwindows_for_group(\n",
    "    final_df_i, \n",
    "    min_window_width, \n",
    "    include_is_first_after_outg_col=True, \n",
    "    t_clean_min_col='t_clean_min', \n",
    "    t_clean_max_col='t_clean_max', \n",
    "    return_t_search_min_col='t_search_min', \n",
    "    return_t_search_max_col='t_search_max'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Designed for use in find_clean_window_for_group when search_window_strategy=='all_subwindows'.\n",
    "    For the clean windows in final_df_i, this will find all acceptable subwindows.\n",
    "      So, e.g., if a clean window is of length 171 days and min_window_width=30 days, this will find 5 acceptable subwindows.\n",
    "      \n",
    "    It is expected that final_df_i contains data for a single group (typically, a single transformer).\n",
    "    The DF will have as many rows as clean periods found (see find_clean_window_for_group for more information).\n",
    "    It is expected that the buffer times have already been taken care of inf final_df_i when defining t_clean_min and max (as\n",
    "      is the case when this function is used within find_clean_window_for_group)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Generate random string to be safe when dong all the index re-naming below\n",
    "    idx_rndm = Utilities.generate_random_string()\n",
    "\n",
    "    # Grab final_df_i index name for later\n",
    "    final_df_i_idx_nm = final_df_i.index.name\n",
    "\n",
    "    # Iterate over each row in final_df_i, find acceptable subwindows, and add to return_dfs collection.\n",
    "    # As noted in the above documentation, final_df_i should contain one row for each clean period\n",
    "    return_dfs = []\n",
    "    for idx, row_i in final_df_i.iterrows():\n",
    "        t_clean_min_i=row_i[t_clean_min_col]\n",
    "        t_clean_max_i=row_i[t_clean_max_col]\n",
    "        n_subwindows_i = np.floor((t_clean_max_i-t_clean_min_i)/min_window_width).astype(int)\n",
    "        #-------------------------\n",
    "        windows_i = []\n",
    "        for i_window in range(n_subwindows_i):\n",
    "            window_i_min = t_clean_min_i + i_window*min_window_width\n",
    "            window_i_max = t_clean_min_i + (i_window+1)*min_window_width\n",
    "            windows_i.append([window_i_min, window_i_max])\n",
    "        #-------------------------\n",
    "        # Sanity check\n",
    "        assert(windows_i[-1][1] <= t_clean_max_i)\n",
    "        #-------------------------\n",
    "        # Create len(windows_i) copies of row_i and merge (concat with axis=1) with windows_i\n",
    "        #-----\n",
    "        # Need to call reset_index on rows_i for merge to work, but want to use original index later,\n",
    "        #   so rename original to more easily grab later\n",
    "        # NOTE: In newer versions of pandas (>=1.5) one can use the names argument of .reset_index,\n",
    "        #       allowing the merge to happen in a single line\n",
    "        rows_i = pd.concat([pd.DataFrame(row_i).T]*len(windows_i))\n",
    "        assert(rows_i.index.nlevels==1)\n",
    "        idx_nm_og = 'index_og'+idx_rndm\n",
    "        rows_i.index.name = idx_nm_og\n",
    "        rows_i = rows_i.reset_index()\n",
    "        #-----\n",
    "        return_df_i = pd.concat([\n",
    "            rows_i, \n",
    "            pd.DataFrame(windows_i, columns=[return_t_search_min_col, return_t_search_max_col])\n",
    "        ], axis=1)\n",
    "        #-----\n",
    "        if include_is_first_after_outg_col:\n",
    "            return_df_i['is_first_after_outg']=0\n",
    "            return_df_i.loc[0, 'is_first_after_outg']=1\n",
    "        #-----\n",
    "        return_dfs.append(return_df_i)\n",
    "    #-------------------------\n",
    "    # Combine all return_dfs into return_df\n",
    "    return_df = pd.concat(return_dfs)\n",
    "    #-------------------------\n",
    "    # Join together the original index and the new index (new index should be 0-len(subwindows_i)-1)\n",
    "    # This will allow one to track where subwindows came from, in case one needs to debug or whatever\n",
    "    # As noted above, in newer versions of pandas (>=1.5) one can use the names argument of .reset_index\n",
    "    assert(return_df.index.nlevels==1)\n",
    "    idx_nm_new = 'index_new'+idx_rndm\n",
    "    return_df.index.name=idx_nm_new\n",
    "    return_df=return_df.reset_index()\n",
    "    #-----\n",
    "    idx_nm_final = 'index_final'+idx_rndm\n",
    "    return_df[idx_nm_final] = return_df[[idx_nm_og, idx_nm_new]].astype(str).agg('_'.join, axis=1)\n",
    "    # Set index to combination of og and new, rename the index to match that of final_df_i, \n",
    "    #   and drop idx_nm_og and idx_nm_new columns as they are no longer needed\n",
    "    return_df = return_df.set_index(idx_nm_final)\n",
    "    return_df.index.name = final_df_i_idx_nm\n",
    "    return_df = return_df.drop(columns=[idx_nm_og, idx_nm_new])\n",
    "    #-------------------------\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def find_clean_window_for_group(\n",
    "    df_i, \n",
    "    min_window_width, \n",
    "    buffer_time_left, \n",
    "    buffer_time_rght, \n",
    "    set_search_window=True, \n",
    "    pd_selection_stategy = 'max', \n",
    "    search_window_strategy = 'centered', \n",
    "    needs_sorted=True, \n",
    "    outg_beg_col='DT_OFF_TS_FULL', \n",
    "    outg_end_col='DT_ON_TS', \n",
    "    record_clean_window_usable=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    INTENDED FOR USE IN .groupby().apply(lambda x) function.\n",
    "    This can still be used on its own, but the user should be aware of the functionality and intent\n",
    "    \n",
    "    FASTEST RUN TIME SUGGESTIONS:\n",
    "        - Sort the DF prior, and set needs_sorted=False\n",
    "        - Use search_window_strategy = 'centered'\n",
    "        \n",
    "    NOTE: If search_window_strategy is a timedelta object, the search period will begin search_window_strategy\n",
    "            after the buffer_time_left (not after the end of the previous outage)\n",
    "    \n",
    "    \n",
    "    needs_sorted:\n",
    "        IF YOU ARE NOT SURE, KEEP needs_sorted=True, as the proper sorting of the DF is vital for the functionality.\n",
    "        When running this within a .groupby().apply(lambda x) function, a little bit of time can be saved by \n",
    "          sorting the overall DataFrame first before the groupby call.\n",
    "        Regardless of needs_sorted, sorting first will save time.\n",
    "        If sorting already done, no need to re-sort here, so a little more time can be saved by setting needs_sorted=False\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(pd_selection_stategy in ['max', 'min', 'rand', 'all'])\n",
    "    assert(search_window_strategy in ['centered', 'rand', 'all_subwindows'] or isinstance(search_window_strategy, datetime.timedelta))\n",
    "    #-------------------------\n",
    "    # For this to function properly, df_i must be sorted according to time\n",
    "    if needs_sorted:\n",
    "        df_i = df_i.sort_values(by=[outg_beg_col, outg_end_col], ascending=True).copy()\n",
    "\n",
    "    #-------------------------\n",
    "    # Find the clean periods of time following each outage by subtracting the beginning time\n",
    "    # of the next outage from the end of the current outage.\n",
    "    clean_windows_after = df_i[outg_beg_col].shift(-1)-df_i[outg_end_col]\n",
    "\n",
    "    # To find the amount of clean time after the last outage, use date_1 as an endpoint\n",
    "    #   i.e., subtract the end time of the current outage from the end of the overall interval, date_1\n",
    "    clean_windows_after.iloc[-1] = pd.to_datetime(date_1) - df_i.iloc[-1][outg_end_col]\n",
    "\n",
    "    #-------------------------\n",
    "    # Find the acceptable periods for which the clean time is greater than the desired length\n",
    "    # NOTE: The buffer_time_left/_rght arguments allow one to ensure the period of time is not \n",
    "    #       immediately proceeding or preceding an outage event\n",
    "    # NOTE: good_clean_windows_after must have a name in order to merge with df_i\n",
    "    good_clean_windows_after = clean_windows_after[clean_windows_after > min_window_width+buffer_time_left+buffer_time_rght]\n",
    "    good_clean_windows_after.name='clean_window_full'\n",
    "    if len(good_clean_windows_after)==0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    #-------------------------\n",
    "    # Construct good_df_i using the entries from good_clean_windows_after\n",
    "    # Merge this with good_clean_windows_after to include clean_window information\n",
    "    good_df_i = df_i.loc[good_clean_windows_after.index]\n",
    "    good_df_i = pd.merge(good_df_i, good_clean_windows_after, left_index=True, right_index=True, how='inner')\n",
    "    if record_clean_window_usable:\n",
    "        good_df_i['clean_window_usable'] = good_df_i['clean_window_full'] - (buffer_time_left+buffer_time_rght)\n",
    "    #-------------------------\n",
    "    # Select subset of good_df_i according to pd_selection_stategy\n",
    "    if pd_selection_stategy=='max':\n",
    "        final_df_i = good_df_i.iloc[[good_df_i['clean_window_full'].argmax()]].copy()\n",
    "    elif pd_selection_stategy=='min':\n",
    "        final_df_i = good_df_i.iloc[[good_df_i['clean_window_full'].argmin()]].copy()\n",
    "    elif pd_selection_stategy=='rand':\n",
    "        final_df_i = good_df_i.sample().copy()\n",
    "    elif pd_selection_stategy=='all':\n",
    "        final_df_i = good_df_i.copy()\n",
    "    else:\n",
    "        assert(0)\n",
    "\n",
    "    #-------------------------\n",
    "    # Create columns to hold the min and max clean times\n",
    "    #   The clean time begins (min) buffer_time_left after the outage ends\n",
    "    #   The clean time ends (max) buffer_time_rght before the next outage\n",
    "    #     (which is equal to the time the current outage ends, plus the clean window, \n",
    "    #      minus the buffer_time_rght)\n",
    "    final_df_i['t_clean_min'] = final_df_i[outg_end_col] + buffer_time_left\n",
    "    final_df_i['t_clean_max'] = final_df_i[outg_end_col] + final_df_i['clean_window_full'] - buffer_time_rght\n",
    "\n",
    "    #-------------------------\n",
    "    if set_search_window:\n",
    "        if search_window_strategy=='centered':\n",
    "            # Mid point of clean time interval = final_df_i[['t_clean_min', 't_clean_max']].mean(numeric_only=False, axis=1)\n",
    "            # ==> Left  point = (final_df_i[['t_clean_min', 't_clean_max']].mean(numeric_only=False, axis=1)) - min_window_width/2\n",
    "            # ==> Right point = (final_df_i[['t_clean_min', 't_clean_max']].mean(numeric_only=False, axis=1)) + min_window_width/2\n",
    "            final_df_i['t_search_min'] = (final_df_i[['t_clean_min', 't_clean_max']].mean(numeric_only=False, axis=1)) - min_window_width/2\n",
    "            final_df_i['t_search_max'] = (final_df_i[['t_clean_min', 't_clean_max']].mean(numeric_only=False, axis=1)) + min_window_width/2\n",
    "        elif search_window_strategy=='rand':\n",
    "            final_df_i['t_search_min'] = pd.NaT\n",
    "            final_df_i['t_search_max'] = pd.NaT\n",
    "            #-----\n",
    "            for idx, row_i in final_df_i.iterrows():\n",
    "                rnd_intrvl_i = Utilities_dt.get_random_datetime_interval_between(\n",
    "                    date_0=row_i['t_clean_min'], \n",
    "                    date_1=row_i['t_clean_max'], \n",
    "                    window_width=min_window_width, \n",
    "                    rand_seed=None        \n",
    "                )\n",
    "                final_df_i.loc[idx, ['t_search_min', 't_search_max']] = rnd_intrvl_i\n",
    "        elif search_window_strategy=='all_subwindows':\n",
    "            final_df_i = find_clean_subwindows_for_group(\n",
    "                final_df_i=final_df_i, \n",
    "                min_window_width=min_window_width, \n",
    "                include_is_first_after_outg_col=True, \n",
    "                t_clean_min_col='t_clean_min', \n",
    "                t_clean_max_col='t_clean_max', \n",
    "                return_t_search_min_col='t_search_min', \n",
    "                return_t_search_max_col='t_search_max'\n",
    "            )\n",
    "        elif isinstance(search_window_strategy, datetime.timedelta):\n",
    "            final_df_i['t_search_min'] = final_df_i['t_clean_min'] + search_window_strategy\n",
    "            final_df_i['t_search_max'] = final_df_i['t_clean_min'] + search_window_strategy + min_window_width\n",
    "        else:\n",
    "            assert(0)\n",
    "    #-------------------------\n",
    "    # Don't need outg_beg_col or outg_end_col anymore.\n",
    "    # These columns contain information about the outage(s) after which the clean period(s) was(were) selected.\n",
    "    # If pd_selection_stategy=='all', I suppose this information would make sense.  But, in any other case, \n",
    "    #   the information isn't really useful, as only one clean period is returned following \n",
    "    #   one of the (possibly randomly) selected outages\n",
    "    final_df_i = final_df_i.drop(columns=[outg_beg_col, outg_end_col])\n",
    "    #-------------------------\n",
    "    return final_df_i\n",
    "\n",
    "def find_clean_windows(\n",
    "    df, \n",
    "    groupby_col, \n",
    "    min_window_width, \n",
    "    buffer_time_left, \n",
    "    buffer_time_rght,  \n",
    "    set_search_window=True, \n",
    "    pd_selection_stategy = 'max', \n",
    "    search_window_strategy = 'centered', \n",
    "    outg_beg_col='DT_OFF_TS_FULL', \n",
    "    outg_end_col='DT_ON_TS', \n",
    "    record_clean_window_usable=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    FASTEST RUN TIME SUGGESTIONS:\n",
    "        - Use search_window_strategy = 'centered' \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Only really need the three columns [groupby_col, outg_beg_col, outg_end_col]\n",
    "    df = df[[groupby_col, outg_beg_col, outg_end_col]].copy()\n",
    "    #-------------------------\n",
    "    # Drop any duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    #-------------------------\n",
    "    # Make sure outg_beg_col/outg_end_col are datetime\n",
    "    if not is_datetime64_dtype(df[outg_beg_col]):\n",
    "        df = Utilities_df.convert_col_type(df=df, column=outg_beg_col, to_type=datetime.datetime)\n",
    "    if not is_datetime64_dtype(df[outg_end_col]):\n",
    "        df = Utilities_df.convert_col_type(df=df, column=outg_end_col, to_type=datetime.datetime)\n",
    "    #-------------------------\n",
    "    # To speed things up, first sort df\n",
    "    df = df.sort_values(by=[groupby_col, outg_beg_col, outg_end_col]).copy()\n",
    "    #-----\n",
    "    return_df = df.groupby(groupby_col, as_index=False, group_keys=False).apply(\n",
    "        lambda x: find_clean_window_for_group(\n",
    "            df_i=x, \n",
    "            min_window_width=min_window_width, \n",
    "            buffer_time_left=buffer_time_left, \n",
    "            buffer_time_rght=buffer_time_rght, \n",
    "            set_search_window=set_search_window, \n",
    "            pd_selection_stategy=pd_selection_stategy, \n",
    "            search_window_strategy=search_window_strategy, \n",
    "            outg_beg_col=outg_beg_col, \n",
    "            outg_end_col=outg_end_col, \n",
    "            record_clean_window_usable=record_clean_window_usable, \n",
    "            needs_sorted=False\n",
    "        )\n",
    "    )\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d57880",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c6673",
   "metadata": {},
   "source": [
    "# Will have two methods for building.  \n",
    "# One using a supplied df_outage with (or, I suppose, without) accompanying meter premise\n",
    "# One building from ground up, given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# VARIABLES TO BE SET BY USER!\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Unless absolutely certain df_mp in csv has all necessary data, use read_dfs_from_file=False\n",
    "save_dfs_to_file   = False\n",
    "read_dfs_from_file = True\n",
    "save_end_events    = True\n",
    "\n",
    "#-------------------------\n",
    "# run_date is used to collect all results from a given acquisiton run together.\n",
    "# As such, run_date should be set to the first date of the acquisition run, and\n",
    "#   SHOULD NOT be changed for each individual date in a run (which typically lasts\n",
    "#   over the course of days/weeks)\n",
    "# run_date = '20221014'\n",
    "# run_date = '20221216'\n",
    "# run_date = '20230512'\n",
    "run_date = '20231003' # Date of data acquisition\n",
    "\n",
    "#-------------------------\n",
    "# date_0 = '2022-01-01'\n",
    "# date_1 = '2022-12-31'\n",
    "\n",
    "date_0 = '2023-04-01' # Lower limit for end events\n",
    "date_1 = '2023-09-30' # Upper limit for end events\n",
    "\n",
    "#-------------------------\n",
    "min_window_width = pd.Timedelta('31 days')\n",
    "buffer_time_left = pd.Timedelta('1 days')\n",
    "buffer_time_rght = pd.Timedelta('31 days')\n",
    "\n",
    "#-------------------------\n",
    "run_using_slim = False\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# If df_mp is read from csv, it will typically contain an outg_rec_nb column\n",
    "#   and entries which are duplicates except for outg_rec_nb\n",
    "# For this process to work correctly, these duplicates must be removed.\n",
    "df_mp_outg_rec_nb_col = 'OUTG_REC_NB'\n",
    "\n",
    "#-------------------------\n",
    "groupby_col = 'trsf_pole_nb'\n",
    "# groupby_col = 'PREMISE_NB'\n",
    "assert(groupby_col in ['trsf_pole_nb', 'PREMISE_NB'])\n",
    "\n",
    "#-------------------------\n",
    "pd_selection_stategy = 'all'\n",
    "# search_window_strategy = 'centered'\n",
    "# search_window_strategy = pd.Timedelta('1 day')\n",
    "search_window_strategy = 'all_subwindows'\n",
    "\n",
    "#--------------------------------------------------\n",
    "# NOTE: below, states and opcos should be consistent!\n",
    "#       i.e., e.g., if states='OH', then opcos should be 'oh' (or None, I suppose)\n",
    "#-------------------------\n",
    "# states used to \n",
    "#   (1) find transformers which suffered at least one outage from DOVS\n",
    "#   (2) find all transformers from MeterPremise\n",
    "# states can be:\n",
    "#   - a single string, e.g. 'OH'\n",
    "#   - a list of strings, e.g., ['OH', 'WV']\n",
    "#   - None\n",
    "# NOTE: states tend to be upper-case!\n",
    "states=['OH']\n",
    "\n",
    "#-------------------------\n",
    "# opcos used with AMIEndEvents to\n",
    "#  (1) find the premise numbers which recorded an event between date_0 and date_1.\n",
    "#  (2) selection/acquisiton of end_device_events\n",
    "# opcos can be:\n",
    "#   - a single string, e.g. 'oh'\n",
    "#   - a list of strings, e.g., ['oh', 'tx']\n",
    "#   - None\n",
    "# NOTE: opcos tend to be lower-case!\n",
    "# NOTE: Acceptable opcos appear to be: ['ap', 'im', 'oh', 'pso', 'swp', 'tx']\n",
    "opcos='oh'\n",
    "\n",
    "#--------------------------------------------------\n",
    "trsf_pole_nbs_to_ignore = [' ', 'TRANSMISSION', 'PRIMARY', 'NETWORK']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b18eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# DFs will be saved in save_dir_base\n",
    "# Collection of end events files will be saved in os.path.join(save_dir_base, 'EndEvents')\n",
    "save_dir_base = os.path.join(\n",
    "    Utilities.get_local_data_dir(), \n",
    "    r'dovs_and_end_events_data', \n",
    "    run_date, \n",
    "    f\"{date_0.replace('-','')}_{date_1.replace('-','')}\", \n",
    "    'NoOutgs'\n",
    ")\n",
    "#-------------------------\n",
    "end_events_save_args = dict(\n",
    "    save_to_file=save_end_events, \n",
    "    save_dir = os.path.join(save_dir_base, 'EndEvents_NEW2'), \n",
    "    save_name=r'end_events.csv', \n",
    "    index=True\n",
    ")\n",
    "#-------------------------\n",
    "print(f\"save_dir_base = {save_dir_base}\")\n",
    "print('end_events_save_args')\n",
    "for k,v in end_events_save_args.items():\n",
    "    print(f\"\\t{k} : {v}\")\n",
    "#-------------------------\n",
    "if save_dfs_to_file or save_end_events:\n",
    "    if not os.path.exists(save_dir_base):\n",
    "        os.makedirs(save_dir_base)\n",
    "    #-----\n",
    "    if save_end_events and not os.path.exists(end_events_save_args['save_dir']):\n",
    "        os.makedirs(end_events_save_args['save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "assert(save_dfs_to_file+read_dfs_from_file <=1) # Should never both read and write!\n",
    "assert(pd.to_datetime(date_1)-pd.to_datetime(date_0) > min_window_width+buffer_time_left+buffer_time_rght)\n",
    "#--------------------------------------------------\n",
    "if not read_dfs_from_file:\n",
    "    conn_outages = Utilities.get_utldb01p_oracle_connection()\n",
    "    conn_aws = Utilities.get_athena_prod_aws_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e61c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "697ad41a",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------\n",
    "# OUTAGES\n",
    "# ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fecfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_outage_full = DOVSOutages_SQL.build_sql_std_outage(\n",
    "    mjr_mnr_cause=None, \n",
    "    include_premise=True, \n",
    "    date_range=[date_0, date_1], \n",
    "    states=states\n",
    ").get_sql_statement()\n",
    "print(sql_outage_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_dfs_from_file:\n",
    "    df_outage_OG = pd.read_csv(os.path.join(save_dir_base, 'df_outage_OG.csv'), dtype=str)\n",
    "    csv_cols_and_types_to_convert_dict = {'CI_NB':np.int32, 'CMI_NB':np.float64, 'OUTG_REC_NB':[np.float64, np.int32]}\n",
    "    df_outage_OG = Utilities_df.convert_col_types(df_outage_OG, csv_cols_and_types_to_convert_dict)\n",
    "else:\n",
    "    df_outage_OG = pd.read_sql_query(sql_outage_full, conn_outages, dtype={'CI_NB':np.int32, \n",
    "                                                                           'CMI_NB':np.float64, \n",
    "                                                                           'OUTG_REC_NB':np.int32})\n",
    "#-------------------------\n",
    "if save_dfs_to_file and not read_dfs_from_file:\n",
    "    df_outage_OG.to_csv(os.path.join(save_dir_base, 'df_outage_OG.csv'), index=False)\n",
    "#-------------------------\n",
    "print(f'df_outage_OG.shape = {df_outage_OG.shape}')\n",
    "# df_outage = df_outage_OG.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee413313",
   "metadata": {},
   "source": [
    "# DELETE CELL BELOW!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transmission_pns = [\n",
    "#     '071511200', '073518871', '101470381', '075619000', '075972000',\n",
    "#     '073209400', '107147577', '079177100', '073936100', '079587200',\n",
    "#     '106865860', '074942500', '077269200'\n",
    "# ]\n",
    "# all_PNs = df_outage_OG['PREMISE_NB'].dropna().unique().tolist()\n",
    "\n",
    "# all_PNs_0 = (\n",
    "#     all_PNs[:1000]+\n",
    "#     transmission_pns+\n",
    "#     ['103280203', '105780203', '106380203']\n",
    "# )\n",
    "\n",
    "# df_outage_OG = df_outage_OG[df_outage_OG['PREMISE_NB'].isin(all_PNs_0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53da5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage = df_outage_OG.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa6697",
   "metadata": {},
   "source": [
    "# Find all outages between date_0 and date_1 for PNs in df_outage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0dfcd2",
   "metadata": {},
   "source": [
    "# Need to find all PNs, which consist not only of those directly from df_outage, but also those not in df_outage who were connected to transformers having entries in df_outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_aws = Utilities.get_athena_prod_aws_connection()\n",
    "PNs = df_outage['PREMISE_NB'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a82bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "# mp_PNs = MeterPremise(\n",
    "#     df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "#     contstruct_df_args=None, \n",
    "#     init_df_in_constructor=True, \n",
    "#     build_sql_function=MeterPremise.build_sql_meter_premise, \n",
    "#     build_sql_function_kwargs=dict(\n",
    "#         cols_of_interest=['DISTINCT trsf_pole_nb'], \n",
    "#         premise_nbs=PNs, \n",
    "#         from_table_alias=None\n",
    "#     )\n",
    "# )\n",
    "# mp_df_PNs = mp_PNs.df.copy()\n",
    "\n",
    "mp_df_PNs = MeterPremise.get_distinct_trsf_pole_nbs_for_PNs(\n",
    "    PNs=PNs, \n",
    "    batch_size=10000, \n",
    "    conn_aws=conn_aws\n",
    ")\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "mp_xfmrs = GenAn(\n",
    "    df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "    contstruct_df_args=dict(conn_db=Utilities.get_athena_prod_aws_connection()), \n",
    "    init_df_in_constructor=True, \n",
    "    build_sql_function=MeterPremise.build_sql_meter_premise, \n",
    "    build_sql_function_kwargs=dict(\n",
    "        cols_of_interest=['trsf_pole_nb', 'prem_nb', 'mfr_devc_ser_nbr', 'inst_ts', 'rmvl_ts'], \n",
    "        trsf_pole_nb=[x for x in mp_df_PNs['trsf_pole_nb'].unique() if x!='TRANSMISSION'], \n",
    "        field_to_split='trsf_pole_nb'\n",
    "    )\n",
    ")\n",
    "print(time.time()-start)\n",
    "mp_df_xfmrs = mp_xfmrs.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07df3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_dfs_to_file:\n",
    "    mp_df_PNs.to_pickle(os.path.join(save_dir_base, 'mp_df_PNs_no_outg.pkl'))\n",
    "    mp_df_xfmrs.to_pickle(os.path.join(save_dir_base, 'mp_df_xfmrs_no_outg.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_df_PNs = pd.read_pickle(os.path.join(save_dir_base, 'mp_df_PNs_no_outg.pkl'))\n",
    "# mp_df_xfmrs = pd.read_pickle(os.path.join(save_dir_base, 'mp_df_xfmrs_no_outg.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc6bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde64de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, for all premise numbers in df_outage, we want to find all outages suffered between date_0 and date_1\n",
    "# There are two methods for achieving this, using either method='query_pns_only' or method='query_all' in find_all_outages_for_pns\n",
    "# NOTE: It is possible that all needed info already contained in df_outage! \n",
    "#       But that is reliant upon the user having everything set up perfectly....so safest just to build...\n",
    "verbose=True\n",
    "n_update=10\n",
    "batch_size=1000\n",
    "\n",
    "# PNs = df_outage['PREMISE_NB'].unique().tolist()\n",
    "PNs = list(set(df_outage['PREMISE_NB'].unique().tolist() + mp_df_xfmrs['prem_nb'].unique().tolist()))\n",
    "\n",
    "mjr_mnr_cause=None\n",
    "method='decide_at_runtime'\n",
    "# addtnl_build_sql_std_outage_kwargs=None\n",
    "addtnl_build_sql_std_outage_kwargs=dict(\n",
    "    states=states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "all_outages_df = find_all_outages_for_pns(\n",
    "    PNs=PNs, \n",
    "    date_0=date_0, \n",
    "    date_1=date_1, \n",
    "    cols_of_interest=None, \n",
    "    mjr_mnr_cause=mjr_mnr_cause, \n",
    "    method=method, \n",
    "    addtnl_build_sql_std_outage_kwargs=addtnl_build_sql_std_outage_kwargs, \n",
    "    verbose=verbose, \n",
    "    n_update=n_update, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "print(f\"Time to run find_all_outages_for_pns: {time.time()-start}\")\n",
    "print(f\"# Unique PNs in df_outage:      {df_outage['PREMISE_NB'].nunique()}\")\n",
    "print(f\"# Unique PNs in all_outages_df: {all_outages_df['PREMISE_NB'].nunique()}\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUCK2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------*************************-------------------------*************************\n",
    "# If grouping by transformer, the trsf_pole_nb from MeterPremise must be merged with all_outages_df\n",
    "# Also, the active meters at the time of outage must be selected by comparing inst_ts,rmvl_ts to \n",
    "#   DT_OFF_TS_FULL,DT_ON_TS.\n",
    "# This is documented in the code below\n",
    "#-------------------------\n",
    "if groupby_col=='trsf_pole_nb':\n",
    "    if read_dfs_from_file:\n",
    "#         df_mp = pd.read_csv(os.path.join(save_dir_base, 'df_mp_dupls_dropped.csv'), dtype=str)\n",
    "        df_mp = pd.read_pickle(os.path.join(save_dir_base, 'df_mp_no_outg.pkl'))\n",
    "        #-----\n",
    "        # Check for df_mp_outg_rec_nb_col in df_mp, regardless of case\n",
    "        #   If contained, must be dropped and duplicates removed\n",
    "        #   *See comment above df_mp_outg_rec_nb_col initial assignment\n",
    "        #TODO: Use Utilities_df.drop_col_case_insensitive instead of if block below!\n",
    "        if df_mp_outg_rec_nb_col.lower() in [x.lower() for x in df_mp.columns]:\n",
    "            tmp_idx = [x.lower() for x in df_mp.columns].index(df_mp_outg_rec_nb_col.lower())\n",
    "            df_mp_outg_rec_nb_col = df_mp.columns.tolist()[tmp_idx]\n",
    "            df_mp = df_mp.drop(columns=[df_mp_outg_rec_nb_col]).drop_duplicates()\n",
    "    else:\n",
    "        df_mp = MeterPremise.build_mp_df_curr_hist_for_PNs(\n",
    "            PNs=PNs, \n",
    "            mp_df_curr=None,\n",
    "            mp_df_hist=None, \n",
    "            join_curr_hist=True, \n",
    "            addtnl_mp_df_curr_cols=None, \n",
    "            addtnl_mp_df_hist_cols=None, \n",
    "            assert_all_PNs_found=False, \n",
    "            assume_one_xfmr_per_PN=True, \n",
    "            drop_approx_duplicates=True\n",
    "        )\n",
    "        if save_dfs_to_file:\n",
    "            df_mp.to_pickle(os.path.join(save_dir_base, 'df_mp_no_outg.pkl'))\n",
    "    #--------------------------------------------------\n",
    "    # Some premise numbers from DOVS are missing from df_mp.\n",
    "    # This is not an issue with the code, I checked. \n",
    "    # This means DOVS says a premise was affected by an outage, but at the time of the outage there were \n",
    "    #   no active meters on the premise.\n",
    "    # My question is: How did DOVS therefore know the premise was affected?\n",
    "    # How are premise numbers in DOVS determined?\n",
    "    #-------------------------\n",
    "    # I want to at least get a count to quantify the situation described above, i.e., how many premises from DOVS\n",
    "    #   did not have any active meters at the time of the outage.\n",
    "    # Note, for this, I cannot simply do, e.g., \n",
    "    #     set(all_outages_df['PREMISE_NB'].unique()).difference(set(df_mp['prem_nb'].unique()))\n",
    "    #   as this might reflect a smaller number of missing PNs than in reality, as df_mp has not yet been chopped down\n",
    "    #   to only those present at time of outage (which is done below comparing 'inst_ts' to 'DT_OFF_TS_FULL' and \n",
    "    #   'rmvl_ts' to 'DT_ON_TS')\n",
    "    #-------------------------\n",
    "    # The meters present at the time of the outages can only be select after all_outages_df and df_mp are merged.\n",
    "    #-------------------------\n",
    "    # Note: A left merge is used below instead of an inner to protect against the case of a df_mp (being read in from a CSV \n",
    "    #       file) which contains extra entries than in all_outages_df\n",
    "    #-------------------------\n",
    "    all_outages_df = DOVSOutages.merge_df_outage_with_mp(\n",
    "        df_outage=all_outages_df, \n",
    "        df_mp=df_mp,  \n",
    "        merge_on_outg=['PREMISE_NB'], \n",
    "        merge_on_mp=['prem_nb'], \n",
    "        cols_to_include_mp=None, \n",
    "        drop_cols = None, \n",
    "        rename_cols=None, \n",
    "        how='left', \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    #-------------------------\n",
    "    # Only include serial numbers which were present at the time of the outage.\n",
    "    #-----\n",
    "    # NOTE the use of .fillna(pd.Timestamp.min) (YES, MIN) below, as this is different from MeterPremise.get_active_SNs_for_PNs_at_datetime_interval\n",
    "    #   and MeterPremise.merge_df_with_active_mp.\n",
    "    # This is needed so the premises missing from df_mp are not removed at this stage (yes, they will ultimately be removed, \n",
    "    #   but I don't want them removed yet because I want to track them!)\n",
    "    # Without this, any entry with 'inst_ts'=NaT would be removed, as a comparison of NaT to anything returns False\n",
    "    #-----\n",
    "    all_outages_df = Utilities_df.convert_col_type_w_pd_to_datetime(all_outages_df, 'inst_ts')\n",
    "    all_outages_df = Utilities_df.convert_col_type_w_pd_to_datetime(all_outages_df, 'rmvl_ts')\n",
    "    #-----\n",
    "    all_outages_df=all_outages_df[(all_outages_df['inst_ts'].fillna(pd.Timestamp.min)<=all_outages_df['DT_OFF_TS_FULL']) & \n",
    "                                  (all_outages_df['rmvl_ts'].fillna(pd.Timestamp.max)>all_outages_df['DT_ON_TS'])]\n",
    "\n",
    "    #-------------------------\n",
    "    # Find the entries with missing df_mp data, i.e., find the entries where DOVS says a premise was affected by an outage, \n",
    "    #   but at the time of the outage there were no active meters on the premise.\n",
    "    all_outages_df_for_non_active_pns = all_outages_df[all_outages_df[df_mp.columns].isna().all(axis=1)].copy()\n",
    "    non_active_pns_from_DOVS = all_outages_df_for_non_active_pns['PREMISE_NB'].unique().tolist()\n",
    "\n",
    "    # And remove the entries with missing df_mp data from all_outages_df\n",
    "    all_outages_df = all_outages_df.dropna(subset=df_mp.columns, how='all')\n",
    "    #-------------------------\n",
    "    print(\"\"\"\n",
    "    Some premise numbers from DOVS are missing from df_mp\n",
    "    This is not an issue with the code, I checked.  \n",
    "    This means DOVS says a premise was affected by an outage, but at the time of the outage there were no active meters on the premise.\n",
    "    My question is: How did DOVS therefore know the premise was affected?\n",
    "    How are premise numbers in DOVS determined?\n",
    "    \"\"\")\n",
    "    print(f\"Number of premise numbers from DOVS without an active meter at outage time: {len(non_active_pns_from_DOVS)}\") \n",
    "    #-------------------------\n",
    "    # At this point, any trsf_pole_nbs to be excluded can be removed\n",
    "    # Remove 'TRANSMISSION', 'PRIMARY', and 'NETWORK' transformers\n",
    "    all_outages_df=all_outages_df[~all_outages_df['trsf_pole_nb'].isin(['TRANSMISSION', 'NETWORK', 'PRIMARY'])] \n",
    "    #--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e3108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------*************************-------------------------*************************\n",
    "# Find the clean windows for each group and build df_no_outage\n",
    "#-------------------------\n",
    "start=time.time()\n",
    "clean_windows_by_grp = find_clean_windows(\n",
    "    df=all_outages_df, \n",
    "    groupby_col=groupby_col, \n",
    "    min_window_width=min_window_width, \n",
    "    buffer_time_left=buffer_time_left, \n",
    "    buffer_time_rght=buffer_time_rght, \n",
    "    set_search_window=True, \n",
    "    pd_selection_stategy = pd_selection_stategy, \n",
    "    search_window_strategy = search_window_strategy, \n",
    "    outg_beg_col='DT_OFF_TS_FULL', \n",
    "    outg_end_col='DT_ON_TS'\n",
    ")\n",
    "print(time.time()-start)\n",
    "#-------------------------\n",
    "# All groups (trsf_pole_nbs, PREMISE_NBs, etc.) in clean_windows_by_grp should also be found in all_outages_df, \n",
    "#   but the reverse is not true\n",
    "assert(len(set(clean_windows_by_grp[groupby_col].unique()).difference(set(all_outages_df[groupby_col].unique())))==0)\n",
    "\n",
    "# Groups where no clean time period was found\n",
    "grps_with_no_clean = all_outages_df[~all_outages_df[groupby_col].isin(clean_windows_by_grp[groupby_col].unique())]\n",
    "\n",
    "print(f\"groupby_col = {groupby_col}\")\n",
    "print(f\"a. # Groups:                      {all_outages_df[groupby_col].nunique()}\")\n",
    "print(f\"b. # Groups with clean period:    {clean_windows_by_grp[groupby_col].nunique()}\")\n",
    "print(f\"c. # Groups without clean period: {len(set(all_outages_df[groupby_col].unique()).difference(set(clean_windows_by_grp[groupby_col].unique())))}\")\n",
    "print(\"NOTE: There may be a difference of 1 between a and b+c due to fact that nunique() does not including NaNs but unique does\")\n",
    "#--------------------------------------------------\n",
    "if save_dfs_to_file:\n",
    "    all_outages_df.to_pickle(os.path.join(save_dir_base, 'all_outages_df.pkl'))\n",
    "    clean_windows_by_grp.to_pickle(os.path.join(save_dir_base, 'clean_windows_by_grp.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0782350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "264c6c76",
   "metadata": {},
   "source": [
    "# MERGING clean_windows_by_grp with df_mp is the winner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43723a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_windows_by_grp_mrg_mp = pd.merge(\n",
    "    clean_windows_by_grp, \n",
    "    df_mp, \n",
    "    left_on='trsf_pole_nb', \n",
    "    right_on='trsf_pole_nb', \n",
    "    how='left'\n",
    ")\n",
    "clean_windows_by_grp_mrg_mp = Utilities_df.convert_col_type_w_pd_to_datetime(df=clean_windows_by_grp_mrg_mp, column='inst_ts', inplace=True)\n",
    "clean_windows_by_grp_mrg_mp = Utilities_df.convert_col_type_w_pd_to_datetime(df=clean_windows_by_grp_mrg_mp, column='rmvl_ts', inplace=True)\n",
    "\n",
    "\n",
    "clean_windows_by_grp_mrg_mp = clean_windows_by_grp_mrg_mp[\n",
    "    (clean_windows_by_grp_mrg_mp['inst_ts'].fillna(pd.Timestamp.min)<=clean_windows_by_grp_mrg_mp['t_search_min']) & \n",
    "    (clean_windows_by_grp_mrg_mp['rmvl_ts'].fillna(pd.Timestamp.max)>clean_windows_by_grp_mrg_mp['t_search_max'])\n",
    "]\n",
    "if save_dfs_to_file:\n",
    "    clean_windows_by_grp_mrg_mp.to_pickle(os.path.join(save_dir_base, 'clean_windows_by_grp_mrg_mp.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outage = clean_windows_by_grp_mrg_mp.copy()\n",
    "df_no_outage = df_no_outage.sort_values(by=[groupby_col, 'prem_nb', 't_search_min'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15225a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed144ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a98a32d",
   "metadata": {},
   "source": [
    "# Add no_outg_rec_nb column to allow easier grouping when building rcpo_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938268d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_pfx = Utilities.generate_random_string(str_len=5, letters=string.ascii_letters + string.digits)\n",
    "df_no_outage['no_outg_rec_nb'] = df_no_outage.groupby(['trsf_pole_nb', 't_search_min', 't_search_max']).ngroup()\n",
    "df_no_outage['no_outg_rec_nb'] = rand_pfx + df_no_outage['no_outg_rec_nb'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f205ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_dfs_to_file:\n",
    "    df_no_outage.to_pickle(os.path.join(save_dir_base, 'df_no_outage_FINAL.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ef19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e4cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df90470",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outages_df=pd.read_pickle(os.path.join(save_dir_base, 'all_outages_df.pkl'))\n",
    "clean_windows_by_grp=pd.read_pickle(os.path.join(save_dir_base, 'clean_windows_by_grp.pkl'))\n",
    "df_no_outage=pd.read_pickle(os.path.join(save_dir_base, 'df_no_outage_FINAL.pkl'))\n",
    "df_mp = pd.read_pickle(os.path.join(save_dir_base, 'df_mp_no_outg.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52534d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outage=df_no_outage.sort_values(by=['no_outg_rec_nb'], key=natsort_keygen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c417523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395507e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c383a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outage=df_no_outage.sort_values(by=['no_outg_rec_nb'], key=natsort_keygen())\n",
    "#-------------------------*************************-------------------------*************************\n",
    "# Convert df_no_outage to consolidated (slim) verion if run_using_slim\n",
    "#-------------------------\n",
    "if run_using_slim:\n",
    "    #--------------------------------------------------\n",
    "    # Convert to slim \n",
    "    cols_shared_by_group = None\n",
    "    cols_to_collect_in_lists = ['prem_nb', 'mfr_devc_ser_nbr', 'trsf_pole_nb']\n",
    "    rename_cols = {\n",
    "        'prem_nb'          : 'premise_nbs', \n",
    "        'mfr_devc_ser_nbr' : 'serial_numbers', \n",
    "        'trsf_pole_nb'     : 'trsf_pole_nbs'\n",
    "    }        \n",
    "    consol_groupby_cols = ['t_search_min', 't_search_max']\n",
    "    #-------------------------\n",
    "    df_no_outage_slim = Utilities_df.consolidate_df(\n",
    "        df=df_no_outage, \n",
    "        groupby_cols=consol_groupby_cols, \n",
    "        cols_shared_by_group=cols_shared_by_group, \n",
    "        cols_to_collect_in_lists=cols_to_collect_in_lists, \n",
    "        rename_cols=rename_cols, \n",
    "        verbose=True\n",
    "    )\n",
    "    #-------------------------\n",
    "    df_no_outage_slim=df_no_outage_slim.reset_index()\n",
    "    #-------------------------\n",
    "    df_no_outage_slim['premise_nbs']    = df_no_outage_slim['premise_nbs'].apply(sorted)\n",
    "    df_no_outage_slim['serial_numbers'] = df_no_outage_slim['serial_numbers'].apply(sorted)\n",
    "    df_no_outage_slim['trsf_pole_nbs']  = df_no_outage_slim['trsf_pole_nbs'].apply(sorted)\n",
    "    #-------------------------\n",
    "    if save_dfs_to_file:\n",
    "        df_no_outage_slim.to_pickle(os.path.join(save_dir_base, 'df_no_outage_slim.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b6f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9965a5e1",
   "metadata": {},
   "source": [
    "# Collect events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ebbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_no_outage = pd.read_pickle(os.path.join(save_dir_base, 'df_no_outage_FINAL.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------\n",
    "usg_split_to_CTEs=True\n",
    "df_construct_type=DFConstructType.kRunSqlQuery\n",
    "contstruct_df_args_end_events=None\n",
    "\n",
    "\n",
    "\n",
    "# cols_of_interest_end_dev_event = TableInfos.AMIEndEvents_TI.std_columns_of_interest\n",
    "cols_of_interest_end_dev_event = ['*']\n",
    "verbose=True\n",
    "n_update=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "addtnl_groupby_cols = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0bd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c007a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "end_events_sql_function_kwargs = dict(\n",
    "    cols_of_interest=cols_of_interest_end_dev_event, \n",
    "    date_only=True, \n",
    "    split_to_CTEs=usg_split_to_CTEs, \n",
    "    join_mp_args=False, \n",
    "    df_args = dict(\n",
    "        addtnl_groupby_cols=addtnl_groupby_cols, \n",
    "        t_search_min_col='t_search_min', \n",
    "        t_search_max_col='t_search_max'\n",
    "    ), \n",
    "    field_to_split='df_mp_no_outg', \n",
    "    field_to_split_location_in_kwargs=['df_mp_no_outg'], \n",
    "    sort_coll_to_split=False,\n",
    "    verbose=verbose, n_update=n_update\n",
    ")\n",
    "#----------\n",
    "addtnl_end_events_sql_function_kwargs = dict(\n",
    "    build_sql_function_kwargs = dict(\n",
    "        schema_name='meter_events', \n",
    "        table_name='events_summary_vw', \n",
    "    )\n",
    ")\n",
    "if opcos is not None:\n",
    "    addtnl_end_events_sql_function_kwargs['build_sql_function_kwargs']['opco'] = opcos\n",
    "end_events_sql_function_kwargs = {**end_events_sql_function_kwargs, \n",
    "                                  **addtnl_end_events_sql_function_kwargs}\n",
    "#--------------------------------------------------\n",
    "if run_using_slim:\n",
    "    batch_size=10\n",
    "    #----------\n",
    "    end_events_sql_function_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "        to_supplmnt_dict=end_events_sql_function_kwargs, \n",
    "        default_values_dict=dict(\n",
    "            df_mp_no_outg=df_no_outage_slim, \n",
    "            batch_size=batch_size, \n",
    "            df_args=dict(\n",
    "                mapping_to_ami={'premise_nbs':'premise_nbs'}, \n",
    "                is_df_consolidated=True\n",
    "            )\n",
    "        ), \n",
    "        extend_any_lists=True,\n",
    "        inplace=True\n",
    "    )\n",
    "#-------------------------\n",
    "else:\n",
    "    batch_size=200\n",
    "    #----------\n",
    "    if groupby_col=='trsf_pole_nb':\n",
    "        df_no_outage=df_no_outage.sort_values(by=['no_outg_rec_nb', 'trsf_pole_nb', 'prem_nb', 't_search_min'], ignore_index=True)\n",
    "    if groupby_col=='PREMISE_NB':\n",
    "        df_no_outage=df_no_outage.sort_values(by=['no_outg_rec_nb', 'PREMISE_NB', 't_search_min'], ignore_index=True)\n",
    "    #----------\n",
    "    end_events_sql_function_kwargs = Utilities.supplement_dict_with_default_values(\n",
    "        to_supplmnt_dict=end_events_sql_function_kwargs, \n",
    "        default_values_dict=dict(\n",
    "            df_mp_no_outg=df_no_outage, \n",
    "            batch_size=batch_size, \n",
    "            df_args=dict(\n",
    "                mapping_to_ami={'prem_nb':'premise_nbs'}, \n",
    "                is_df_consolidated=False\n",
    "            )\n",
    "        ), \n",
    "        extend_any_lists=True,\n",
    "        inplace=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--------------------------------------------------\n",
    "# #--------------------------------------------------\n",
    "# start=time.time()\n",
    "# end_events = AMIEndEvents(\n",
    "#     df_construct_type=df_construct_type, \n",
    "#     contstruct_df_args = contstruct_df_args_end_events, \n",
    "#     build_sql_function=AMIEndEvents_SQL.build_sql_end_events_for_no_outages, \n",
    "#     build_sql_function_kwargs=end_events_sql_function_kwargs, \n",
    "#     init_df_in_constructor=True, \n",
    "#     save_args=end_events_save_args\n",
    "# )\n",
    "# end_events_build_time = time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        end_events = AMIEndEvents(\n",
    "            df_construct_type=df_construct_type, \n",
    "            contstruct_df_args = contstruct_df_args_end_events, \n",
    "            build_sql_function=AMIEndEvents_SQL.build_sql_end_events_for_no_outages, \n",
    "            build_sql_function_kwargs=end_events_sql_function_kwargs, \n",
    "            init_df_in_constructor=True, \n",
    "            save_args=end_events_save_args\n",
    "        )\n",
    "        break # stop the loop if the function completes sucessfully\n",
    "    except Exception as e:\n",
    "        print(\"Function errored out!\", e)\n",
    "        print(\"Retrying ... \")\n",
    "        \n",
    "build_time = time.time()-start\n",
    "print(build_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea9039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b5f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ee196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f389b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
