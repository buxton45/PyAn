{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fde5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "# NOTE: To reload a class imported as, e.g., \n",
    "# from module import class\n",
    "# One must call:\n",
    "#   1. import module\n",
    "#   2. reload module\n",
    "#   3. from module import class\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns\n",
    "from packaging import version\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "#sys.path.insert(0, os.path.join(os.path.realpath('..'), 'Utilities'))\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "from Utilities_df import DFConstructType\n",
    "import Utilities_dt\n",
    "import Plot_Box_sns\n",
    "import GrubbsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c83686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dfs_to_csv = False\n",
    "read_dfs_from_csv = True\n",
    "save_dir_base_csvs = os.path.join(Utilities.get_local_data_dir(), r'dovs_and_end_events_data')\n",
    "\n",
    "assert(save_dfs_to_csv+read_dfs_from_csv <=1) # Should never both read and write!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not read_dfs_from_csv:\n",
    "    conn_outages = Utilities.get_utldb01p_oracle_connection()\n",
    "    conn_aws = Utilities.get_athena_prod_aws_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_0 = '2021-01-01'\n",
    "date_1 = '2021-12-31'\n",
    "\n",
    "# date_0 = '2022-01-01'\n",
    "# date_1 = '2022-05-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c11485",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_subdir_csvs = f\"{date_0.replace('-','')}_{date_1.replace('-','')}\"\n",
    "save_dir_csvs = os.path.join(save_dir_base_csvs, save_subdir_csvs)\n",
    "if not os.path.exists(save_dir_csvs):\n",
    "    os.makedirs(save_dir_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest_met_prem = [\n",
    "    'mfr_devc_ser_nbr',\n",
    "    'state_cd', \n",
    "    'prem_nb',\n",
    "    'trsf_pole_nb',\n",
    "    'annual_kwh',\n",
    "    'annual_max_dmnd', \n",
    "    'mtr_stat_cd',\n",
    "    'mtr_stat_cd_desc', \n",
    "    'devc_stat_cd', \n",
    "    'devc_stat_cd_desc'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_slim = DOVSOutages.read_df_outage_slim_from_csv(os.path.join(save_dir_csvs, 'df_outage_slim.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_time_half_window = datetime.timedelta(days=30)\n",
    "#-------------------------\n",
    "cols_of_interest_end_dev_event = TableInfos.AMIEndEvents_TI.std_columns_of_interest\n",
    "#-----\n",
    "cols_of_interest_met_prem      = TableInfos.MeterPremise_TI.std_columns_of_interest\n",
    "#-------------------------\n",
    "usg_split_to_CTEs=True\n",
    "match_events_in_df_to_outages=False\n",
    "combine_kwh_delivered_and_received=True\n",
    "#-------------------------\n",
    "if not usg_split_to_CTEs:\n",
    "    match_events_in_df_to_outages = True\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_slim = DOVSOutages.set_search_time_in_outage_df(\n",
    "    df_outage=df_outage_slim, \n",
    "    search_time_half_window=search_time_half_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_args = dict(save_to_file=False, \n",
    "                 save_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\EndEvents_FUCK', \n",
    "                 save_name=r'end_events.csv', \n",
    "                 index=True)\n",
    "\n",
    "# save_args = dict(save_to_file=True, \n",
    "#                  save_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\EndEvents_prim_strict', \n",
    "#                  save_name=r'end_events.csv', \n",
    "#                  index=True)\n",
    "batch_size=10\n",
    "verbose=True\n",
    "n_update=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_construct_type=DFConstructType.kRunSqlQuery\n",
    "contstruct_df_args_end_events=None\n",
    "end_events_sql_function_kwargs = dict(\n",
    "    cols_of_interest=cols_of_interest_end_dev_event, \n",
    "    df_outage=df_outage_slim, \n",
    "    split_to_CTEs=usg_split_to_CTEs, \n",
    "    join_mp_args=dict(\n",
    "        join_with_CTE=True, \n",
    "        build_mp_kwargs=dict(cols_of_interest=cols_of_interest_met_prem), \n",
    "        join_type='LEFT'\n",
    "    ), \n",
    "    df_args = dict(mapping_to_ami={'PREMISE_NBS':'premise_nbs'}, \n",
    "                   is_df_consolidated=True), \n",
    "    field_to_split='df_outage', \n",
    "    field_to_split_location_in_kwargs=['df_outage'], \n",
    "    sort_coll_to_split=True,\n",
    "    batch_size=batch_size, verbose=verbose, n_update=n_update\n",
    ")\n",
    "addtnl_end_events_sql_function_kwargs = dict(\n",
    "    build_sql_function_kwargs=dict(opco='oh')\n",
    ")\n",
    "end_events_sql_function_kwargs = {**end_events_sql_function_kwargs, \n",
    "                                  **addtnl_end_events_sql_function_kwargs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5cabd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22395b13",
   "metadata": {},
   "source": [
    "# Want to re-work AMI_SQL.build_sql_ami_for_df_with_search_time_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO This might make more sense if I also build a build_sql_ami_for_df method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_sql_ami_for_df_with_search_time_window_FINAL(\n",
    "        cols_of_interest, \n",
    "        df_with_search_time_window, \n",
    "        build_sql_function=None, \n",
    "        build_sql_function_kwargs={}, \n",
    "        sql_alias_base='USG_', \n",
    "        return_args = dict(return_statement=True, \n",
    "                           insert_n_tabs_to_each_line=0), \n",
    "        final_table_alias=None, \n",
    "        prepend_with_to_stmnt=True, \n",
    "        split_to_CTEs=True, \n",
    "        max_n_for_ami_queries=None, \n",
    "        join_mp_args=False, \n",
    "        df_args={}\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        The basic idea here is that a SQL query is built using information contained inside of df_with_search_time_window.\n",
    "        ------------------------- High Level Overview -------------------------\n",
    "        The input parameter df_args essentially determines how df_with_search_time_window will be utilized in the construction\n",
    "          and the final form of the query.\n",
    "        -----\n",
    "        The final query is composed of sub-queries (see below for description of general query sql_gnrl), which are dependent \n",
    "          on how df_with_search_time_window will be grouped, as dictated in df_args['addtnl_groupby_cols'].  \n",
    "        The DF will always be grouped by the minimum and maximum time columns, given in df_args['t_search_min_col'] \n",
    "          and df_args['t_search_max_col'].\n",
    "        Any additional columns in df_args['addtnl_groupby_cols'] will be prepended to the groupby_cols list.\n",
    "        There will be (at least) one sub-query per DF group.  There can be multiple sub-queries per DF group if\n",
    "          max_n_for_ami_queries is implemented in such a manner that batches are required.\n",
    "        -----\n",
    "        As stated above, (at least one) sub-query will be generated in each DF group.  For the explanation below, consider\n",
    "          the DF corresponding to a single group in df_with_search_time_window to be sub_df.\n",
    "        The information to be extracted from sub_df and inserted into the SQL query is encoded in df_args['mapping_to_ami'], \n",
    "          which is a dict whose :\n",
    "              keys give columns of sub_df from which to extact data and\n",
    "              values give the corresponding kwarg to use in build_sql_function to inject the data into SQL (see below for more info)\n",
    "        -----\n",
    "        A general query, sql_gnrl, is built first from which all sub-queries draw.  \n",
    "          - If MeterPremise is to be joined (depending on join_mp_args), it will be joined at the sql_gnrl level.\n",
    "        This is built using build_sql_function with cols_of_interest, join_mp_args, and build_sql_function_kwargs.\n",
    "        Additionally, a WHERE statement is added containing the collection of unique date ranges for all the sub-queries.\n",
    "        -----------------------------------------------------------------------\n",
    "        NOTE: The datetime arguments are taken from df_with_search_time_window\n",
    "                Therefore, no datetime arguments should be found in build_sql_function_kwargs.\n",
    "              Similarly, df_args['mapping_to_ami'] will be taken care of for the various collections.\n",
    "                Therefore, the value in df_args['mapping_to_ami'].values() should not be found in build_sql_function_kwargs\n",
    "        \n",
    "        Input arguments:\n",
    "            cols_of_interest:\n",
    "                Columns of interest to be extracted from query.\n",
    "                Fed as argument to build_sql_function\n",
    "            \n",
    "            df_with_search_time_window:\n",
    "                A datetime containing the search time window information.\n",
    "            \n",
    "            build_sql_function:\n",
    "                Default: AMI_SQL.build_sql_ami\n",
    "                NOTE: Must return a SQLQuery object (cannot simply return a string)\n",
    "                \n",
    "            build_sql_function_kwargs:\n",
    "                Default: Listed below\n",
    "                Keyword arguments for build_sql_function.\n",
    "                These are used mainly for the construction of sql_gnrl, from which all sub-queries draw.\n",
    "                  -Note: The sub-queries really only use this explicitly for datetime_pattern and from_table_alias,\n",
    "                         although they all obviously use all build_sql_function_kwargs implicitly!\n",
    "                The exact acceptable arguments depend on the function used for build_sql_function.\n",
    "                Default key/value pairs:\n",
    "                    serialnumber_col: 'serialnumber'\n",
    "                    from_table_alias: 'un_rin'\n",
    "                    date_col:         'aep_usage_dt'\n",
    "                    datetime_col:     'starttimeperiod'\n",
    "                    datetime_pattern: r\"([0-9]{4}-[0-9]{2}-[0-9]{2})T([0-9]{2}:[0-9]{2}:[0-9]{2}).*\"\n",
    "                \n",
    "            sql_alias_base:\n",
    "                Default: 'USG_'\n",
    "                \n",
    "            return_args: \n",
    "                Default: dict(return_statement=True, insert_n_tabs_to_each_line=0)\n",
    "                \n",
    "            final_table_alias:\n",
    "                Default: None\n",
    "                \n",
    "            prepend_with_to_stmnt:\n",
    "                Default: True\n",
    "                \n",
    "            split_to_CTEs:\n",
    "                Default: True\n",
    "                \n",
    "            max_n_for_ami_queries:\n",
    "                Default: None\n",
    "                The maximum number dictates how the sub-queries will further be divided into batches.\n",
    "                NOTE: This can only be enforced for a single field, otherwise one would need to implement\n",
    "                      a multi-dimensional batching.\n",
    "                This should be a tuple/list of length two (or a single int, see below) whose 0th element\n",
    "                  gives the build_sql_function input parameter to be used for batching, and whose 1st\n",
    "                  element gives the maximum number per batch.\n",
    "                  The 0th element must be contained in  df_args['mapping_to_ami'].values()\n",
    "                  e.g., max_n_for_ami_queries = ['premise_nbs':200]\n",
    "                  \n",
    "                max_n_for_ami_queries as an int:\n",
    "                    max_n_for_ami_queries can be an int ONLY WHEN there is a single mapping in df_args['mapping_to_ami'],\n",
    "                    in which case the field to which the int value corresponds is unambiguous.\n",
    "                \n",
    "            join_mp_args:\n",
    "                Default: False\n",
    "                \n",
    "            df_args:\n",
    "                Default: Listed below\n",
    "                Contains arguments which are used to extract the correct information from df_with_search_time_window.\n",
    "                The acceptable keys for df_args are:\n",
    "                    t_search_min_col:\n",
    "                        Default: 't_search_min'\n",
    "                        \n",
    "                    t_search_max_col:\n",
    "                        Default: 't_search_max'\n",
    "                        \n",
    "                    addtnl_groupby_cols:\n",
    "                        Default: ['OUTG_REC_NB']\n",
    "                        Any additional columns in df_with_search_time_window to group by.\n",
    "                        The DF is always grouped by t_search_min_col and t_search_max_col.\n",
    "                        When additional columns are added, these are grouped first, i.e. the time columns are always\n",
    "                          grouped last.\n",
    "                        These groups basically dictate how the SQL query is divided into sub-queries.  The DF \n",
    "                          df_with_search_time_window is first grouped by addtnl_groupby_cols+[t_search_min_col, t_search_max_col].\n",
    "                          The groups are then iterated over, and a subquery is built using each group.\n",
    "                          For each subquery, the values of addtnl_groupby_cols are included as constant values, and can be\n",
    "                            identified in the ultimate resulting DF as they are tagged with '_GPD_FOR_SQL'\n",
    "                        \n",
    "                    is_df_consolidated:\n",
    "                        Default: False\n",
    "                        \n",
    "                    mapping_to_ami:\n",
    "                        Default: {'PREMISE_NB':'premise_nbs'}\n",
    "                        This basically dictates what will be taken from df_with_search_time_window and used as inputs\n",
    "                          to the queries run by build_sql_function.\n",
    "                        The keys give the column in df_with_search_time_window from which to extract the information.\n",
    "                        The values contain the parameter to be set in build_sql_function with the information extracted\n",
    "                          from df_with_search_time_window\n",
    "                        e.g., mapping_to_ami = {'PREMISE_NB':'premise_nbs'}\n",
    "                            NOTE: OVERSIMPLIFIED, NOT TO BE TAKEN AS EXACT\n",
    "                            NOTE: key_0                 = 'PREMISE_NB' \n",
    "                                  mapping_to_ami[key_0] = 'premise_nbs'\n",
    "                            vals_from_df = df_with_search_time_window[key_0].unique()\n",
    "                            --> sql = build_sql_function(... mapping_to_ami[key_0] = vals_from_df, ...)\n",
    "                            \n",
    "                    mapping_to_mp:\n",
    "                        Default: {'STATE_ABBR_TX':'states'}\n",
    "                        Only used when joining with MeterPremise (i.e., only when join_mp_args)\n",
    "                        Similar in spirit to mapping_to_ami, except the extracted information is used in the MeterPremise\n",
    "                          query (through join_mp_args['build_mp_kwargs'])\n",
    "                          \n",
    "                        This basically dictates what will be taken from df_with_search_time_window and used as inputs\n",
    "                          to the queries run by AMI.join_sql_ami_with_mp.\n",
    "                        The keys give the column in df_with_search_time_window from which to extract the information.\n",
    "                        The values contain the parameter to be set in join_mp_args['build_mp_kwargs'] (and ultimately used\n",
    "                          in AMI.join_sql_ami_with_mp) with the information extracted from df_with_search_time_window\n",
    "\n",
    "        \"\"\"\n",
    "        #--------------------------------------------------\n",
    "        # The whole purpose of this functionality is to take the datetime parameters from df_with_search_time_window\n",
    "        # Therefore, no datetime arguments should be found in build_sql_function_kwargs\n",
    "        datetime_kwargs = ['date_range', 'datetime_range']\n",
    "        if len(set(datetime_kwargs).intersection(set(build_sql_function_kwargs.keys()))) != 0:\n",
    "            print(f'ERROR: In build_sql_ami_for_df_with_search_time_window, datetime argument(s)={found_dts_in_build_sql_function_kwargs}'\\\n",
    "                  ' found in build_sql_function_kwargs\\nCRASH IMMINENT')\n",
    "            assert(0)\n",
    "        #--------------------------------------------------\n",
    "        # build_sql_function default = AMI_SQL.build_sql_ami \n",
    "        if build_sql_function is None:\n",
    "            build_sql_function=AMI_SQL.build_sql_ami\n",
    "        #--------------------------------------------------\n",
    "        # Set up default arguments in build_sql_function_kwargs\n",
    "        if build_sql_function_kwargs is None:\n",
    "            build_sql_function_kwargs = {}\n",
    "        build_sql_function_kwargs['serialnumber_col'] = build_sql_function_kwargs.get('serialnumber_col', 'serialnumber')\n",
    "        build_sql_function_kwargs['from_table_alias'] = build_sql_function_kwargs.get('from_table_alias', 'un_rin')\n",
    "        build_sql_function_kwargs['date_col']         = build_sql_function_kwargs.get('date_col', 'aep_usage_dt')\n",
    "        build_sql_function_kwargs['datetime_col']     = build_sql_function_kwargs.get('datetime_col', 'starttimeperiod')\n",
    "        # NOTE: Below, for datetime_pattern, using [0-9] instead of \\\\d (or \\d) seems more robust.\n",
    "        #       EMR accepts \\\\d but not \\d, whereas Athena accepts \\d but not \\\\d.  Both accept [0-9]\n",
    "        build_sql_function_kwargs['datetime_pattern'] = build_sql_function_kwargs.get('datetime_pattern', \n",
    "                                                                                      r\"([0-9]{4}-[0-9]{2}-[0-9]{2})T([0-9]{2}:[0-9]{2}:[0-9]{2}).*\")\n",
    "        #--------------------------------------------------\n",
    "        # Set up default arguments in df_args.  These are used to essentially translate the information in \n",
    "        # df_with_search_time_window (i.e., used to extract the correct information from DF)\n",
    "        df_args['t_search_min_col']    = df_args.get('t_search_min_col', 't_search_min')\n",
    "        df_args['t_search_max_col']    = df_args.get('t_search_max_col', 't_search_max')\n",
    "        df_args['addtnl_groupby_cols'] = df_args.get('addtnl_groupby_cols', ['OUTG_REC_NB'])\n",
    "        df_args['is_df_consolidated']  = df_args.get('is_df_consolidated', False)\n",
    "        df_args['mapping_to_ami']      = df_args.get('mapping_to_ami', {'PREMISE_NB':'premise_nbs'})\n",
    "        df_args['mapping_to_mp']       = df_args.get('mapping_to_mp', {'STATE_ABBR_TX':'states'})\n",
    "        #--------------------------------------------------\n",
    "        # df_args['mapping_to_ami'] dictates what will be taken from df_with_search_time_window and used as inputs to the \n",
    "        #   queries run by build_sql_function.  The df_args['mapping_to_ami'] contains key,val pairs which correspond to \n",
    "        #   df_col and ami_field (see description at top).\n",
    "        # Since the ami_fields will be handled for each sql_i in collection, they should be absent from build_sql_function_kwargs.\n",
    "        # Also, all keys in ami_field in df_args['mapping_to_ami'] should be contained in df_with_search_time_window.columns\n",
    "        for df_col, ami_field in df_args['mapping_to_ami'].items():\n",
    "            assert(ami_field not in build_sql_function_kwargs)\n",
    "            assert(df_col in df_with_search_time_window.columns.tolist())\n",
    "        #--------------------------------------------------\n",
    "        #--------------------------------------------------\n",
    "        # Determine the columns by which df_with_search_time_window will be grouped.\n",
    "        # The DF is always grouped by t_search_min_col and t_search_max_col.\n",
    "        #   df_args['addtnl_groupby_cols'] prepends columns to the collection\n",
    "        # These groups basically dictate how the SQL query is divided into sub-queries.\n",
    "        #   The groups are iterated over, and a subquery is built using each group.\n",
    "        #   NOTE: For each subquery, the values of addtnl_groupby_cols are included as constant values, \n",
    "        #         and can be identified in the ultimate resulting DF as they are tagged with '_GPD_FOR_SQL'\n",
    "        if df_args['addtnl_groupby_cols'] is None:\n",
    "            df_args['addtnl_groupby_cols'] = []\n",
    "            groupby_cols = []\n",
    "        elif Utilities.is_object_one_of_types(df_args['addtnl_groupby_cols'], [list, tuple]):\n",
    "            # NOTE: Need to make copy of df_args['addtnl_groupby_cols'] below.\n",
    "            #       Cannot simply call, e.g., groupby_cols = df_args['addtnl_groupby_cols'], as this would\n",
    "            #         cause t_search_min/max to also be appended to df_args['addtnl_groupby_cols'] when they \n",
    "            #         are appended to groupby_cols, which is not desired\n",
    "            #       Not sure why I was so careful here?  Doesn't really matter, as this should be harmless performance-wise\n",
    "            groupby_cols = copy.deepcopy(df_args['addtnl_groupby_cols'])\n",
    "        else:\n",
    "            assert(isinstance(df_args['addtnl_groupby_cols'], str))\n",
    "            groupby_cols = [df_args['addtnl_groupby_cols']]\n",
    "        #----------\n",
    "        groupby_cols.extend([df_args['t_search_min_col'], df_args['t_search_max_col']])\n",
    "        #--------------------------------------------------\n",
    "        # Build the grouped version of df_with_search_time_window\n",
    "        # Grab both the group keys and the number of groups\n",
    "        df_gpd = df_with_search_time_window.groupby(groupby_cols)\n",
    "        group_keys = list(df_gpd.groups.keys())\n",
    "        n_groups = df_gpd.ngroups\n",
    "        assert(len(group_keys)==n_groups)\n",
    "        #--------------------------------------------------\n",
    "        # group_keys contains the list of groups in df_with_search_time_window.groupby(groupby_cols)\n",
    "        #   The length of group_keys is the number of groups.\n",
    "        #   The length of each group in group_keys should equal the length of groupby_cols, which is equal\n",
    "        #     to the length of df_args['addtnl_groupby_cols'] plus 2 (for t_min and t_max)\n",
    "        #   In each group, t_min and t_max are the last two elements.  Using the t_min,t_max values from all\n",
    "        #     groups, build the unique date ranges.\n",
    "        #   unique_date_ranges will be used in the general SQL statement, before the sub-queries.  Therefore, since\n",
    "        #     there can be duplicate date ranges, and since many date ranges likely overlap, get only the unique\n",
    "        #     overlap intervals for efficiency\n",
    "        unique_date_ranges = [(pd.to_datetime(x[-2]).date(), pd.to_datetime(x[-1]).date()) \n",
    "                              for x in group_keys]\n",
    "        unique_date_ranges = Utilities.get_overlap_intervals(unique_date_ranges)\n",
    "        #--------------------------------------------------\n",
    "        #--------------------------------------------------\n",
    "        # Build general SQL statement, sql_gnrl\n",
    "        #-------------------------\n",
    "\n",
    "        # Need build_sql_function to return a SQLQuery object, not a string or anything else (-->return_statement=False)\n",
    "        build_sql_function_kwargs['return_args'] = dict(return_statement=False)\n",
    "        #-------------------------\n",
    "        # If join_mp_args, use df_args['mapping_to_mp'] to fully set up join_mp_args['build_mp_kwargs']\n",
    "        # NOTE: If join_mp_args is False or an empty dict, no join will occur\n",
    "        # NOTE: In df_args['mapping_to_mp'], the keys give the column in df_with_search_time_window from which to \n",
    "        #       extract the information, and the values contain the parameter to be set in join_mp_args['build_mp_kwargs'] \n",
    "        #       with the information extracted\n",
    "        assert(Utilities.is_object_one_of_types(join_mp_args, [bool, dict]))\n",
    "        if join_mp_args:\n",
    "            if isinstance(join_mp_args, bool):\n",
    "                join_mp_args = {}\n",
    "            if 'build_mp_kwargs' not in join_mp_args:\n",
    "                join_mp_args['build_mp_kwargs'] = {}\n",
    "            #-----\n",
    "            for df_col, mp_field in df_args['mapping_to_mp'].items():\n",
    "                join_mp_args['build_mp_kwargs'][mp_field] = df_with_search_time_window[df_col].unique().tolist()\n",
    "        else:\n",
    "            join_mp_args = False\n",
    "        #-------------------------\n",
    "        # Build sql_gnrl\n",
    "        sql_gnrl = build_sql_function(cols_of_interest=cols_of_interest, \n",
    "                                      join_mp_args=join_mp_args, \n",
    "                                      **build_sql_function_kwargs)\n",
    "        #-------------------------\n",
    "        # If joined with MP, grab sql_mp and sql_gnrl\n",
    "        if isinstance(sql_gnrl, dict):\n",
    "            assert(join_mp_args) # Should only happen when joining with MP\n",
    "            assert('usg_sql' in sql_gnrl)\n",
    "            sql_mp   = sql_gnrl['mp_sql']\n",
    "            sql_gnrl = sql_gnrl['usg_sql']\n",
    "        else: \n",
    "            sql_mp = None\n",
    "        #-------------------------\n",
    "        # At where statements for all the date ranges in unique_date_ranges\n",
    "        sql_gnrl_where = sql_gnrl.sql_where\n",
    "        if sql_gnrl_where is None:\n",
    "            sql_gnrl_where = SQLWhere()\n",
    "        for unq_date_range in unique_date_ranges:\n",
    "            sql_gnrl_where.add_where_statement(field_desc=build_sql_function_kwargs['date_col'] , comparison_operator='BETWEEN', \n",
    "                                               value=[f'{unq_date_range[0]}', f'{unq_date_range[1]}'], needs_quotes=True, \n",
    "                                               table_alias_prefix=build_sql_function_kwargs['from_table_alias'])\n",
    "        sql_gnrl_where.combine_last_n_where_elements(last_n=len(unique_date_ranges), join_operator = 'OR')\n",
    "        sql_gnrl.sql_where = sql_gnrl_where # Probably not necessary, but to be safe\n",
    "        sql_gnrl.alias = f'{sql_alias_base}gnrl'\n",
    "        #--------------------------------------------------\n",
    "\n",
    "\n",
    "        #--------------------------------------------------\n",
    "        # Build the collection of SQL sub-queries for each group in df_gpd\n",
    "        #-------------------------\n",
    "        sql_i_coll = []\n",
    "        count=0 # Count is used as appendix to give each sub-query a unique alias (=f'{sql_alias_base}{count}')\n",
    "        for addtnl_groupby_vals_w_t_search_min_max, sub_df in df_gpd:\n",
    "            addtnl_groupby_vals = addtnl_groupby_vals_w_t_search_min_max[:-2]\n",
    "            t_search_min = addtnl_groupby_vals_w_t_search_min_max[-2]\n",
    "            t_search_max = addtnl_groupby_vals_w_t_search_min_max[-1]\n",
    "            #----------   \n",
    "            \n",
    "            # Below, df_col is the column in df_with_search_time_window from which to extract the information\n",
    "            #        ami_field is the parameter to be set in build_sql_function with the information extracted\n",
    "            ami_input = {}\n",
    "            for df_col, ami_field in df_args['mapping_to_ami'].items():\n",
    "                assert(ami_field not in ami_input)\n",
    "                if df_args['is_df_consolidated']:\n",
    "                    assert(sub_df.shape[0]==1)\n",
    "                    ami_field_values = sub_df.iloc[0][df_col]\n",
    "                    assert(Utilities.is_object_one_of_types(ami_field_values, [list, tuple]))\n",
    "                    ami_field_values = [x for x in ami_field_values if pd.notna(x)] # In test case, first entry had all NaN prem nbs\n",
    "                else:\n",
    "                    ami_field_values = sub_df[df_col].unique().tolist()\n",
    "                #-----\n",
    "                if len(ami_field_values)==0:\n",
    "                    continue\n",
    "                ami_input[ami_field] = ami_field_values\n",
    "            #----------\n",
    "            if not ami_input:\n",
    "                continue\n",
    "            #----------\n",
    "            #--------------------------------------------------\n",
    "            # Enforce max_n_for_ami_queries, running by batches if the collection size exceeds max_n\n",
    "            batch_idxs=None\n",
    "            if max_n_for_ami_queries is not None:\n",
    "                assert(Utilities.is_object_one_of_types(max_n_for_ami_queries, [list, tuple, int]))\n",
    "                if isinstance(max_n_for_ami_queries, int):\n",
    "                    # This only makes sense if df_args['mapping_to_ami'] has only a single mapping\n",
    "                    #   Otherwise, it would be uncertain as to which ami_field the max is intended for\n",
    "                    assert(len(df_args['mapping_to_ami'])==1)\n",
    "                    max_n_for_ami_queries = [list(df_args['mapping_to_ami'].values())[0], max_n_for_ami_queries]\n",
    "                #----------\n",
    "                assert(\n",
    "                    Utilities.is_object_one_of_types(max_n_for_ami_queries, [list, tuple]) and \n",
    "                    len(max_n_for_ami_queries)==2 and \n",
    "                    isinstance(max_n_for_ami_queries[0], str) and \n",
    "                    isinstance(max_n_for_ami_queries[1], int)\n",
    "                )\n",
    "                assert(max_n_for_ami_queries[0] in df_args['mapping_to_ami'].values())\n",
    "                #----------\n",
    "                ami_field_to_batch = max_n_for_ami_queries[0]\n",
    "                max_n              = max_n_for_ami_queries[1]\n",
    "                ami_field_values_to_batch = ami_input[ami_field_to_batch]\n",
    "                if len(ami_field_values_to_batch)>max_n:\n",
    "                    batch_idxs = Utilities.get_batch_idx_pairs(len(ami_field_values_to_batch), max_n)\n",
    "                else:\n",
    "                    batch_idxs = [None]\n",
    "            else:\n",
    "                batch_idxs=[None]\n",
    "            #--------------------------------------------------\n",
    "            # Generate the sub-query.\n",
    "            # If batch_idxs != [None], generate a sub-query for each batch in batch_idxs (or, I guess one could think \n",
    "            #   of it as a sub-sub-query as the query for this group is split into multiple when len(batch_idxs)>1)\n",
    "            # NOTE: Made batch_idxs = [None] instead of None so I could keep all code in 'for batch_i in batch_idxs'\n",
    "            #       loop regardless of whether or not batching \n",
    "            #-------------------------\n",
    "            # In any case, batch_idxs should be a list\n",
    "            assert(Utilities.is_object_one_of_types(batch_idxs, [list, tuple]))\n",
    "            for batch_i in batch_idxs:\n",
    "                assert(batch_i is None or \n",
    "                       Utilities.is_object_one_of_types(batch_i, [list, tuple]))\n",
    "                if batch_i is None:\n",
    "                    # Should only happen when not batching\n",
    "                    assert(len(batch_idxs)==1)\n",
    "                    ami_input_i = ami_input\n",
    "                else:\n",
    "                    assert(len(batch_i)==2)\n",
    "                    i_beg = batch_i[0]\n",
    "                    i_end = batch_i[1]\n",
    "                    # Grab ami_input_i from ami_input, keeping only the appropriate subset of ami_field_to_batch\n",
    "                    ami_input_i = {k:(v if k!=ami_field_to_batch else v[i_beg:i_end]) for k,v in ami_input.items()}\n",
    "                #----------\n",
    "                sql_i = build_sql_function(\n",
    "                    cols_of_interest=[f\"*\"], \n",
    "                    date_range=[pd.to_datetime(t_search_min).date(), \n",
    "                               pd.to_datetime(t_search_max).date()], \n",
    "                    datetime_range=[t_search_min, t_search_max], \n",
    "                    datetime_pattern=build_sql_function_kwargs['datetime_pattern'], \n",
    "                    table_name=sql_gnrl.alias, \n",
    "                    schema_name=None, \n",
    "                    from_table_alias=build_sql_function_kwargs['from_table_alias'], \n",
    "                    **ami_input_i\n",
    "                )\n",
    "                #----------\n",
    "                #sql_i.sql_from = SQLFrom(table_name=sql_gnrl.alias)\n",
    "                #----------\n",
    "                # For each subquery, the values of addtnl_groupby_cols are included as constant values, and can be\n",
    "                #   identified in the ultimate resulting DF as they are tagged with '_GPD_FOR_SQL'\n",
    "                assert(len(df_args['addtnl_groupby_cols'])==len(addtnl_groupby_vals)) # Sanity check\n",
    "                for i_gpby_col in range(len(df_args['addtnl_groupby_cols'])):\n",
    "                    gpby_col = df_args['addtnl_groupby_cols'][i_gpby_col]\n",
    "                    gpby_col_val = addtnl_groupby_vals[i_gpby_col]\n",
    "                    #TODO In build_sql_ami_for_outages, had field_desc=f\"'{int(outg_rec_nb)}'\"\n",
    "                    sql_i.sql_select.add_select_element(field_desc=f\"'{gpby_col_val}'\", alias=f'{gpby_col}_GPD_FOR_SQL')\n",
    "                #-----\n",
    "                sql_i.alias = f'{sql_alias_base}{count}'\n",
    "                #-----\n",
    "                sql_i_coll.append(sql_i)\n",
    "                count += 1\n",
    "        #--------------------------------------------------\n",
    "        return_dict = {'sql_gnrl':sql_gnrl, \n",
    "                       'sql_i_coll':sql_i_coll, \n",
    "                       'sql_mp':sql_mp}\n",
    "        #--------------------------------------------------\n",
    "        return_statement = return_args.get('return_statement', False)\n",
    "        insert_n_tabs_to_each_line = return_args.get('insert_n_tabs_to_each_line', 0)\n",
    "        prepend_with_to_stmnt = return_args.get('prepend_with_to_stmnt', True)\n",
    "        if return_statement:\n",
    "            return AMI_SQL.assemble_sql_ami_for_outages_statement(\n",
    "                return_dict, \n",
    "                final_table_alias=final_table_alias, \n",
    "                prepend_with_to_stmnt=prepend_with_to_stmnt, \n",
    "                split_to_CTEs=split_to_CTEs\n",
    "            )\n",
    "        else:\n",
    "            return return_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e18095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae73160",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = AMIEndEvents_SQL.build_sql_end_events_for_df_with_search_time_window(\n",
    "            cols_of_interest=cols_of_interest_end_dev_event, \n",
    "            df_with_search_time_window=df_outage_slim.iloc[:10], \n",
    "            build_sql_function=AMIEndEvents_SQL.build_sql_end_events, \n",
    "            build_sql_function_kwargs=dict(opco='oh'), \n",
    "            sql_alias_base='USG_', \n",
    "            return_args = dict(return_statement=True, \n",
    "                               insert_n_tabs_to_each_line=0), \n",
    "            final_table_alias=None, \n",
    "            prepend_with_to_stmnt=True, \n",
    "            split_to_CTEs=usg_split_to_CTEs, \n",
    "            max_n_prem_per_outg=None, \n",
    "            join_mp_args=dict(\n",
    "                join_with_CTE=True, \n",
    "                build_mp_kwargs=dict(cols_of_interest=cols_of_interest_met_prem), \n",
    "                join_type='LEFT'\n",
    "            ), \n",
    "            df_args = dict(mapping_to_ami={'PREMISE_NBS':'premise_nbs'}, \n",
    "                               is_df_consolidated=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc06fb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start=time.time()\n",
    "# end_events = AMIEndEvents(\n",
    "#     df_construct_type=df_construct_type, \n",
    "#     contstruct_df_args = contstruct_df_args_end_events, \n",
    "#     build_sql_function=AMIEndEvents_SQL.build_sql_end_events_for_outages, \n",
    "#     build_sql_function_kwargs=end_events_sql_function_kwargs, \n",
    "#     init_df_in_constructor=True, \n",
    "#     save_args=save_args\n",
    "# )\n",
    "# end_events_build_time = time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5374177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde27749",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest=cols_of_interest_end_dev_event\n",
    "df_with_search_time_window=df_outage_slim.iloc[:10]\n",
    "build_sql_function=AMIEndEvents_SQL.build_sql_end_events\n",
    "build_sql_function_kwargs=dict(opco='oh')\n",
    "sql_alias_base='USG_'\n",
    "return_args = dict(return_statement=True, \n",
    "                   insert_n_tabs_to_each_line=0)\n",
    "final_table_alias=None\n",
    "prepend_with_to_stmnt=True\n",
    "split_to_CTEs=usg_split_to_CTEs\n",
    "max_n_prem_per_outg=None\n",
    "join_mp_args=dict(\n",
    "    join_with_CTE=True, \n",
    "    build_mp_kwargs=dict(cols_of_interest=cols_of_interest_met_prem), \n",
    "    join_type='LEFT'\n",
    ")\n",
    "df_args = dict(mapping_to_ami={'PREMISE_NBS':'premise_nbs'}, \n",
    "                   is_df_consolidated=True)\n",
    "\n",
    "max_n_for_ami_queries=max_n_prem_per_outg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sql_function_kwargs['serialnumber_col'] = build_sql_function_kwargs.get('serialnumber_col', 'serialnumber')\n",
    "build_sql_function_kwargs['from_table_alias'] = build_sql_function_kwargs.get('from_table_alias', 'un_rin')\n",
    "build_sql_function_kwargs['datetime_col']     = build_sql_function_kwargs.get('datetime_col', 'valuesinterval')\n",
    "build_sql_function_kwargs['datetime_pattern'] = build_sql_function_kwargs.get('datetime_pattern', \n",
    "                                                                               r\"([0-9]{4}-[0-9]{2}-[0-9]{2})T([0-9]{2}:[0-9]{2}:[0-9]{2}).*\")\n",
    "build_sql_function_kwargs['date_col']         = build_sql_function_kwargs.get('date_col', 'aep_event_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17baa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2 = AMI_SQL.build_sql_ami_for_df_with_search_time_window(\n",
    "            cols_of_interest=cols_of_interest, \n",
    "            df_with_search_time_window=df_with_search_time_window, \n",
    "            build_sql_function=build_sql_function, \n",
    "            build_sql_function_kwargs=build_sql_function_kwargs, \n",
    "            sql_alias_base=sql_alias_base, \n",
    "            return_args = return_args, \n",
    "            final_table_alias=final_table_alias, \n",
    "            prepend_with_to_stmnt=prepend_with_to_stmnt, \n",
    "            split_to_CTEs=split_to_CTEs, \n",
    "            max_n_prem_per_outg=max_n_prem_per_outg, \n",
    "            join_mp_args=join_mp_args, \n",
    "            df_args = df_args\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833bcf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2==sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2b = AMI_SQL.build_sql_ami_for_df_with_search_time_window(\n",
    "            cols_of_interest=cols_of_interest, \n",
    "            df_with_search_time_window=df_with_search_time_window, \n",
    "            build_sql_function=build_sql_function, \n",
    "            build_sql_function_kwargs=build_sql_function_kwargs, \n",
    "            sql_alias_base=sql_alias_base, \n",
    "            return_args = return_args, \n",
    "            final_table_alias=final_table_alias, \n",
    "            prepend_with_to_stmnt=prepend_with_to_stmnt, \n",
    "            split_to_CTEs=split_to_CTEs, \n",
    "            max_n_prem_per_outg=max_n_prem_per_outg, \n",
    "            join_mp_args=join_mp_args, \n",
    "            df_args = df_args\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72784fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2b==sql2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf67152",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql3 = build_sql_ami_for_df_with_search_time_window_FINAL(\n",
    "            cols_of_interest=cols_of_interest, \n",
    "            df_with_search_time_window=df_with_search_time_window, \n",
    "            build_sql_function=build_sql_function, \n",
    "            build_sql_function_kwargs=build_sql_function_kwargs, \n",
    "            sql_alias_base=sql_alias_base, \n",
    "            return_args = return_args, \n",
    "            final_table_alias=final_table_alias, \n",
    "            prepend_with_to_stmnt=prepend_with_to_stmnt, \n",
    "            split_to_CTEs=split_to_CTEs, \n",
    "            max_n_for_ami_queries=max_n_for_ami_queries, \n",
    "            join_mp_args=join_mp_args, \n",
    "            df_args = df_args\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql3==sql2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a59da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afeba80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_OG = pd.read_csv(os.path.join(save_dir_csvs, 'df_outage_OG.csv'), dtype=str)\n",
    "csv_cols_and_types_to_convert_dict = {'CI_NB':np.int32, 'CMI_NB':np.float64, 'OUTG_REC_NB':[np.float64, np.int32]}\n",
    "df_outage_OG = Utilities_df.convert_col_types(df_outage_OG, csv_cols_and_types_to_convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage = df_outage_OG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_time_half_window = datetime.timedelta(days=30)\n",
    "df_outage = DOVSOutages.set_search_time_in_outage_df(\n",
    "    df_outage=df_outage, \n",
    "    search_time_half_window=search_time_half_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50935b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest=cols_of_interest_end_dev_event\n",
    "df_with_search_time_window=df_outage[df_outage['OUTG_REC_NB'].isin(df_outage_slim.iloc[:10]['OUTG_REC_NB'].unique().tolist())]\n",
    "build_sql_function=AMIEndEvents_SQL.build_sql_end_events\n",
    "build_sql_function_kwargs=dict(opco='oh')\n",
    "sql_alias_base='USG_'\n",
    "return_args = dict(return_statement=True, \n",
    "                   insert_n_tabs_to_each_line=0)\n",
    "final_table_alias=None\n",
    "prepend_with_to_stmnt=True\n",
    "split_to_CTEs=usg_split_to_CTEs\n",
    "max_n_prem_per_outg=None\n",
    "join_mp_args=dict(\n",
    "    join_with_CTE=True, \n",
    "    build_mp_kwargs=dict(cols_of_interest=cols_of_interest_met_prem), \n",
    "    join_type='LEFT'\n",
    ")\n",
    "df_args = dict(mapping_to_ami={'PREMISE_NB':'premise_nbs'}, \n",
    "                   is_df_consolidated=False)\n",
    "\n",
    "max_n_for_ami_queries=max_n_prem_per_outg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e2b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sql_function_kwargs['serialnumber_col'] = build_sql_function_kwargs.get('serialnumber_col', 'serialnumber')\n",
    "build_sql_function_kwargs['from_table_alias'] = build_sql_function_kwargs.get('from_table_alias', 'un_rin')\n",
    "build_sql_function_kwargs['datetime_col']     = build_sql_function_kwargs.get('datetime_col', 'valuesinterval')\n",
    "build_sql_function_kwargs['datetime_pattern'] = build_sql_function_kwargs.get('datetime_pattern', \n",
    "                                                                               r\"([0-9]{4}-[0-9]{2}-[0-9]{2})T([0-9]{2}:[0-9]{2}:[0-9]{2}).*\")\n",
    "build_sql_function_kwargs['date_col']         = build_sql_function_kwargs.get('date_col', 'aep_event_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878be57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql4 = build_sql_ami_for_df_with_search_time_window_FINAL(\n",
    "            cols_of_interest=cols_of_interest, \n",
    "            df_with_search_time_window=df_with_search_time_window, \n",
    "            build_sql_function=build_sql_function, \n",
    "            build_sql_function_kwargs=build_sql_function_kwargs, \n",
    "            sql_alias_base=sql_alias_base, \n",
    "            return_args = return_args, \n",
    "            final_table_alias=final_table_alias, \n",
    "            prepend_with_to_stmnt=prepend_with_to_stmnt, \n",
    "            split_to_CTEs=split_to_CTEs, \n",
    "            max_n_for_ami_queries=max_n_for_ami_queries, \n",
    "            join_mp_args=join_mp_args, \n",
    "            df_args = df_args\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9b631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sql4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f069dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql4==sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f38e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5692bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNs = DOVSOutages.get_serial_numbers_for_outages(\n",
    "    outg_rec_nbs=df_outage_slim['OUTG_REC_NB'].unique().tolist(), \n",
    "    return_premise_nbs_for_outages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc51c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNs_df = pd.DataFrame(SNs)\n",
    "SNs_df.index = SNs_df.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d46a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_outage_slim, SNs_df, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43660792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
