{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0556a65b",
   "metadata": {},
   "source": [
    "# Originally adapted from IT_DEMO_MODEL_DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22918598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051005d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\s346557\\Anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./model_end_events_for_outages_METHODS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9764e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "# NOTE: To reload a class imported as, e.g., \n",
    "# from module import class\n",
    "# One must call:\n",
    "#   1. import module\n",
    "#   2. reload module\n",
    "#   3. from module import class\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "import sys, os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns, natsort_keygen\n",
    "from packaging import version\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import adjustText\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm #e.g. for cmap=cm.jet\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "from EEMSP import EEMSP\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIEDE_DEV import AMIEDE_DEV\n",
    "from MECPODf import MECPODf\n",
    "from MECPOAn import MECPOAn\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "from OutagePredictor import OutagePredictor\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "#sys.path.insert(0, os.path.join(os.path.realpath('..'), 'Utilities'))\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "from Utilities_df import DFConstructType\n",
    "import Utilities_dt\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import Plot_Hist\n",
    "import Plot_Bar\n",
    "import GrubbsTest\n",
    "import DataFrameSubsetSlicer\n",
    "from DataFrameSubsetSlicer import DataFrameSubsetSlicer as DFSlicer\n",
    "#---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132b712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6687d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_probs_for_trsf_i(\n",
    "    fig_num, \n",
    "    trsf_pole_nb_i, \n",
    "    y_prob1_by_date_df, \n",
    "    dovs_df, \n",
    "    trsf_pole_nb_col='trsf_pole_nb', \n",
    "    outg_rec_nb_col='OUTG_REC_NB', \n",
    "    dt_off_ts_full_col='DT_OFF_TS_FULL', \n",
    "    plot_maxima=False, \n",
    "    maxima_colors=['green', 'lawngreen']\n",
    "):\n",
    "    r\"\"\"\n",
    "    dovs_df:\n",
    "        Must be merged with MeterPremise so that it contains trsf_pole_nb_col\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    fig,ax = Plot_General.default_subplots(fig_num=fig_num)\n",
    "    #-------------------------\n",
    "    y_prob1_by_date_df.loc[trsf_pole_nb_i].T.plot.line(ax=ax)\n",
    "    ax.axhline(0.5, color='cyan')\n",
    "    #-------------------------\n",
    "    dovs_df_i = dovs_df[dovs_df[trsf_pole_nb_col]==trsf_pole_nb_i][[outg_rec_nb_col, dt_off_ts_full_col]].drop_duplicates()\n",
    "    if dovs_df_i.shape[0]>0:\n",
    "        for idx_ij in range(dovs_df_i.shape[0]):\n",
    "            dt_off_ts_full_ij = dovs_df_i.iloc[idx_ij][dt_off_ts_full_col]\n",
    "            ax.axvline(dt_off_ts_full_ij, color='red', lw=5, alpha=0.5)\n",
    "    ax.set_title(f'trsf_pole_nb = {trsf_pole_nb_i}')\n",
    "    #-------------------------\n",
    "    ax.set_ylim(0,1);\n",
    "    #-------------------------\n",
    "    if plot_maxima:\n",
    "        idx_max_i = y_prob1_by_date_df.loc[trsf_pole_nb_i].idxmax()\n",
    "        ax.axvline(idx_max_i, color=maxima_colors[0])\n",
    "        if(\n",
    "            idx_max_i > dovs_df_i[dt_off_ts_full_col].max() and\n",
    "            y_prob1_by_date_df.loc[trsf_pole_nb_i].index[0] < dovs_df_i[dt_off_ts_full_col].max() #Make sure there are actually data to grab\n",
    "        ):\n",
    "            idx_max_i = y_prob1_by_date_df.loc[trsf_pole_nb_i][:dovs_df_i[dt_off_ts_full_col].max()].idxmax()\n",
    "            ax.axvline(idx_max_i, color=maxima_colors[1])\n",
    "    #-------------------------\n",
    "    if dovs_df_i[dt_off_ts_full_col].min() < y_prob1_by_date_df.loc[trsf_pole_nb_i].index[0]:\n",
    "        ax.set_xlim(left=dovs_df_i[dt_off_ts_full_col].min()-pd.Timedelta('1D'))    \n",
    "    #-------------------------\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096600c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b827c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_rec_nbs_and_trsf_pole_nbs = [\n",
    "    ('13275190', '40820482D20097'),\n",
    "    ('13276382', '41810769C10019'),\n",
    "    ('13516272', '1887822760551'),\n",
    "    ('13281137', '41810677B20017'),\n",
    "    ('13276382', '41810769C30027'),\n",
    "    ('13284541', '41810796A10038'),\n",
    "    ('13415438', '1916206716391'),\n",
    "    ('13431370', '1884744712610'),\n",
    "    ('13295601', '1852023757802'),\n",
    "    ('13276753', '41810724B30053'),\n",
    "    ('13417871', '1893731751927'),\n",
    "    ('13417871', '1895111751920'),\n",
    "    ('13274063', '41810723A20123'),\n",
    "    ('13275201', '40820707C40124'),\n",
    "    ('13275190', '40820483B10262'),\n",
    "    ('13415340', '1912458713764'),\n",
    "    ('13499728', '1852582750272'),\n",
    "    ('13275205', '40820731A30084'),\n",
    "    ('13393559', '1834040741037'),\n",
    "    ('13417871', '1893939751371'),\n",
    "    ('13276382', '41810770A30092'),\n",
    "    ('13276382', '41810745D20026'),\n",
    "    ('13283050', '41810892000151'),\n",
    "    ('13275190', '40820482D30197'),\n",
    "    ('13275190', '40820482C40019'),\n",
    "    ('13393559', '1838888738583'),\n",
    "    ('13415438', '1914777719791'),\n",
    "    ('13281046', '41810771A40026'),\n",
    "    ('13415340', '1912185712890'),\n",
    "    ('13526085', '1860302692180'),\n",
    "    ('13275895', '40820507C30177'),\n",
    "    ('13276382', '41810769C30018'),\n",
    "    ('13275201', '40820707D20093'),\n",
    "    ('13514788', '1911697717715'),\n",
    "    ('13460850', '1848168697905'),\n",
    "    ('13403703', '1834467736881'),\n",
    "    ('13514788', '1914335720034'),\n",
    "    ('13412376', '1893308707792'),\n",
    "    ('13467591', '40830120D20010'),\n",
    "    ('13389129', '1867427734381'),\n",
    "    ('13403703', '1837283735919'),\n",
    "    ('13276382', '41810769000350'),\n",
    "    ('13314058', '41830806B20055'),\n",
    "    ('13275190', '40820482D20111'),\n",
    "    ('13393559', '1838734738009'),\n",
    "    ('13276753', '41810724D40119'),\n",
    "    ('13485854', '1868344715875'),\n",
    "    ('13277285', '40820508A30379'),\n",
    "    ('13415340', '1913460716874'),\n",
    "    ('13514788', '1914362717373'),\n",
    "    ('13278651', '40810001C30061'),\n",
    "    ('13276996', '40820506B40143'),\n",
    "    ('13406470', '1871656735225'),\n",
    "    ('13415438', '1916295718725'),\n",
    "    ('13276996', '40820506B40088'),\n",
    "    ('13275190', '40820482D40089'),\n",
    "    ('13431370', '1886191712330'),\n",
    "    ('13276382', '41810769000088'),\n",
    "    ('13275895', '40820507C30126'),\n",
    "    ('13438478', '1819164739684'),\n",
    "    ('13544115', '1833725729244'),\n",
    "    ('13415340', '1912003713975'),\n",
    "    ('13514722', '41840982B20074'),\n",
    "    ('13282527', '41810700B40030'),\n",
    "    ('13275201', '40820707C40185'),\n",
    "    ('13275190', '40820482D10088'),\n",
    "    ('13276382', '41810769C40053'),\n",
    "    ('13417871', '1892806750265'),\n",
    "    ('13393559', '1838220737735'),\n",
    "    ('13408697', '1870422694331'),\n",
    "    ('13275190', '40820506C30146'),\n",
    "    ('13514788', '1913256717164'),\n",
    "    ('13275190', '40820482D20119'),\n",
    "    ('13400225', '1902452700640'),\n",
    "    ('13499690', '1827080796078'),\n",
    "    ('13499690', '1827588789150'),\n",
    "    ('13402650', '1871972735001'),\n",
    "    ('13279018', '39830880A20086'),\n",
    "    ('13276382', '41810745D30057'),\n",
    "    ('13275853', '40820685000017'),\n",
    "    ('13276382', '41810746A10066'),\n",
    "    ('13403703', '1830532740214'),\n",
    "    ('13275201', '40820707C30132'),\n",
    "    ('13275201', '40820707D10133'),\n",
    "    ('13417871', '1894735750035'),\n",
    "    ('13275205', '40820706D40013'),\n",
    "    ('13384109', '1872593713120'),\n",
    "    ('13514788', '1916894718759'),\n",
    "    ('13516272', '1887338760527'),\n",
    "    ('13415340', '1911710713780'),\n",
    "    ('13465713', '1876383744479'),\n",
    "    ('13282147', '1894307719021'),\n",
    "    ('13276382', '41810769000233'),\n",
    "    ('13403703', '1835580734953'),\n",
    "    ('13514788', '1913682718651'),\n",
    "    ('13516879', '1874886723070'),\n",
    "    ('13276382', '41810770A40029'),\n",
    "    ('13276753', '41810724D40113'),\n",
    "    ('13275201', '40820707D10101'),\n",
    "    ('13431370', '1884944712035')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ea4668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_coll = 100\n",
      "batch_size = 1000\n",
      "n_batches = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s346557\\Documents\\Analysis\\GenAn.py:656: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql, conn_db, **read_sql_args)\n"
     ]
    }
   ],
   "source": [
    "outg_rec_nbs = [x[0] for x in outg_rec_nbs_and_trsf_pole_nbs]\n",
    "dovs_dev = DOVSOutages(\n",
    "    df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "    contstruct_df_args=None, \n",
    "    init_df_in_constructor=True,\n",
    "    build_sql_function=DOVSOutages_SQL.build_sql_std_outage, \n",
    "    build_sql_function_kwargs=dict(\n",
    "        outg_rec_nbs=outg_rec_nbs, \n",
    "        field_to_split='outg_rec_nbs', \n",
    "        include_premise=False\n",
    "    ), \n",
    "    build_consolidated=False\n",
    ")\n",
    "dovs_df_dev = dovs_dev.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a3aa145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dovs_df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64defb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New meter_events.events_summary_vw table became live on 2023-04-01\n",
    "# Therefore, we can only really assess outages occurring on/after 2023-05-02 (due to 31-day look)\n",
    "outg_rec_nbs = dovs_df_dev[dovs_df_dev['DT_OFF_TS'] >= pd.to_datetime('2023-05-02')]['OUTG_REC_NB'].unique().tolist()\n",
    "outg_rec_nbs_and_trsf_pole_nbs = [x for x in outg_rec_nbs_and_trsf_pole_nbs if x[0] in outg_rec_nbs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c897a-a619-49c9-a07e-ed6a6eaa3a61",
   "metadata": {},
   "source": [
    "# BEG DEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0637a2d-13c1-4c6a-b38a-a1f90d47d627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_coll = 51\n",
      "batch_size = 10000\n",
      "n_batches = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s346557\\Documents\\Analysis\\GenAn.py:656: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql, conn_db, **read_sql_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s346557\\Documents\\Analysis\\EEMSP.py:698: FutureWarning: The provided callable <function mean at 0x000001EF16BA1790> is currently using Series.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  srs_i = df_eemsp_i.agg(agg_dict)\n",
      "C:\\Users\\s346557\\Documents\\Analysis\\EEMSP.py:698: FutureWarning: The provided callable <function mean at 0x000001EF16BA1790> is currently using Series.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  srs_i = df_eemsp_i.agg(agg_dict)\n",
      "C:\\Users\\s346557\\Documents\\Analysis\\EEMSP.py:698: FutureWarning: The provided callable <function mean at 0x000001EF16BA1790> is currently using Series.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  srs_i = df_eemsp_i.agg(agg_dict)\n",
      "C:\\Users\\s346557\\Documents\\Analysis\\EEMSP.py:698: FutureWarning: The provided callable <function mean at 0x000001EF16BA1790> is currently using Series.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  srs_i = df_eemsp_i.agg(agg_dict)\n"
     ]
    }
   ],
   "source": [
    "df_1 = OutagePredictor.build_eemsp_df(\n",
    "    trsf_pole_nbs = [x[1] for x in outg_rec_nbs_and_trsf_pole_nbs], \n",
    "    date_range    =  [pd.to_datetime('2023-07-01'), pd.to_datetime('2023-09-01')], \n",
    "    addtnl_kwargs=None, \n",
    "    mult_strategy='agg', \n",
    "    include_n_eemsp=True, \n",
    "    cols_of_interest_eemsp=None, \n",
    "    numeric_cols = ['kva_size'], \n",
    "    dt_cols = ['install_dt', 'removal_dt'], \n",
    "    ignore_cols = ['serial_nb'], \n",
    "    batch_size=10000, \n",
    "    verbose=True, \n",
    "    n_update=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6a12aa-6d93-4b21-b2d9-5c64b67c29e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_coll = 51\n",
      "batch_size = 10000\n",
      "n_batches = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s346557\\Documents\\Analysis\\OutagePredictor.py:1649: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  eemsp_df_i = pd.read_sql_query(sql_EEMSP_i, conn_aws)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s346557\\Documents\\Analysis\\EEMSP.py:698: FutureWarning: The provided callable <function mean at 0x000001EF16BA1790> is currently using Series.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  srs_i = df_eemsp_i.agg(agg_dict)\n",
      "C:\\Users\\s346557\\Documents\\Analysis\\EEMSP.py:698: FutureWarning: The provided callable <function mean at 0x000001EF16BA1790> is currently using Series.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  srs_i = df_eemsp_i.agg(agg_dict)\n",
      "C:\\Users\\s346557\\Documents\\Analysis\\EEMSP.py:698: FutureWarning: The provided callable <function mean at 0x000001EF16BA1790> is currently using Series.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  srs_i = df_eemsp_i.agg(agg_dict)\n",
      "C:\\Users\\s346557\\Documents\\Analysis\\EEMSP.py:698: FutureWarning: The provided callable <function mean at 0x000001EF16BA1790> is currently using Series.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  srs_i = df_eemsp_i.agg(agg_dict)\n"
     ]
    }
   ],
   "source": [
    "df_2 = OutagePredictor.build_eemsp_df_OLD(\n",
    "    trsf_pole_nbs = [x[1] for x in outg_rec_nbs_and_trsf_pole_nbs], \n",
    "    date_range    =  [pd.to_datetime('2023-07-01'), pd.to_datetime('2023-09-01')], \n",
    "    conn_aws=None,  \n",
    "    mult_strategy='agg', \n",
    "    include_n_eemsp=True, \n",
    "    cols_of_interest_eemsp=None, \n",
    "    numeric_cols = ['kva_size'], \n",
    "    dt_cols = ['install_dt', 'removal_dt'], \n",
    "    ignore_cols = ['serial_nb'], \n",
    "    batch_size=10000, \n",
    "    verbose=True, \n",
    "    n_update=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "265d2e51-30fd-4adb-a237-3a3cdf3d936d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d243300-9d92-4b74-bdf6-f759bb1d11ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 19)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43196ecb-1558-48a7-b8b6-7a2f6202aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.equals(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02ce8a3a-e4a6-46ac-b867-3a561217bd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_1['LOCATION_NB'].unique()).symmetric_difference(set(df_2['LOCATION_NB'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdfe3521-580f-4fcd-bf9b-4f6182bf6224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOCATION_NB</th>\n",
       "      <th>KVA_SIZE</th>\n",
       "      <th>INSTALL_DT</th>\n",
       "      <th>REMOVAL_DT</th>\n",
       "      <th>COOLANT</th>\n",
       "      <th>EQTYPE_ID</th>\n",
       "      <th>INFO</th>\n",
       "      <th>LAST_TRANS_DESC</th>\n",
       "      <th>LATEST_STATUS</th>\n",
       "      <th>MFGR_NM</th>\n",
       "      <th>PHASE_CNT</th>\n",
       "      <th>PRIM_VOLTAGE</th>\n",
       "      <th>PROTECTION</th>\n",
       "      <th>PRU_NUMBER</th>\n",
       "      <th>SEC_VOLTAGE</th>\n",
       "      <th>SPECIAL_CHAR</th>\n",
       "      <th>TAPS</th>\n",
       "      <th>XFTYPE</th>\n",
       "      <th>n_eemsp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [LOCATION_NB, KVA_SIZE, INSTALL_DT, REMOVAL_DT, COOLANT, EQTYPE_ID, INFO, LAST_TRANS_DESC, LATEST_STATUS, MFGR_NM, PHASE_CNT, PRIM_VOLTAGE, PROTECTION, PRU_NUMBER, SEC_VOLTAGE, SPECIAL_CHAR, TAPS, XFTYPE, n_eemsp]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1[df_1['LOCATION_NB']=='1916894718759']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4381ffdd-18cd-4035-8516-3bcf37d4315a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1887822760551',\n",
       " '1916206716391',\n",
       " '1884744712610',\n",
       " '1893731751927',\n",
       " '1895111751920',\n",
       " '1912458713764',\n",
       " '1852582750272',\n",
       " '1834040741037',\n",
       " '1893939751371',\n",
       " '1838888738583',\n",
       " '1914777719791',\n",
       " '1912185712890',\n",
       " '1860302692180',\n",
       " '1911697717715',\n",
       " '1848168697905',\n",
       " '1834467736881',\n",
       " '1914335720034',\n",
       " '1893308707792',\n",
       " '40830120D20010',\n",
       " '1867427734381',\n",
       " '1837283735919',\n",
       " '1838734738009',\n",
       " '1868344715875',\n",
       " '1913460716874',\n",
       " '1914362717373',\n",
       " '1871656735225',\n",
       " '1916295718725',\n",
       " '1886191712330',\n",
       " '1819164739684',\n",
       " '1833725729244',\n",
       " '1912003713975',\n",
       " '41840982B20074',\n",
       " '1892806750265',\n",
       " '1838220737735',\n",
       " '1870422694331',\n",
       " '1913256717164',\n",
       " '1902452700640',\n",
       " '1827080796078',\n",
       " '1827588789150',\n",
       " '1871972735001',\n",
       " '1830532740214',\n",
       " '1894735750035',\n",
       " '1872593713120',\n",
       " '1916894718759',\n",
       " '1887338760527',\n",
       " '1911710713780',\n",
       " '1876383744479',\n",
       " '1835580734953',\n",
       " '1913682718651',\n",
       " '1874886723070',\n",
       " '1884944712035']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[1] for x in outg_rec_nbs_and_trsf_pole_nbs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d765a574-d0d0-4a13-a7b9-5f41b0a956a1",
   "metadata": {},
   "source": [
    "# END DEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a039e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_date   = pd.to_datetime('2023-07-27')\n",
    "# prediction_date   = pd.to_datetime('2023-09-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084f0d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "self = OutagePredictor(prediction_date=prediction_date)\n",
    "#-----\n",
    "self.set_model_dir(r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231201\\Models\\All_EEMSP_agg_Top10_v2_NoMonth')\n",
    "#-----\n",
    "trsf_pole_nbs = [x[1] for x in outg_rec_nbs_and_trsf_pole_nbs]\n",
    "self.set_trsf_pole_nbs(trsf_pole_nbs)\n",
    "# self.set_trsf_pole_nbs_from_sql(\n",
    "#     n_trsf_pole_nbs=10000, \n",
    "#     states='OH'\n",
    "# )\n",
    "# print(f\"n_trsf_pole_nbs = {len(self.trsf_pole_nbs)}\")\n",
    "# print(f\"\\nSQL statement:\\n{self.trsf_pole_nbs_sql}\")\n",
    "#-----\n",
    "self.initialize_data(\n",
    "    evsSum_sql_fcn=AMIEndEvents_SQL.build_sql_end_events,  \n",
    "    evsSum_sql_kwargs=dict(opco='oh'), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94750f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = self.model_clf.predict(self.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd717c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = self.model_clf.predict_proba(self.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc37354",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(y_pred, y_scores[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d54dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "threshold=0.5\n",
    "y_pred_from_proba = [int(x>threshold) for x in y_scores[:, 1]]\n",
    "assert(all(y_pred_from_proba==y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in y_scores[:, 1]]==[x[1] for x in y_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41081bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d79119",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.rcpx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9153e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9bba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_dates = pd.date_range(\n",
    "#     start = pd.to_datetime('2023-07-01'), \n",
    "#     end   = pd.to_datetime('2023-08-01'), \n",
    "#     freq  = pd.Timedelta('1D')\n",
    "# )\n",
    "# prediction_dates\n",
    "\n",
    "prediction_dates = pd.date_range(\n",
    "    start = pd.to_datetime('2023-05-02'), \n",
    "    end   = pd.to_datetime('2023-09-30'), \n",
    "    freq  = pd.Timedelta('1D')\n",
    ")\n",
    "prediction_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cbb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./model_end_events_for_outages_METHODS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5cb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_prdctr = OutagePredictor(prediction_date=prediction_dates[0])\n",
    "#-----\n",
    "outg_prdctr.set_model_dir(r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231201\\Models\\All_EEMSP_agg_Top10_v2_NoMonth')\n",
    "#-----\n",
    "trsf_pole_nbs = [x[1] for x in outg_rec_nbs_and_trsf_pole_nbs]\n",
    "outg_prdctr.set_trsf_pole_nbs(trsf_pole_nbs)\n",
    "# outg_prdctr.set_trsf_pole_nbs_from_sql(\n",
    "#     n_trsf_pole_nbs=10000, \n",
    "#     states='OH'\n",
    "# )\n",
    "# print(f\"n_trsf_pole_nbs = {len(outg_prdctr.trsf_pole_nbs)}\")\n",
    "# print(f\"\\nSQL statement:\\n{outg_prdctr.trsf_pole_nbs_sql}\")\n",
    "#-----\n",
    "outg_prdctr.prep_multiple_prediction_dates(\n",
    "    prediction_dates  = prediction_dates, \n",
    "    predictions_range = None, \n",
    "    evsSum_sql_fcn    = AMIEndEvents_SQL.build_sql_end_events,  \n",
    "    evsSum_sql_kwargs = dict(opco='oh'), \n",
    "    save_args         = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e00ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict = dict()\n",
    "for prediction_date_i in prediction_dates:\n",
    "    print(prediction_date_i)\n",
    "    assert(prediction_date_i not in results_dict)\n",
    "    outg_prdctr.change_prediction_date(prediction_date=prediction_date_i)\n",
    "    y_pred_i   = outg_prdctr.model_clf.predict(outg_prdctr.X_test)\n",
    "    y_scores_i = outg_prdctr.model_clf.predict_proba(outg_prdctr.X_test)\n",
    "    #-----\n",
    "    assert(outg_prdctr.rcpx_df.shape[0]==len(y_pred_i))\n",
    "    assert(outg_prdctr.rcpx_df.shape[0]==len(y_scores_i))\n",
    "    res_i = pd.DataFrame(\n",
    "        index=outg_prdctr.rcpx_df.index, \n",
    "        data=dict(\n",
    "            y_pred=y_pred_i, \n",
    "            y_prob_0=[x[0] for x in y_scores_i], \n",
    "            y_prob_1=[x[1] for x in y_scores_i]\n",
    "        )\n",
    "    )    \n",
    "    #-----\n",
    "    results_dict[prediction_date_i] = res_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eab3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f16add",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = results_dict[pd.to_datetime('2023-07-01 00:00:00')].copy()\n",
    "df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = Utilities_df.prepend_level_to_MultiIndex(\n",
    "    df=df_i, \n",
    "    level_val=pd.to_datetime('2023-07-01 00:00:00'), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0faf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (prediction_date_i, df_i) in enumerate(results_dict.items()):\n",
    "    df_i = Utilities_df.prepend_level_to_MultiIndex(\n",
    "        df=df_i, \n",
    "        level_val=prediction_date_i, \n",
    "        axis=1\n",
    "    )\n",
    "    if i==0:\n",
    "        return_df = df_i.copy()\n",
    "    else:\n",
    "        return_df = pd.merge(\n",
    "            return_df, \n",
    "            df_i, \n",
    "            left_index=True, \n",
    "            right_index=True, \n",
    "            how='outer'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55f7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_by_date_df  = return_df.xs('y_pred', level=1, axis=1)\n",
    "y_prob1_by_date_df = return_df.xs('y_prob_1', level=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "y_prob1_by_date_df.T.plot.line(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349573bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "y_prob1_by_date_df.iloc[:10].T.plot.line(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c3ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d95c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9363b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp_install_time_col = 'inst_ts'\n",
    "df_mp_removal_time_col = 'rmvl_ts'\n",
    "dt_0 = prediction_dates[0]\n",
    "dt_1 = prediction_dates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df = MeterPremise.build_mp_df_curr_hist_for_xfmrs(\n",
    "    trsf_pole_nbs=self.rcpx_df.index.tolist(), \n",
    "    join_curr_hist=True, \n",
    "    addtnl_mp_df_curr_cols=None, \n",
    "    addtnl_mp_df_hist_cols=None, \n",
    "    assume_one_xfmr_per_PN=True, \n",
    "    drop_approx_duplicates=True, \n",
    "    drop_approx_duplicates_args=None, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb'\n",
    ")\n",
    "\n",
    "# Only want meters active at the relevant time period\n",
    "mp_df = mp_df[(mp_df[df_mp_install_time_col]<=pd.to_datetime(dt_0)) & \n",
    "              (mp_df[df_mp_removal_time_col].fillna(pd.Timestamp.max)>pd.to_datetime(dt_1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c62ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df[mp_df['trsf_pole_nb']=='1817472740442']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dovs_df\n",
    "dovs = DOVSOutages(\n",
    "    df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "    contstruct_df_args=None, \n",
    "    init_df_in_constructor=True,\n",
    "    build_sql_function=DOVSOutages_SQL.build_sql_std_outage, \n",
    "    build_sql_function_kwargs=dict(\n",
    "        premise_nbs=mp_df['prem_nb'].unique().tolist(), \n",
    "        date_range=[dt_0, dt_1], \n",
    "        field_to_split='premise_nbs', \n",
    "        include_premise=True\n",
    "    ), \n",
    "    build_consolidated=False\n",
    ")\n",
    "dovs_df = dovs.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8267267",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_rec_nbs = [x[0] for x in outg_rec_nbs_and_trsf_pole_nbs]\n",
    "missing_outg_rec_nbs = list(set(outg_rec_nbs).difference(set(dovs_df['OUTG_REC_NB'].unique().tolist())))\n",
    "if len(missing_outg_rec_nbs) > 0:\n",
    "    addtnl_dovs = DOVSOutages(\n",
    "        df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "        contstruct_df_args=None, \n",
    "        init_df_in_constructor=True,\n",
    "        build_sql_function=DOVSOutages_SQL.build_sql_std_outage, \n",
    "        build_sql_function_kwargs=dict(\n",
    "            outg_rec_nbs=missing_outg_rec_nbs, \n",
    "            field_to_split='outg_rec_nbs', \n",
    "            include_premise=False\n",
    "        ), \n",
    "        build_consolidated=False\n",
    "    )\n",
    "    addtnl_dovs_df = addtnl_dovs.df.copy()\n",
    "    #-----\n",
    "    dovs_df = pd.concat([dovs_df, addtnl_dovs_df])\n",
    "assert(set(outg_rec_nbs).difference(set(dovs_df['OUTG_REC_NB'].unique().tolist()))==set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df = pd.merge(\n",
    "    dovs_df, \n",
    "    mp_df[['prem_nb', 'trsf_pole_nb']].drop_duplicates(), \n",
    "    left_on='PREMISE_NB', \n",
    "    right_on='prem_nb', \n",
    "    how='left'\n",
    ")\n",
    "dovs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set predictions column in self.rcpx_df\n",
    "assert(self.rcpx_df.shape[0]==y_pred.shape[0])\n",
    "self.rcpx_df['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b47e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df_pred1 = mp_df[mp_df['trsf_pole_nb'].isin(self.rcpx_df[self.rcpx_df['y_pred']==1].index.tolist())].copy()\n",
    "dovs_df_pred1 = dovs_df[dovs_df['PREMISE_NB'].isin(mp_df_pred1['prem_nb'].unique().tolist())]\n",
    "#-----\n",
    "mp_df_pred0 = mp_df[mp_df['trsf_pole_nb'].isin(self.rcpx_df[self.rcpx_df['y_pred']==0].index.tolist())].copy()\n",
    "dovs_df_pred0 = dovs_df[dovs_df['PREMISE_NB'].isin(mp_df_pred0['prem_nb'].unique().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num=0\n",
    "\n",
    "trsf_pole_nb_i = y_prob1_by_date_df.index[0]\n",
    "# trsf_pole_nb_i = '1916206716391'\n",
    "\n",
    "fig,ax = plot_pred_probs_for_trsf_i(\n",
    "    fig_num            = fig_num, \n",
    "    trsf_pole_nb_i     = trsf_pole_nb_i, \n",
    "    y_prob1_by_date_df = y_prob1_by_date_df, \n",
    "    dovs_df            = dovs_df, \n",
    "    trsf_pole_nb_col   = 'trsf_pole_nb', \n",
    "    outg_rec_nb_col    = 'OUTG_REC_NB', \n",
    "    dt_off_ts_full_col = 'DT_OFF_TS_FULL', \n",
    "    plot_maxima=True\n",
    ")\n",
    "\n",
    "fig_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c57ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf94d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c234a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b42073",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num=0\n",
    "for i, trsf_pole_nb_i in enumerate(y_prob1_by_date_df.index.unique().tolist()):\n",
    "    outg_rec_nbs_i = [x[0] for x in outg_rec_nbs_and_trsf_pole_nbs if x[1]==trsf_pole_nb_i]\n",
    "    #-----\n",
    "    fig,ax = plot_pred_probs_for_trsf_i(\n",
    "        fig_num            = fig_num, \n",
    "        trsf_pole_nb_i     = trsf_pole_nb_i, \n",
    "        y_prob1_by_date_df = y_prob1_by_date_df, \n",
    "        dovs_df            = dovs_df, \n",
    "        trsf_pole_nb_col   = 'trsf_pole_nb', \n",
    "        outg_rec_nb_col    = 'OUTG_REC_NB', \n",
    "        dt_off_ts_full_col = 'DT_OFF_TS_FULL', \n",
    "        plot_maxima=True\n",
    "    )\n",
    "    #-----\n",
    "    fig_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf163eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_prdctr.model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231201\\Models\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45673ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_infos_df_outg          = pd.read_pickle(os.path.join(r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231201\\Models', 'time_infos_df_outg.pkl'))\n",
    "no_outg_time_infos_df       = pd.read_pickle(os.path.join(r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231201\\Models', 'no_outg_time_infos_df.pkl'))\n",
    "no_outg_time_infos_prstn_df = pd.read_pickle(os.path.join(r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231201\\Models', 'no_outg_time_infos_prstn_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0efb349",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_infos_df_outg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda85a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outg_time_infos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outg_time_infos_prstn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7b494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90eed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7d34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cdc2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0d837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba89ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277b8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# 1A. Make sure the transformer pole numbers have been set (whether done explicitly with\n",
    "#      set_trsf_pole_nbs or via sql with set_trsf_pole_nbs_from_sql)\n",
    "assert(self.trsf_pole_nbs is not None and len(self.trsf_pole_nbs)>0)\n",
    "\n",
    "# 1B. Make sure the model_dir has been set (and, through set_model_dir, all needed components\n",
    "#       have been extracted)\n",
    "assert(self.model_dir is not None and os.path.exists(self.model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# 2. Build events summary (self.evsSum)\n",
    "self.build_events_summary(\n",
    "        evsSum_sql_fcn=AMIEndEvents_SQL.build_sql_end_events,  \n",
    "        evsSum_sql_kwargs=dict(opco='oh'), \n",
    "        init_df_in_constructor=True, \n",
    "        save_args=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44871a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c04dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evsSum_df = self.evsSum_df\n",
    "data_structure_df = self.data_structure_df\n",
    "td_min=self.idk_name_2\n",
    "td_max=self.idk_name_1\n",
    "cr_trans_dict = self.cr_trans_dict\n",
    "trsf_pole_nbs = self.trsf_pole_nbs\n",
    "eemsp_enc = self.eemsp_enc\n",
    "scaler = self.scaler\n",
    "eemsp_mult_strategy = self.eemsp_mult_strategy\n",
    "merge_eemsp = self.merge_eemsp\n",
    "include_month = self.include_month\n",
    "date_range = self.date_range\n",
    "\n",
    "freq='5D'\n",
    "group_cols=['trsf_pole_nb']\n",
    "date_col = 'aep_event_dt'\n",
    "normalize_by_SNs=True\n",
    "normalize_by_time=True\n",
    "\n",
    "include_power_down_minus_up=False\n",
    "regex_patterns_to_remove=['.*cleared.*', '.*Test Mode.*']\n",
    "combine_cpo_df_reasons=True\n",
    "include_n_eemsp=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d94544",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_aws = Utilities.get_athena_prod_aws_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc2791",
   "metadata": {},
   "source": [
    "# BEG DEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7cadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_date = self.prediction_date\n",
    "\n",
    "xf_meter_cnt_col = 'xf_meter_cnt'\n",
    "events_tot_col = 'events_tot'\n",
    "trsf_pole_nb_col = 'trsf_pole_nb'\n",
    "other_reasons_col = 'Other Reasons'\n",
    "total_counts_col = 'total_counts'\n",
    "nSNs_col         = 'nSNs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561374d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_rcpx_from_evsSum_df(\n",
    "        evsSum_df, \n",
    "        data_structure_df, \n",
    "        prediction_date, \n",
    "        td_min, \n",
    "        td_max, \n",
    "        cr_trans_dict, \n",
    "        freq='5D', \n",
    "        group_cols=['trsf_pole_nb'], \n",
    "        date_col='aep_event_dt', \n",
    "        normalize_by_SNs=True, \n",
    "        normalize_by_time=True, \n",
    "        include_power_down_minus_up=False, \n",
    "        regex_patterns_to_remove=['.*cleared.*', '.*Test Mode.*'], \n",
    "        combine_cpo_df_reasons=True, \n",
    "        xf_meter_cnt_col = 'xf_meter_cnt', \n",
    "        events_tot_col = 'events_tot', \n",
    "        trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "        other_reasons_col = 'Other Reasons',  # From data_structure_df\n",
    "        total_counts_col = 'total_counts', \n",
    "        nSNs_col         = 'nSNs', \n",
    "    ):\n",
    "        r\"\"\"\n",
    "        This function only permits uniform time periods; i.e., all periods will have length equal to freq.\n",
    "        If one wants to use variable spacing (e.g., maybe the first group is one day in width, the second is\n",
    "          three days, all others equal to five days), a new function will need to be built.\n",
    "          \n",
    "        NOTE: td_min, td_max, and freq must all be in DAYS\n",
    "        \"\"\"\n",
    "        #--------------------------------------------------\n",
    "        # 0. Need data_structure_df\n",
    "        #     In general, not all curated reasons will be included in the model.\n",
    "        #     Typically, 10 commong curated reasons will be included, and all others will be grouped together in \"Other Reasons\".\n",
    "        #     Furthermore, some reasons may be combined together, others may be completely removed.\n",
    "        #     For these reasons, it is beneficial to have some sample data (taken from when the model was created) to utilize \n",
    "        #       in structuring the new data in the same fashion.\n",
    "        #     Additionally, the data will be used to ensure the ordering of columns is correct before the data are fed into \n",
    "        #       the model.\n",
    "        assert(isinstance(data_structure_df, pd.DataFrame) and data_structure_df.shape[0]>0)\n",
    "\n",
    "        #--------------------------------------------------\n",
    "        #--------------------------------------------------\n",
    "        # 1. Build rcpo_0\n",
    "        #     Construct rcpx_0 by aggregating evsSum_df by group_cols and by freq on date_col\n",
    "        #--------------------------------------------------\n",
    "        evsSum_df = evsSum_df.copy()\n",
    "        #-------------------------\n",
    "        if not isinstance(group_cols, list):\n",
    "            group_cols = [group_cols]\n",
    "        assert(len(set(group_cols).difference(set(evsSum_df.columns.tolist())))==0)\n",
    "        #-------------------------\n",
    "        # Need to set origin in pd.Grouper to ensure proper grouping\n",
    "        freq = pd.Timedelta(freq)\n",
    "        assert((td_max-td_min) % freq==pd.Timedelta(0))\n",
    "        #-----\n",
    "        time_grps = pd.date_range(\n",
    "            start = prediction_date-td_max, \n",
    "            end   = prediction_date-td_min, \n",
    "            freq  = freq\n",
    "        )\n",
    "        time_grps = [(time_grps[i], time_grps[i+1]) for i in range(len(time_grps)-1)]\n",
    "        assert(len(time_grps) == (td_max-td_min)/pd.Timedelta(freq))\n",
    "        #-------------------------\n",
    "        group_freq=pd.Grouper(freq=freq, key=date_col, origin=time_grps[0][0])\n",
    "        #-------------------------\n",
    "        cr_cols = Utilities.find_in_list_with_regex(\n",
    "            lst=evsSum_df.columns.tolist(), \n",
    "            regex_pattern=r'cr\\d*', \n",
    "            ignore_case=False\n",
    "        )\n",
    "        #-----\n",
    "        cols_to_drop = set(evsSum_df.columns.tolist()).difference(\n",
    "            set(cr_cols+group_cols+[date_col, xf_meter_cnt_col, events_tot_col])\n",
    "        )\n",
    "        cols_to_drop = list(cols_to_drop)\n",
    "        #-------------------------\n",
    "        # Make sure date_col is datetime object\n",
    "        evsSum_df[date_col] = pd.to_datetime(evsSum_df[date_col])\n",
    "        \n",
    "        #-------------------------\n",
    "        # No need in wasting time grouping data we won't use\n",
    "        # So, reduce evsSum_df to only the dates we're interested in \n",
    "        evsSum_df = evsSum_df[\n",
    "            (evsSum_df[date_col] >= prediction_date-td_max) & \n",
    "            (evsSum_df[date_col] <= prediction_date-td_min)\n",
    "        ]\n",
    "\n",
    "\n",
    "        # All of the cr# columns will be aggregated with np.sum, as will events_tot_col\n",
    "        # xf_meter_cnt_col will be aggregated using np.max\n",
    "        agg_dict = {col:np.sum for col in cr_cols+[events_tot_col, xf_meter_cnt_col]}\n",
    "        agg_dict[xf_meter_cnt_col] = np.max\n",
    "        #-------------------------\n",
    "        rcpx_0 = evsSum_df.drop(columns=cols_to_drop).groupby(group_cols+[group_freq]).agg(agg_dict)\n",
    "\n",
    "        #--------------------------------------------------\n",
    "        # 2. Grab meter_cnt_per_gp_srs and all_groups\n",
    "        #--------------------------------------------------\n",
    "        # Project out the meter count per group, as it will be used later\n",
    "        #   This information will be stored in the pd.Series object meter_cnt_per_gp_srs, where the index will\n",
    "        #   contain the group_cols\n",
    "        meter_cnt_per_gp_srs = rcpx_0.reset_index()[group_cols+[xf_meter_cnt_col]].drop_duplicates().set_index(group_cols).squeeze()\n",
    "        assert(meter_cnt_per_gp_srs.shape[0]==meter_cnt_per_gp_srs.index.nunique())\n",
    "        meter_cnt_per_gp_srs.name = nSNs_col\n",
    "\n",
    "        # Will also need the unique groups in rcpx_0\n",
    "        #   This will be used later (see no_events_pd_i below)\n",
    "        #   These can be grabbed from the index of rcpx_0 (excluding the date_col level)\n",
    "        all_groups = rcpx_0.droplevel(date_col, axis=0).index.unique().tolist()\n",
    "\n",
    "        #--------------------------------------------------\n",
    "        # 3. Transform rcpx_0 to the form expected by the model\n",
    "        #     i.e., similar to data_structure_df.\n",
    "        #     This is essentially just changing rcpo_0 from long form to wide form\n",
    "        #--------------------------------------------------\n",
    "        #-------------------------\n",
    "        # 3a. Build time_pds_rename\n",
    "        #      Need to convert the time periods, which are currently housed in the date_col index of \n",
    "        #        rcpx_0 from their specific dates to the names expected by the model.\n",
    "        #      In rcpx_0, after grouping by the freq intervals, the values of date_col are equal to the beginning\n",
    "        #        dates of the given interval.\n",
    "        #      These will be converted to the titles contained in final_time_pds below\n",
    "        #      NOTE: This is probably not 100% necessary, but is useful nonetheless\n",
    "        #-------------------------\n",
    "        curr_time_pds = [x[0] for x in time_grps]\n",
    "        time_pds_rename = OutagePredictor.get_time_pds_rename(\n",
    "            curr_time_pds=curr_time_pds, \n",
    "            td_min=td_min, \n",
    "            td_max=td_max, \n",
    "            freq=freq\n",
    "        )\n",
    "        final_time_pds = list(time_pds_rename.values())\n",
    "        # final_time_pds should all be found in data_structure_df to help\n",
    "        #   ensure the alignment between the current data and data used when modelling\n",
    "        assert(set(final_time_pds).difference(data_structure_df.columns.get_level_values(0).unique())==set())\n",
    "        #-------------------------\n",
    "        # Overkill here (since all time windows are of length freq), but something similar will \n",
    "        #   be needed if I want to move to non-uniform period lengths\n",
    "        time_grps_dict = dict()\n",
    "        assert(len(curr_time_pds) == len(time_grps))\n",
    "        # Each element in curr_time_pds should match exactly one of the 0th elements \n",
    "        #   in time_grps (which is a list of length-2 tuples)\n",
    "        # Make sure this is so while building time_grps_dict\n",
    "        for curr_time_pd_i in curr_time_pds:\n",
    "            time_grp_i = [x for x in time_grps if x[0]==curr_time_pd_i]\n",
    "            assert(len(time_grp_i)==1)\n",
    "            assert(curr_time_pd_i not in time_grps_dict.keys())\n",
    "            time_grps_dict[curr_time_pd_i] = time_grp_i[0]\n",
    "\n",
    "        #-------------------------\n",
    "        # 3b. Transform rcpx_0 to the form expected by the model\n",
    "        #      As stated above, this is essentially just changing rcpo_0 from long form to wide form\n",
    "        #      This will probably be formalized further in the future (i.e., function(s) developed to handle)\n",
    "        rename_cols = {\n",
    "            events_tot_col:total_counts_col, \n",
    "            xf_meter_cnt_col:nSNs_col\n",
    "        }\n",
    "        rcpx_0=rcpx_0.rename(columns=rename_cols)\n",
    "        #-------------------------\n",
    "        total_counts_col = total_counts_col\n",
    "        nSNs_col         = nSNs_col\n",
    "        non_reason_cols = [nSNs_col, total_counts_col]\n",
    "        #------------------------- \n",
    "        pd_dfs = []\n",
    "        for date_pd_i in curr_time_pds:\n",
    "            # Grab the proper time period name from final_time_pd_i\n",
    "            final_time_pd_i = time_pds_rename[date_pd_i]\n",
    "            #-------------------------\n",
    "            rcpx_0_pd_i = OutagePredictor.project_time_pd_from_rcpx_0_and_prepare(\n",
    "                rcpx_0                      = rcpx_0, \n",
    "                date_pd_i                   = date_pd_i, \n",
    "                final_time_pd_i             = final_time_pd_i, \n",
    "                data_structure_df           = data_structure_df, \n",
    "                meter_cnt_per_gp_srs        = meter_cnt_per_gp_srs, \n",
    "                all_groups                  = all_groups, \n",
    "                cr_trans_dict               = cr_trans_dict, \n",
    "                non_reason_cols             = non_reason_cols, \n",
    "                other_reasons_col           = other_reasons_col, \n",
    "                group_cols                  = group_cols, \n",
    "                date_col                    = date_col, \n",
    "                regex_patterns_to_remove    = regex_patterns_to_remove, \n",
    "                combine_cpo_df_reasons      = combine_cpo_df_reasons, \n",
    "                include_power_down_minus_up = include_power_down_minus_up, \n",
    "                total_counts_col            = total_counts_col, \n",
    "                nSNs_col                    = nSNs_col\n",
    "            )\n",
    "            #-------------------------\n",
    "            # Overkill here (since all time windows are of length freq), but something similar will \n",
    "            #   be needed if I want to move to non-uniform period lengths\n",
    "            # One could, e.g., simply divide by length of freq in days\n",
    "            if normalize_by_time:\n",
    "                time_grp_i = time_grps_dict[date_pd_i]\n",
    "                #-----\n",
    "                days_min_outg_td_window_i = prediction_date - time_grp_i[1]\n",
    "                days_max_outg_td_window_i = prediction_date - time_grp_i[0]\n",
    "                #-----\n",
    "                OutagePredictor.assert_timedelta_is_days(days_min_outg_td_window_i)\n",
    "                OutagePredictor.assert_timedelta_is_days(days_max_outg_td_window_i)\n",
    "                #-----\n",
    "                days_min_outg_td_window_i = days_min_outg_td_window_i.days\n",
    "                days_max_outg_td_window_i = days_max_outg_td_window_i.days\n",
    "                #-------------------------\n",
    "                rcpx_0_pd_i = MECPODf.normalize_rcpo_df_by_time_interval(\n",
    "                    rcpo_df                 = rcpx_0_pd_i, \n",
    "                    days_min_outg_td_window = days_min_outg_td_window_i, \n",
    "                    days_max_outg_td_window = days_max_outg_td_window_i, \n",
    "                    cols_to_adjust          = None, \n",
    "                    SNs_tags                = None, \n",
    "                    inplace                 = True\n",
    "                )\n",
    "            #-------------------------\n",
    "            pd_dfs.append(rcpx_0_pd_i)\n",
    "\n",
    "        # Make sure all dfs in pd_dfs look correct\n",
    "        shape_0 = pd_dfs[0].shape\n",
    "        index_0 = pd_dfs[0].index\n",
    "        for i in range(len(pd_dfs)):\n",
    "            if i==0:\n",
    "                continue\n",
    "            assert(pd_dfs[i].shape==shape_0)\n",
    "            assert(len(set(index_0).symmetric_difference(set(pd_dfs[i].index)))==0)\n",
    "            #-----\n",
    "            # Aligning the indices is not strictly necessary, as pd.concat should handle that\n",
    "            # But, it's best to be safe\n",
    "            pd_dfs[i] = pd_dfs[i].loc[index_0]\n",
    "\n",
    "        # Build rcpx_final by combining all dfs in pd_dfs\n",
    "        rcpx_final = pd.concat(pd_dfs, axis=1)\n",
    "\n",
    "        # Include back in the number of SNs per group (from meter_cnt_per_gp_srs)\n",
    "        rcpx_final=rcpx_final.merge(\n",
    "            meter_cnt_per_gp_srs.to_frame(name=(nSNs_col, nSNs_col)), \n",
    "            left_index=True, \n",
    "            right_index=True, \n",
    "            how='left'\n",
    "        )\n",
    "        # Sanity check on the merge\n",
    "        assert(rcpx_final[nSNs_col].notna().all().all())\n",
    "\n",
    "        #--------------------------------------------------\n",
    "        # 4. Normalize by nSNs\n",
    "        #--------------------------------------------------\n",
    "        if normalize_by_SNs:\n",
    "            # Kind of silly, but below I cannot simply use 'rcpx_final[final_time_pds] = ...'\n",
    "            #   This will result in: \"ValueError: Columns must be same length as key\", because final_time_pds\n",
    "            #   has only, e.g., 6 elements but rcpx_final[final_time_pds] contains, e.g., 72 columns\n",
    "            # Instead, must use 'rcpx_final[rcpx_final[final_time_pds].columns] = ..'\n",
    "            rcpx_final[rcpx_final[final_time_pds].columns] = rcpx_final[final_time_pds].divide(rcpx_final[(nSNs_col, nSNs_col)], axis=0)\n",
    "\n",
    "        #--------------------------------------------------\n",
    "        return rcpx_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(isinstance(data_structure_df, pd.DataFrame) and data_structure_df.shape[0]>0)\n",
    "\n",
    "#--------------------------------------------------\n",
    "#--------------------------------------------------\n",
    "# 1. Build rcpo_0\n",
    "#     Construct rcpx_0 by aggregating evsSum_df by group_cols and by freq on date_col\n",
    "#--------------------------------------------------\n",
    "evsSum_df = evsSum_df.copy()\n",
    "#-------------------------\n",
    "if not isinstance(group_cols, list):\n",
    "    group_cols = [group_cols]\n",
    "assert(len(set(group_cols).difference(set(evsSum_df.columns.tolist())))==0)\n",
    "#-------------------------\n",
    "# Need to set origin in pd.Grouper to ensure proper grouping\n",
    "freq = pd.Timedelta(freq)\n",
    "assert((td_max-td_min) % freq==pd.Timedelta(0))\n",
    "#-----\n",
    "time_grps = pd.date_range(\n",
    "    start = prediction_date-td_max, \n",
    "    end   = prediction_date-td_min, \n",
    "    freq  = freq\n",
    ")\n",
    "time_grps = [(time_grps[i], time_grps[i+1]) for i in range(len(time_grps)-1)]\n",
    "assert(len(time_grps) == (td_max-td_min)/pd.Timedelta(freq))\n",
    "#-----\n",
    "group_freq=pd.Grouper(freq=freq, key=date_col, origin=time_grps[0][0])\n",
    "#-------------------------\n",
    "cr_cols = Utilities.find_in_list_with_regex(\n",
    "    lst=evsSum_df.columns.tolist(), \n",
    "    regex_pattern=r'cr\\d*', \n",
    "    ignore_case=False\n",
    ")\n",
    "#-----\n",
    "cols_to_drop = set(evsSum_df.columns.tolist()).difference(\n",
    "    set(cr_cols+group_cols+[date_col, xf_meter_cnt_col, events_tot_col])\n",
    ")\n",
    "cols_to_drop = list(cols_to_drop)\n",
    "#-------------------------\n",
    "# Make sure date_col is datetime object\n",
    "evsSum_df[date_col] = pd.to_datetime(evsSum_df[date_col])\n",
    "\n",
    "#-------------------------\n",
    "# No need in wasting time grouping data we won't use\n",
    "# So, reduce evsSum_df to only the dates we're interested in \n",
    "evsSum_df = evsSum_df[\n",
    "    (evsSum_df[date_col] >= prediction_date-td_max) & \n",
    "    (evsSum_df[date_col] <= prediction_date-td_min)\n",
    "]\n",
    "\n",
    "\n",
    "# All of the cr# columns will be aggregated with np.sum, as will events_tot_col\n",
    "# xf_meter_cnt_col will be aggregated using np.max\n",
    "agg_dict = {col:np.sum for col in cr_cols+[events_tot_col, xf_meter_cnt_col]}\n",
    "agg_dict[xf_meter_cnt_col] = np.max\n",
    "#-------------------------\n",
    "rcpx_0 = evsSum_df.drop(columns=cols_to_drop).groupby(group_cols+[group_freq]).agg(agg_dict)\n",
    "\n",
    "#--------------------------------------------------\n",
    "# 2. Grab meter_cnt_per_gp_srs and all_groups\n",
    "#--------------------------------------------------\n",
    "# Project out the meter count per group, as it will be used later\n",
    "#   This information will be stored in the pd.Series object meter_cnt_per_gp_srs, where the index will\n",
    "#   contain the group_cols\n",
    "meter_cnt_per_gp_srs = rcpx_0.reset_index()[group_cols+[xf_meter_cnt_col]].drop_duplicates().set_index(group_cols).squeeze()\n",
    "assert(meter_cnt_per_gp_srs.shape[0]==meter_cnt_per_gp_srs.index.nunique())\n",
    "meter_cnt_per_gp_srs.name = nSNs_col\n",
    "\n",
    "# Will also need the unique groups in rcpx_0\n",
    "#   This will be used later (see no_events_pd_i below)\n",
    "#   These can be grabbed from the index of rcpx_0 (excluding the date_col level)\n",
    "all_groups = rcpx_0.droplevel(date_col, axis=0).index.unique().tolist()\n",
    "\n",
    "#--------------------------------------------------\n",
    "# 3. Transform rcpx_0 to the form expected by the model\n",
    "#     i.e., similar to data_structure_df.\n",
    "#     This is essentially just changing rcpo_0 from long form to wide form\n",
    "#--------------------------------------------------\n",
    "#-------------------------\n",
    "# 3a. Build time_pds_rename\n",
    "#      Need to convert the time periods, which are currently housed in the date_col index of \n",
    "#        rcpx_0 from their specific dates to the names expected by the model.\n",
    "#      In rcpx_0, after grouping by the freq intervals, the values of date_col are equal to the beginning\n",
    "#        dates of the given interval.\n",
    "#      These will be converted to the titles contained in final_time_pds below\n",
    "#      NOTE: This is probably not 100% necessary, but is useful nonetheless\n",
    "#-------------------------\n",
    "curr_time_pds = [x[0] for x in time_grps]\n",
    "time_pds_rename = OutagePredictor.get_time_pds_rename(\n",
    "    curr_time_pds=curr_time_pds, \n",
    "    td_min=td_min, \n",
    "    td_max=td_max, \n",
    "    freq=freq\n",
    ")\n",
    "final_time_pds = list(time_pds_rename.values())\n",
    "# final_time_pds should all be found in data_structure_df to help\n",
    "#   ensure the alignment between the current data and data used when modelling\n",
    "assert(set(final_time_pds).difference(data_structure_df.columns.get_level_values(0).unique())==set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf186e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grps_dict = dict()\n",
    "assert(len(curr_time_pds) == len(time_grps))\n",
    "# Each element in curr_time_pds should match exactly one of the 0th elements \n",
    "#   in time_grps (which is a list of length-2 tuples)\n",
    "# Make sure this is so while building time_grps_dict\n",
    "for curr_time_pd_i in curr_time_pds:\n",
    "    time_grp_i = [x for x in time_grps if x[0]==curr_time_pd_i]\n",
    "    assert(len(time_grp_i)==1)\n",
    "    assert(curr_time_pd_i not in time_grps_dict.keys())\n",
    "    time_grps_dict[curr_time_pd_i] = time_grp_i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661588ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pd_i =  curr_time_pds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fa797",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grp_i = time_grps_dict[date_pd_i]\n",
    "#-----\n",
    "days_min_outg_td_window_i = prediction_date - time_grp_i[1]\n",
    "days_max_outg_td_window_i = prediction_date - time_grp_i[0]\n",
    "#-----\n",
    "OutagePredictor.assert_timedelta_is_days(days_min_outg_td_window_i)\n",
    "OutagePredictor.assert_timedelta_is_days(days_max_outg_td_window_i)\n",
    "#-----\n",
    "days_min_outg_td_window_i = days_min_outg_td_window_i.days\n",
    "days_max_outg_td_window_i = days_max_outg_td_window_i.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4852068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93eeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830bfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c663e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# 3. Build reason counts per x df (self.rcpx_df)\n",
    "rcpx_df_OG = OutagePredictor.build_rcpx_from_evsSum_df(\n",
    "    evsSum_df=evsSum_df, \n",
    "    data_structure_df=data_structure_df, \n",
    "    prediction_date=prediction_date, \n",
    "    td_min=self.idk_name_2, \n",
    "    td_max=self.idk_name_1, \n",
    "    cr_trans_dict=cr_trans_dict, \n",
    "    freq=freq, \n",
    "    group_cols=group_cols, \n",
    "    date_col=date_col, \n",
    "    normalize_by_SNs=normalize_by_SNs, \n",
    "    normalize_by_time=normalize_by_time, \n",
    "    include_power_down_minus_up=include_power_down_minus_up, \n",
    "    regex_patterns_to_remove=regex_patterns_to_remove, \n",
    "    combine_cpo_df_reasons=combine_cpo_df_reasons, \n",
    "    xf_meter_cnt_col = 'xf_meter_cnt', \n",
    "    events_tot_col = 'events_tot', \n",
    "    trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "    other_reasons_col = 'Other Reasons',  # From data_structure_df\n",
    "    total_counts_col = 'total_counts', \n",
    "    nSNs_col         = 'nSNs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1240df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.idk_name_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e233e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.idk_name_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1befbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_df_OG.loc['40820650C10054']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c67a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evsSum_df[evsSum_df['trsf_pole_nb']=='40820650C10054'].sort_values(by=['aep_event_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0d1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        assert(isinstance(data_structure_df, pd.DataFrame) and data_structure_df.shape[0]>0)\n",
    "\n",
    "        #--------------------------------------------------\n",
    "        #--------------------------------------------------\n",
    "        # 1. Build rcpo_0\n",
    "        #     Construct rcpx_0 by aggregating evsSum_df by group_cols and by freq on date_col\n",
    "        #--------------------------------------------------\n",
    "        evsSum_df = evsSum_df.copy()\n",
    "        #-------------------------\n",
    "        if not isinstance(group_cols, list):\n",
    "            group_cols = [group_cols]\n",
    "        assert(len(set(group_cols).difference(set(evsSum_df.columns.tolist())))==0)\n",
    "        #-------------------------\n",
    "        # Need to set origin in pd.Grouper to ensure proper grouping\n",
    "        freq = pd.Timedelta(freq)\n",
    "        assert((td_max-td_min) % freq==pd.Timedelta(0))\n",
    "        #-----\n",
    "        time_grps = pd.date_range(\n",
    "            start = prediction_date-td_max, \n",
    "            end   = prediction_date-td_min, \n",
    "            freq  = freq\n",
    "        )\n",
    "        time_grps = [(time_grps[i], time_grps[i+1]) for i in range(len(time_grps)-1)]\n",
    "        assert(len(time_grps) == (td_max-td_min)/pd.Timedelta(freq))\n",
    "        #-----\n",
    "        group_freq=pd.Grouper(freq=freq, key=date_col, origin=time_grps[0][0])\n",
    "        #-------------------------\n",
    "        cr_cols = Utilities.find_in_list_with_regex(\n",
    "            lst=evsSum_df.columns.tolist(), \n",
    "            regex_pattern=r'cr\\d*', \n",
    "            ignore_case=False\n",
    "        )\n",
    "        #-----\n",
    "        cols_to_drop = set(evsSum_df.columns.tolist()).difference(\n",
    "            set(cr_cols+group_cols+[date_col, xf_meter_cnt_col, events_tot_col])\n",
    "        )\n",
    "        cols_to_drop = list(cols_to_drop)\n",
    "        #-------------------------\n",
    "        # Make sure date_col is datetime object\n",
    "        evsSum_df[date_col] = pd.to_datetime(evsSum_df[date_col])\n",
    "        \n",
    "        #-------------------------\n",
    "        # No need in wasting time grouping data we won't use\n",
    "        # So, reduce evsSum_df to only the dates we're interested in \n",
    "        evsSum_df = evsSum_df[\n",
    "            (evsSum_df[date_col] >= prediction_date-td_max) & \n",
    "            (evsSum_df[date_col] <= prediction_date-td_min)\n",
    "        ]\n",
    "\n",
    "\n",
    "        # All of the cr# columns will be aggregated with np.sum, as will events_tot_col\n",
    "        # xf_meter_cnt_col will be aggregated using np.max\n",
    "        agg_dict = {col:np.sum for col in cr_cols+[events_tot_col, xf_meter_cnt_col]}\n",
    "        agg_dict[xf_meter_cnt_col] = np.max\n",
    "        #-------------------------\n",
    "        rcpx_0 = evsSum_df.drop(columns=cols_to_drop).groupby(group_cols+[group_freq]).agg(agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcbabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_0.loc['40820650C10054']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "evsSum_df[evsSum_df['trsf_pole_nb']=='40820650C10054'].sort_values(by=['aep_event_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ebfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f2161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f0650af",
   "metadata": {},
   "source": [
    "# END DEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# 3. Build reason counts per x df (self.rcpx_df)\n",
    "rcpx_df_OG = OutagePredictor.build_rcpx_from_evsSum_df(\n",
    "    evsSum_df=evsSum_df, \n",
    "    data_structure_df=data_structure_df, \n",
    "    td_min=self.idk_name_2, \n",
    "    td_max=self.idk_name_1, \n",
    "    cr_trans_dict=cr_trans_dict, \n",
    "    freq=freq, \n",
    "    group_cols=group_cols, \n",
    "    date_col=date_col, \n",
    "    normalize_by_SNs=normalize_by_SNs, \n",
    "    normalize_by_time=normalize_by_time, \n",
    "    include_power_down_minus_up=include_power_down_minus_up, \n",
    "    regex_patterns_to_remove=regex_patterns_to_remove, \n",
    "    combine_cpo_df_reasons=combine_cpo_df_reasons, \n",
    "    xf_meter_cnt_col = 'xf_meter_cnt', \n",
    "    events_tot_col = 'events_tot', \n",
    "    trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "    other_reasons_col = 'Other Reasons',  # From data_structure_df\n",
    "    total_counts_col = 'total_counts', \n",
    "    nSNs_col         = 'nSNs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_df = rcpx_df_OG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# 4. If including EEMSP, build and merge with rcpx\n",
    "if merge_eemsp:\n",
    "    rcpx_df, eemsp_df = OutagePredictor.build_eemsp_df_and_merge_rcpx( \n",
    "            rcpx_df=rcpx_df, \n",
    "            trsf_pole_nbs=trsf_pole_nbs, \n",
    "            date_range=date_range, \n",
    "            merge_on_rcpx=['index_0'], \n",
    "            merge_on_eems=['LOCATION_NB'], \n",
    "            conn_aws=None, \n",
    "            mult_strategy=eemsp_mult_strategy, \n",
    "            include_n_eemsp=include_n_eemsp, \n",
    "            cols_of_interest_eemsp=None, \n",
    "            numeric_cols = ['kva_size'], \n",
    "            dt_cols = ['install_dt', 'removal_dt'], \n",
    "            ignore_cols = ['serial_nb'], \n",
    "            batch_size=10000, \n",
    "            verbose=True, \n",
    "            n_update=10, \n",
    "        )\n",
    "    #-----\n",
    "    rcpx_df = OutagePredictor.convert_install_dt_to_years(\n",
    "        rcpx_df=rcpx_df, \n",
    "        prediction_date=prediction_date, \n",
    "        install_dt_col=('EEMSP_0', 'INSTALL_DT'), \n",
    "        assert_col_found=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7acfd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# 5. Include month?\n",
    "if include_month:\n",
    "    rcpx_df = OutagePredictor.add_predict_month_to_rcpx_df(\n",
    "        rcpx_df=rcpx_df, \n",
    "        prediction_date=prediction_date, \n",
    "        month_col=('dummy_lvl_0', 'outg_month'), \n",
    "        dummy_col_levels_prefix='dummy'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# 6. Make sure final form of rcpx_df agrees with data_structure_df\n",
    "rcpx_df = OutagePredictor.assert_rcpx_has_correct_form(\n",
    "    rcpx_df=rcpx_df, \n",
    "    data_structure_df=data_structure_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d53cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# 7. Build X_test\n",
    "X_test_no1 = OutagePredictor.build_X_test(\n",
    "    rcpx_df=rcpx_df, \n",
    "    data_structure_df=data_structure_df, \n",
    "    eemsp_args=dict(eemsp_enc=eemsp_enc), \n",
    "    scaler=scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_df_no1 = rcpx_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d22721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65322b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_test_no1==self.X_test).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3173fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_df_no1.equals(self.rcpx_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bf973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e058468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccecd08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_no1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bac30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = self.model_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# Outages Predicted: {y_pred.sum()}\")\n",
    "print(f\"# Predictions:       {y_pred.shape[0]}\")\n",
    "print(f\"%:                   {100*y_pred.sum()/y_pred.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b088a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_final = rcpx_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set predictions column in rcpx_final\n",
    "assert(rcpx_final.shape[0]==y_pred.shape[0])\n",
    "rcpx_final['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25193b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_final[rcpx_final['y_pred']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45c151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp_install_time_col = 'inst_ts'\n",
    "df_mp_removal_time_col = 'rmvl_ts'\n",
    "dt_0 = prediction_date\n",
    "dt_1 = prediction_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df = MeterPremise.build_mp_df_curr_hist_for_xfmrs(\n",
    "    trsf_pole_nbs=rcpx_final.index.tolist(), \n",
    "    join_curr_hist=True, \n",
    "    addtnl_mp_df_curr_cols=None, \n",
    "    addtnl_mp_df_hist_cols=None, \n",
    "    assume_one_xfmr_per_PN=True, \n",
    "    drop_approx_duplicates=True, \n",
    "    drop_approx_duplicates_args=None, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb'\n",
    ")\n",
    "\n",
    "# Only want meters active at the relevant time period\n",
    "mp_df = mp_df[(mp_df[df_mp_install_time_col]<=pd.to_datetime(dt_0)) & \n",
    "              (mp_df[df_mp_removal_time_col].fillna(pd.Timestamp.max)>pd.to_datetime(dt_1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66857285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dovs_df\n",
    "dovs = DOVSOutages(\n",
    "    df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "    contstruct_df_args=None, \n",
    "    init_df_in_constructor=True,\n",
    "    build_sql_function=DOVSOutages_SQL.build_sql_std_outage, \n",
    "    build_sql_function_kwargs=dict(\n",
    "        premise_nbs=mp_df['prem_nb'].unique().tolist(), \n",
    "        date_range=[\n",
    "            prediction_date-pd.Timedelta('31D'), \n",
    "            prediction_date+pd.Timedelta('31D')\n",
    "        ], \n",
    "        field_to_split='premise_nbs', \n",
    "        include_premise=True\n",
    "    ), \n",
    "    build_consolidated=False\n",
    ")\n",
    "dovs_df = dovs.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f38531",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df = pd.merge(\n",
    "    dovs_df, \n",
    "    mp_df[['prem_nb', 'trsf_pole_nb']].drop_duplicates(), \n",
    "    left_on='PREMISE_NB', \n",
    "    right_on='prem_nb', \n",
    "    how='left'\n",
    ")\n",
    "dovs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df_pred1 = mp_df[mp_df['trsf_pole_nb'].isin(rcpx_final[rcpx_final['y_pred']==1].index.tolist())].copy()\n",
    "dovs_df_pred1 = dovs_df[dovs_df['PREMISE_NB'].isin(mp_df_pred1['prem_nb'].unique().tolist())]\n",
    "#-----\n",
    "mp_df_pred0 = mp_df[mp_df['trsf_pole_nb'].isin(rcpx_final[rcpx_final['y_pred']==0].index.tolist())].copy()\n",
    "dovs_df_pred0 = dovs_df[dovs_df['PREMISE_NB'].isin(mp_df_pred0['prem_nb'].unique().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ef423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_pred1['DT_OFF_TS_FULL'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74091b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_pred0['DT_OFF_TS_FULL'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67ddc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_pred1['DT_OFF_TS_FULL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "natsorted(dovs_df_pred1['DT_OFF_TS_FULL'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred1.plot.scatter(ax=ax, x='DT_OFF_TS_FULL', y='CI_NB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32801d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred1.plot.scatter(ax=ax, x='DT_OFF_TS_FULL', y='CMI_NB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_pred1.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658085d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['OUTG_REC_NB'].count().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred1.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['OUTG_REC_NB'].count().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred0.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['OUTG_REC_NB'].count().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a278e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441883a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred1.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['CI_NB'].sum().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred0.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['CI_NB'].sum().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501cd7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e1a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred1.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['CMI_NB'].sum().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d99c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred0.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['CMI_NB'].sum().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028ba2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8c7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6c5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df['OUTG_REC_NB'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df['trsf_pole_nb'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_final[rcpx_final['y_pred']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dovs_df['trsf_pole_nb'].unique()).intersection(set(rcpx_final[rcpx_final['y_pred']==1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9411dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "natsorted(rcpx_final[rcpx_final['y_pred']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294956d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "natsorted(dovs_df['trsf_pole_nb'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c591171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dovs_df['trsf_pole_nb'].unique()).intersection(set(rcpx_final.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601ead81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d8cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90a6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a635ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df[dovs_df['DT_OFF_TS']=='2023-06-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5376431",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_df_test_i = pd.read_pickle(r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231201\\Models\\All_EEMSP_agg_Top10_v2\\full_data_df_test_i.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda73f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_df_test_i.loc[\n",
    "    (full_data_df_test_i.index.get_level_values(1)=='1870612751127') & \n",
    "    (full_data_df_test_i.index.get_level_values(0).isin(['13382076', '13382693']))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40686f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_df[rcpx_df.index=='1870612751127']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe7fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_df[rcpx_df.index=='1870612751127']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4904d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7d5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtf = full_data_df_test_i.loc[\n",
    "    (full_data_df_test_i.index.get_level_values(1)=='1870612751127') & \n",
    "    (full_data_df_test_i.index.get_level_values(0).isin(['13382076', '13382693']))\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfbdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtf.iloc[0]!=wtf.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtf.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a90db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aeaa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = wtf.iloc[0]!=wtf.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ece4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm[hmm].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bba7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtf[('EEMSP_0', 'INSTALL_DT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtf[hmm[hmm].index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4454081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feefc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4af7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6aab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cab461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f694a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a34e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0da44278",
   "metadata": {},
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# ----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400cfdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe383fee",
   "metadata": {},
   "source": [
    "# SET model_dir TO YOUR LOCAL VALUE!!!!!\n",
    "This directory should house the following files:\n",
    "- forest_clf.joblib\n",
    "- scaler.joblib\n",
    "- eemsp_encoder.joblib\n",
    "- data_structure_df.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a0aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20230615\\Models\\All_EEMSP_agg_Top10_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a527d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4e904af",
   "metadata": {},
   "source": [
    "# Randomly chosen trsf_pole_nbs\n",
    "I randomly choses the trsf_pole_nbs below from a dataset I was working with.\n",
    "</br>The purpose is simply to create a smaller, more manageable, dataset to work with for this demo (as opposed to, e.g., taking all Ohio data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e760a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_aws = Utilities.get_athena_prod_aws_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f937af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trsf_pole_nbs_df = MeterPremise.get_distinct_trsf_pole_nbs(\n",
    "#     conn_aws=conn_aws, \n",
    "#     states='OH'\n",
    "# )\n",
    "# trsf_pole_nbs = trsf_pole_nbs_df.sample(n=10000)['trsf_pole_nb'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trsf_pole_nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07e0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4c327c",
   "metadata": {},
   "source": [
    "# Prediction Date\n",
    "This corresponds essentially to the day on which the model will be run/data evaluated.\n",
    "</br>Data will be collected for a period spanning 31 days before the prediction date up to 1 day before.\n",
    "</br>Eventually, the data will be grouped into the 5-day periods:'01-06 Days', '06-11 Days', '11-16 Days', '16-21 Days', '21-26 Days','26-31 Days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_date = pd.to_datetime('2023-06-01')\n",
    "date_range = [\n",
    "    prediction_date-pd.Timedelta('31D'), \n",
    "    prediction_date-pd.Timedelta('1D')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trsf_pole_nbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ce167",
   "metadata": {},
   "source": [
    "# Grab the data from meter_events.events_summary_vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_aws = Utilities.get_athena_prod_aws_connection()\n",
    "#-----\n",
    "end_events_sql_function_kwargs=dict(\n",
    "    schema_name='meter_events', \n",
    "    table_name='events_summary_vw', \n",
    "    cols_of_interest=['*'], \n",
    "    date_range=date_range, \n",
    "    trsf_pole_nbs=trsf_pole_nbs, \n",
    "    opco='oh'\n",
    ")\n",
    "#-----\n",
    "end_events = AMIEndEvents(\n",
    "    df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "    contstruct_df_args = dict(conn_db=conn_aws), \n",
    "    build_sql_function=AMIEndEvents_SQL.build_sql_end_events, \n",
    "    build_sql_function_kwargs=end_events_sql_function_kwargs, \n",
    "    init_df_in_constructor=True, \n",
    "    save_args=False\n",
    ")\n",
    "ede_df = end_events.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ede_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfab684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ede_df = self.evsSum_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41376294",
   "metadata": {},
   "source": [
    "# Also need meter_events.event_summ_regex_setup\n",
    "to convert the column names in rcpx from cr# to curated reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cr_trans_dict = curated reasons translation dictionary\n",
    "sql = \"\"\"\n",
    "SELECT * FROM meter_events.event_summ_regex_setup\n",
    "\"\"\"\n",
    "regex_setup_df = pd.read_sql(sql, conn_aws, dtype=str)\n",
    "cr_trans_dict = {x[0]:x[1] for x in regex_setup_df[['pivot_id', 'regex_report_title']].values.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_trans_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c60c6",
   "metadata": {},
   "source": [
    "# Build rcpx_0\n",
    "Construct rcpx_0 by aggregating ede_df by trsf_pole_nb and by 5-day frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='5D'\n",
    "group_cols=['trsf_pole_nb']\n",
    "group_freq=pd.Grouper(freq=freq, key='aep_event_dt')\n",
    "#-------------------------\n",
    "# Convert aep_event_dt to datetime object\n",
    "ede_df['aep_event_dt'] = pd.to_datetime(ede_df['aep_event_dt'])\n",
    "\n",
    "# Will no longer need the following columns\n",
    "cols_to_drop = ['serialnumber', 'aep_premise_nb', 'aep_opco']\n",
    "\n",
    "agg_dict = {col:np.sum for col in ede_df.drop(columns=cols_to_drop+['trsf_pole_nb', 'aep_event_dt']).columns.tolist()}\n",
    "agg_dict['xf_meter_cnt'] = np.max\n",
    "#-------------------------\n",
    "rcpx_0 = ede_df.drop(columns=cols_to_drop).groupby(group_cols+[group_freq]).agg(agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04728a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_0.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3d400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e9e984b",
   "metadata": {},
   "source": [
    "# Project out xf_meter_cnt, as it will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "xf_meter_cnt_srs = rcpx_0.droplevel(1, axis=0)['xf_meter_cnt'].reset_index().drop_duplicates().set_index('trsf_pole_nb').squeeze()\n",
    "assert(xf_meter_cnt_srs.shape[0]==xf_meter_cnt_srs.index.nunique())\n",
    "all_trsf_pole_nbs = rcpx_0.index.get_level_values(0).unique().tolist()\n",
    "xf_meter_cnt_srs.name='nSNs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ad0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xf_meter_cnt_srs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb81e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ad9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need data_structure_df\n",
    "# In general, not all curated reasons will be included in the model.\n",
    "# Typically, 10 commong curated reasons will be included, and all others will be grouped together in \"Other Reasons\".\n",
    "# Furthermore, some reasons may be combined together, others may be completely removed.\n",
    "# For these reasons, it is beneficial to have some sample data (taken from when the model was created) to utilize in structuring the new data in the same fashion.\n",
    "# Additionally, the data will be used to ensure the ordering of columns is correct before the data are fed into the model.\n",
    "data_structure_df = pd.read_pickle(os.path.join(model_dir, 'data_structure_df.pkl'))\n",
    "data_structure_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2e616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bf5e5c7",
   "metadata": {},
   "source": [
    "# Transform rcpx_0 to the form expected by the model\n",
    "i.e., similar to data_structure_df.\n",
    "</br>This is essentially just changing rcpo_0 from long form to wide form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2269d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build time_pds_rename\n",
    "#-----\n",
    "# We will need to convert the time periods, which are currently housed in the 'aep_event_dt' index of \n",
    "#   rcpx_0 from their specific dates to the names expected by the model.\n",
    "# In rcpx_0, after grouping by the 5-day intervals, the values of 'aep_event_dt' are equal to the beginning\n",
    "#   dates of the given interval.\n",
    "# These will be converted to the titles contained in final_time_pds below\n",
    "# NOTE: This is probably not 100% necessary, but is useful nonetheless\n",
    "#-------------------------\n",
    "curr_time_pds = natsorted(rcpx_0.index.get_level_values(1).unique())\n",
    "# There should be 6 time periods, each of width 5 days\n",
    "for i in range(len(curr_time_pds)):\n",
    "    if i==0:\n",
    "        continue\n",
    "    assert(curr_time_pds[i]-curr_time_pds[i-1]==pd.Timedelta('5D'))\n",
    "#-----\n",
    "final_time_pds = [\n",
    "    '01-06 Days',\n",
    "    '06-11 Days',\n",
    "    '11-16 Days',\n",
    "    '16-21 Days',\n",
    "    '21-26 Days',\n",
    "    '26-31 Days',\n",
    "]\n",
    "#-----\n",
    "time_pds_rename = dict(zip(curr_time_pds, final_time_pds))\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00f6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated above, this is essentially just changing rcpo_0 from long form to wide form\n",
    "# This will probably be formalized further in the future (i.e., function(s) developed to handle)\n",
    "rename_cols = {\n",
    "    'events_tot':'total_counts', \n",
    "    'xf_meter_cnt':'nSNs'\n",
    "}\n",
    "\n",
    "total_counts_col = 'total_counts'\n",
    "nSNs_col         = 'nSNs'\n",
    "non_reason_cols = [nSNs_col, total_counts_col]\n",
    "\n",
    "include_power_down_minus_up=False\n",
    "#-------------------------\n",
    "rcpx_0=rcpx_0.rename(columns=rename_cols)\n",
    "#-------------------------\n",
    "pd_dfs = []\n",
    "for date_pd_i in curr_time_pds:\n",
    "    # Grab the proper time period name from final_time_pd_i\n",
    "    final_time_pd_i = time_pds_rename[date_pd_i]\n",
    "    #-----\n",
    "    # Get the expected columns for this time period from data_structure_df\n",
    "    final_reason_cols_i = data_structure_df[final_time_pd_i].columns.tolist()\n",
    "    final_reason_cols_i = [x for x in final_reason_cols_i if x not in non_reason_cols+['Other Reasons']]\n",
    "    #-------------------------\n",
    "    # Project out the current time period (date_pd_i) from rcpx_0 by selecting the appropriate\n",
    "    #   values from the 'aep_event_dt' index (i.e., index level 1)\n",
    "    rcpx_0_pd_i = rcpx_0[rcpx_0.index.get_level_values(1)==date_pd_i].copy()\n",
    "    rcpx_0_pd_i = rcpx_0_pd_i.droplevel(1, axis=0)\n",
    "    #-------------------------\n",
    "    # Make sure all trsf_pole_nbs have an entry in rcpx_0_pd_i:\n",
    "    #   If a trsf_pole_nb didn't register any events in a given time period, it will not be included in the projection.\n",
    "    #   However, the final format requires each transformer have entries for each time period\n",
    "    #   Therefore, we identify the trsf_pole_nbs missing from rcpx_0_pd_i (no_events_pd_i) and add approriate rows\n",
    "    #     containing all 0 values for the counts\n",
    "    no_events_pd_i = list(set(all_trsf_pole_nbs).difference(set(rcpx_0_pd_i.index.get_level_values(0).unique())))\n",
    "    no_events_pd_i_df = pd.DataFrame(\n",
    "        columns=rcpx_0.columns, \n",
    "        index=no_events_pd_i, \n",
    "        data=np.zeros((len(no_events_pd_i), rcpx_0.shape[1]))\n",
    "    )\n",
    "    #-----\n",
    "    # Use xf_meter_cnt_srs to fill the 'nSNs' column in no_events_pd_i_df\n",
    "    # NOTE: This is probably not strictly necessary, as the 'nSNs' column won't be used here,\n",
    "    #         since the data are not normalized.\n",
    "    no_events_pd_i_df = no_events_pd_i_df.drop(columns=['nSNs']).merge(\n",
    "        xf_meter_cnt_srs, \n",
    "        left_index=True, \n",
    "        right_index=True, \n",
    "        how='left'\n",
    "    )\n",
    "    # Sanity check on the merge\n",
    "    assert(no_events_pd_i_df['nSNs'].notna().all())\n",
    "    #-----\n",
    "    # Combine rcpx_0_pd_i and no_events_pd_i_df\n",
    "    assert(len(set(rcpx_0_pd_i.columns).symmetric_difference(set(no_events_pd_i_df.columns)))==0)\n",
    "    no_events_pd_i_df = no_events_pd_i_df[rcpx_0_pd_i.columns]\n",
    "    rcpx_0_pd_i = pd.concat([rcpx_0_pd_i, no_events_pd_i_df])\n",
    "    #-------------------------\n",
    "    # Rename the cr# columns to their full curated reasons\n",
    "    rcpx_0_pd_i=rcpx_0_pd_i.rename(columns=cr_trans_dict)\n",
    "    #--------------------------------------------------\n",
    "    #--------------------------------------------------\n",
    "    # Any columns without a curated reason (i.e., those with column name = ''), have not been observed\n",
    "    #   yet in the data, and therefore the sume of the counts should be 0.\n",
    "    # These empty columns are not needed, so drop\n",
    "    assert(rcpx_0_pd_i[''].sum().sum()==0)\n",
    "    rcpx_0_pd_i=rcpx_0_pd_i.drop(columns=[''])\n",
    "    #-------------------------\n",
    "    # Any curated reasons containing 'cleared' or 'Test Mode' or not included in the analysis, so remove\n",
    "    rcpx_0_pd_i = MECPODf.remove_reasons_from_rcpo_df(\n",
    "        rcpo_df=rcpx_0_pd_i, \n",
    "        regex_patterns_to_remove=['.*cleared.*', '.*Test Mode.*'], \n",
    "        ignore_case=True\n",
    "    )\n",
    "    #-----\n",
    "    # After irrelevant cleared and test columns removed, need to recalculate events_tot to accurately\n",
    "    #   reflect the total number of relevant events\n",
    "    assert(total_counts_col in non_reason_cols)\n",
    "    rcpx_0_pd_i[total_counts_col] = rcpx_0_pd_i.drop(columns=non_reason_cols).sum(axis=1)\n",
    "    #-------------------------\n",
    "    # Combine similar reasons (e.g., all 'Tamper' type reasons are combined into 1)\n",
    "    # See MECPODf.combine_cpo_df_reasons for more information\n",
    "    rcpx_0_pd_i = MECPODf.combine_cpo_df_reasons(rcpo_df=rcpx_0_pd_i)\n",
    "    #-------------------------\n",
    "    # Include the difference in power-up and power-down, if desired (typically turned off) \n",
    "    if include_power_down_minus_up:\n",
    "        rcpx_0_pd_i = MECPODf.delta_cpo_df_reasons(\n",
    "            rcpo_df=rcpx_0_pd_i, \n",
    "            reasons_1='Primary Power Down',\n",
    "            reasons_2='Primary Power Up',\n",
    "            delta_reason_name='Power Down Minus Up'\n",
    "        )\n",
    "    #-------------------------\n",
    "    # Make sure rcpx_0_pd_i contains the expected final reason columns.\n",
    "    # Once this is assured, project out these reasons and combine all other reasons into\n",
    "    #   the 'Other Reasons' columns\n",
    "    # See MECPODf.get_reasons_subset_from_cpo_df for more info\n",
    "    assert(len(set(final_reason_cols_i).difference(set(rcpx_0_pd_i.columns.tolist())))==0)\n",
    "    rcpx_0_pd_i = MECPODf.get_reasons_subset_from_cpo_df(\n",
    "        cpo_df=rcpx_0_pd_i, \n",
    "        reasons_to_include=final_reason_cols_i, \n",
    "        combine_others=True, \n",
    "        output_combine_others_col='Other Reasons', \n",
    "        SNs_tags=None, \n",
    "        is_norm=False, \n",
    "        counts_col='nSNs', \n",
    "        normalize_by_nSNs_included=False, \n",
    "        level_0_raw_col = 'counts', \n",
    "        level_0_nrm_col = 'counts_norm', \n",
    "        cols_to_ignore = ['total_counts'], \n",
    "        include_counts_col_in_output=True\n",
    "    )    \n",
    "    #--------------------------------------------------\n",
    "    #--------------------------------------------------\n",
    "    # Don't want nSNs in each pd individually\n",
    "    rcpx_0_pd_i = rcpx_0_pd_i.drop(columns=[nSNs_col])\n",
    "    #-------------------------\n",
    "    # Add the correct time period name as level 0 of the columns\n",
    "    rcpx_0_pd_i = Utilities_df.prepend_level_to_MultiIndex(\n",
    "        df=rcpx_0_pd_i, \n",
    "        level_val=final_time_pd_i, \n",
    "        level_name=None, \n",
    "        axis=1\n",
    "    )\n",
    "    #-------------------------\n",
    "    pd_dfs.append(rcpx_0_pd_i)\n",
    "    \n",
    "# Make sure all dfs in pd_dfs look correct\n",
    "shape_0 = pd_dfs[0].shape\n",
    "index_0 = pd_dfs[0].index\n",
    "for i in range(len(pd_dfs)):\n",
    "    if i==0:\n",
    "        continue\n",
    "    assert(pd_dfs[i].shape==shape_0)\n",
    "    assert(len(set(index_0).symmetric_difference(set(pd_dfs[i].index)))==0)\n",
    "    #-----\n",
    "    # Aligning the indices is not strictly necessary, as pd.concat should handle that\n",
    "    # But, it's best to be safe\n",
    "    pd_dfs[i] = pd_dfs[i].loc[index_0]\n",
    "    \n",
    "# Build rcpx_final by combining all dfs in pd_dfs\n",
    "rcpx_final = pd.concat(pd_dfs, axis=1)\n",
    "\n",
    "# Include back in the number of SNs per transformer (from xf_meter_cnt_srs)\n",
    "rcpx_final=rcpx_final.merge(\n",
    "    xf_meter_cnt_srs.to_frame(name=('nSNs', 'nSNs')), \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    how='left'\n",
    ")\n",
    "# Sanity check on the merge\n",
    "assert(rcpx_final['nSNs'].notna().all().all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401148db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9278f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3912673",
   "metadata": {},
   "source": [
    "# Normalize by nSNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kind of silly, but below I cannot simply use 'rcpx_final[final_time_pds] = ...'\n",
    "#   This will result in: \"ValueError: Columns must be same length as key\", because final_time_pds\n",
    "#   has only, e.g., 6 elements but rcpx_final[final_time_pds] contains, e.g., 72 columns\n",
    "# Instead, must use 'rcpx_final[rcpx_final[final_time_pds].columns] = ..'\n",
    "rcpx_final[rcpx_final[final_time_pds].columns] = rcpx_final[final_time_pds].divide(rcpx_final[('nSNs', 'nSNs')], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8472e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10c1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2246ee7",
   "metadata": {},
   "source": [
    "# Build EEMSP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79315f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_aws = Utilities.get_athena_prod_aws_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682519d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_eemsp = True\n",
    "mult_strategy='agg'\n",
    "#-------------------------\n",
    "cols_of_interest_eemsp = [\n",
    "    'location_nb', \n",
    "    'mfgr_nm', \n",
    "    'install_dt', \n",
    "    'last_trans_desc', \n",
    "    'eqtype_id', \n",
    "    'coolant', \n",
    "    'info', \n",
    "    'kva_size',\n",
    "    'phase_cnt', \n",
    "    'prim_voltage', \n",
    "    'protection', \n",
    "    'pru_number', \n",
    "    'sec_voltage', \n",
    "    'special_char', \n",
    "    'taps', \n",
    "    'xftype'\n",
    "]\n",
    "cols_of_interest_eemsp_full = cols_of_interest_eemsp + ['latest_status', 'removal_dt', 'serial_nb']\n",
    "#-------------------------\n",
    "sql_EEMSP = \"\"\"\n",
    "SELECT {} \n",
    "FROM meter_events.eems_transformer_nameplate\n",
    "WHERE location_nb IN ({})\n",
    "AND install_dt <= '{}'\n",
    "AND (removal_dt IS NULL OR removal_dt > '{}')\n",
    "\"\"\".format(\n",
    "    Utilities_sql.join_list(cols_of_interest_eemsp_full, quotes_needed=False), \n",
    "    Utilities_sql.join_list(trsf_pole_nbs, quotes_needed=True), \n",
    "    date_range[0], \n",
    "    date_range[1]\n",
    ")\n",
    "print(sql_EEMSP)\n",
    "#-------------------------\n",
    "df_eemsp = pd.read_sql_query(sql_EEMSP, conn_aws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eemsp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f4c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb9c4ff1",
   "metadata": {},
   "source": [
    "# Reduce down df_eemsp so there is a single entry for each transformer\n",
    "reduce1_eemsp_for_outg_trsf reduces df_eemsp down to contain only entries for transformers which were active during the date(s) in question.\n",
    "</br>No need to run reduce1_eemsp_for_outg_trsf for this case, as all share the same date restrictions which were already imposed in sql_EEMSP.\n",
    "</br>(For model development/training, this step would be necessary, as the data utilized there have many different date restrictions, and df_eemsp cannot simply be built with the date restrictions)\n",
    "\n",
    "reduce2_eemsp_for_outg_trsf futher reduces df_eemsp down so there is a single entry for each transformer.\n",
    "</br>How exactly this is achieved is dictated mainly by the \"mult_strategy\" parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60797c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce2_eemsp_for_outg_trsf was designed to be used with outg_rec_nb/no_outg_rec_nb.\n",
    "# outg_rec_nb is not necessary here, but we need a temporary column anyway to make the function happy.\n",
    "# I'll update the code in the future so this unnecessary step won't be needed\n",
    "df_eemsp['outg_rec_nb'] = df_eemsp['location_nb']\n",
    "#-----\n",
    "df_eemsp_reduce2 = reduce2_eemsp_for_outg_trsf_OLD(\n",
    "    df_eemsp=df_eemsp, \n",
    "    mult_strategy='agg', \n",
    "    include_n_eemsp=True, \n",
    "    outg_rec_nb_col='outg_rec_nb', \n",
    "    location_nb_col='location_nb', \n",
    "    numeric_cols = ['kva_size'], \n",
    "    dt_cols = ['install_dt', 'removal_dt'], \n",
    "    ignore_cols = ['serial_nb'], \n",
    "    cat_cols_as_strings=True\n",
    ")\n",
    "#-------------------------\n",
    "# No matter of the mult_strategy used, at this point df_eemsp_reduce2 should only have a single\n",
    "#   entry for each outg_rec_nb, location_nb pair\n",
    "assert(all(df_eemsp_reduce2[['outg_rec_nb', 'location_nb']].value_counts()==1))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Clean up df_eemsp_reduce2 and merge with rcpx_final\n",
    "#--------------------------------------------------\n",
    "# Can't simply take df_eemsp_reduce2[cols_of_interest_eemsp] because we need also the new column\n",
    "#   OUTG_REC_NB_TO_MERGE (and any others which may be added in the future)\n",
    "cols_to_drop = list(set(cols_of_interest_eemsp_full).difference(set(cols_of_interest_eemsp)))\n",
    "cols_to_drop = [x for x in cols_to_drop if x in df_eemsp_reduce2.columns]\n",
    "if len(cols_to_drop)>0:\n",
    "    df_eemsp_reduce2 = df_eemsp_reduce2.drop(columns=cols_to_drop)\n",
    "#-------------------------\n",
    "assert(df_eemsp_reduce2.shape[0]==df_eemsp_reduce2.groupby(['outg_rec_nb', 'location_nb']).ngroups)\n",
    "print(f\"df_eemsp_reduce2['location_nb'].nunique() = {df_eemsp_reduce2['location_nb'].nunique()}\")\n",
    "print(f\"len(trsf_pole_nbs)                        = {len(trsf_pole_nbs)}\")\n",
    "print(f\"Diff                                      = {len(trsf_pole_nbs)-df_eemsp_reduce2['location_nb'].nunique()}\")\n",
    "print()\n",
    "#-------------------------\n",
    "# Make all EEMSP columns (except n_eemsp) uppercase to match what was done in model development (where EEMSP)\n",
    "#   data were grabbed from the Oracle database, and columns were all uppercase)\n",
    "df_eemsp_reduce2 = Utilities_df.make_all_column_names_uppercase(df_eemsp_reduce2, cols_to_exclude=['n_eemsp'])\n",
    "\n",
    "# Similar to the case with 'outg_rec_nb' column in df_eemsp above, merge_rcpx_with_eemsp was designed to be \n",
    "#   used with outg_rec_nb/no_outg_rec_nb.\n",
    "# As such, rcpx_final needs an additional column (in this case, it is easier to add another level to the index)\n",
    "# I'll update the code in the future so this unnecessary step won't be needed\n",
    "rcpx_final = rcpx_final.set_index([rcpx_final.index, rcpx_final.index])\n",
    "#-------------------------\n",
    "print(\"\\nShapes BEFORE merging\")\n",
    "print(f\"rcpx_final.shape = {rcpx_final.shape}\")\n",
    "#-------------------------\n",
    "rcpx_final = merge_rcpx_with_eemsp_OLD(\n",
    "    df_rcpx=rcpx_final, \n",
    "    df_eemsp=df_eemsp_reduce2, \n",
    "    outg_rec_nb_idfr_rcpx ='index_0', \n",
    "    trsf_pole_nb_idfr_rcpx='index_1', \n",
    "    outg_rec_nb_idfr_eemsp='OUTG_REC_NB', \n",
    "    location_nb_idfr_eemsp='LOCATION_NB', \n",
    "    set_index=True\n",
    ")\n",
    "#-------------------------\n",
    "print(\"\\nShapes AFTER merging\")\n",
    "print(f\"rcpx_final.shape = {rcpx_final.shape}\")\n",
    "#-------------------------\n",
    "# Drop the unnecessary index level that was added above and is no longer needed\n",
    "rcpx_final=rcpx_final.droplevel(0, axis=0)\n",
    "\n",
    "# Convert INSTALL_DT to age in years\n",
    "rcpx_final[('EEMSP_0', 'INSTALL_DT')] = (prediction_date-rcpx_final[('EEMSP_0', 'INSTALL_DT')]).dt.total_seconds()/(60*60*24*365)\n",
    "\n",
    "# Add month\n",
    "rcpx_final[('dummy_lvl_0', 'outg_month')] = prediction_date.month\n",
    "#-------------------------\n",
    "# Make sure rcpx_final has the correct columns in the correct order\n",
    "assert(len(set(data_structure_df.columns).symmetric_difference(set(rcpx_final.columns)))==0)\n",
    "rcpx_final=rcpx_final[data_structure_df.columns]\n",
    "X_test = rcpx_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_final.equals(rcpx_df_no1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35219b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580fb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8495b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87996e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdcf754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e08c24d1",
   "metadata": {},
   "source": [
    "# Load Model and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa4b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = joblib.load(os.path.join(model_dir, 'forest_clf.joblib'))\n",
    "scaler     = joblib.load(os.path.join(model_dir, 'scaler.joblib'))\n",
    "eemsp_enc  = joblib.load(os.path.join(model_dir, 'eemsp_encoder.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2da13",
   "metadata": {},
   "source": [
    "# Transformations/scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "cols_to_encode = data_structure_df['EEMSP_0'].columns\n",
    "numeric_cols = ['KVA_SIZE', 'INSTALL_DT']\n",
    "cols_to_encode = [x for x in cols_to_encode if x not in numeric_cols]\n",
    "assert(len(set(eemsp_enc.feature_names_in_).symmetric_difference(cols_to_encode))==0)\n",
    "assert(set(X_test['EEMSP_0'].columns).difference(eemsp_enc.feature_names_in_)==set(numeric_cols))\n",
    "#-----\n",
    "cols_to_encode = [('EEMSP_0', x) for x in cols_to_encode if x not in numeric_cols]\n",
    "X_test[cols_to_encode] = X_test[cols_to_encode].astype(str)\n",
    "X_test[cols_to_encode] = eemsp_enc.transform(X_test[cols_to_encode].droplevel(0, axis=1))\n",
    "#----------\n",
    "X_test = scaler.transform(X_test)\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b6912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b06895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5dbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# Outages Predicted: {y_pred.sum()}\")\n",
    "print(f\"# Predictions:       {y_pred.shape[0]}\")\n",
    "print(f\"%:                   {100*y_pred.sum()/y_pred.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e282d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dfda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set predictions column in rcpx_final\n",
    "assert(rcpx_final.shape[0]==y_pred.shape[0])\n",
    "rcpx_final['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab28bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_final[rcpx_final['y_pred']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fe777",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92feff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp_install_time_col = 'inst_ts'\n",
    "df_mp_removal_time_col = 'rmvl_ts'\n",
    "dt_0 = prediction_date\n",
    "dt_1 = prediction_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df = MeterPremise.build_mp_df_curr_hist_for_xfmrs(\n",
    "    trsf_pole_nbs=rcpx_final.index.tolist(), \n",
    "    join_curr_hist=True, \n",
    "    addtnl_mp_df_curr_cols=None, \n",
    "    addtnl_mp_df_hist_cols=None, \n",
    "    assume_one_xfmr_per_PN=True, \n",
    "    drop_approx_duplicates=True, \n",
    "    drop_approx_duplicates_args=None, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb'\n",
    ")\n",
    "\n",
    "# Only want meters active at the relevant time period\n",
    "mp_df = mp_df[(mp_df[df_mp_install_time_col]<=pd.to_datetime(dt_0)) & \n",
    "              (mp_df[df_mp_removal_time_col].fillna(pd.Timestamp.max)>pd.to_datetime(dt_1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b21ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dovs_df\n",
    "dovs = DOVSOutages(\n",
    "    df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "    contstruct_df_args=None, \n",
    "    init_df_in_constructor=True,\n",
    "    build_sql_function=DOVSOutages_SQL.build_sql_std_outage, \n",
    "    build_sql_function_kwargs=dict(\n",
    "        premise_nbs=mp_df['prem_nb'].unique().tolist(), \n",
    "        date_range=[\n",
    "            prediction_date-pd.Timedelta('31D'), \n",
    "            prediction_date+pd.Timedelta('31D')\n",
    "        ], \n",
    "        field_to_split='premise_nbs', \n",
    "        include_premise=True\n",
    "    ), \n",
    "    build_consolidated=False\n",
    ")\n",
    "dovs_df = dovs.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df = pd.merge(\n",
    "    dovs_df, \n",
    "    mp_df[['prem_nb', 'trsf_pole_nb']].drop_duplicates(), \n",
    "    left_on='PREMISE_NB', \n",
    "    right_on='prem_nb', \n",
    "    how='left'\n",
    ")\n",
    "dovs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df_pred1 = mp_df[mp_df['trsf_pole_nb'].isin(rcpx_final[rcpx_final['y_pred']==1].index.tolist())].copy()\n",
    "dovs_df_pred1 = dovs_df[dovs_df['PREMISE_NB'].isin(mp_df_pred1['prem_nb'].unique().tolist())]\n",
    "#-----\n",
    "mp_df_pred0 = mp_df[mp_df['trsf_pole_nb'].isin(rcpx_final[rcpx_final['y_pred']==0].index.tolist())].copy()\n",
    "dovs_df_pred0 = dovs_df[dovs_df['PREMISE_NB'].isin(mp_df_pred0['prem_nb'].unique().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fbb403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_pred1['DT_OFF_TS_FULL'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_pred0['DT_OFF_TS_FULL'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611c17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_pred1['DT_OFF_TS_FULL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a61a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "natsorted(dovs_df_pred1['DT_OFF_TS_FULL'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ba83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred1.plot.scatter(ax=ax, x='DT_OFF_TS_FULL', y='CI_NB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred1.plot.scatter(ax=ax, x='DT_OFF_TS_FULL', y='CMI_NB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201071d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_pred1.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f157e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred1.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['OUTG_REC_NB'].count().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbc08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred0.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['OUTG_REC_NB'].count().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605b79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a14814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred1.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['CI_NB'].sum().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b430c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred0.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['CI_NB'].sum().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb2f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a57697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc459532",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred1.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['CMI_NB'].sum().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aacdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = Plot_General.default_subplots()\n",
    "dovs_df_pred0.groupby(pd.Grouper(freq='1D', key='DT_OFF_TS_FULL'))['CMI_NB'].sum().plot(ax=ax, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097b8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457628e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ab4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df['OUTG_REC_NB'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a95c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df['trsf_pole_nb'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416747fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpx_final[rcpx_final['y_pred']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dovs_df['trsf_pole_nb'].unique()).intersection(set(rcpx_final[rcpx_final['y_pred']==1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e3fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "natsorted(rcpx_final[rcpx_final['y_pred']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "natsorted(dovs_df['trsf_pole_nb'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbaead9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6170db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dovs_df['trsf_pole_nb'].unique()).intersection(set(rcpx_final.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63b961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73891e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9dc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a98c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039677d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b836f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OG = GenAn.read_df_from_csv_dir_batches(\n",
    "    files_dir=r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231005\\20230401_20230930\\Outgs_Full\\EndEvents', \n",
    "    file_path_glob=r'end_events_[0-9]*.csv', \n",
    "    file_path_regex=None, \n",
    "    cols_and_types_to_convert_dict=None, \n",
    "    to_numeric_errors='coerce', \n",
    "    drop_unnamed0_col=True, \n",
    "    pd_read_csv_kwargs={}, \n",
    "    assert_all_cols_equal=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_OG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6267dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_rec_nbs = df['OUTG_REC_NB_GPD_FOR_SQL'].unique().tolist()\n",
    "dovs = DOVSOutages(\n",
    "    df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "    contstruct_df_args=None, \n",
    "    init_df_in_constructor=True,\n",
    "    build_sql_function=DOVSOutages_SQL.build_sql_std_outage, \n",
    "    build_sql_function_kwargs=dict(\n",
    "        outg_rec_nbs=outg_rec_nbs, \n",
    "        field_to_split='outg_rec_nbs', \n",
    "        include_premise=True\n",
    "    ), \n",
    "    build_consolidated=False\n",
    ")\n",
    "dovs_df = dovs.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755421ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df = dovs.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4a70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_len = df.shape[0]\n",
    "#-----\n",
    "df = pd.merge(\n",
    "    df, \n",
    "    dovs_df[['OUTG_REC_NB', 'PREMISE_NB', 'DT_OFF_TS_FULL', 'DT_ON_TS']], \n",
    "    left_on=['OUTG_REC_NB_GPD_FOR_SQL', 'aep_premise_nb'], \n",
    "    right_on=['OUTG_REC_NB', 'PREMISE_NB'], \n",
    "    how='left'\n",
    ")\n",
    "#-----\n",
    "assert(df.shape[0]==og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ed55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7ba5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.rcpx_df = OutagePredictor.build_rcpx_from_evsSum_df(\n",
    "            evsSum_df                   = self.evsSum_df, \n",
    "            data_structure_df           = self.data_structure_df, \n",
    "            td_min                      = self.idk_name_2, \n",
    "            td_max                      = self.idk_name_1, \n",
    "            cr_trans_dict               = self.cr_trans_dict, \n",
    "            freq                        = freq, \n",
    "            group_cols                  = group_cols, \n",
    "            date_col                    = date_col, \n",
    "            normalize_by_SNs            = normalize_by_SNs, \n",
    "            include_power_down_minus_up = include_power_down_minus_up, \n",
    "            regex_patterns_to_remove    = regex_patterns_to_remove, \n",
    "            combine_cpo_df_reasons      = combine_cpo_df_reasons, \n",
    "            xf_meter_cnt_col            = 'xf_meter_cnt', \n",
    "            events_tot_col              = 'events_tot', \n",
    "            trsf_pole_nb_col            = 'trsf_pole_nb', \n",
    "            other_reasons_col           = 'Other Reasons', \n",
    "            total_counts_col            = 'total_counts', \n",
    "            nSNs_col                    = 'nSNs'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f998eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "        freq                        = '5D', \n",
    "        group_cols                  = ['trsf_pole_nb'], \n",
    "        date_col                    = 'aep_event_dt', \n",
    "        normalize_by_SNs            = True, \n",
    "        include_power_down_minus_up = False, \n",
    "        regex_patterns_to_remove    = ['.*cleared.*', '.*Test Mode.*'], \n",
    "        combine_cpo_df_reasons      = True, \n",
    "        include_n_eemsp             = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db579335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c5e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e452ffa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f1196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_events_summary_df_from_csvs(\n",
    "    files_dir, \n",
    "    file_path_glob, \n",
    "    file_path_regex, \n",
    "    batch_kwargs=None, \n",
    "    cols_and_types_to_convert_dict=None, \n",
    "    to_numeric_errors='coerce', \n",
    "    assert_all_cols_equal=True, \n",
    "    verbose=True, \n",
    "    n_update=1,\n",
    "):\n",
    "    r\"\"\"\n",
    "    \n",
    "    batch_kwargs:\n",
    "        Keys and default values:\n",
    "            batch_size_MB        = 1024\n",
    "            tolerance_pct        = 0.01\n",
    "            absorb_last_pair_pct = None\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    paths = Utilities.find_all_paths(\n",
    "        base_dir      = files_dir, \n",
    "        glob_pattern  = file_path_glob, \n",
    "        regex_pattern = file_path_regex\n",
    "    )\n",
    "    if len(paths)==0:\n",
    "        print(f'No paths found in files_dir = {files_dir}')\n",
    "        return None\n",
    "    paths=natsorted(paths)    \n",
    "    #--------------------------------------------------\n",
    "    # Find the smallest file in paths and check to see if it is empty\n",
    "    # If it is found to be empty, any files of that size can be skipped\n",
    "    min_size, min_file = Utilities.get_smallest_file_size_MB(\n",
    "        paths           = paths, \n",
    "        return_min_file = True\n",
    "    )\n",
    "    smallest_df = pd.read_csv(min_file)\n",
    "    # If smallest_df is not empty, set min_size equal to None so that\n",
    "    #   no files will be skipped\n",
    "    if smallest_df.shape[0]!=0:\n",
    "        min_size = None\n",
    "    #--------------------------------------------------\n",
    "    if batch_kwargs is None:\n",
    "        batch_kwargs = {}\n",
    "    assert(isinstance(batch_kwargs, dict))\n",
    "    #-----\n",
    "    batch_idxs = Utilities.get_files_split_locations(\n",
    "        paths                = paths, \n",
    "        batch_size_MB        = batch_kwargs.get('batch_size_MB',        1024), \n",
    "        tolerance_pct        = batch_kwargs.get('tolerance_pct',        0.01), \n",
    "        absorb_last_pair_pct = batch_kwargs.get('absorb_last_pair_pct', None)\n",
    "    )\n",
    "    n_batches = len(batch_idxs)\n",
    "    #-------------------------\n",
    "    if verbose:\n",
    "        print(f'n_paths       = {len(paths)}')\n",
    "        print(f'batch_size_MB = {batch_kwargs.get(\"batch_size_MB\", 1024)}')\n",
    "        print(f'n_batches     = {n_batches}')\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    evsSum_dfs = []\n",
    "    for i, batch_i in enumerate(batch_idxs):\n",
    "        if verbose and (i+1)%n_update==0:\n",
    "            print(f'{i+1}/{n_batches}')\n",
    "        i_beg = batch_i[0]\n",
    "        i_end = batch_i[1]\n",
    "        #-----\n",
    "        evsSum_df_i = GenAn.read_df_from_csv_batch(\n",
    "            paths                          = paths[i_beg:i_end], \n",
    "            cols_and_types_to_convert_dict = cols_and_types_to_convert_dict, \n",
    "            to_numeric_errors              = to_numeric_errors, \n",
    "            drop_na_rows_when_exception    = True, \n",
    "            drop_unnamed0_col              = True, \n",
    "            pd_read_csv_kwargs             = None, \n",
    "            make_all_columns_lowercase     = False, \n",
    "            assert_all_cols_equal          = assert_all_cols_equal, \n",
    "            min_fsize_MB                   = min_size\n",
    "        )\n",
    "        if evsSum_df_i.shape[0]>0:\n",
    "            evsSum_dfs.append(evsSum_df_i)\n",
    "    #-------------------------        \n",
    "    evsSum_cols = evsSum_dfs[0].columns.tolist()\n",
    "    for i_df in range(len(evsSum_dfs)):\n",
    "        # Make sure columns are same\n",
    "        assert(set(evsSum_dfs[i_df].columns.tolist()).symmetric_difference(set(evsSum_cols))==set())\n",
    "        # Make sure order is same\n",
    "        evsSum_dfs[i_df] = evsSum_dfs[i_df][evsSum_cols]\n",
    "    #-------------------------\n",
    "    evsSum_df = pd.concat(evsSum_dfs)\n",
    "    return evsSum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb187d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922de2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir       = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231004\\20230401_20230930\\Outgs_Full\\EndEvents'\n",
    "files_dir_2       = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231005\\20230401_20230930\\Outgs_Full\\EndEvents'\n",
    "file_path_glob  = r'end_events_[0-9]*.csv'\n",
    "file_path_regex = None\n",
    "\n",
    "# batch_kwargs = None\n",
    "batch_kwargs = dict(\n",
    "    batch_size_MB = 64\n",
    "#     batch_size_MB = 5.623506\n",
    ")\n",
    "\n",
    "cols_and_types_to_convert_dict=None\n",
    "to_numeric_errors='coerce'\n",
    "assert_all_cols_equal=True\n",
    "verbose=True\n",
    "n_update=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e922635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "evsSum_df = build_events_summary_df_from_csvs(\n",
    "    files_dir=files_dir, \n",
    "    file_path_glob=file_path_glob, \n",
    "    file_path_regex=file_path_regex, \n",
    "    batch_kwargs=batch_kwargs, \n",
    "    cols_and_types_to_convert_dict=cols_and_types_to_convert_dict, \n",
    "    to_numeric_errors=to_numeric_errors, \n",
    "    assert_all_cols_equal=assert_all_cols_equal, \n",
    "    verbose=verbose, \n",
    "    n_update=n_update,\n",
    ")\n",
    "# evsSum_df.to_pickle(r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231004\\20230401_20230930\\Outgs_Full\\evsSum_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ac728",
   "metadata": {},
   "outputs": [],
   "source": [
    "evsSum_df_2 = build_events_summary_df_from_csvs(\n",
    "    files_dir=files_dir_2, \n",
    "    file_path_glob=file_path_glob, \n",
    "    file_path_regex=file_path_regex, \n",
    "    batch_kwargs=batch_kwargs, \n",
    "    cols_and_types_to_convert_dict=cols_and_types_to_convert_dict, \n",
    "    to_numeric_errors=to_numeric_errors, \n",
    "    assert_all_cols_equal=assert_all_cols_equal, \n",
    "    verbose=verbose, \n",
    "    n_update=n_update,\n",
    ")\n",
    "# evsSum_df_2.to_pickle(r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20231005\\20230401_20230930\\Outgs_Full\\evsSum_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb12a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff352531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evsSum_df_1.shape[0])\n",
    "print(evsSum_df_2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18812c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "evsSum_df_2.shape[0]-evsSum_df_1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evsSum_df_1 = evsSum_df_1.sort_values(by=['OUTG_REC_NB_GPD_FOR_SQL', 'trsf_pole_nb', 'serialnumber', 'aep_event_dt'], ignore_index=True)\n",
    "evsSum_df_2 = evsSum_df_2.sort_values(by=['OUTG_REC_NB_GPD_FOR_SQL', 'trsf_pole_nb', 'serialnumber', 'aep_event_dt'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d787d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_1 = list(evsSum_df_1.groupby(['serialnumber', 'trsf_pole_nb', 'OUTG_REC_NB_GPD_FOR_SQL']).groups.keys())\n",
    "gps_2 = list(evsSum_df_2.groupby(['serialnumber', 'trsf_pole_nb', 'OUTG_REC_NB_GPD_FOR_SQL']).groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(gps_1).symmetric_difference(set(gps_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_gps = list(set(gps_1).intersection(set(gps_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca749c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlap_1 = evsSum_df_1.groupby(['serialnumber', 'trsf_polae_nb', 'OUTG_REC_NB_GPD_FOR_SQL']).apply(lambda x: x.name in overlap_gps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "\n",
    "overlap_1 = evsSum_df_1[\n",
    "    (evsSum_df_1['serialnumber']            == overlap_gps[idx][0]) & \n",
    "    (evsSum_df_1['trsf_pole_nb']            == overlap_gps[idx][1]) & \n",
    "    (evsSum_df_1['OUTG_REC_NB_GPD_FOR_SQL'] == overlap_gps[idx][2])\n",
    "][overlap_2.columns]\n",
    "\n",
    "overlap_2 = evsSum_df_2[\n",
    "    (evsSum_df_2['serialnumber']            == overlap_gps[idx][0]) & \n",
    "    (evsSum_df_2['trsf_pole_nb']            == overlap_gps[idx][1]) & \n",
    "    (evsSum_df_2['OUTG_REC_NB_GPD_FOR_SQL'] == overlap_gps[idx][2])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e025d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e43ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_1.equals(overlap_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92957d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_1 = evsSum_df_1.set_index(['serialnumber', 'trsf_pole_nb', 'OUTG_REC_NB_GPD_FOR_SQL']).loc[overlap_gps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_2 = evsSum_df_2.set_index(['serialnumber', 'trsf_pole_nb', 'OUTG_REC_NB_GPD_FOR_SQL']).loc[overlap_gps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0064a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_1.equals(overlap_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d8a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame( \n",
    "    { \n",
    "        \"Date\": [ \n",
    "            pd.Timestamp(\"2000-11-02\"), \n",
    "            pd.Timestamp(\"2000-11-03\"), \n",
    "            pd.Timestamp(\"2000-11-04\"), \n",
    "            pd.Timestamp(\"2000-11-05\"), \n",
    "            pd.Timestamp(\"2000-11-06\"), \n",
    "            pd.Timestamp(\"2000-11-07\") \n",
    "        ], \n",
    "        \"ID\": [1, 2, 3, 4, 5, 6], \n",
    "        \"Price\": [140, 120, 230, 40, 100, 450] \n",
    "    } \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.Grouper(key='Date', axis=0,  \n",
    "                      freq='2D', sort=True)).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22990a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa457f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fea12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28a8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c40f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a962f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad197dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa44b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig_num=0\n",
    "for trsf_pole_nb_i in y_prob1_by_date_df.index.unique().tolist():\n",
    "    outg_rec_nbs_i = [x[0] for x in outg_rec_nbs_and_trsf_pole_nbs if x[1]==trsf_pole_nb_i]\n",
    "    #-----\n",
    "    fig,ax = Plot_General.default_subplots(fig_num=fig_num)\n",
    "    y_prob1_by_date_df.loc[trsf_pole_nb_i].T.plot.line(ax=ax)\n",
    "    #-----\n",
    "    dovs_df_dev_i = dovs_df_dev[dovs_df_dev['OUTG_REC_NB'].isin(outg_rec_nbs_i)].copy()\n",
    "    for idx_ij in range(dovs_df_dev_i.shape[0]):\n",
    "        dt_off_ts_full_i, dt_on_ts_i = dovs_df_dev_i.iloc[idx_ij][['DT_OFF_TS_FULL', 'DT_ON_TS']]\n",
    "        ax.axvline(dt_off_ts_full_i, color='red')\n",
    "    #-----\n",
    "    idx_max_i = y_prob1_by_date_df.loc[trsf_pole_nb_i].idxmax()\n",
    "    ax.axvline(idx_max_i, color='green')\n",
    "    if(\n",
    "        idx_max_i > dovs_df_dev_i['DT_OFF_TS_FULL'].max() and\n",
    "        y_prob1_by_date_df.loc[trsf_pole_nb_i].index[0] < dovs_df_dev_i['DT_OFF_TS_FULL'].max() #Make sure there are actually data to grab\n",
    "    ):\n",
    "        idx_max_i = y_prob1_by_date_df.loc[trsf_pole_nb_i][:dovs_df_dev_i['DT_OFF_TS_FULL'].max()].idxmax()\n",
    "        ax.axvline(idx_max_i, color='lawngreen')\n",
    "    #-----\n",
    "    if dovs_df_dev_i['DT_OFF_TS_FULL'].min() < y_prob1_by_date_df.loc[trsf_pole_nb_i].index[0]:\n",
    "        ax.set_xlim(left=dovs_df_dev_i['DT_OFF_TS_FULL'].min()-pd.Timedelta('1D'))\n",
    "    #-----\n",
    "    fig_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3091163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trsf_pole_nb_i = y_prob1_by_date_df.index.unique().tolist()[0]\n",
    "outg_rec_nbs_i = [x[0] for x in outg_rec_nbs_and_trsf_pole_nbs if x[1]==trsf_pole_nb_i]\n",
    "y_prob1_i = y_prob1_by_date_df.loc[trsf_pole_nb_i].copy()\n",
    "dovs_df_dev_i = dovs_df_dev[dovs_df_dev['OUTG_REC_NB'].isin(outg_rec_nbs_i)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c41b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob1_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_dev_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max_i = y_prob1_by_date_df.loc[trsf_pole_nb_i].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "        idx_max_i > dovs_df_dev_i['DT_OFF_TS_FULL'].max() and\n",
    "        y_prob1_by_date_df.loc[trsf_pole_nb_i].index[0] < dovs_df_dev_i['DT_OFF_TS_FULL'].max() #Make sure there are actually data to grab\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When failures occur, what are number of days above threshold?  Median, mean, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob1_i>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_dev_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da63ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dovs_df_dev_i.iloc[0]['DT_OFF_TS_FULL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = dovs_df_dev_i.iloc[0]['DT_OFF_TS_FULL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef53927",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob1_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5de8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob1_i.loc[:date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ca9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_len=pd.Timedelta('7D')\n",
    "exclude_day_of=True\n",
    "threshold=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_consec_len_in_bool_srs(\n",
    "    bool_srs\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a pd.Series, srs, comprised of boolean values, determine the longest streak of consecutive True values.\n",
    "    Assumption is that bool_srs has been sorted to fit needs before being fit into this function.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(bool_srs.dtype==bool)\n",
    "    #-------------------------\n",
    "    longest = 0\n",
    "    streak = 0\n",
    "    for idx_i, bool_i in bool_srs.items():\n",
    "        if bool_i==True:\n",
    "            streak += 1\n",
    "        else:\n",
    "            longest = max(longest, streak)\n",
    "            streak = 0\n",
    "    # Make sure the last streak isn't ignored if series ends with True\n",
    "    longest = max(longest, streak)\n",
    "    #-------------------------\n",
    "    return longest\n",
    "\n",
    "def find_longest_consec_idxs_in_bool_srs(\n",
    "    bool_srs, \n",
    "    return_ilocs=False, \n",
    "    return_len=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a pd.Series, srs, comprised of boolean values, determine the longest streak of consecutive True values and return the indices.\n",
    "    Returns:\n",
    "        A pair of indices representing the beginning and ending of the True block\n",
    "        The returned indices are INCLUSIVE, meaning bool_srs.loc[return_idxs[0]]==True and bool_srs.loc[return_idxs[1]]==True\n",
    "        \n",
    "    return_ilocs:\n",
    "        If True, instead of returning the values in bool_srs.index, return integers between 0 and bool_srs.shape[0]-1\n",
    "        \n",
    "    return_len:\n",
    "        If True, also return the length of the block, in integer form\n",
    "    \n",
    "    Assumption is that bool_srs has been sorted to fit needs before being fit into this function.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(bool_srs.dtype==bool)\n",
    "    #-------------------------\n",
    "    longest = 0\n",
    "    streak = 0\n",
    "    #-----\n",
    "    locs = [np.nan, np.nan]\n",
    "    curr_beg_loc = bool_srs.index[0]\n",
    "    #-----\n",
    "    ilocs = [np.nan, np.nan]\n",
    "    curr_beg_iloc = 0\n",
    "    #-------------------------\n",
    "    for i in range(bool_srs.shape[0]):\n",
    "        idx_i  = bool_srs.index[i]\n",
    "        bool_i = bool_srs.iloc[i]\n",
    "        #-----\n",
    "        if bool_i==True:\n",
    "            streak += 1\n",
    "        else:\n",
    "            if streak >= longest:\n",
    "                longest = streak\n",
    "                locs  = [curr_beg_loc,  idx_i]\n",
    "                ilocs = [curr_beg_iloc, i]\n",
    "            #-----\n",
    "            streak = 0\n",
    "            if i < bool_srs.shape[0]-1:\n",
    "                curr_beg_loc  = bool_srs.index[i+1]\n",
    "                curr_beg_iloc = i+1\n",
    "    # Make sure the last streak isn't ignored if series ends with True\n",
    "    if streak >= longest:\n",
    "        longest = streak\n",
    "        locs  = [curr_beg_loc, idx_i]\n",
    "        ilocs = [curr_beg_iloc, bool_srs.shape[0]-1] \n",
    "    #-------------------------\n",
    "    return_idxs = locs\n",
    "    if return_ilocs:\n",
    "        return_idxs = ilocs\n",
    "    #-------------------------\n",
    "    if return_len:\n",
    "        return return_idxs, longest\n",
    "    else:\n",
    "        return return_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob1_i_stats_preceding_date(\n",
    "    y_prob1_i, \n",
    "    date, \n",
    "    pd_len=pd.Timedelta('7D'),\n",
    "    exclude_day_of=True, \n",
    "    threshold=0.5, \n",
    "    return_series=True, \n",
    "    cols_to_drop=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    Return information regarding the probability prediction in the period of length pd_len preceding date\n",
    "    \n",
    "    date:\n",
    "        If not supplied, will be randomly selected from y_prob1_i.index UNLESS pd_len is also not supplied (see NEITHER date/pd_len supplied)\n",
    "        \n",
    "    pd_len:\n",
    "        If not supplied, all available data preceding date will be used UNLESS date is also not supplied (see NEITHER date/pd_len supplied)\n",
    "        \n",
    "    NEITHER date/pd_len supplied:\n",
    "        If neither is supplied, entire series y_prob1_i is evaluated\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    assert(isinstance(y_prob1_i, pd.Series))\n",
    "    y_prob1_i = copy.deepcopy(y_prob1_i)\n",
    "    if cols_to_drop is not None:\n",
    "        y_prob1_i = y_prob1_i.drop(cols_to_drop)\n",
    "    #--------------------------------------------------\n",
    "    # If date is None and pd_len is None, no slicing done on y_prob1_i (i.e., y_prob1_i=y_prob1_i instead\n",
    "    #   of y_prob1_i = y_prob1_i.loc[pd.Timestamp(date-pd_len) : pd.Timestamp(date)])\n",
    "    # Put differently, slicing is needed if date is not None or pd_len is not None\n",
    "    if(\n",
    "        date   is not None or \n",
    "        pd_len is not None\n",
    "    ):\n",
    "        #-------------------------\n",
    "        if date is None:\n",
    "            date_0 = y_prob1_i.index.min()\n",
    "            date_1 = y_prob1_i.index.max()\n",
    "            if exclude_day_of:\n",
    "                # To ensure date_0 remains within bounds of y_prob1_i\n",
    "                date_0 = date_0+pd.Timedelta('1D')\n",
    "            #-----\n",
    "            date = Utilities_dt.get_random_date_interval_between(\n",
    "                date_0       = date_0, \n",
    "                date_1       = date_1, \n",
    "                window_width = pd_len, \n",
    "                rand_seed    = None\n",
    "            )\n",
    "            date = date[1]\n",
    "        #-------------------------\n",
    "        if pd_len is None:\n",
    "            pd_len = date-y_prob1_i.index.min()\n",
    "            if exclude_day_of:\n",
    "                pd_len = pd_len-pd.Timedelta('1D')\n",
    "        #-------------------------\n",
    "        # Being extra safe, don't want safe changed outside of function\n",
    "        date = copy.deepcopy(date)\n",
    "        #-------------------------\n",
    "        # Ensure date is a date object, and not datetime\n",
    "        assert(Utilities.is_object_one_of_types(date, [pd.Timestamp, datetime.date]))\n",
    "        if isinstance(date, pd.Timestamp):\n",
    "            date = date.date()\n",
    "        assert(isinstance(date, datetime.date))\n",
    "        #-------------------------\n",
    "        if exclude_day_of:\n",
    "            date = date-pd.Timedelta('1D')\n",
    "        #-------------------------\n",
    "        if(\n",
    "            pd.Timestamp(date-pd_len) < y_prob1_i.index.min() or \n",
    "            pd.Timestamp(date) > y_prob1_i.index.max()\n",
    "        ):\n",
    "            return None\n",
    "        #-------------------------\n",
    "        y_prob1_i = y_prob1_i.loc[pd.Timestamp(date-pd_len) : pd.Timestamp(date)]\n",
    "    #--------------------------------------------------\n",
    "    n_nan                          = y_prob1_i.isna().sum()\n",
    "    n_abv_thrsh                    = (y_prob1_i > threshold).sum()\n",
    "    idxs, longest_streak_abv_thrsh = find_longest_consec_idxs_in_bool_srs(\n",
    "        bool_srs     = y_prob1_i>threshold, \n",
    "        return_ilocs = False, \n",
    "        return_len   = True\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    if date is None:\n",
    "        longest_streak_beg = y_prob1_i.index.max()-idxs[0]\n",
    "        longest_streak_end = y_prob1_i.index.max()-idxs[1]\n",
    "    else:\n",
    "        longest_streak_beg = pd.Timestamp(date)-idxs[0]\n",
    "        longest_streak_end = pd.Timestamp(date)-idxs[1]\n",
    "    #-----\n",
    "    return_dict = dict(\n",
    "        n_nan                    = n_nan, \n",
    "        n_abv_thrsh              = n_abv_thrsh, \n",
    "        longest_streak_abv_thrsh = longest_streak_abv_thrsh, \n",
    "        longest_streak_beg       = longest_streak_beg, \n",
    "        longest_streak_end       = longest_streak_end\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    if return_series:\n",
    "        return pd.Series(return_dict)\n",
    "    else:\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_df.xs('y_prob_1', level=1, axis=1).to_pickle(r'C:\\Users\\s346557\\Downloads\\tmp_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob1_by_date_df = return_df.xs('y_prob_1', level=1, axis=1).copy()\n",
    "# y_prob1_by_date_df = pd.read_pickle(r'C:\\Users\\s346557\\Downloads\\tmp_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "dovs_aid_df = pd.DataFrame(data=outg_rec_nbs_and_trsf_pole_nbs, columns=['outg_rec_nb', 'trsf_pole_nb'])\n",
    "#-----\n",
    "assert(dovs_df_dev['OUTG_REC_NB'].nunique()==dovs_df_dev.shape[0])\n",
    "#-----\n",
    "dovs_aid_df = pd.merge(\n",
    "    dovs_aid_df, \n",
    "    dovs_df_dev[['OUTG_REC_NB', 'DT_OFF_TS_FULL', 'DT_ON_TS']], \n",
    "    left_on='outg_rec_nb', \n",
    "    right_on='OUTG_REC_NB', \n",
    "    how='left'\n",
    ")\n",
    "#-------------------------\n",
    "y_prob1_by_date_df = pd.merge(\n",
    "    y_prob1_by_date_df.reset_index(), \n",
    "    dovs_aid_df, \n",
    "    left_on='trsf_pole_nb', \n",
    "    right_on='trsf_pole_nb', \n",
    "    how='left'\n",
    ").set_index('trsf_pole_nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe130827",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob1_stats = y_prob1_by_date_df.apply(\n",
    "    lambda x: get_prob1_i_stats_preceding_date(\n",
    "        y_prob1_i = x, \n",
    "        date = x['DT_OFF_TS_FULL'], \n",
    "        pd_len=pd.Timedelta('7D'),\n",
    "        exclude_day_of=True, \n",
    "        threshold=0.5, \n",
    "        return_series=True, \n",
    "        cols_to_drop=['outg_rec_nb', 'OUTG_REC_NB', 'DT_OFF_TS_FULL', 'DT_ON_TS']\n",
    "    ), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe74143",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob1_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6380885",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob1_stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f82eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b19691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dovs_df.to_pickle(r'C:\\Users\\s346557\\Documents\\OutgPredictions\\NoMonth\\dovs_df.pkl')\n",
    "# return_df.to_pickle(r'C:\\Users\\s346557\\Documents\\OutgPredictions\\NoMonth\\return_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab31deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
