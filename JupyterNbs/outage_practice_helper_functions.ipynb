{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17087ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pandas.api.types import is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns\n",
    "from packaging import version\n",
    "import itertools\n",
    "from dateutil.parser import parse\n",
    "from operator import itemgetter\n",
    "\n",
    "from pmdarima import auto_arima\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import acovf, acf, pacf, pacf_yw, pacf_ols\n",
    "from pandas.plotting import lag_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.graphics.tsaplots import month_plot, quarter_plot, seasonal_plot\n",
    "from statsmodels.tsa.arima_model import ARMA, ARIMA, ARMAResults, ARIMAResults\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from arch import arch_model\n",
    "\n",
    "from scipy.stats.mstats import trim\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "# import constants for the days of the week\n",
    "from matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "import Utilities_dt\n",
    "import Plot_Box_sns\n",
    "import GrubbsTest\n",
    "import DickeyFullerTest as dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed602ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7e9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp_to_utc_in_df(df, timestamp_col, placement_col=None, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    if placement_col is None:\n",
    "        placement_col = f'{timestamp_col}_from_timestamp'\n",
    "    df[placement_col] = df[timestamp_col].apply(datetime.datetime.utcfromtimestamp)\n",
    "    return df\n",
    "\n",
    "def build_utc_time_column(df, time_col, placement_col=None, naive=True, inplace=False):\n",
    "    # If naive=True, the timezone information is dropped from each entrty\n",
    "    # If naive=False, the timezone information is kept, but will always be \n",
    "    #   equal to +00:00 as all returned times will be in UTC\n",
    "    #\n",
    "    # NOTE: time_col can be a single column or a list of columns\n",
    "    #-----------------------------------\n",
    "    if not inplace:\n",
    "        df = df.copy()    \n",
    "    #-----------------------------------\n",
    "    assert(isinstance(time_col, str) or isinstance(time_col, list))\n",
    "    if isinstance(time_col, list):\n",
    "        # Note: inplace already taken care of above, so don't need to waste memory\n",
    "        #       copying df for each iteration, which is why inplace=True below\n",
    "        assert((isinstance(placement_col, list) and len(placement_col)==len(time_col)) \n",
    "               or placement_col is None)\n",
    "        for i,col in enumerate(time_col):\n",
    "            df = build_utc_time_column(df, time_col=col, \n",
    "                                       placement_col=None if placement_col is None else placement_col[i], \n",
    "                                       naive=naive, inplace=True)\n",
    "        return df\n",
    "    #-----------------------------------    \n",
    "    if placement_col is None:\n",
    "        placement_col = f'{time_col}_utc'\n",
    "    # NOTE: dt.tz_localize(None) drops the timezone information, creating timezone naive entries\n",
    "    #       Calling simply .tz_localize(None) doesn't seem to do anything\n",
    "    df[placement_col] = pd.to_datetime(df[time_col], utc=True)\n",
    "    if naive:\n",
    "        df[placement_col] = df[placement_col].dt.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def convert_timezoneoffset_col_to_timedelta(df, timezoneoffset_col, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    if not is_timedelta64_dtype(df[timezoneoffset_col]):\n",
    "        df[timezoneoffset_col] = df[timezoneoffset_col].apply(lambda x: Utilities_dt.get_timedelta_from_timezoneoffset(x))    \n",
    "    return df\n",
    "\n",
    "def strip_tz_info_and_convert_to_dt(df, time_col, placement_col=None, \n",
    "                                    run_quick=True, n_strip=6, inplace=True):\n",
    "    # Entries in time_col should be strings of format e.g., '2020-01-01T00:00:00-05:00'\n",
    "    # n_strip is the number of elements to strip off the back of the time\n",
    "    # For the example given, n_strip=6 as len('-05:00')==6\n",
    "    #\n",
    "    # NOTE: time_col can be a single column or a list of columns\n",
    "    #-----------------------------------\n",
    "    if not inplace:\n",
    "        df = df.copy()    \n",
    "    #-----------------------------------\n",
    "    assert(isinstance(time_col, str) or isinstance(time_col, list))\n",
    "    if isinstance(time_col, list):\n",
    "        # Note: inplace already taken care of above, so don't need to waste memory\n",
    "        #       copying df for each iteration, which is why inplace=True below\n",
    "        assert((isinstance(placement_col, list) and len(placement_col)==len(time_col)) \n",
    "               or placement_col is None)\n",
    "        for i,col in enumerate(time_col):\n",
    "            df = strip_tz_info_and_convert_to_dt(df, time_col=col, \n",
    "                                                 placement_col=None if placement_col is None else placement_col[i], \n",
    "                                                 run_quick=run_quick, \n",
    "                                                 n_strip=n_strip, \n",
    "                                                 inplace=True)\n",
    "        return df\n",
    "    #-----------------------------------  \n",
    "    if placement_col is None:\n",
    "        placement_col = time_col\n",
    "    if run_quick:\n",
    "        df[placement_col] = df[time_col].str[:-n_strip]\n",
    "        df[placement_col] = pd.to_datetime(df[placement_col])\n",
    "    else:\n",
    "        df[placement_col] = df[time_col].apply(Utilities_dt.clean_timeperiod_entry)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d4d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_usage_around_outage(fig, ax, \n",
    "                             data, x, y, hue, \n",
    "                             out_t_beg, out_t_end, expand_time, data_label='', \n",
    "                             title_args=None, ax_args=None, \n",
    "                             xlabel_args=None, ylabel_args=None, \n",
    "                             df_mean=None, df_mean_col=None, mean_args=None, \n",
    "                             draw_outage_limits=True, draw_without_hue_also=False, \n",
    "                             seg_line_freq=None, palette='colorblind'):\n",
    "    # Setting hue=None will aggregate over repeated values to show the mean and 95% confidence interval\n",
    "    # seg_line_freq can be set to e.g., seg_line_freq='D'\n",
    "    if df_mean_col is None:\n",
    "        df_mean_col = y\n",
    "    sns.lineplot(ax=ax, data=data[out_t_beg-expand_time:out_t_end+expand_time], \n",
    "                 x=x, y=y, hue=hue, \n",
    "                 palette=palette, label=data_label)\n",
    "    if draw_without_hue_also and hue is not None:\n",
    "        sns.lineplot(ax=ax, data=data[out_t_beg-expand_time:out_t_end+expand_time], \n",
    "                     x=x, y=y, hue=None, \n",
    "                     color='deeppink', linestyle='--', label='AVG')\n",
    "    #----------------------------\n",
    "    # Note: if hue=None is drawn, then average will already be drawn!\n",
    "    if (df_mean is not None \n",
    "        and hue is not None \n",
    "        and not draw_without_hue_also):\n",
    "        if mean_args is None:\n",
    "            avg_label = 'AVG'\n",
    "            if data_label:\n",
    "                avg_label = f'{data_label} AVG'\n",
    "            mean_args = dict(style='--', linewidth=3, alpha=0.50, color='deeppink', label=avg_label, legend=True)\n",
    "        df_mean[out_t_beg-expand_time:out_t_end+expand_time][df_mean_col].plot(ax=ax, **mean_args)\n",
    "    #----------------------------\n",
    "    if draw_outage_limits:\n",
    "        ax.axvline(out_t_beg, color='red')\n",
    "        ax.axvline(out_t_end, color='lawngreen')\n",
    "    #----------------------------\n",
    "    if seg_line_freq is not None:\n",
    "        seg_min = pd.to_datetime(out_t_beg-expand_time).round(seg_line_freq)\n",
    "        seg_max = pd.to_datetime(out_t_end+expand_time).round(seg_line_freq)\n",
    "        all_segs = pd.date_range(seg_min, seg_max, freq=seg_line_freq)\n",
    "        for seg in all_segs:\n",
    "            ax.axvline(seg, color='black', linestyle='--');\n",
    "    #----------------------------\n",
    "    if isinstance(title_args, str):\n",
    "        title_args = dict(label=title_args)\n",
    "    if title_args is not None:\n",
    "        ax.set_title(**title_args)\n",
    "    #----------------------------\n",
    "    if ax_args is not None:\n",
    "        ax.set(**ax_args)\n",
    "    if xlabel_args is not None:\n",
    "        ax.set_xlabel(**xlabel_args)\n",
    "    if ylabel_args is not None:\n",
    "        ax.set_ylabel(**ylabel_args)\n",
    "        \n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b16ed8",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfadbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_cols_for_rounds_of_aggregation(agg_cols, agg_types, mix_agg_functions, n_rounds=2, identifiers=None):\n",
    "    # Note: Keys in all_rounds from 1 to n_rounds (not 0 to n_rounds-1)\n",
    "    # identifiers are tags that will be added to the aggregate names in each round\n",
    "    #   If identifiers is not None, it must be a list whose length equals at least n_rounds\n",
    "    #   Typical case: First, aggregate of meters for each time.  Second, time resampling\n",
    "    #               ---> identifiers = ['_mtrs', '_TRS']\n",
    "    #                                  '_mtrs' represents aggregate of meters\n",
    "    #                                  '_TRS' stands for Time ReSampled\n",
    "    #---------------------------\n",
    "    if identifiers is not None:\n",
    "        assert(len(identifiers)>=n_rounds)\n",
    "    all_rounds = {}\n",
    "    rd_1 = {}\n",
    "    rd_1_cols = []\n",
    "    idfr=''\n",
    "    if identifiers is not None:\n",
    "        idfr = identifiers[1-1]\n",
    "    for agg_col in agg_cols:\n",
    "        for agg_type in agg_types:\n",
    "            curr_mult = (agg_col, agg_type)\n",
    "            curr_flat = f'{agg_type}{idfr} {agg_col}'\n",
    "            rd_1_cols.append({'mult':curr_mult, 'flat':curr_flat})\n",
    "    rd_1['columns'] = rd_1_cols\n",
    "    rd_1['rename_dict'] = {col['mult']:col['flat'] for col in rd_1['columns']}\n",
    "    #-----\n",
    "    all_rounds[1] = rd_1\n",
    "    #-----------------\n",
    "    for i_round in list(range(2,n_rounds+1)):\n",
    "        rd_i = {}\n",
    "        rd_i_cols = []\n",
    "        rd_im1_cols = all_rounds[i_round-1]['columns']\n",
    "        idfr=''\n",
    "        if identifiers is not None:\n",
    "            idfr = identifiers[i_round-1]\n",
    "        for prev_col in rd_im1_cols:\n",
    "            for agg_type in agg_types:\n",
    "                prev_agg_type = prev_col['mult'][1]\n",
    "                if not mix_agg_functions and prev_agg_type!=agg_type:\n",
    "                    continue\n",
    "                prev_col_flat = prev_col['flat']\n",
    "                #-----\n",
    "                curr_agg_type = agg_type\n",
    "                curr_mult = (prev_col_flat, curr_agg_type)\n",
    "                curr_flat = f'{curr_agg_type}{idfr} {prev_col_flat}'\n",
    "                #-----\n",
    "                rd_i_cols.append({'mult':curr_mult, 'flat':curr_flat})\n",
    "        rd_i['columns'] = rd_i_cols\n",
    "        rd_i['rename_dict'] = {col['mult']:col['flat'] for col in rd_i['columns']}\n",
    "        #-----\n",
    "        all_rounds[i_round] = rd_i\n",
    "    return all_rounds\n",
    "\n",
    "\n",
    "def rename_agg_and_other_cols_for_round_of_aggregation(df, agg_cols, agg_types, mix_agg_functions, t_round, \n",
    "                                                       identifiers=None, other_cols_to_keep=[]):\n",
    "    # Note: Keys in agg_cols_for_rounds from 1 to n_rounds\n",
    "    # t_round stands for 'this round' and should be 1 or greater\n",
    "    # It is assumed that the 'first' aggregation function called on other_cols_to_keep\n",
    "    agg_cols_for_rounds = get_agg_cols_for_rounds_of_aggregation(agg_cols, agg_types, \n",
    "                                                                 mix_agg_functions=mix_agg_functions, \n",
    "                                                                 n_rounds=t_round, identifiers=identifiers)\n",
    "    full_rename_dict = {**agg_cols_for_rounds[t_round]['rename_dict'], **{(f'{col}', 'first'):col for col in other_cols_to_keep}}\n",
    "    df.columns = df.columns.to_flat_index()\n",
    "    df = df.rename(columns=full_rename_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a4b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Probably don't use ensure_other_cols_to_keep_are_appropriate or decide_other_cols_to_keep\n",
    "#     Either improve methods or remove functionality\n",
    "def build_df_aggregated_for_each_time_index(df_15T, time_col_for_agg, agg_cols, agg_types, other_cols_to_keep=[], \n",
    "                                            ensure_other_cols_to_keep_are_appropriate=False, \n",
    "                                            decide_other_cols_to_keep=False):\n",
    "    # TODO should I implement methods allowing one to group by index?  e.g. through  df.groupby(level=0)\n",
    "    #---------------------------------\n",
    "    # This groups the DataFrame by the time (in time_col_for_agg) and outputs a DataFrame built by aggregating \n",
    "    #   each group by the functions specified in agg_types.\n",
    "    # Typically, this corresponds an average over all meters in the collection for each time index.\n",
    "    # -----\n",
    "    # The time column by which to group is specified in time_col_for_agg.\n",
    "    #   Note: If the timestamp is in the index of df_15T, the easiest method will be to give the index\n",
    "    #           a name (e.g. df_15T.index.name = 'time_idx'), and then feed the index name into this \n",
    "    #           function via time_col_for_agg.\n",
    "    # -----\n",
    "    # The columns to be aggregated are specified in agg_cols.\n",
    "    # -----\n",
    "    # The aggregate functions to use are specified in agg_types\n",
    "    #   e.g. agg_types = ['mean', 'sum', 'std']\n",
    "    # -----\n",
    "    # If other_cols_to_keep=[] or other_cols_to_keep=None, only time_col_for_agg (as the index) and agg_cols \n",
    "    #   will be contained in df_15T_agg.\n",
    "    # If other_cols_to_keep are included, these columns should only have a single unique value for each \n",
    "    #   group, as the value from the first row in the group will be used\n",
    "    # The user can ensure other_cols_to_keep are appropriate by setting \n",
    "    #   ensure_other_cols_to_keep_are_appropriate=True\n",
    "    # The user can let the program decide which columns to keep by setting \n",
    "    #   decide_other_cols_to_keep=True\n",
    "    #   Note: if decide_other_cols_to_keep=True, anything provided in other_cols_to_keep will \n",
    "    #         be ignored\n",
    "    #-------------------------------------------------------------------------\n",
    "    # For the code below, assume the following to understand explanations:\n",
    "    # time_col_for_agg='endtimeperiod_utc'\n",
    "    # agg_cols = ['value']\n",
    "    # agg_types=['mean', 'sum']\n",
    "    # other_cols_to_keep = ['srvc_pole_nb', 'aep_derived_uom', 'aep_srvc_qlty_idntfr']\n",
    "    #-------------------------------------------------------------------------\n",
    "    if other_cols_to_keep is None:\n",
    "        other_cols_to_keep=[]\n",
    "    if decide_other_cols_to_keep:\n",
    "        other_cols_to_keep = decide_which_other_cols_to_keep(df_15T, time_col_for_agg, other_cols_to_keep)\n",
    "    if ensure_other_cols_to_keep_are_appropriate and not decide_other_cols_to_keep:\n",
    "        # Don't need to check if decide_which_other_cols_to_keep run already\n",
    "        assert(are_other_cols_to_keep_appropriate(df_15T, time_col_for_agg))\n",
    "    #---------------------------        \n",
    "    # Create initial agg_dict, which will cause agg_types to be applied to all columns in agg_cols\n",
    "    # and the 'first' aggregate function to be used on all columns in other_cols_to_keep\n",
    "    # --- The assumption is each grouping has only a single unique value for each\n",
    "    #     column in other_cols_to_keep, therefore using 'first' is appropriate\n",
    "    agg_dict = {**{x:agg_types for x in agg_cols}, **{x:'first' for x in other_cols_to_keep}}\n",
    "    df_15T_agg = df_15T.groupby(time_col_for_agg).agg(agg_dict)\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # Flatten down the columns\n",
    "    # After aggregating, the columns of df_15T_agg become multi-levelled (i.e. becomes a multiindex)\n",
    "    # level 0: the original name of the column\n",
    "    # level 1: the corresponding aggregate function applied\n",
    "    # ===> column = 'value' --> columns [('value', 'mean'), ('value', 'sum')]\n",
    "    # The code below flattens down the columns and names them 'orignal name' + ' agg function'\n",
    "    # ===> ('value', 'mean') --> mean value\n",
    "    # ===> ('value', 'sum') --> sum value\n",
    "    # For columns not included in aggregation (i.e. those not in agg_cols, for which the 'first'\n",
    "    #   aggregation funciton was used), the original column name is retained\n",
    "    identifiers = ['_mtrs', '_TRS']\n",
    "    df_15T_agg = rename_agg_and_other_cols_for_round_of_aggregation(df_15T_agg, agg_cols, agg_types, \n",
    "                                                                    mix_agg_functions=False, \n",
    "                                                                    t_round=1, identifiers=identifiers, \n",
    "                                                                    other_cols_to_keep=other_cols_to_keep)\n",
    "    return df_15T_agg\n",
    "\n",
    "\n",
    "def build_resampled_df_aggregated_for_each_time_index(df_15T_agg, agg_cols, agg_types, \n",
    "                                                      other_cols_to_keep=[], freq='H', mix_agg_functions=True):\n",
    "    #-----------------------------------\n",
    "    # Time resample df_15T_agg\n",
    "    # The intent of this function is to be used on a DataFrame built with build_df_aggregated_for_each_time_index.\n",
    "    #   - df_15T contains multiple meters for multiple time period\n",
    "    #   - df_15T_agg = build_df_aggregated_for_each_time_index(df_15T, ...) then contains aggregate meter values\n",
    "    #     for each time period\n",
    "    #   - This function will resample df_15T_agg to, e.g. df_H_agg (hourly)\n",
    "    #-----------------------------------\n",
    "    # NOTE: freq can be a single frequency (of type str, e.g. 'H')\n",
    "    #         In this case, a single pd.DataFrame is returned\n",
    "    #       OR freq can be a list of frequencies (e.g., ['H', '4H', 'D', 'MS'])\n",
    "    #         In this case, a dict is returned whose keys are the frequencies\n",
    "    #         and whose values are the corresponding pd.DataFrames\n",
    "    assert(isinstance(freq, str) or isinstance(freq, list))\n",
    "    if isinstance(freq, list):\n",
    "        return_dict = {}\n",
    "        for f in freq:\n",
    "            return_dict[f] = build_resampled_df_aggregated_for_each_time_index(df_15T_agg, agg_cols, agg_types, \n",
    "                                                                               other_cols_to_keep, freq=f, \n",
    "                                                                               mix_agg_functions=mix_agg_functions)\n",
    "        return return_dict\n",
    "    #-----------------------------------\n",
    "    # For the code below, assume the following to understand explanations:\n",
    "    # agg_cols = ['value']\n",
    "    # agg_types=['mean', 'sum']\n",
    "    # other_cols_to_keep = ['srvc_pole_nb', 'aep_derived_uom', 'aep_srvc_qlty_idntfr']\n",
    "    #-----------------------------------\n",
    "    # The same agg_types will be used to build df_15T_agg as df_resampled_agg.\n",
    "    # mix_agg_functions allows the user to set how these two aggregations will work.\n",
    "    # If mix_agg_functions=True:\n",
    "    #   Final aggregate columns will be: 'mean mean value', 'sum mean value', 'mean sum value', 'sum sum value'\n",
    "    # If mix_agg_functions=False:\n",
    "    #   Final aggregate columns will be: 'mean mean value' and 'sum sum value'\n",
    "    #-----------------------------------------------------------------------------\n",
    "    identifiers = ['_mtrs', '_TRS']\n",
    "    agg_cols_for_rounds = get_agg_cols_for_rounds_of_aggregation(agg_cols, agg_types, mix_agg_functions, n_rounds=2, identifiers=identifiers)\n",
    "    agg_dict_2 = {}\n",
    "    for col in agg_cols_for_rounds[1]['columns']:\n",
    "        assert(col['flat'] not in agg_dict_2)\n",
    "        agg_dict_2[col['flat']] = agg_types\n",
    "    agg_dict_2 = {**agg_dict_2, **{x:'first' for x in other_cols_to_keep}}\n",
    "    #----------------------\n",
    "    df_resampled_agg = df_15T_agg.resample(freq).agg(agg_dict_2)\n",
    "    # Flatten down the columns of df_resampled_agg\n",
    "    df_resampled_agg = rename_agg_and_other_cols_for_round_of_aggregation(df_resampled_agg, agg_cols, agg_types, \n",
    "                                                                          mix_agg_functions=mix_agg_functions, \n",
    "                                                                          t_round=2, identifiers=identifiers, \n",
    "                                                                          other_cols_to_keep=other_cols_to_keep)\n",
    "    return df_resampled_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resampled_df(df_15T, freq, other_grouper_cols, agg_cols, agg_types, other_cols_to_keep=[], identifier=['_TRS']):\n",
    "    #-----------------------------------\n",
    "    # NOTE: freq can be a single frequency (of type str, e.g. 'H')\n",
    "    #         In this case, a single pd.DataFrame is returned\n",
    "    #       OR freq can be a list of frequencies (e.g., ['H', '4H', 'D', 'MS'])\n",
    "    #         In this case, a dict is returned whose keys are the frequencies\n",
    "    #         and whose values are the corresponding pd.DataFrames\n",
    "    assert(isinstance(freq, str) or isinstance(freq, list))\n",
    "    if isinstance(freq, list):\n",
    "        return_dict = {}\n",
    "        for f in freq:\n",
    "            return_dict[f] = build_resampled_df(df_15T, f, \n",
    "                                                other_grouper_cols, agg_cols, agg_types, other_cols_to_keep, identifier)\n",
    "        return return_dict\n",
    "    #-----------------------------------\n",
    "    #-----------------------------------\n",
    "    # Create initial agg_dict, which will cause agg_types to be applied to all columns in agg_cols\n",
    "    # and the 'first' aggregate function to be used on all columns in other_cols_to_keep\n",
    "    # --- The assumption is each grouping has only a single unique value for each\n",
    "    #     column in other_cols_to_keep, therefore using 'first' is appropriate\n",
    "    agg_dict = {**{x:agg_types for x in agg_cols}, **{x:'first' for x in other_cols_to_keep}}\n",
    "    df_resampled = df_15T.groupby([pd.Grouper(freq=freq)] + other_grouper_cols).agg(agg_dict)\n",
    "    #--------------------------------------------------------------\n",
    "    # Flatten down the columns\n",
    "    # After aggregating, the columns of df_resampled become multi-levelled (i.e. becomes a multiindex)\n",
    "    # level 0: the original name of the column\n",
    "    # level 1: the corresponding aggregate function applied\n",
    "    # ===> column = 'value' --> columns [('value', 'mean'), ('value', 'sum')]\n",
    "    # The code below flattens down the columns and names them 'orignal name' + ' agg function'\n",
    "    # ===> ('value', 'mean') --> mean value\n",
    "    # ===> ('value', 'sum') --> sum value\n",
    "    # For columns not included in aggregation (i.e. those not in agg_cols, for which the 'first'\n",
    "    #   aggregation funciton was used), the original column name is retained\n",
    "    df_resampled = rename_agg_and_other_cols_for_round_of_aggregation(df_resampled, agg_cols, agg_types, \n",
    "                                                                      mix_agg_functions=False, \n",
    "                                                                      t_round=1, identifiers=identifier, \n",
    "                                                                      other_cols_to_keep=other_cols_to_keep)\n",
    "    \n",
    "    return df_resampled    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f804fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resampled_dfs(df_15T, base_freq='15T', freqs=['H', '4H', 'D', 'MS'], \n",
    "                      other_grouper_cols=['serialnumber'], other_cols_to_keep=[], flatten_idxs=True, \n",
    "                      build_agg_dfs=True, time_col_for_agg='endtimeperiod_utc', agg_cols=['value'], agg_types=['mean'], \n",
    "                      other_cols_to_keep_agg=[], mix_agg_functions=True, \n",
    "                      df_key='df', df_agg_key='df_agg'):\n",
    "    # By default, when grouping the grouped columns become indices\n",
    "    # If flatten_idxs = True, the indices will be flattened back out\n",
    "    # to one-dimensional\n",
    "    #   Note: As df_15T is not grouped, it does not need to be flattened\n",
    "    #-------------------------------------------------------------\n",
    "    return_dict = {}\n",
    "    return_dict[base_freq] = {df_key:df_15T}\n",
    "    #------------------\n",
    "    # Build resampled dfs and add to return_dict\n",
    "    resampled_dfs_dict = build_resampled_df(df_15T, freqs, other_grouper_cols, agg_cols, agg_types, other_cols_to_keep)\n",
    "    for freq in resampled_dfs_dict.keys():\n",
    "        assert(freq not in return_dict and freq in freqs)\n",
    "        return_dict[freq] = {df_key:resampled_dfs_dict[freq]}\n",
    "    # Add 'date' column to each df, as that was done in first version of code\n",
    "    for freq in return_dict.keys():\n",
    "        return_dict[freq][df_key]['date'] = return_dict[freq][df_key].index.get_level_values(0)\n",
    "        if flatten_idxs and freq != base_freq:\n",
    "            n_levels = return_dict[freq][df_key].index.nlevels\n",
    "            return_dict[freq][df_key] = return_dict[freq][df_key].reset_index(level=list(range(1, n_levels)))\n",
    "        if not flatten_idxs and freq==base_freq:\n",
    "            return_dict[freq][df_key] = return_dict[freq][df_key].set_index(other_grouper_cols, append=True)\n",
    "    #------------------\n",
    "    # Build resampled aggregate dfs if build_agg_dfs==True\n",
    "    if build_agg_dfs:\n",
    "        # First, build 15_T aggregate df, which has all entries for each time index aggregated\n",
    "        df_15T_agg = build_df_aggregated_for_each_time_index(df_15T, time_col_for_agg, agg_cols, agg_types, other_cols_to_keep_agg)\n",
    "        return_dict[base_freq][df_agg_key] = df_15T_agg\n",
    "        # Resample df_15T_agg to other desired frequencies\n",
    "        resampled_agg_dfs_dict = build_resampled_df_aggregated_for_each_time_index(df_15T_agg, agg_cols, agg_types, \n",
    "                                                                                   other_cols_to_keep_agg, freqs, mix_agg_functions)\n",
    "        # Add resampled agg dfs to return_dict\n",
    "        for freq in resampled_agg_dfs_dict.keys():\n",
    "            assert(freq in return_dict)\n",
    "            return_dict[freq][df_agg_key] = resampled_agg_dfs_dict[freq]\n",
    "        # Add index name and create time_col_for_agg for each df_agg, as that was done in first version of code\n",
    "        for freq in return_dict.keys():\n",
    "            return_dict[freq][df_agg_key].index.name='time_idx'\n",
    "            return_dict[freq][df_agg_key][time_col_for_agg] = return_dict[freq][df_agg_key].index\n",
    "        #------------------        \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20d57c",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5820d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_level(index_name, df):\n",
    "    index_names = list(df.index.names)\n",
    "    index_level=None\n",
    "    for i,idx_name_i in enumerate(index_names):\n",
    "        if idx_name_i==index_name:\n",
    "            assert(index_level is None)\n",
    "            index_level = i\n",
    "    assert(index_level is not None)\n",
    "    return index_level\n",
    "\n",
    "def find_datetime_idx(df):\n",
    "    # NOTE: For now, this will fail if more than one datetime index is found\n",
    "    # This is enforced through the line: assert(datetime_idx is None)\n",
    "    datetime_idx = None\n",
    "    grouped_cols = list(df.index.names)\n",
    "    if len(grouped_cols)==1:\n",
    "        if isinstance(df.index[0], datetime.datetime):\n",
    "            datetime_idx = {'idx_level':0, 'idx_name':grouped_cols[0]}\n",
    "    else:\n",
    "        assert(len(grouped_cols)==len(df.index[0]))\n",
    "        for i in range(len(grouped_cols)):\n",
    "            if isinstance(df.index[0][i], datetime.datetime):\n",
    "                assert(datetime_idx is None)\n",
    "                datetime_idx = {'idx_level':i, 'idx_name':grouped_cols[i]}\n",
    "    return datetime_idx\n",
    "\n",
    "def find_in_df_columns_or_indices(df, names):\n",
    "    # Determine where names are located in df (special case if name found in index and is datetime)\n",
    "    # Returns dict {'names_in_cols':names_in_cols, 'names_in_idxs':names_in_idxs}\n",
    "    #     names_in_cols is a list of all names found in the columns of df\n",
    "    #     names_in_idxs is a dictionary with:\n",
    "    #         'datetime_idx' = None or {'idx_level':idx_level, 'idx_name':idx_name} if found to be datetime\n",
    "    #         'regular_idxs' = list of non-datetime indices, where eadch element\n",
    "    #                          is a dict of the form {'idx_level':idx_level, 'idx_name':idx_name}\n",
    "    #\n",
    "    # NOTE: For now, this will fail if more than one datetime index is found\n",
    "    # This is enforced through the line: assert(datetime_idx is None)\n",
    "    #-------------------------\n",
    "    names_in_idxs = {'datetime_idx':None, 'regular_idxs':[]}\n",
    "    names_in_cols = []\n",
    "    #-------------------------\n",
    "    col_names = df.columns.tolist()\n",
    "    idx_names = list(df.index.names)\n",
    "    sample_idx = df.index[0] # Used for determining if datetime element exists\n",
    "    #-------------------------\n",
    "    for name in names:\n",
    "        if name in col_names:\n",
    "            names_in_cols.append(name)\n",
    "        elif name in idx_names:\n",
    "            if len(idx_names)==1:\n",
    "                if isinstance(sample_idx, datetime.datetime):\n",
    "                    assert(names_in_idxs['datetime_idx'] is None)\n",
    "                    names_in_idxs['datetime_idx'] = {'idx_level':0, 'idx_name':name}\n",
    "            else:\n",
    "                idx_level = idx_names.index(name)\n",
    "                if isinstance(sample_idx[idx_level], datetime.datetime):\n",
    "                    assert(names_in_idxs['datetime_idx'] is None)\n",
    "                    names_in_idxs['datetime_idx'] = {'idx_level':idx_level, 'idx_name':name}\n",
    "                else:\n",
    "                    regular_idx = {'idx_level':idx_level, 'idx_name':name}\n",
    "                    names_in_idxs['regular_idxs'].append(regular_idx)\n",
    "        else:\n",
    "            print(f'Name: {name} NOT FOUND in df columns or indices')\n",
    "            assert(0)\n",
    "    names_in_cols_and_idxs_dict = {'names_in_cols':names_in_cols, 'names_in_idxs':names_in_idxs}\n",
    "    return names_in_cols_and_idxs_dict\n",
    "\n",
    "def get_list_of_idx_level_name_value_dicts(list_of_idx_level_name_dicts, names_vals_dict):\n",
    "    # names_vals_dict = {'idx_name_1':idx_val_1, 'idx_name_2':idx_val_2, ... , 'idx_name_n':idx_val_n}\n",
    "    # list_of_idx_level_name_dicts = [{'idx_level':idx_level_a, 'idx_name': idx_name_a}, \n",
    "    #                                 {'idx_level':idx_level_b, 'idx_name': idx_name_b}, \n",
    "    #                                                    ...\n",
    "    #                                 {'idx_level':idx_level_m, 'idx_name': idx_name_m}]\n",
    "    # \n",
    "    # return_list_dicts = [{'idx_level':idx_level_a, 'idx_name': idx_name_a, 'idx_value':idx_val_a}, \n",
    "    #                      {'idx_level':idx_level_b, 'idx_name': idx_name_b, 'idx_value':idx_val_b}, \n",
    "    #                                         ...\n",
    "    #                      {'idx_level':idx_level_m, 'idx_name': idx_name_m, 'idx_value':idx_val_m}]    \n",
    "    \n",
    "    # Only the members included in list_of_idx_level_name_dicts will be in the returned list.\n",
    "    # This implies each 'idx_name' value in list_of_idx_level_name_dicts must be a key in names_vals_dict.\n",
    "    # -----> names_vals_dict must include at least all members in list_of_idx_level_name_dicts, \n",
    "    #        but can also include more without effect.\n",
    "    # -----------------------------------------------------------------------\n",
    "    return_list_dicts = []\n",
    "    for level_name_dict in list_of_idx_level_name_dicts:\n",
    "        assert(level_name_dict['idx_name'] in names_vals_dict)\n",
    "        return_list_dicts.append({**level_name_dict, \n",
    "                                  **{'idx_value':names_vals_dict[level_name_dict['idx_name']]}\n",
    "                                 })\n",
    "    return return_list_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_idx_select_arg_list(list_of_idx_level_name_value_dicts, df_idx_names):\n",
    "    # list_of_idx_level_name_value_dicts can include entries for all levels in df, but does not have to.\n",
    "    # Any missing level will be assigned a value of slice(None), which is equivalent for no slicing on that level\n",
    "    #\n",
    "    # list_of_idx_level_name_value_dicts = [{'idx_level':0, 'idx_name':'time_idx', 'idx_value':pd.to_datetime('2018-10-04 12:00:00')}, \n",
    "    #                                       {'idx_level':2, 'idx_name': 'prem_nb', 'idx_value':109826791}]\n",
    "    # idx_select_arg_list = (pd.to_datetime('2018-10-04 12:00:00'), slice(None), 109826791)\n",
    "    # ----------------------------------------------------------------\n",
    "    n_idx_levels_in_df = len(df_idx_names)\n",
    "    assert(len(list_of_idx_level_name_value_dicts) <= n_idx_levels_in_df)\n",
    "    assert(all(x['idx_name'] in df_idx_names for x in list_of_idx_level_name_value_dicts))\n",
    "    #-------------------------\n",
    "    if len(list_of_idx_level_name_value_dicts) < n_idx_levels_in_df:\n",
    "        for level,df_idx_name in enumerate(df_idx_names):\n",
    "            if df_idx_name not in [x['idx_name'] for x in list_of_idx_level_name_value_dicts]:\n",
    "                list_of_idx_level_name_value_dicts.append({'idx_level':level, 'idx_name': df_idx_name, 'idx_value':slice(None)})\n",
    "    \n",
    "    # Make sure each level has a value\n",
    "    assert(all(x['idx_name'] in df_idx_names for x in list_of_idx_level_name_value_dicts))\n",
    "    assert(all(x['idx_level'] in list(range(len(df_idx_names))) for x in list_of_idx_level_name_value_dicts))\n",
    "\n",
    "    # Get a list of the index values on which to select\n",
    "    # sorted by the index level\n",
    "    list_of_idx_level_value_dicts = {x['idx_level']:x['idx_value'] for x in list_of_idx_level_name_value_dicts}\n",
    "    idx_select_arg_list = []\n",
    "    for level in range(len(df_idx_names)):\n",
    "        idx_select_arg_list.append(list_of_idx_level_value_dicts[level])\n",
    "    # Convert idx_select_arg_list to usable form\n",
    "    idx_select_arg_list = tuple(x if x is not None else slice(None) for x in idx_select_arg_list)\n",
    "    return idx_select_arg_list\n",
    "\n",
    "\n",
    "def replace_cols_in_gpd_df_with_values_from_df_singlegroup(gpd_vals_dict, gpd_df, df, resample_freq, cols_to_replace):\n",
    "    # TODO!!!!!!!!!!!!!!\n",
    "    # if datetime index included, resample_freq MUST match the frequency used for gpd_df\n",
    "    #\n",
    "    # gpd_vals_dict should be a dictionary containing keys which are indice names from gpd_vals_dict\n",
    "    #   and values are corresponding values to group\n",
    "    # If all indices from gpd_vals_dict are included, this is the same thing as replacing the column\n",
    "    #   values row by row.  This can be very time consuming\n",
    "    # In most cases (indeed in the case for which this was designed) the time index can be excluded\n",
    "    #\n",
    "    # cols_to_replace should be columns in gpd_df, not indices\n",
    "    # Enforce this\n",
    "    tmp_dict = find_in_df_columns_or_indices(gpd_df, cols_to_replace)\n",
    "    assert(tmp_dict['names_in_idxs']['datetime_idx'] is None)\n",
    "    assert(len(tmp_dict['names_in_idxs']['regular_idxs'])==0)\n",
    "\n",
    "    # All keys in gpd_vals_dict should be indices in gpd_df\n",
    "    # Assert this\n",
    "    # At the same time, find if any datetime indices supplied in gpd_vals_dict\n",
    "    gpd_df_idx_names = list(gpd_df.index.names)\n",
    "    gpd_datetime_idx=None\n",
    "    for key in gpd_vals_dict:\n",
    "        assert(key in gpd_df_idx_names)\n",
    "        if isinstance(gpd_vals_dict[key], datetime.datetime):\n",
    "            gpd_datetime_idx = {'idx_level': gpd_df_idx_names.index(key), 'idx_name': key}\n",
    "\n",
    "    # Determine where in df the gpd_vals_dict.keys() are located\n",
    "    names_in_cols_and_idxs_dict = find_in_df_columns_or_indices(df, gpd_vals_dict.keys())\n",
    "    gpd_cols_in_df_cols = names_in_cols_and_idxs_dict['names_in_cols']\n",
    "    gpd_cols_in_df_idxs = names_in_cols_and_idxs_dict['names_in_idxs']\n",
    "\n",
    "    # If datetime index found in gpd_df, one should also be found in df\n",
    "    # More generally, gpd_datetime_idx should equal gpd_cols_in_df_idxs['datetime_idx'], \n",
    "    #   whether they equal a datetime idx, or both equal None\n",
    "    assert(gpd_cols_in_df_idxs['datetime_idx']==gpd_datetime_idx)\n",
    "    #------------------------------------------------------------------\n",
    "    # First, apply selection via indices, then columns\n",
    "    # If datetime index, apply first\n",
    "    if gpd_datetime_idx is not None:\n",
    "        # The methodology here also only will work of the time index is the first index\n",
    "        # TODO THIS COULD BE CHANGED USING THE 'idx_level' info!\n",
    "        assert(gpd_cols_in_df_idxs['datetime_idx']['idx_level']==0)\n",
    "        t_slice_beg = gpd_vals_dict[gpd_datetime_idx['idx_name']]\n",
    "        t_slice_end = t_slice_beg + pd.to_timedelta(resample_freq if resample_freq[0].isnumeric() else '1'+resample_freq)\n",
    "        df = df.loc[t_slice_beg:t_slice_end]\n",
    "    # Now, other indices\n",
    "    regular_idxs = gpd_cols_in_df_idxs['regular_idxs']\n",
    "    if len(regular_idxs)>0:\n",
    "        # Make sure entries in regular_idxs are in correct order\n",
    "        regular_idxs = sorted(regular_idxs, key=lambda x: x['idx_level'])\n",
    "        # Get values for indices\n",
    "        regular_idx_vals = [gpd_vals_dict[x['idx_name']] for x in regular_idxs]\n",
    "        regular_list_of_idx_level_name_value_dicts = get_list_of_idx_level_name_value_dicts(regular_idxs, gpd_vals_dict)\n",
    "        regular_idx_select_arg_list = build_idx_select_arg_list(regular_list_of_idx_level_name_value_dicts, list(df.index.names))\n",
    "                \n",
    "        df = df.loc(axis=0)[pd.IndexSlice[regular_idx_select_arg_list]]\n",
    "    # ------------------------------------------------------------\n",
    "    # Now, apply selection via columns\n",
    "    col_bool_mask = [True]*df.shape[0]\n",
    "    for col in gpd_cols_in_df_cols:\n",
    "        col_bool_mask = (col_bool_mask) & (df[col]==gpd_vals_dict[col])\n",
    "    df = df.loc[col_bool_mask]\n",
    "    # ------------------------------------------------------------\n",
    "    # Now, df is reduced down to only the elements of interest\n",
    "    # Need to grab the values for all columns in cols_to_replace\n",
    "    # There should only be one unique value for each, let's enforce this with asset\n",
    "    replace_dict = {}\n",
    "    for col in cols_to_replace:\n",
    "        value_counts = df[col].value_counts()\n",
    "        n_unique = len(value_counts)\n",
    "        # n_unique should be 1, with the only exception being\n",
    "        # when a column is full of NaNs, in which case n_unique will be 0\n",
    "        assert(n_unique==1 or n_unique==0)\n",
    "        if n_unique==0:\n",
    "            # All values should be NaNs\n",
    "            assert(all(df[col].isna()))\n",
    "            value = np.nan\n",
    "        else:\n",
    "            value = value_counts.index[0]\n",
    "        assert(col not in replace_dict)\n",
    "        replace_dict[col] = value\n",
    "    # ------------------------------------------------------------\n",
    "    # Finally, replace the value(s) in gpd_df with replace_dict\n",
    "    list_of_idx_level_name_dicts_to_include = [{'idx_level': get_index_level(x, gpd_df), 'idx_name': x} for x in gpd_vals_dict]\n",
    "    list_of_idx_level_name_value_dicts = get_list_of_idx_level_name_value_dicts(list_of_idx_level_name_dicts_to_include, gpd_vals_dict)\n",
    "    idx_select_arg_list = build_idx_select_arg_list(list_of_idx_level_name_value_dicts, list(gpd_df.index.names))\n",
    "    gpd_df.loc[pd.IndexSlice[idx_select_arg_list], cols_to_replace] = itemgetter(*tuple(cols_to_replace))(replace_dict)\n",
    "    \n",
    "    # Could also return df is wanted for debugging/answer checking\n",
    "    return gpd_df\n",
    "\n",
    "\n",
    "# For each entry in gpd_df, I need to grab the values of the variables used for grouping.\n",
    "# Then, I need to get that group from df_kwh_15T to find the values of all other columns to be set.\n",
    "#\n",
    "# Typically (but not always): \n",
    "#     the time_idx grouped will be the index of df_kwh_15T\n",
    "#     any other grouping variables are columns in df_kwh_15T\n",
    "# However, it may occur that df_kwh_15T has a multi-index, in which the grouping\n",
    "# variable are contained.  Any of these cases should work\n",
    "\n",
    "def replace_cols_in_gpd_df_with_values_from_df(gpd_df, df, resample_freq, cols_to_replace, exclude_time_idx=True):\n",
    "    # Not recommended to set exclude_time_idx=False unless dfs are small\n",
    "    #   as this will perform the replacement row-by-row.\n",
    "    # exclude_time_idx=True is far more efficient \n",
    "    #TODO build in option to allow user to input the names of indices to be included\n",
    "    #    Right now, all indices in gpd_df are included, except possibly the time index\n",
    "    gpd_datetime_idx = find_datetime_idx(gpd_df)\n",
    "    n_idxs = len(gpd_df.index[0])\n",
    "    level = []\n",
    "    for i in range(n_idxs):\n",
    "        if (exclude_time_idx and \n",
    "            gpd_datetime_idx is not None and \n",
    "            gpd_datetime_idx['idx_level']==i):\n",
    "            continue\n",
    "        level.append(i)\n",
    "    #-------------------------\n",
    "    all_groups = list(gpd_df.groupby(level=level).groups.keys())\n",
    "    idx_names_to_include = [list(gpd_df.index.names)[lvl] for lvl in level]\n",
    "    assert(len(all_groups[0])==len(idx_names_to_include))\n",
    "    gpd_vals_dicts = [dict(zip(idx_names_to_include, grp)) for grp in all_groups]    \n",
    "    #-------------------------\n",
    "    for gpd_vals_dict in gpd_vals_dicts:\n",
    "        gpd_df = replace_cols_in_gpd_df_with_values_from_df_singlegroup(gpd_vals_dict=gpd_vals_dict, \n",
    "                                                                        gpd_df=gpd_df, \n",
    "                                                                        df=df, \n",
    "                                                                        resample_freq=resample_freq, \n",
    "                                                                        cols_to_replace=cols_to_replace)\n",
    "    return gpd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02af3d",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_us_timezone(shifts, assert_found=False):\n",
    "    # shifts should either be a single (negative) integer or a list/tuple of two (negative) integers\n",
    "    #   Note: A list/tuple of one (negative) integer will work too\n",
    "    # If only a single shift is given, this function can at best return two possible timezones\n",
    "    utc_shifts = {\n",
    "        'US/Eastern':  [-5, -4], \n",
    "        'US/Central':  [-6, -5], \n",
    "        'US/Mountain': [-7, -6], \n",
    "        'US/Pacific':  [-8, -7], \n",
    "        'US/Alaska':   [-9, -8], \n",
    "        'US/Hawaii':   [-10, -10]\n",
    "    }\n",
    "    #-------------------------------\n",
    "    found_tz = None\n",
    "    if isinstance(shifts,int) or len(shifts)==1:\n",
    "        if not isinstance(shifts,int):\n",
    "            shifts = shifts[0]\n",
    "        found_tz = [tz for tz,tz_shifts in utc_shifts.items() if shifts>=tz_shifts[0] and shifts<=tz_shifts[1]]\n",
    "    else:\n",
    "        assert(len(shifts)==2)\n",
    "        shifts=sorted(shifts)\n",
    "        found_tz = [tz for tz,tz_shifts in utc_shifts.items() if shifts==tz_shifts]\n",
    "    #-------------------------------\n",
    "    if len(found_tz)==0:\n",
    "        found_tz=None\n",
    "    if found_tz is None:\n",
    "        if assert_found:\n",
    "            assert(0)\n",
    "        else:\n",
    "            return found_tz\n",
    "    #-----\n",
    "    if len(found_tz)==1:\n",
    "        found_tz=found_tz[0]\n",
    "    return found_tz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3435b",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7855d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_local_to_utc_time(t_local, timezone):\n",
    "    # timezone should be e.g., 'US/Eastern'\n",
    "    #-----------------------------------\n",
    "    if isinstance(t_local, list) or isinstance(t_local, tuple):\n",
    "        return_list = []\n",
    "        for t in t_local:\n",
    "            return_list.append(convert_local_to_utc_time(t, timezone))\n",
    "        return return_list\n",
    "    #-----------------------------------    \n",
    "    t_utc = pd.to_datetime(t_local).tz_localize(timezone).tz_convert(None)\n",
    "    return t_utc\n",
    "\n",
    "def determine_timezone_and_convert_local_to_utc_time(t_local, unique_tz_offsets, \n",
    "                                                     timezone_aware_times=None, **kwargs):\n",
    "    # Should be complete hour \n",
    "    #  (there do exist half-hour and 45-minute time zones in t\n",
    "    #   the world, but there shouldn't be any in AEP data)\n",
    "    #\n",
    "    # As convert_local_to_utc_time works whether t_local is single time or a list\n",
    "    # of times, this function does as well.\n",
    "    #--------------------\n",
    "    # Preferable to use unique_tz_offsets, \n",
    "    #   which is a list of timezone offsets\n",
    "    #     can be strings (e.g., '-05:00'), datetime.timedeltas, or pd./np.Timedeltas\n",
    "    # HOWEVER, can set unique_tz_offsets to None and use timezone_aware_times instead\n",
    "    #--------------------\n",
    "    if unique_tz_offsets is None:\n",
    "        assert(timezone_aware_times is not None)\n",
    "        dflt_pattern = r'(\\d{4}-\\d{2}-\\d{2})T(\\d{2}:\\d{2}:\\d{2})([+-]\\d{2}:\\d{2})'\n",
    "        pattern = kwargs.get('pattern', dflt_pattern)\n",
    "        unique_tz_offsets = [Utilities_dt.extract_tz_from_tz_aware_dt_str(x, pattern=pattern) for x in timezone_aware_times]\n",
    "        unique_tz_offsets = list(set(unique_tz_offsets))        \n",
    "    #--------------------\n",
    "    # Make sure elements in unique_tz_offsets are of proper type, and all are unique\n",
    "    unq_tz_offsets = []\n",
    "    for x in unique_tz_offsets:\n",
    "        if is_timedelta64_dtype(x):\n",
    "            unq_tz_offsets.append(x)\n",
    "        elif isinstance(x, datetime.timedelta):\n",
    "            unq_tz_offsets.append(pd.to_timedelta(x))\n",
    "        else:\n",
    "            assert(isinstance(x, str))\n",
    "            unq_tz_offsets.append(pd.to_timedelta(Utilities_dt.get_timedelta_from_timezoneoffset(x)))\n",
    "    # Make sure unq_tz_offsets truly is unique\n",
    "    unq_tz_offsets = list(set(unq_tz_offsets))    \n",
    "    #--------------------\n",
    "    assert(all(x.total_seconds()%3600==0 for x in unq_tz_offsets))\n",
    "    unq_tz_offsets = [round(x.total_seconds()/3600) for x in unq_tz_offsets]\n",
    "    found_tz = determine_us_timezone(unq_tz_offsets, assert_found=True)\n",
    "    #--------------------\n",
    "    return convert_local_to_utc_time(t_local, found_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0f44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ba04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf915765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be32d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_df = pd.read_csv(os.path.join(Utilities.get_local_data_dir(), r'sample_outages\\outg_rec_nb_11751094\\outg_rec_nb_11751094_2019_q4.csv')\n",
    "# read_df = read_df[read_df['serialnumber']==880687439]\n",
    "# read_df = read_df[(read_df['aep_derived_uom']=='KWH') & (read_df['aep_srvc_qlty_idntfr']=='TOTAL')]\n",
    "# read_df = read_df[['endtimeperiod', 'endtimeperiod','aep_endtime_utc', 'timezoneoffset']]\n",
    "# read_df = read_df.sort_values(by='aep_endtime_utc', ignore_index=True)\n",
    "# read_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
