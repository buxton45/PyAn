{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444718d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns\n",
    "from packaging import version\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "import Utilities_dt\n",
    "from Utilities_df import DFConstructType\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import GrubbsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ev_submeter_in_pair(\n",
    "    ami_df_i, \n",
    "    pct_0_thresh_main=0.1, \n",
    "    pct_0_thresh_subm=0.6, \n",
    "    enforce_corr=True, \n",
    "    corr_thresh=0.5, \n",
    "    #remove_undetermined=True, \n",
    "    value_col='value', \n",
    "    time_col='index', \n",
    "    PN_col='aep_premise_nb', \n",
    "    SN_col='serialnumber'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a pair of meter connected to a single premise, this tries to determine which is the EV submeter in the pair, \n",
    "      and keeps that which is not.\n",
    "    Basically, the submeter only monitors the EV, whereas the other monitors the usage of the entire premise.\n",
    "    Thus, the EV signal should be 0 for a majority of the time, and the two meters should be correlated.\n",
    "    \n",
    "    This is a somewhat specialized function for use with engineering the EV dataset\n",
    "    \n",
    "    ami_df_i:\n",
    "        An AMI DF containing data for a single premise which has two meters\n",
    "        \n",
    "    pct_0_thresh_main:\n",
    "        The maximum allowed percentage of 0 values for a meter to be considered the main meter\n",
    "    pct_0_thresh_subm:\n",
    "        The minimum allowed percentage of 0 values for a meter to be considered the submeter\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(ami_df_i[PN_col].nunique()==1)\n",
    "    assert(pct_0_thresh_main < pct_0_thresh_subm)\n",
    "    #-------------------------\n",
    "    SNs_i = ami_df_i[SN_col].unique().tolist()\n",
    "    assert(len(SNs_i)<=2)\n",
    "    #-------------------------\n",
    "    if len(SNs_i)==1:\n",
    "        return ami_df_i\n",
    "    #-------------------------\n",
    "    ami_df_i_1 = ami_df_i[ami_df_i[SN_col]==SNs_i[0]]\n",
    "    ami_df_i_2 = ami_df_i[ami_df_i[SN_col]==SNs_i[1]]\n",
    "    if time_col=='index':\n",
    "        ami_df_i_1 = ami_df_i_1.sort_index()\n",
    "        ami_df_i_2 = ami_df_i_2.sort_index()\n",
    "    else:\n",
    "        ami_df_i_1 = ami_df_i_1.sort_values(by=[time_col])\n",
    "        ami_df_i_2 = ami_df_i_2.sort_values(by=[time_col])\n",
    "    #--------------------------------------------------\n",
    "    # NOTE: I have seen some SNs which have a bunch of 0.002 values, which, for the purposes here should be treated as 0s\n",
    "    #       This is why I use <0.005 instead of ==0 and >=0.005 instead of !=0 below\n",
    "    #--------------------------------------------------\n",
    "    pct_1 = (ami_df_i_1[value_col]<0.005).sum()/ami_df_i_1.shape[0]\n",
    "    pct_2 = (ami_df_i_2[value_col]<0.005).sum()/ami_df_i_2.shape[0]\n",
    "    #-------------------------\n",
    "    if pct_1 <= pct_0_thresh_main:\n",
    "        defn_1 = 1\n",
    "    elif pct_1 >= pct_0_thresh_subm:\n",
    "        defn_1 = 0\n",
    "    else:\n",
    "        defn_1 = -1\n",
    "    #-----\n",
    "    if pct_2 <= pct_0_thresh_main:\n",
    "        defn_2 = 1\n",
    "    elif pct_2 >= pct_0_thresh_subm:\n",
    "        defn_2 = 0\n",
    "    else:\n",
    "        defn_2 = -1\n",
    "    #-------------------------\n",
    "    if defn_1+defn_2==1:\n",
    "        pass_pcts=True\n",
    "    else:\n",
    "        pass_pcts=False\n",
    "    #-------------------------\n",
    "    if not pass_pcts:\n",
    "        return\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    pass_corr=True\n",
    "    if enforce_corr:\n",
    "        if time_col=='index':\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_index=True, right_index=True, how='outer')\n",
    "        else:\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_on=time_col, right_on=time_col, how='outer')\n",
    "        # For the correlation only using values which are non-zero (and aligning, meaning non-NaN)\n",
    "        ami_df_i_12 = ami_df_i_12[(ami_df_i_12[f'{value_col}_x']>=0.005) & (ami_df_i_12[f'{value_col}_y']>=0.005)]\n",
    "        corr_val = ami_df_i_12[f'{value_col}_x'].corr(ami_df_i_12[f'{value_col}_y'])\n",
    "        if corr_val >= corr_thresh:\n",
    "            pass_corr = True\n",
    "        else:\n",
    "            pass_corr = False\n",
    "            \n",
    "    #--------------------------------------------------\n",
    "    if not (pass_pcts and pass_corr):\n",
    "        return\n",
    "    #-------------------------\n",
    "    if defn_1==1:\n",
    "        assert(defn_2==0) # Sanity check, not needed\n",
    "        return ami_df_i[ami_df_i[SN_col]==SNs_i[0]]\n",
    "    else:\n",
    "        assert(defn_1==0 and defn_2==1) # Sanity check, not needed\n",
    "        return ami_df_i[ami_df_i[SN_col]==SNs_i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f81243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_pairs_w_submeter(\n",
    "    ami_df_i, \n",
    "    pct_0_thresh_main=0.1, \n",
    "    pct_0_thresh_subm=0.6, \n",
    "    enforce_corr=True, \n",
    "    corr_thresh=0.5, \n",
    "    #remove_undetermined=True, \n",
    "    value_col='value', \n",
    "    time_col='index', \n",
    "    PN_col='aep_premise_nb', \n",
    "    SN_col='serialnumber'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a pair of meter connected to a single premise, this tries to determine which is the EV submeter in the pair, \n",
    "      and keeps that which is not.\n",
    "    Basically, the submeter only monitors the EV, whereas the other monitors the usage of the entire premise.\n",
    "    Thus, the EV signal should be 0 for a majority of the time, and the two meters should be correlated.\n",
    "    \n",
    "    This is a somewhat specialized function for use with engineering the EV dataset\n",
    "    \n",
    "    ami_df_i:\n",
    "        An AMI DF containing data for a single premise which has two meters\n",
    "        \n",
    "    pct_0_thresh_main:\n",
    "        The maximum allowed percentage of 0 values for a meter to be considered the main meter\n",
    "    pct_0_thresh_subm:\n",
    "        The minimum allowed percentage of 0 values for a meter to be considered the submeter\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(ami_df_i[PN_col].nunique()==1)\n",
    "    assert(pct_0_thresh_main < pct_0_thresh_subm)\n",
    "    #-------------------------\n",
    "    SNs_i = ami_df_i[SN_col].unique().tolist()\n",
    "    assert(len(SNs_i)<=2)\n",
    "    #-------------------------\n",
    "    if len(SNs_i)==1:\n",
    "        return \n",
    "    #-------------------------\n",
    "    ami_df_i_1 = ami_df_i[ami_df_i[SN_col]==SNs_i[0]]\n",
    "    ami_df_i_2 = ami_df_i[ami_df_i[SN_col]==SNs_i[1]]\n",
    "    if time_col=='index':\n",
    "        ami_df_i_1 = ami_df_i_1.sort_index()\n",
    "        ami_df_i_2 = ami_df_i_2.sort_index()\n",
    "    else:\n",
    "        ami_df_i_1 = ami_df_i_1.sort_values(by=[time_col])\n",
    "        ami_df_i_2 = ami_df_i_2.sort_values(by=[time_col])\n",
    "    #--------------------------------------------------\n",
    "    # NOTE: I have seen some SNs which have a bunch of 0.002 values, which, for the purposes here should be treated as 0s\n",
    "    #       This is why I use <0.005 instead of ==0 and >=0.005 instead of !=0 below\n",
    "    #--------------------------------------------------\n",
    "    pct_1 = (ami_df_i_1[value_col]<0.005).sum()/ami_df_i_1.shape[0]\n",
    "    pct_2 = (ami_df_i_2[value_col]<0.005).sum()/ami_df_i_2.shape[0]\n",
    "    #-------------------------\n",
    "    if pct_1 <= pct_0_thresh_main:\n",
    "        defn_1 = 1\n",
    "    elif pct_1 >= pct_0_thresh_subm:\n",
    "        defn_1 = 0\n",
    "    else:\n",
    "        defn_1 = -1\n",
    "    #-----\n",
    "    if pct_2 <= pct_0_thresh_main:\n",
    "        defn_2 = 1\n",
    "    elif pct_2 >= pct_0_thresh_subm:\n",
    "        defn_2 = 0\n",
    "    else:\n",
    "        defn_2 = -1\n",
    "    #-------------------------\n",
    "    if defn_1+defn_2==1:\n",
    "        pass_pcts=True\n",
    "    else:\n",
    "        pass_pcts=False\n",
    "    #-------------------------\n",
    "    if not pass_pcts:\n",
    "        return\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    pass_corr=True\n",
    "    if enforce_corr:\n",
    "        if time_col=='index':\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_index=True, right_index=True, how='outer')\n",
    "        else:\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_on=time_col, right_on=time_col, how='outer')\n",
    "        # For the correlation only using values which are non-zero (and aligning, meaning non-NaN)\n",
    "        ami_df_i_12 = ami_df_i_12[(ami_df_i_12[f'{value_col}_x']>=0.005) & (ami_df_i_12[f'{value_col}_y']>=0.005)]\n",
    "        corr_val = ami_df_i_12[f'{value_col}_x'].corr(ami_df_i_12[f'{value_col}_y'])\n",
    "        if corr_val >= corr_thresh:\n",
    "            pass_corr = True\n",
    "        else:\n",
    "            pass_corr = False\n",
    "            \n",
    "    #--------------------------------------------------\n",
    "    if not (pass_pcts and pass_corr):\n",
    "        return\n",
    "    #-------------------------\n",
    "    if defn_1==1:\n",
    "        assert(defn_2==0) # Sanity check, not needed\n",
    "        ami_df_i_main = ami_df_i[ami_df_i[SN_col]==SNs_i[0]].copy()\n",
    "        ami_df_i_subm = ami_df_i[ami_df_i[SN_col]==SNs_i[1]].copy()\n",
    "    else:\n",
    "        assert(defn_1==0 and defn_2==1) # Sanity check, not needed\n",
    "        ami_df_i_main =  ami_df_i[ami_df_i[SN_col]==SNs_i[1]].copy()\n",
    "        ami_df_i_subm =  ami_df_i[ami_df_i[SN_col]==SNs_i[0]].copy()\n",
    "    #-------------------------\n",
    "    if time_col=='index':\n",
    "        return_ami_df_i = pd.merge(\n",
    "            ami_df_i_main, \n",
    "            ami_df_i_subm[[value_col, SN_col]], \n",
    "            left_index=True, \n",
    "            right_index=True, \n",
    "            how='inner'\n",
    "        )\n",
    "    else:\n",
    "        return_ami_df_i = pd.merge(\n",
    "            ami_df_i_main, \n",
    "            ami_df_i_subm[[value_col, time_col, SN_col]], \n",
    "            left_on=time_col, \n",
    "            right_on=time_col, \n",
    "            how='inner'\n",
    "        )\n",
    "    #----------\n",
    "    return_ami_df_i = return_ami_df_i.rename(columns={\n",
    "        f'{value_col}_x': f'{value_col}_main', \n",
    "        f'{value_col}_y': f'{value_col}_subm', \n",
    "        f'{SN_col}_x': f'{SN_col}_main', \n",
    "        f'{SN_col}_y': f'{SN_col}_subm', \n",
    "    })\n",
    "    #----------\n",
    "    return_ami_df_i[f'{value_col}_delt'] = return_ami_df_i[f'{value_col}_main']-return_ami_df_i[f'{value_col}_subm']\n",
    "    #-------------------------\n",
    "    return return_ami_df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637ef95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e3342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_ami_df = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472da5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if build_ami_df:\n",
    "    ami_df = GenAn.read_df_from_csv_dir_batches(\n",
    "        files_dir=r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Data', \n",
    "        file_path_glob=r'*.csv'\n",
    "    )\n",
    "    #-------------------------\n",
    "    ami_df = ami_df[\n",
    "        (ami_df['aep_derived_uom']=='KWH') & \n",
    "        (ami_df['aep_srvc_qlty_idntfr']=='TOTAL')\n",
    "    ].copy()\n",
    "    #-----\n",
    "    ami_df['timezoneoffset'] = ami_df['starttimeperiod'].str[-6:]\n",
    "    #-------------------------\n",
    "    ami_df = AMINonVee.perform_std_initiation_and_cleaning(\n",
    "        ami_df, \n",
    "        timestamp_col=None\n",
    "    )\n",
    "    #-----\n",
    "    ami_df = Utilities_dt.strip_tz_info_and_convert_to_dt(\n",
    "        df=ami_df, \n",
    "        time_col='starttimeperiod', \n",
    "        placement_col='starttimeperiod_local', \n",
    "        run_quick=True, \n",
    "        n_strip=6, \n",
    "        inplace=False\n",
    "    )\n",
    "    ami_df = Utilities_dt.strip_tz_info_and_convert_to_dt(\n",
    "        df=ami_df, \n",
    "        time_col='endtimeperiod', \n",
    "        placement_col='endtimeperiod_local', \n",
    "        run_quick=True, \n",
    "        n_strip=6, \n",
    "        inplace=False\n",
    "    )\n",
    "    ami_df=ami_df.set_index('starttimeperiod_local', drop=False)\n",
    "    #-------------------------\n",
    "    ami_df.to_pickle(r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\ami_df.pkl')\n",
    "else:\n",
    "    ami_df = pd.read_pickle(r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\ami_df.pkl')\n",
    "#-------------------------\n",
    "all_trff_dfs = pd.read_pickle(r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\all_trff_dfs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6cf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_full = ami_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc95004",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df['aep_premise_nb'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebed084",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trff_dfs['PREM_NB'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trff_dfs_found     = all_trff_dfs[all_trff_dfs['PREM_NB'].isin(ami_df['aep_premise_nb'].unique().tolist())]\n",
    "all_trff_dfs_not_found = all_trff_dfs[~all_trff_dfs['PREM_NB'].isin(ami_df['aep_premise_nb'].unique().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_trff_dfs_found['PREM_NB'].nunique())\n",
    "print(all_trff_dfs_not_found['PREM_NB'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ddda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ami_df['aep_premise_nb'].nunique())\n",
    "print(ami_df['serialnumber'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5835881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cdb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evs_prems = all_trff_dfs[all_trff_dfs['EV']==1]['PREM_NB'].unique().tolist()\n",
    "non_prems = all_trff_dfs[all_trff_dfs['EV']==0]['PREM_NB'].unique().tolist()\n",
    "\n",
    "# aep_premise_nb in ami_df is of type object (i.e., a string), whereas PREM_NB in trff_df is int64\n",
    "evs_prems = [str(x) for x in evs_prems]\n",
    "non_prems = [str(x) for x in non_prems]\n",
    "#-----\n",
    "ami_df_evs = ami_df[ami_df['aep_premise_nb'].isin(evs_prems)].copy()\n",
    "ami_df_non = ami_df[ami_df['aep_premise_nb'].isin(non_prems)].copy()\n",
    "#----\n",
    "assert(ami_df.shape[0]==ami_df_evs.shape[0]+ami_df_non.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d56aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df = ami_df_evs.groupby('aep_premise_nb', as_index=False, group_keys=False).apply(\n",
    "    lambda x: select_pairs_w_submeter(\n",
    "        ami_df_i=x, \n",
    "        pct_0_thresh_main=0.1, \n",
    "        pct_0_thresh_subm=0.6, \n",
    "        enforce_corr=True, \n",
    "        corr_thresh=0.5, \n",
    "        value_col='value', \n",
    "        time_col='index', \n",
    "        PN_col='aep_premise_nb', \n",
    "        SN_col='serialnumber'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf3ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b404b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ami_df['aep_premise_nb'].nunique())\n",
    "# print(ami_df['serialnumber'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3496a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df = ami_df.sort_index()\n",
    "ami_df['date'] = ami_df.index\n",
    "#-------------------------\n",
    "ami_df_main = ami_df[['aep_premise_nb', 'serialnumber_main', 'value_main', 'date']].copy()\n",
    "ami_df_delt = ami_df[['aep_premise_nb', 'serialnumber_main', 'value_delt', 'date']].copy()\n",
    "ami_df_subm = ami_df[['aep_premise_nb', 'serialnumber_subm', 'value_subm', 'date']].copy()\n",
    "#-------------------------\n",
    "ami_df_main = ami_df_main.rename(columns={\n",
    "    'serialnumber_main':'serialnumber', \n",
    "    'value_main':'value'\n",
    "})\n",
    "\n",
    "ami_df_delt = ami_df_delt.rename(columns={\n",
    "    'serialnumber_main':'serialnumber', \n",
    "    'value_delt':'value'\n",
    "})\n",
    "\n",
    "ami_df_subm = ami_df_subm.rename(columns={\n",
    "    'serialnumber_subm':'serialnumber', \n",
    "    'value_subm':'value'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb945c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77292cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "351597f9",
   "metadata": {},
   "source": [
    "# SEE WEBPAGES\n",
    "https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data\n",
    "https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data/43512887#43512887\n",
    "https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data/56451135#56451135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36655af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class real_time_peak_detection():\n",
    "#     def __init__(self, array, lag, threshold, influence):\n",
    "#         self.y = list(array)\n",
    "#         self.length = len(self.y)\n",
    "#         self.lag = lag\n",
    "#         self.threshold = threshold\n",
    "#         self.influence = influence\n",
    "#         self.signals = [0] * len(self.y)\n",
    "#         self.filteredY = np.array(self.y).tolist()\n",
    "#         self.avgFilter = [0] * len(self.y)\n",
    "#         self.stdFilter = [0] * len(self.y)\n",
    "#         self.avgFilter[self.lag - 1] = np.mean(self.y[0:self.lag]).tolist()\n",
    "#         self.stdFilter[self.lag - 1] = np.std(self.y[0:self.lag]).tolist()\n",
    "\n",
    "#     def thresholding_algo(self, new_value):\n",
    "#         self.y.append(new_value)\n",
    "#         i = len(self.y) - 1\n",
    "#         self.length = len(self.y)\n",
    "#         if i < self.lag:\n",
    "#             return 0\n",
    "#         elif i == self.lag:\n",
    "#             self.signals = [0] * len(self.y)\n",
    "#             self.filteredY = np.array(self.y).tolist()\n",
    "#             self.avgFilter = [0] * len(self.y)\n",
    "#             self.stdFilter = [0] * len(self.y)\n",
    "#             self.avgFilter[self.lag] = np.mean(self.y[0:self.lag]).tolist()\n",
    "#             self.stdFilter[self.lag] = np.std(self.y[0:self.lag]).tolist()\n",
    "#             return 0\n",
    "\n",
    "#         self.signals += [0]\n",
    "#         self.filteredY += [0]\n",
    "#         self.avgFilter += [0]\n",
    "#         self.stdFilter += [0]\n",
    "\n",
    "#         if abs(self.y[i] - self.avgFilter[i - 1]) > (self.threshold * self.stdFilter[i - 1]):\n",
    "\n",
    "#             if self.y[i] > self.avgFilter[i - 1]:\n",
    "#                 self.signals[i] = 1\n",
    "#             else:\n",
    "#                 self.signals[i] = -1\n",
    "\n",
    "#             self.filteredY[i] = self.influence * self.y[i] + \\\n",
    "#                 (1 - self.influence) * self.filteredY[i - 1]\n",
    "#             self.avgFilter[i] = np.mean(self.filteredY[(i - self.lag):i])\n",
    "#             self.stdFilter[i] = np.std(self.filteredY[(i - self.lag):i])\n",
    "#         else:\n",
    "#             self.signals[i] = 0\n",
    "#             self.filteredY[i] = self.y[i]\n",
    "#             self.avgFilter[i] = np.mean(self.filteredY[(i - self.lag):i])\n",
    "#             self.stdFilter[i] = np.std(self.filteredY[(i - self.lag):i])\n",
    "\n",
    "#         return self.signals[i]\n",
    "\n",
    "\n",
    "def thresholding_algo_OLD(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0 #DOESN'T DO ANYTHING IN THIS FUNCTION\n",
    "):\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    for i in range(lag, len(y)):\n",
    "        if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter [i-1]:\n",
    "            if y[i] > avgFilter[i-1]:\n",
    "                signals[i] = 1\n",
    "            else:\n",
    "                signals[i] = -1\n",
    "\n",
    "            filteredY[i] = influence * y[i] + (1 - influence) * filteredY[i-1]\n",
    "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "        else:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))\n",
    "\n",
    "\n",
    "def thresholding_algo(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.mean(y)),\n",
    "                    stdFilter = np.asarray(np.std(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.mean(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b38c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding_algo_median(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.median(y)),\n",
    "                    stdFilter = np.asarray(np.std(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.median(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.median(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.median(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa017496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(y):\n",
    "    return np.mean(np.abs(y - np.mean(y)))\n",
    "\n",
    "def thresholding_algo_mad(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.mean(y)),\n",
    "                    stdFilter = np.asarray(mad(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = mad(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.mean(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = mad(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))\n",
    "\n",
    "\n",
    "def thresholding_algo_median_mad(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.median(y)),\n",
    "                    stdFilter = np.asarray(mad(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.median(y[0:lag])\n",
    "    stdFilter[lag - 1] = mad(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.median(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.median(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = mad(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc810545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df = ami_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ami_df_main[ami_df_main['aep_premise_nb']=='020093989'].sort_index().copy()\n",
    "\n",
    "rtpd = thresholding_algo(\n",
    "    y=tmp_df['value'].tolist(), \n",
    "    lag=48,\n",
    "    threshold=10, \n",
    "    influence=0.0, \n",
    "    signal_abs_threshold=1.0\n",
    ")\n",
    "tmp_df['signals'] = rtpd['signals']\n",
    "#-------------------------\n",
    "fig, ax = Plot_General.default_subplots()\n",
    "sns.lineplot(ax=ax, x='date', y='value', hue='serialnumber', data=tmp_df)\n",
    "sns.scatterplot(ax=ax, x='date', y='value', hue='serialnumber', data=tmp_df[tmp_df['signals']==1], palette='hls')\n",
    "ax.grid(True, which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e798d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ami_df_subm[ami_df_subm['aep_premise_nb']=='020093989'].sort_index().copy()\n",
    "\n",
    "rtpd = thresholding_algo(\n",
    "    y=tmp_df['value'].tolist(), \n",
    "    lag=48,\n",
    "    threshold=10, \n",
    "    influence=0.0, \n",
    "    signal_abs_threshold=1.0\n",
    ")\n",
    "tmp_df['signals'] = rtpd['signals']\n",
    "#-------------------------\n",
    "fig, ax = Plot_General.default_subplots()\n",
    "sns.lineplot(ax=ax, x='date', y='value', hue='serialnumber', data=tmp_df)\n",
    "sns.scatterplot(ax=ax, x='date', y='value', hue='serialnumber', data=tmp_df[tmp_df['signals']==1], palette='hls')\n",
    "ax.grid(True, which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369e00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ami_df_delt[ami_df_delt['aep_premise_nb']=='020093989'].sort_index().copy()\n",
    "\n",
    "rtpd = thresholding_algo(\n",
    "    y=tmp_df['value'].tolist(), \n",
    "    lag=48,\n",
    "    threshold=10, \n",
    "    influence=0.0, \n",
    "    signal_abs_threshold=1.0\n",
    ")\n",
    "tmp_df['signals'] = rtpd['signals']\n",
    "#-------------------------\n",
    "fig, ax = Plot_General.default_subplots()\n",
    "sns.lineplot(ax=ax, x='date', y='value', hue='serialnumber', data=tmp_df)\n",
    "sns.scatterplot(ax=ax, x='date', y='value', hue='serialnumber', data=tmp_df[tmp_df['signals']==1], palette='hls')\n",
    "ax.grid(True, which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b0b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760da86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38873d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_signal_groups_in_df(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber',\n",
    "    signal_col='signals', \n",
    "    return_signal_group_col='signal_grp'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    rtpd_i = thresholding_algo(\n",
    "        y=df_i[value_col].tolist(), \n",
    "        lag=lag,\n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold\n",
    "    )\n",
    "    df_i[signal_col] = rtpd_i['signals']    \n",
    "    \n",
    "    #-------------------------\n",
    "    signals_df_i = df_i.reset_index()[df_i.reset_index()[signal_col]==1]\n",
    "    #-----\n",
    "    tmp_signal_grp_col = Utilities.generate_random_string()\n",
    "    signals_df_i[tmp_signal_grp_col] = np.nan\n",
    "    #-----\n",
    "    tmp_idx_col = Utilities.generate_random_string()\n",
    "    signals_df_i[tmp_idx_col] = signals_df_i.index\n",
    "    #-----\n",
    "    signals_df_i[tmp_signal_grp_col] = signals_df_i[tmp_idx_col].diff().ne(1).cumsum()\n",
    "    #-------------------------\n",
    "    return_df = df_i.copy()\n",
    "    return_df[return_signal_group_col] = np.nan\n",
    "    return_df.iloc[signals_df_i.index,-1] = signals_df_i[tmp_signal_grp_col]\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328faf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_i_signals_gpd(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col='signals'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    assert(df_i.index.name == time_col)\n",
    "    df_i = df_i.sort_index()\n",
    "    #-------------------------\n",
    "    signal_group_col='signal_grp'\n",
    "    df_i = set_signal_groups_in_df(\n",
    "        df_i=df_i, \n",
    "        lag=lag, \n",
    "        threshold=threshold,\n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col,\n",
    "        signal_col=signal_col, \n",
    "        return_signal_group_col=signal_group_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    df_i_signals = df_i[df_i[signal_group_col].notna()]\n",
    "    df_i_signals = df_i_signals.drop(columns=[signal_col])\n",
    "    df_i_signals[signal_group_col] = df_i_signals[signal_group_col].astype(int)\n",
    "    df_i_signals = df_i_signals.reset_index()\n",
    "    assert(time_col in df_i_signals.columns.tolist())\n",
    "    #-------------------------\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "\n",
    "    df_i_signals_gpd = df_i_signals.groupby([signal_group_col]).agg({\n",
    "        time_col:['mean', 'std', 'min', 'max'], \n",
    "        value_col:['mean', 'std', 'max']\n",
    "    })\n",
    "    df_i_signals_gpd=df_i_signals_gpd.sort_values(by=[(time_col, 'mean')])\n",
    "    df_i_signals_gpd[(time_col, 'max_m_min')] = df_i_signals_gpd[(time_col, 'max')] - df_i_signals_gpd[(time_col, 'min')]\n",
    "    #-------------------------\n",
    "    return df_i_signals_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd0da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_SN(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col='signals'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "    \n",
    "    df_i_signals_gpd = build_df_i_signals_gpd(\n",
    "        df_i=df_i, \n",
    "        lag=lag, \n",
    "        threshold=threshold,\n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col, \n",
    "        time_col=time_col, \n",
    "        signal_col=signal_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    peak_max_mean     = df_i_signals_gpd[(value_col, 'max')].mean()\n",
    "    peak_width_mean   = df_i_signals_gpd[(time_col, 'max_m_min')].mean()\n",
    "    peak_spacing_mean = df_i_signals_gpd[(time_col, 'mean')].diff().mean()\n",
    "    # Below, the date being used is of no matter, any random date works, it's simply\n",
    "    #   to make pd.to_datetime happy\n",
    "    if df_i_signals_gpd.shape[0]>0:\n",
    "        peak_hour_mean    = pd.to_datetime(\n",
    "            '2023-01-01 ' + df_i_signals_gpd[(time_col, 'mean')].dt.strftime('%H:%M:%S'), \n",
    "            format=\"%Y-%m-%d %H:%M:%S\"\n",
    "        ).mean().round('H').time().hour\n",
    "    else:\n",
    "        peak_hour_mean = np.nan\n",
    "    #-------------------------\n",
    "    features_srs = pd.Series({\n",
    "        'peak_max_mean':     peak_max_mean, \n",
    "        'peak_width_mean':   peak_width_mean, \n",
    "        'peak_spacing_mean': peak_spacing_mean, \n",
    "        'peak_hour_mean':    peak_hour_mean, \n",
    "    })\n",
    "    features_srs.name = df_i[SN_col].unique()[0]\n",
    "    return features_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d1a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_SN_v2(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col='signals'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    assert(df_i.index.name == time_col)\n",
    "    df_i = df_i.sort_index()\n",
    "    #-------------------------\n",
    "    signal_group_col='signal_grp'\n",
    "    df_i = find_signals_and_set_signal_groups_in_df_i(\n",
    "        df_i=df_i, \n",
    "        lag=lag, \n",
    "        threshold=threshold,\n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col,\n",
    "        signal_col=signal_col, \n",
    "        return_signal_group_col=signal_group_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    df_i_signals = df_i[df_i[signal_group_col].notna()]\n",
    "    df_i_signals = df_i_signals.drop(columns=[signal_col])\n",
    "    df_i_signals[signal_group_col] = df_i_signals[signal_group_col].astype(int)\n",
    "    #-----   \n",
    "    drop_idx = False\n",
    "    if df_i_signals.index.name in df_i_signals.columns:\n",
    "        drop_idx = True\n",
    "    df_i_signals = df_i_signals.reset_index(drop=drop_idx)\n",
    "    assert(time_col in df_i_signals.columns.tolist())\n",
    "    #-------------------------\n",
    "    # Features split into:\n",
    "    #   1. Using all data in peaks pooled together\n",
    "    #   2. First grouping by signal group, then extracting features\n",
    "    #-------------------------\n",
    "    # Features (1):\n",
    "    peak_mean = df_i_signals[value_col].mean()\n",
    "    peak_std  = df_i_signals[value_col].std()\n",
    "    #-------------------------\n",
    "    # Features (2):\n",
    "    #-----\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "\n",
    "    df_i_signals_gpd = df_i_signals.groupby([signal_group_col]).agg({\n",
    "        time_col:['mean', 'std', 'min', 'max'], \n",
    "        value_col:['mean', 'std', 'max']\n",
    "    })\n",
    "    df_i_signals_gpd=df_i_signals_gpd.sort_values(by=[(time_col, 'mean')])\n",
    "    df_i_signals_gpd[(time_col, 'max_m_min')] = df_i_signals_gpd[(time_col, 'max')] - df_i_signals_gpd[(time_col, 'min')]\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    peak_max_mean     = df_i_signals_gpd[(value_col, 'max')].mean()\n",
    "    peak_max_std      = df_i_signals_gpd[(value_col, 'max')].std()\n",
    "    #-----\n",
    "    peak_width_mean   = df_i_signals_gpd[(time_col, 'max_m_min')].mean()\n",
    "    peak_width_std    = df_i_signals_gpd[(time_col, 'max_m_min')].std()\n",
    "    #-----\n",
    "    peak_spacing_mean = df_i_signals_gpd[(time_col, 'mean')].diff().mean()\n",
    "    peak_spacing_std  = df_i_signals_gpd[(time_col, 'mean')].diff().std()\n",
    "    #-----\n",
    "    # Below, the date being used is of no matter, any random date works, it's simply\n",
    "    #   to make pd.to_datetime happy\n",
    "    if df_i_signals_gpd.shape[0]>0:\n",
    "        peak_hour_mean    = pd.to_datetime(\n",
    "            '2023-01-01 ' + df_i_signals_gpd[(time_col, 'mean')].dt.strftime('%H:%M:%S'), \n",
    "            format=\"%Y-%m-%d %H:%M:%S\"\n",
    "        ).mean().round('H').time().hour\n",
    "    else:\n",
    "        peak_hour_mean = np.nan\n",
    "    #-------------------------\n",
    "    features_srs = pd.Series({\n",
    "        'peak_mean':         peak_mean, \n",
    "        'peak_std':          peak_std, \n",
    "        #-----\n",
    "        'peak_max_mean':     peak_max_mean, \n",
    "        'peak_max_std':      peak_max_std, \n",
    "        #-----\n",
    "        'peak_width_mean':   peak_width_mean, \n",
    "        'peak_width_std':    peak_width_std, \n",
    "        #-----\n",
    "        'peak_spacing_mean': peak_spacing_mean,\n",
    "        'peak_spacing_std':  peak_spacing_std,\n",
    "        #-----\n",
    "        'peak_hour_mean':    peak_hour_mean, \n",
    "    })\n",
    "    features_srs.name = df_i[SN_col].unique()[0]\n",
    "    return features_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4aa22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "peak_df_evs = ami_df_main.groupby(['serialnumber']).apply(\n",
    "    lambda x: extract_features_for_SN_v2(\n",
    "        df_i=x, \n",
    "        lag=48, \n",
    "        threshold=10, \n",
    "        influence=0.0, \n",
    "        signal_abs_threshold=2.0, \n",
    "        value_col='value'\n",
    "    )\n",
    ")\n",
    "\n",
    "peak_df_evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56334166",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df_non = ami_df_delt.groupby(['serialnumber']).apply(\n",
    "    lambda x: extract_features_for_SN_v2(\n",
    "        df_i=x, \n",
    "        lag=48, \n",
    "        threshold=10, \n",
    "        influence=0.0, \n",
    "        signal_abs_threshold=2.0, \n",
    "        value_col='value'\n",
    "    )\n",
    ")\n",
    "\n",
    "peak_df_non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc6b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beaaeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df_evs['target']=1\n",
    "peak_df_non['target']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95502a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df=pd.concat([peak_df_evs, peak_df_non])\n",
    "peak_df=peak_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60085d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8bb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df_OG = peak_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b972ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df = peak_df_OG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak_df['peak_max_mean']     = peak_df['peak_max_mean'].fillna(0)\n",
    "# peak_df['peak_width_mean']   = peak_df['peak_width_mean'].fillna(pd.Timedelta.max)\n",
    "# peak_df['peak_spacing_mean'] = peak_df['peak_spacing_mean'].fillna(pd.Timedelta.max)\n",
    "# peak_df['peak_hour_mean']    = peak_df['peak_hour_mean'].fillna(-1)\n",
    "\n",
    "peak_df['peak_mean']         = peak_df['peak_mean'].fillna(0)\n",
    "peak_df['peak_std']          = peak_df['peak_std'].fillna(-1)\n",
    "#-----\n",
    "peak_df['peak_max_mean']     = peak_df['peak_max_mean'].fillna(0)\n",
    "peak_df['peak_max_std']      = peak_df['peak_max_std'].fillna(-1)\n",
    "#-----\n",
    "peak_df['peak_width_mean']   = peak_df['peak_width_mean'].fillna(pd.Timedelta(0))\n",
    "peak_df['peak_width_std']    = peak_df['peak_width_std'].fillna(pd.Timedelta(-1))\n",
    "# peak_df['peak_width_std']    = peak_df['peak_width_std'].fillna(pd.Timedelta(0))\n",
    "# peak_df['peak_width_std']    = peak_df['peak_width_std'].fillna(pd.Timedelta(pd.Timedelta.max))\n",
    "#-----\n",
    "peak_df['peak_spacing_mean'] = peak_df['peak_spacing_mean'].fillna(pd.Timedelta(0))\n",
    "peak_df['peak_spacing_std']  = peak_df['peak_spacing_std'].fillna(pd.Timedelta(-1))\n",
    "# peak_df['peak_spacing_std']  = peak_df['peak_spacing_std'].fillna(pd.Timedelta(0))\n",
    "# peak_df['peak_spacing_std']  = peak_df['peak_spacing_std'].fillna(pd.Timedelta(pd.Timedelta.max))\n",
    "#-----\n",
    "peak_df['peak_hour_mean']    = peak_df['peak_hour_mean'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb7aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df['peak_width_mean']   = peak_df['peak_width_mean'].dt.total_seconds()/60\n",
    "peak_df['peak_width_std']    = peak_df['peak_width_std'].dt.total_seconds()/60\n",
    "#-----\n",
    "peak_df['peak_spacing_mean'] = peak_df['peak_spacing_mean'].dt.total_seconds()/60\n",
    "peak_df['peak_spacing_std']  = peak_df['peak_spacing_std'].dt.total_seconds()/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136c847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16141a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4656c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak_df = peak_df.drop(columns=[\n",
    "#     'peak_mean', 'peak_std', \n",
    "#     'peak_max_std', \n",
    "#     'peak_width_std', \n",
    "#     'peak_spacing_std'\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6b257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59660f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df_train, peak_df_test = train_test_split(peak_df, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = peak_df_train[[x for x in peak_df_train.columns.tolist() if x!='target']]\n",
    "y_train = peak_df_train['target']\n",
    "#-----\n",
    "X_test = peak_df_test[[x for x in peak_df_test.columns.tolist() if x!='target']]\n",
    "y_test = peak_df_test['target']\n",
    "#-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52747393",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators = 10, max_depth=None, n_jobs=None)\n",
    "start = time.time()\n",
    "forest_clf.fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed01dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = forest_clf.predict(X_train)\n",
    "print('*****'*5)\n",
    "print('TRAINING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_train.sum()}\")\n",
    "print(f\"#(target==0): {y_train.shape[0]-y_train.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_train.sum()/(y_train.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_train, y_pred_train))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_train, y_pred_train))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_train, y_pred_train))\n",
    "print()\n",
    "\n",
    "y_pred = forest_clf.predict(X_test)\n",
    "print('*****'*5)\n",
    "print('TESTING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_test.sum()}\")\n",
    "print(f\"#(target==0): {y_test.shape[0]-y_test.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_test.sum()/(y_test.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_test, y_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d110ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_train, y_pred_train, normalize=None), \n",
    "    display_labels=['Not EV','EV']\n",
    ")\n",
    "cmd.plot(values_format='.3e')\n",
    "# cmd.plot(values_format='')\n",
    "# cmd.ax_.set(xlabel='Predicted', ylabel='True')\n",
    "cmd.ax_.set_xlabel('Predicted', fontsize=16)\n",
    "cmd.ax_.set_ylabel('True', fontsize=16)\n",
    "cmd.ax_.set_title('Train', fontsize=20, fontweight='semibold')\n",
    "# cmd.figure_.set_size_inches(12, 10)\n",
    "\n",
    "cmd.figure_.text(1.0, 0.9, \"# Entries:    {:.4e}\".format(y_train.shape[0]), fontsize=14)\n",
    "cmd.figure_.text(1.0, 0.8, \"# Outages:  {:.4e}\".format(y_train.sum()), fontsize=14)\n",
    "cmd.figure_.text(1.0, 0.7, \"# Baseline:  {:.4e}\".format(y_train.shape[0]-y_train.sum()), fontsize=14)\n",
    "cmd.figure_.text(1.0, 0.6, \"% Outages:  {:.4f}\".format(100*y_train.sum()/y_train.shape[0]), fontsize=14)\n",
    "\n",
    "\n",
    "cmd.figure_.text(1.0, 0.4, \"ACCURACY:  {:.4f}\".format(accuracy_score(y_train, y_pred_train)), fontsize=14)\n",
    "cmd.figure_.text(1.0, 0.3, \"PRECISION:  {:.4f}\".format(precision_score(y_train, y_pred_train)), fontsize=14)\n",
    "cmd.figure_.text(1.0, 0.2, \"RECALL:       {:.4f}\".format(recall_score(y_train, y_pred_train)), fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_test, y_pred, normalize=None), \n",
    "    display_labels=['Not EV','EV']\n",
    ")\n",
    "cmd.plot(values_format='.3e')\n",
    "# cmd.plot(values_format='')\n",
    "# cmd.ax_.set(xlabel='Predicted', ylabel='True')\n",
    "cmd.ax_.set_xlabel('Predicted', fontsize=16)\n",
    "cmd.ax_.set_ylabel('True', fontsize=16)\n",
    "cmd.ax_.set_title('Test', fontsize=20, fontweight='semibold')\n",
    "# cmd.figure_.set_size_inches(12, 10)\n",
    "\n",
    "cmd.figure_.text(1.0, 0.9, \"# Entries:    {:.4e}\".format(y_test.shape[0]), fontsize=14)\n",
    "cmd.figure_.text(1.0, 0.8, \"# Outages:  {:.4e}\".format(y_test.sum()), fontsize=14)\n",
    "cmd.figure_.text(1.0, 0.7, \"# Baseline:  {:.4e}\".format(y_test.shape[0]-y_test.sum()), fontsize=14)\n",
    "cmd.figure_.text(1.0, 0.6, \"% Outages:  {:.4f}\".format(100*y_test.sum()/y_test.shape[0]), fontsize=14)\n",
    "\n",
    "cmd.figure_.text(1.0, 0.4, \"ACCURACY:  {:.4f}\".format(accuracy_score(y_test, y_pred)), fontsize=14)\n",
    "cmd.figure_.text(1.0, 0.3, \"PRECISION:  {:.4f}\".format(precision_score(y_test, y_pred)), fontsize=14)\n",
    "cmd.figure_.text(1.0, 0.2, \"RECALL:       {:.4f}\".format(recall_score(y_test, y_pred)), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e231a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(peak_df.columns[:-1])==len(forest_clf.feature_importances_))\n",
    "importances = list(zip(peak_df.columns[:-1], forest_clf.feature_importances_))\n",
    "importances_srtd = sorted(importances, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_srtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43051148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9bc68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e749ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27845a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f109605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04037dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_it_funky(df_i, n_entries=500, val_col='value', SN_col='serialnumber'):\n",
    "    if df_i.shape[0] < n_entries:\n",
    "        return None\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    vals_i = df_i[val_col].sort_index().iloc[:n_entries].tolist()\n",
    "    return_i = pd.Series(vals_i)\n",
    "    return_i.name = df_i[SN_col].unique()[0]\n",
    "    return return_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4bf023",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_evs = ami_df_evs.groupby(['serialnumber']).apply(\n",
    "    lambda x: make_it_funky(x)\n",
    ")\n",
    "keras_df_non = ami_df_non.groupby(['serialnumber']).apply(\n",
    "    lambda x: make_it_funky(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_evs['target']=1\n",
    "keras_df_non['target']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bf8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df = pd.concat([keras_df_evs, keras_df_non])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df=keras_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_train, keras_df_test = train_test_split(keras_df, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4cce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cae561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed587bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ce2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras_df_train[[x for x in keras_df_train.columns.tolist() if x != 'target']].values\n",
    "y_train = keras_df_train['target'].values\n",
    "x_train=x_train.reshape((x_train.shape[0], x_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = keras_df_test[[x for x in keras_df_test.columns.tolist() if x != 'target']].values\n",
    "y_test = keras_df_test['target'].values\n",
    "x_test=x_test.reshape((x_test.shape[0], x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No longer needed because train_test_splot\n",
    "# idx = np.random.permutation(len(x_train))\n",
    "# x_train = x_train[idx]\n",
    "# y_train = y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab718c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    num_classes=2\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d44287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(input_shape=x_train.shape[1:])\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a33841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"best_model.h5\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef81b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model.evaluate(x_train, y_train)\n",
    "print(\"Train accuracy\", train_acc)\n",
    "print(\"Train loss\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5478e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febcffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04faa10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a85d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c79502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfa0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4af99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nSNs_per_PN = ami_df_full[['aep_premise_nb', 'serialnumber']].drop_duplicates()['aep_premise_nb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "nSNs_per_PN[nSNs_per_PN==1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_HOLDOUT = ami_df_full[ami_df_full['aep_premise_nb'].isin(nSNs_per_PN[nSNs_per_PN==1].index.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ami_df_HOLDOUT['aep_premise_nb'].tolist()).intersection(ami_df['aep_premise_nb'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_HOLDOUT = ami_df_HOLDOUT.groupby(['serialnumber', 'aep_premise_nb']).apply(\n",
    "    lambda x: make_it_funky(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_HOLDOUT['target']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf029502",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_HOLDOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f51a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_HOLDOUT.loc[keras_df_HOLDOUT.index.get_level_values(1).isin(evs_prems), 'target']=1\n",
    "keras_df_HOLDOUT.loc[keras_df_HOLDOUT.index.get_level_values(1).isin(non_prems), 'target']=0\n",
    "assert(pd.isna(keras_df_HOLDOUT['target']).sum()==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ecc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_HOLDOUT=keras_df_HOLDOUT.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961563cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_HOLDOUT = keras_df_HOLDOUT[[x for x in keras_df_HOLDOUT.columns.tolist() if x != 'target']].values\n",
    "y_HOLDOUT = keras_df_HOLDOUT['target'].values\n",
    "x_HOLDOUT=x_HOLDOUT.reshape((x_HOLDOUT.shape[0], x_HOLDOUT.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b278e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_HOLDOUT = model.predict(x_HOLDOUT)\n",
    "y_pred_HOLDOUT = np.argmax(y_pred_HOLDOUT, axis=1)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_HOLDOUT, y_pred_HOLDOUT))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_HOLDOUT, y_pred_HOLDOUT))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_HOLDOUT, y_pred_HOLDOUT))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_HOLDOUT, y_pred_HOLDOUT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c1fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ecfb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37765e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df = pd.read_pickle(r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\ami_df.pkl')\n",
    "all_trff_dfs = pd.read_pickle(r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\all_trff_dfs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evs_prems = all_trff_dfs[all_trff_dfs['EV']==1]['PREM_NB'].unique().tolist()\n",
    "non_prems = all_trff_dfs[all_trff_dfs['EV']==0]['PREM_NB'].unique().tolist()\n",
    "\n",
    "# aep_premise_nb in ami_df is of type object (i.e., a string), whereas PREM_NB in trff_df is int64\n",
    "evs_prems = [str(x) for x in evs_prems]\n",
    "non_prems = [str(x) for x in non_prems]\n",
    "#-----\n",
    "ami_df_evs = ami_df[ami_df['aep_premise_nb'].isin(evs_prems)].copy()\n",
    "ami_df_non = ami_df[ami_df['aep_premise_nb'].isin(non_prems)].copy()\n",
    "#----\n",
    "assert(ami_df.shape[0]==ami_df_evs.shape[0]+ami_df_non.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7b4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_it_funky(df_i, n_entries=500, val_col='value', SN_col='serialnumber'):\n",
    "    if df_i.shape[0] < n_entries:\n",
    "        return None\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    vals_i = df_i[val_col].sort_index().iloc[:n_entries].tolist()\n",
    "    return_i = pd.Series(vals_i)\n",
    "    return_i.name = df_i[SN_col].unique()[0]\n",
    "    return return_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35bf02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_evs = ami_df_evs.groupby(['serialnumber']).apply(\n",
    "    lambda x: make_it_funky(x)\n",
    ")\n",
    "keras_df_non = ami_df_non.groupby(['serialnumber']).apply(\n",
    "    lambda x: make_it_funky(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b10efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_evs['target']=1\n",
    "keras_df_non['target']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df = pd.concat([keras_df_evs, keras_df_non])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df=keras_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051edec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_train, keras_df_test = train_test_split(keras_df, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c7530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ca52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4551269",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320dce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4703310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ce833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d827203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503fe7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding_algo_v2(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.mean(y)),\n",
    "                    stdFilter = np.asarray(np.std(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                #-------------------------\n",
    "                # If stdFilter[i-1]==0, try np.mean(stdFilter)\n",
    "                # If both==0, use raw value, y[i] - avgFilter[i-1]\n",
    "                if stdFilter[i-1]>0:\n",
    "                    signals[i] = (y[i] - avgFilter[i-1])/stdFilter[i-1]\n",
    "                else:\n",
    "                    if np.mean(stdFilter)>0:\n",
    "                        signals[i] = (y[i] - avgFilter[i-1])/np.mean(stdFilter)\n",
    "                    else:\n",
    "                        signals[i] = (y[i] - avgFilter[i-1])\n",
    "                #-------------------------\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.mean(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return np.asarray(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fa51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding_algo_v3(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.mean(y)),\n",
    "                    stdFilter = np.asarray(np.std(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                signals[i]=1\n",
    "                #-------------------------\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.mean(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return np.asarray(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_signal_in_df_i(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber',\n",
    "    signal_col='signals', \n",
    "    return_signal_group_col='signal_grp'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    rtpd_i = thresholding_algo_v3(\n",
    "        y=df_i[value_col].tolist(), \n",
    "        lag=lag,\n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold\n",
    "    )\n",
    "    df_i[signal_col] = rtpd_i\n",
    "    return df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b17e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52840bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d7eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df['serialnumber'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025faf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = ami_df[ami_df['serialnumber'].isin(['794400462', '780269113', '764972211', '784554124', '762443496'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16afc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "idk2 = dev_df.groupby(['serialnumber']).apply(\n",
    "    lambda x: set_signal_in_df_i(\n",
    "        df_i=x, \n",
    "        lag=48,\n",
    "        threshold=5, \n",
    "        influence=0.0, \n",
    "        signal_abs_threshold=1.0, \n",
    "        value_col='value', \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5db75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e725d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ami_df_evs = ami_df_evs.copy()\n",
    "dev_ami_df_non = ami_df_non.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ami_df_evs = dev_ami_df_evs.groupby(['serialnumber']).apply(\n",
    "    lambda x: set_signal_in_df_i(\n",
    "        df_i=x, \n",
    "        lag=48,\n",
    "        threshold=5, \n",
    "        influence=0.0, \n",
    "        signal_abs_threshold=1.0, \n",
    "        value_col='value', \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951e51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_ami_df_non = dev_ami_df_non.groupby(['serialnumber']).apply(\n",
    "    lambda x: set_signal_in_df_i(\n",
    "        df_i=x, \n",
    "        lag=48,\n",
    "        threshold=5, \n",
    "        influence=0.0, \n",
    "        signal_abs_threshold=1.0, \n",
    "        value_col='value', \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd8773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_evs = dev_ami_df_evs.reset_index(drop=True).groupby(['serialnumber']).apply(\n",
    "    lambda x: make_it_funky(\n",
    "        df_i=x, \n",
    "        n_entries=500, \n",
    "        val_col='signals', \n",
    "        SN_col='serialnumber'\n",
    "    )\n",
    ")\n",
    "keras_df_non = dev_ami_df_non.reset_index(drop=True).groupby(['serialnumber']).apply(\n",
    "    lambda x: make_it_funky(\n",
    "        df_i=x, \n",
    "        n_entries=500, \n",
    "        val_col='signals', \n",
    "        SN_col='serialnumber'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_evs['target']=1\n",
    "keras_df_non['target']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df = pd.concat([keras_df_evs, keras_df_non])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ceaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd53848",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df=keras_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e79c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_train, keras_df_test = train_test_split(keras_df, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d826b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1568890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7777a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fac6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras_df_train[[x for x in keras_df_train.columns.tolist() if x != 'target']].values\n",
    "y_train = keras_df_train['target'].values\n",
    "x_train=x_train.reshape((x_train.shape[0], x_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = keras_df_test[[x for x in keras_df_test.columns.tolist() if x != 'target']].values\n",
    "y_test = keras_df_test['target'].values\n",
    "x_test=x_test.reshape((x_test.shape[0], x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd05b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6acf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e883b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(input_shape=x_train.shape[1:])\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e63bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68eb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"best_model.h5\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d868b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c580bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model.evaluate(x_train, y_train)\n",
    "print(\"Train accuracy\", train_acc)\n",
    "print(\"Train loss\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4ee3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae4b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f596e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfeae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df[ami_df['serialnumber']=='791084258']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfef45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265b1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d2330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691428b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013df57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
