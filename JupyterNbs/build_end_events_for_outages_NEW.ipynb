{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "# NOTE: To reload a class imported as, e.g., \n",
    "# from module import class\n",
    "# One must call:\n",
    "#   1. import module\n",
    "#   2. reload module\n",
    "#   3. from module import class\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns\n",
    "from packaging import version\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "#sys.path.insert(0, os.path.join(os.path.realpath('..'), 'Utilities'))\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "from Utilities_df import DFConstructType\n",
    "import Utilities_dt\n",
    "import Plot_Box_sns\n",
    "import GrubbsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In MeterPremise, can I create a function which will build mp_df_hist given mp_df_curr?\n",
    "In DOVSOutages I need to build new build_mp_for_outg (and update its use throughout)\n",
    "    APPARENTLY I already have this, build_active_MP_for_outages or one of other similar functions\n",
    "    \n",
    "I need to basically replace everything in DOVSOutages which uses build_mp_for_outg\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_active_MP_for_outages_df(\n",
    "    df_outage, \n",
    "    prem_nb_col, \n",
    "    df_mp_curr=None, \n",
    "    df_mp_hist=None, \n",
    "    assert_all_PNs_found=True, \n",
    "    drop_inst_rmvl_cols=False, \n",
    "    outg_rec_nb_col='OUTG_REC_NB',  #TODO!!!!!!!!!!!!!!!!!!!!!!! what if index?!\n",
    "    is_slim=False, \n",
    "    dt_on_ts_col='DT_ON_TS', \n",
    "    df_off_ts_full_col='DT_OFF_TS_FULL', \n",
    "    consolidate_PNs_batch_size=1000, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Similar to build_active_MP_for_outages\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(prem_nb_col in df_outage.columns and \n",
    "           dt_on_ts_col in df_outage.columns and \n",
    "           df_off_ts_full_col in df_outage.columns)\n",
    "    #-------------------------\n",
    "    if not is_slim:\n",
    "        PNs = df_outage[prem_nb_col].unique().tolist()\n",
    "    else:\n",
    "        PNs = Utilities_df.consolidate_column_of_lists(\n",
    "            df=df_outage, \n",
    "            col=prem_nb_col, \n",
    "            sort=True,\n",
    "            include_None=False,\n",
    "            batch_size=consolidate_PNs_batch_size, \n",
    "            verbose=False\n",
    "        )\n",
    "    #-----\n",
    "    PNs = [x for x in PNs if pd.notna(x)]\n",
    "    #-------------------------\n",
    "    mp_df_curr_hist_dict = MeterPremise.build_mp_df_curr_hist_for_PNs(\n",
    "        PNs=PNs, \n",
    "        mp_df_curr=df_mp_curr,\n",
    "        mp_df_hist=df_mp_hist, \n",
    "        join_curr_hist=False, \n",
    "        addtnl_mp_df_curr_cols=None, \n",
    "        addtnl_mp_df_hist_cols=None, \n",
    "        assert_all_PNs_found=assert_all_PNs_found, \n",
    "        assume_one_xfmr_per_PN=True, \n",
    "        drop_approx_duplicates=True\n",
    "    )\n",
    "    df_mp_curr = mp_df_curr_hist_dict['mp_df_curr']\n",
    "    df_mp_hist = mp_df_curr_hist_dict['mp_df_hist']\n",
    "    #-------------------------\n",
    "    # Only reason for making dict is to ensure outg_rec_nbs are not repeated \n",
    "    active_SNs_in_outgs_dfs_dict = {}\n",
    "\n",
    "    if not is_slim:\n",
    "        for outg_rec_nb_i, df_i in df_outage.groupby(outg_rec_nb_col):\n",
    "            # Don't want to include outg_rec_nb_i=-2147483648\n",
    "            if int(outg_rec_nb_i) < 0:\n",
    "                continue\n",
    "            # There should only be a single unique dt_on_ts and dt_off_ts_full for each outage\n",
    "            if(df_i[dt_on_ts_col].nunique()!=1 or \n",
    "               df_i[df_off_ts_full_col].nunique()!=1):\n",
    "                print(f'outg_rec_nb_i = {outg_rec_nb_i}')\n",
    "                print(f'df_i[dt_on_ts_col].nunique()       = {df_i[dt_on_ts_col].nunique()}')\n",
    "                print(f'df_i[df_off_ts_full_col].nunique() = {df_i[df_off_ts_full_col].nunique()}')\n",
    "                print('CRASH IMMINENT!')\n",
    "                assert(0)\n",
    "            # Grab power out/on time and PNs from df_i\n",
    "            dt_on_ts_i       = df_i[dt_on_ts_col].unique()[0]\n",
    "            df_off_ts_full_i = df_i[df_off_ts_full_col].unique()[0]\n",
    "            PNs_i            = df_i[prem_nb_col].unique().tolist()\n",
    "\n",
    "            # Just as was done above for PNs, NaN values must be removed from PNs_i\n",
    "            #   The main purpose here is to remove instances where PNs_i = [nan]\n",
    "            #   NOTE: For case of slim df, the NaNs should already be removed\n",
    "            # After removal, if len(PNs_i)==0, contine\n",
    "            PNs_i = [x for x in PNs_i if pd.notna(x)]\n",
    "            if len(PNs_i)==0:\n",
    "                continue\n",
    "            \n",
    "            # Build active_SNs_df_i and add it to active_SNs_in_outgs_dfs_dict\n",
    "            # NOTE: assume_one_xfmr_per_PN=True above in MeterPremise.build_mp_df_curr_hist_for_PNs,\n",
    "            #       so does not need to be set again (i.e., assume_one_xfmr_per_PN=False below)\n",
    "            active_SNs_df_i = MeterPremise.get_active_SNs_for_PNs_at_datetime_interval(\n",
    "                PNs=PNs_i,\n",
    "                df_mp_curr=df_mp_curr, \n",
    "                df_mp_hist=df_mp_hist, \n",
    "                dt_0=df_off_ts_full_i,\n",
    "                dt_1=dt_on_ts_i,\n",
    "                assume_one_xfmr_per_PN=False, \n",
    "                output_index=None,\n",
    "                output_groupby=None, \n",
    "                assert_all_PNs_found=False\n",
    "            )\n",
    "            active_SNs_df_i[outg_rec_nb_col] = outg_rec_nb_i\n",
    "            assert(outg_rec_nb_i not in active_SNs_in_outgs_dfs_dict)\n",
    "            active_SNs_in_outgs_dfs_dict[outg_rec_nb_i] = active_SNs_df_i\n",
    "    else:\n",
    "        for outg_rec_nb_i, row_i in df_outage.iterrows():\n",
    "            # NOTE: assume_one_xfmr_per_PN=True above in MeterPremise.build_mp_df_curr_hist_for_PNs,\n",
    "            #       so does not need to be set again (i.e., assume_one_xfmr_per_PN=False below)\n",
    "            active_SNs_df_i = MeterPremise.get_active_SNs_for_PNs_at_datetime_interval(\n",
    "                PNs=row_i[prem_nb_col],\n",
    "                df_mp_curr=df_mp_curr, \n",
    "                df_mp_hist=df_mp_hist, \n",
    "                dt_0=row_i[df_off_ts_full_col],\n",
    "                dt_1=row_i[dt_on_ts_col],\n",
    "                assume_one_xfmr_per_PN=False, \n",
    "                output_index=None,\n",
    "                output_groupby=None, \n",
    "                assert_all_PNs_found=False\n",
    "            )\n",
    "            active_SNs_df_i[outg_rec_nb_col] = outg_rec_nb_i\n",
    "            assert(outg_rec_nb_i not in active_SNs_in_outgs_dfs_dict)\n",
    "            active_SNs_in_outgs_dfs_dict[outg_rec_nb_i] = active_SNs_df_i\n",
    "    #-------------------------\n",
    "    active_SNs_df = pd.concat(list(active_SNs_in_outgs_dfs_dict.values()))\n",
    "    #-------------------------\n",
    "    if drop_inst_rmvl_cols:\n",
    "        active_SNs_df = active_SNs_df.drop(columns=[df_mp_install_time_col, df_mp_removal_time_col])\n",
    "    #-------------------------\n",
    "    return active_SNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a980870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a3d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def offset_int_tagged_files_in_dir(\n",
    "    files_dir, \n",
    "    file_name_regex, \n",
    "    offset_int=None, \n",
    "    new_0_int=None, \n",
    "    new_dir=None, \n",
    "    file_name_glob=None, \n",
    "    copy_and_rename=False, \n",
    "    return_rename_summary=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Offset all of the files in files_dir by offset_int.\n",
    "    The directory files_dir is expected to contain files of the form [file_idx_0, file_idx_1, ..., file_idx_n] where\n",
    "        idx_0, idx_1, ..., idx_n are integers.\n",
    "    The files can either be renamed using the offset_int argument OR the new_0_int argument, BUT NOT BOTH.\n",
    "        For the case of offset_int:\n",
    "            The files in this directory will be renamed [file_{idx_0+offset_int}, file_{idx_1+offset_int}, ..., file_{idx_n+offset_int}]\n",
    "        For the case of new_0_int:\n",
    "            The files in this directory will be renamed [file_{new_0_int}, file_{new_0_int+1}, ..., file_{new_0_int+n_files-1}]\n",
    "    The files can simply be moved/renamed (copy_and_rename==False), or copied to the new directory (copy_and_rename==True and \n",
    "        new_dir not None)\n",
    "    -------------------------\n",
    "    files_dir:\n",
    "        The directory housing the files to be renamed\n",
    "        \n",
    "    file_name_regex:\n",
    "        A regex patten used to both identify the files to be renamed and to find the integer tag for each file.\n",
    "        NOTE: file_name_regex MUST have some sort of digit capture (e.g., contain '(\\d*)')\n",
    "              e.g., for the case of end events, one would use file_name_regex = r'end_events_(\\d*).csv'\n",
    "              \n",
    "    offset_int/new_0_int:\n",
    "        These direct how the files will be renamed.  \n",
    "        ONLY ONE OF THESE SHOULD BE USED ==> one should always be set to None and the other should be set to some int value\n",
    "        offset_int:\n",
    "            Take the identifier/tag ints, and simply shift them by offset_int.\n",
    "        new_0_int:\n",
    "            Start the identifier tags at new_0_int, and label from new_0_int to new_0_int + len(files in files_dir)-1\n",
    "            \n",
    "    new_dir:\n",
    "        The directory to which the renamed files will be saved.\n",
    "        Default value is None, which means the files will be saved in the input files_dir\n",
    "              \n",
    "    file_name_glob:\n",
    "        Used in Utilities.find_all_paths to help find the paths to be renamed.  By default this is set to None, which is then\n",
    "            changed to = '*', meaning the glob portion doesn't trim down the list of files at all, but returns all contained\n",
    "            in the directory.  Therefore, file_name_regex does all of the work, which is fine and really as designed\n",
    "            \n",
    "    copy_and_rename:\n",
    "        Directs whether to call rename or copy.\n",
    "        copy_and_rename=False:\n",
    "            Default behavior, which means the files will be renamed and replaced.\n",
    "        copy_and_rename=False:\n",
    "            The files will be copied and renamed, with the originals kept intact.  This is only possible if new_dir is not None\n",
    "            and new_dir != files_dir\n",
    "        \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Exclusive or for offset_int and new_0_int (meaning, one but not both must not be None)\n",
    "    assert(offset_int is not None or new_0_int is not None)\n",
    "    assert(not(offset_int is not None and new_0_int is not None))\n",
    "    #-------------------------\n",
    "    assert(os.path.isdir(files_dir))\n",
    "    if file_name_glob is None:\n",
    "        file_name_glob = '*'\n",
    "    if new_dir is None:\n",
    "        new_dir = files_dir\n",
    "    #-------------------------\n",
    "    if new_dir==files_dir:\n",
    "        copy_and_rename = False\n",
    "    #-------------------------\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    #-------------------------\n",
    "    paths = Utilities.find_all_paths(\n",
    "        base_dir=files_dir, \n",
    "        glob_pattern=file_name_glob, \n",
    "        regex_pattern=file_name_regex\n",
    "    )\n",
    "    #-------------------------\n",
    "    paths_w_tags = []\n",
    "    for path in paths:\n",
    "        tag = re.findall(file_name_regex, path)\n",
    "        print(path)\n",
    "        print(file_name_regex)\n",
    "        print(tag)\n",
    "        print()\n",
    "        # Should have only been one tag found per path\n",
    "        if len(tag)>1:\n",
    "            print(tag)\n",
    "        assert(len(tag)==1)\n",
    "        tag=int(tag[0])\n",
    "        paths_w_tags.append((path, tag))\n",
    "    # NOTE: Want to sort in reverse so that highest is first.  This is so there are no naming issues when the rename occurs.  \n",
    "    #       E.g., imaging there are 10 members in paths, tagged 0 through nine, and they are to be offset by 1\n",
    "    #         If we started with the lowest tag, file_0, shifting it by 1 would make it file_1, which already exists!\n",
    "    #         If, instead, we start with the highest tag, file_9, it shifts by 1 to file_10, which is not an issue.\n",
    "    paths_w_tags = natsorted(paths_w_tags, key=lambda x: x[1], reverse=True)\n",
    "    #-------------------------\n",
    "    rename_summary = {}\n",
    "    for i,path_w_tag_i in enumerate(paths_w_tags):\n",
    "        path_i = path_w_tag_i[0]\n",
    "        tag_i  = path_w_tag_i[1]\n",
    "        file_name_i = os.path.basename(path_i)\n",
    "        #-----\n",
    "        assert(str(tag_i) in file_name_i)\n",
    "        if offset_int is not None:\n",
    "            repl_int_i = str(tag_i+offset_int)\n",
    "        elif new_0_int is not None:\n",
    "            # Remember, sorted in descending order\n",
    "            repl_int_i = str(len(paths_w_tags)+new_0_int-(i+1))\n",
    "        else:\n",
    "            assert(0)\n",
    "        new_file_name_i=file_name_i.replace(str(tag_i), str(repl_int_i))\n",
    "        #-----\n",
    "        assert(new_file_name_i not in os.listdir(new_dir))\n",
    "        new_path_i = os.path.join(new_dir, new_file_name_i)\n",
    "        #-----\n",
    "        assert(path_i not in rename_summary.keys())\n",
    "        rename_summary[path_i] = new_path_i\n",
    "        #-----\n",
    "        if copy_and_rename:\n",
    "            shutil.copy(src=path_i, dst=new_path_i)\n",
    "        else:\n",
    "            os.rename(path_i, new_path_i)\n",
    "    #-------------------------\n",
    "    if return_rename_summary:\n",
    "        return rename_summary\n",
    "    \n",
    "    \n",
    "    \n",
    "def offset_int_tagged_files_w_summaries_in_dir(\n",
    "    files_dir, \n",
    "    file_name_regex, \n",
    "    offset_int=None, \n",
    "    new_0_int=None, \n",
    "    new_dir=None, \n",
    "    file_name_glob=None, \n",
    "    \n",
    "    summary_files_dir=None,\n",
    "    summary_file_name_regex=None,\n",
    "    summary_file_name_glob=None, \n",
    "    summary_new_dir=None, \n",
    "    \n",
    "    copy_and_rename=False, \n",
    "    return_rename_summary=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if summary_files_dir is None:\n",
    "        summary_files_dir = os.path.join(files_dir, 'summary_files')\n",
    "    if summary_file_name_regex is None:\n",
    "        summary_file_name_regex = file_name_regex.replace('_(\\d*).csv', '_([0-9]*)_summary.json')\n",
    "    if summary_file_name_glob is None:\n",
    "        if file_name_glob is None:\n",
    "            summary_file_name_glob = '*'\n",
    "        else:\n",
    "            summary_file_name_glob = file_name_glob.replace('_*.csv', '*_summary.json')\n",
    "    if summary_new_dir is None:\n",
    "        if new_dir is None:\n",
    "            summary_new_dir = os.path.join(files_dir, 'summary_files')\n",
    "        else:\n",
    "            summary_new_dir = os.path.join(new_dir, 'summary_files')\n",
    "    #-------------------------\n",
    "    files_rename_summary = offset_int_tagged_files_in_dir(\n",
    "        files_dir=files_dir, \n",
    "        file_name_regex=file_name_regex, \n",
    "        offset_int=offset_int, \n",
    "        new_0_int=new_0_int, \n",
    "        new_dir=new_dir, \n",
    "        file_name_glob=file_name_glob, \n",
    "        copy_and_rename=copy_and_rename, \n",
    "        return_rename_summary=True\n",
    "    )\n",
    "    #-------------------------\n",
    "    summaries_rename_summary = offset_int_tagged_files_in_dir(\n",
    "        files_dir=summary_files_dir, \n",
    "        file_name_regex=summary_file_name_regex, \n",
    "        offset_int=offset_int, \n",
    "        new_0_int=new_0_int, \n",
    "        new_dir=summary_new_dir, \n",
    "        file_name_glob=summary_file_name_glob, \n",
    "        copy_and_rename=copy_and_rename, \n",
    "        return_rename_summary=True\n",
    "    )\n",
    "    #-------------------------\n",
    "    assert(len(files_rename_summary)==len(summaries_rename_summary))\n",
    "    if return_rename_summary:\n",
    "        return (files_rename_summary, summaries_rename_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01df82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724868bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_drct_subset_dir_from_end_events_dir(\n",
    "    files_dir,\n",
    "    save_dir, \n",
    "    drct_subset_type='drct_strict', \n",
    "    xfmr_equip_typ_nms_of_interest = ['TRANSFORMER, OH', 'TRANSFORMER, UG'], \n",
    "    file_path_glob = r'end_events_[0-9]*.csv', \n",
    "    file_path_regex = None, \n",
    "    outg_rec_nb_col='outg_rec_nb', \n",
    "    trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "    batch_size=100, \n",
    "    verbose=True, \n",
    "    n_update=1, \n",
    "    cols_and_types_to_convert_dict=None, \n",
    "    to_numeric_errors='coerce', \n",
    "    assert_all_cols_equal=True    \n",
    "):\n",
    "    r\"\"\"\n",
    "    From an EndEvents directory, save the subset of direct outage transformers to a new directory.\n",
    "    NOTE: end_events_dfs must have trsf_pole_nb information!!!!!\n",
    "            So, older versions, which don't include this information, will not work!\n",
    "    NOTE: summary_files subdirectory will not be transferred. \n",
    "          So, if one needs that info, grab it from the original directory\n",
    "          \n",
    "    xfmr_equip_typ_nms_of_interest:\n",
    "        Used only for the case of drct_subset_type=='drct_strict'\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(drct_subset_type in ['drct_strict', 'drct'])\n",
    "    #-------------------------\n",
    "    paths = Utilities.find_all_paths(\n",
    "        base_dir=files_dir, \n",
    "        glob_pattern=file_path_glob, \n",
    "        regex_pattern=file_path_regex\n",
    "    )\n",
    "    if len(paths)==0:\n",
    "        print(f'No paths found in files_dir = {files_dir}')\n",
    "        return None\n",
    "    paths=natsorted(paths)\n",
    "    #-------------------------\n",
    "    save_args = dict(\n",
    "        save_to_file=True, \n",
    "        save_dir=save_dir, \n",
    "        save_name=r'end_events.csv', \n",
    "        save_summary=False, \n",
    "        index=True\n",
    "    )\n",
    "    #-----\n",
    "    save_args = GenAn.prepare_save_args(save_args)\n",
    "    save_args['offset_int'] = GenAn.get_next_summary_file_tag_int(save_args)\n",
    "    # This is really intended to build new directories with subsets of data\n",
    "    #   ==> offset_int should be 0\n",
    "    assert(save_args['offset_int']==0)\n",
    "    #-------------------------\n",
    "    batch_idxs = Utilities.get_batch_idx_pairs(len(paths), batch_size)\n",
    "    n_batches = len(batch_idxs)    \n",
    "    if verbose:\n",
    "        print(f'n_paths = {len(paths)}')\n",
    "        print(f'batch_size = {batch_size}')\n",
    "        print(f'n_batches = {n_batches}')\n",
    "    #-------------------------\n",
    "    counter = 0\n",
    "    for i, batch_i in enumerate(batch_idxs):\n",
    "        if verbose and (i+1)%n_update==0:\n",
    "            print(f'{i+1}/{n_batches}')\n",
    "        i_beg = batch_i[0]\n",
    "        i_end = batch_i[1]\n",
    "        #-----\n",
    "        # NOTE: make_all_columns_lowercase=True because...\n",
    "        #   EMR would return lowercase outg_rec_nb or outg_rec_nb_gpd_for_sql\n",
    "        #   Athena maintains the original case, and does not conver to lower case,\n",
    "        #     so it returns OUTG_REC_NB or OUTG_REC_NB_GPD_FOR_SQL\n",
    "        end_events_df_i = GenAn.read_df_from_csv_batch(\n",
    "            paths=paths[i_beg:i_end], \n",
    "            cols_and_types_to_convert_dict=cols_and_types_to_convert_dict, \n",
    "            to_numeric_errors=to_numeric_errors, \n",
    "            make_all_columns_lowercase=True, \n",
    "            assert_all_cols_equal=assert_all_cols_equal\n",
    "        )\n",
    "        if end_events_df_i.shape[0]==0:\n",
    "            continue\n",
    "        #-------------------------\n",
    "        if outg_rec_nb_col not in end_events_df_i.columns.tolist():\n",
    "            outg_rec_nb_col = f'{outg_rec_nb_col}_gpd_for_sql'\n",
    "        assert(outg_rec_nb_col in end_events_df_i.columns.tolist())\n",
    "        #-----\n",
    "        if trsf_pole_nb_col not in end_events_df_i.columns.tolist():\n",
    "            trsf_pole_nb_col = f'{trsf_pole_nb_col}_gpd_for_sql'\n",
    "        assert(trsf_pole_nb_col in end_events_df_i.columns.tolist())\n",
    "        #-------------------------\n",
    "        cols_b4_merge = end_events_df_i.columns.tolist()\n",
    "        end_events_df_i = DOVSOutages.append_outg_info_to_df(\n",
    "            df=end_events_df_i, \n",
    "            outg_rec_nb_idfr=outg_rec_nb_col, \n",
    "            contstruct_df_args=None, \n",
    "            build_sql_function = DOVSOutages_SQL.build_sql_outage, \n",
    "            build_sql_function_kwargs=dict(\n",
    "                include_DOVS_EQUIPMENT_TYPES_DIM=True, \n",
    "                cols_of_interest=['OUTG_REC_NB', 'LOCATION_ID'], \n",
    "                select_cols_DOVS_EQUIPMENT_TYPES_DIM=['EQUIP_TYP_NM']\n",
    "            )\n",
    "        )\n",
    "        #-------------------------\n",
    "        if drct_subset_type=='drct_strict':\n",
    "            end_events_df_i = end_events_df_i[(end_events_df_i['LOCATION_ID']==end_events_df_i[trsf_pole_nb_col]) & \n",
    "                                              (end_events_df_i['EQUIP_TYP_NM'].isin(xfmr_equip_typ_nms_of_interest))]\n",
    "        elif drct_subset_type=='drct':\n",
    "            end_events_df_i = end_events_df_i[end_events_df_i['LOCATION_ID']==end_events_df_i[trsf_pole_nb_col]]\n",
    "        else:\n",
    "            assert(0)\n",
    "        #-------------------------\n",
    "        # Drop the columns which were added from DOVS (via DOVSOutages.append_outg_info_to_df) and used to \n",
    "        #   identify drct or drct_strict types\n",
    "        end_events_df_i = end_events_df_i.drop(columns=set(end_events_df_i.columns.tolist()).difference(set(cols_b4_merge)))\n",
    "        #-------------------------\n",
    "        save_args_i = copy.deepcopy(save_args)\n",
    "        batch_int = counter + save_args_i['offset_int']\n",
    "        save_args_i['save_name'] = Utilities.append_to_path(save_args_i['save_name'], appendix=f'_{batch_int}', \n",
    "                                                            ext_to_find=save_args_i['save_ext'], append_to_end_if_ext_no_found=True)\n",
    "        # Call prepare_save_args again to compile save_path, save_summary_path, etc.                                          \n",
    "        save_args_i = GenAn.prepare_save_args(save_args_i, make_save_dir_if_dne=True)        \n",
    "        #-----\n",
    "        end_events_df_i.to_csv(save_args_i['save_path'], index=save_args_i['index'])\n",
    "        #-------------------------\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d57880",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e943a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# VARIABLES TO BE SET BY USER!\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "save_dfs_to_file   = False\n",
    "read_dfs_from_file = True\n",
    "save_end_events    = True\n",
    "\n",
    "#-------------------------\n",
    "# run_date is used to collect all results from a given acquisiton run together.\n",
    "# As such, run_date should be set to the first date of the acquisition run, and\n",
    "#   SHOULD NOT be changed for each individual date in a run (which typically lasts\n",
    "#   over the course of days/weeks)\n",
    "run_date = '20231004'\n",
    "\n",
    "#-------------------------\n",
    "date_0 = '2023-04-01'\n",
    "date_1 = '2023-09-30'\n",
    "search_time_half_window = datetime.timedelta(days=31)\n",
    "\n",
    "#--------------------------------------------------\n",
    "# NOTE: below, states and opcos should be consistent!\n",
    "#       i.e., e.g., if states='OH', then opcos should be 'oh' (or None, I suppose)\n",
    "#-------------------------\n",
    "# states used to \n",
    "#   (1) find transformers which suffered at least one outage from DOVS\n",
    "#   (2) find all transformers from MeterPremise\n",
    "# states can be:\n",
    "#   - a single string, e.g. 'OH'\n",
    "#   - a list of strings, e.g., ['OH', 'WV']\n",
    "#   - None\n",
    "# NOTE: states tend to be upper-case!\n",
    "states=['OH']\n",
    "\n",
    "#-------------------------\n",
    "# opcos used with AMIEndEvents to\n",
    "#  (1) find the premise numbers which recorded an event between date_0 and date_1.\n",
    "#  (2) selection/acquisiton of end_device_events\n",
    "# opcos can be:\n",
    "#   - a single string, e.g. 'oh'\n",
    "#   - a list of strings, e.g., ['oh', 'tx']\n",
    "#   - None\n",
    "# NOTE: opcos tend to be lower-case!\n",
    "# NOTE: Acceptable opcos appear to be: ['ap', 'im', 'oh', 'pso', 'swp', 'tx']\n",
    "opcos='oh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d944979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde35de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# DFs will be saved in save_dir_base\n",
    "# Collection of end events files will be saved in os.path.join(save_dir_base, 'EndEvents')\n",
    "save_dir_base = os.path.join(\n",
    "    Utilities.get_local_data_dir(), \n",
    "    r'dovs_and_end_events_data', \n",
    "    run_date, \n",
    "    f\"{date_0.replace('-','')}_{date_1.replace('-','')}\", \n",
    "    'Outgs_Full'\n",
    ")\n",
    "#-------------------------\n",
    "end_events_save_args = dict(\n",
    "    save_to_file=save_end_events, \n",
    "    save_dir = os.path.join(save_dir_base, 'EndEvents'), \n",
    "    save_name=r'end_events.csv', \n",
    "    index=True\n",
    ")\n",
    "#-------------------------\n",
    "print(f\"save_dir_base = {save_dir_base}\")\n",
    "print('end_events_save_args')\n",
    "for k,v in end_events_save_args.items():\n",
    "    print(f\"\\t{k} : {v}\")\n",
    "#-------------------------\n",
    "if save_dfs_to_file or save_end_events:\n",
    "    if not os.path.exists(save_dir_base):\n",
    "        os.makedirs(save_dir_base)\n",
    "    #-----\n",
    "    if save_end_events and not os.path.exists(end_events_save_args['save_dir']):\n",
    "        os.makedirs(end_events_save_args['save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e1e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "assert(save_dfs_to_file+read_dfs_from_file <=1) # Should never both read and write!\n",
    "assert(pd.to_datetime(date_1)-pd.to_datetime(date_0) > 2*search_time_half_window)\n",
    "#--------------------------------------------------\n",
    "if not read_dfs_from_file:\n",
    "    conn_outages = Utilities.get_utldb01p_oracle_connection()\n",
    "    conn_aws = Utilities.get_athena_prod_aws_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7a6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "697ad41a",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------\n",
    "# OUTAGES\n",
    "# ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fecfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# Find outages between date_0 and date_1 for states\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "start=time.time()\n",
    "print('-----'*20+f'\\nFinding outages between {date_0} and {date_1} for states={states}\\n'+'-----'*10)\n",
    "if read_dfs_from_file:\n",
    "    print(f\"Reading df_outage_OG from file: {os.path.join(save_dir_base, 'df_outage_OG.pkl')}\")\n",
    "    df_outage_OG = pd.read_pickle(os.path.join(save_dir_base, 'df_outage_OG.pkl'))\n",
    "else:\n",
    "    sql_outage_full = DOVSOutages_SQL.build_sql_std_outage(\n",
    "        mjr_mnr_cause=None, \n",
    "        include_premise=True, \n",
    "        date_range=[date_0, date_1], \n",
    "        states=states\n",
    "    ).get_sql_statement()\n",
    "    #-----\n",
    "    print(f'sql_outage_full:\\n{sql_outage_full}\\n\\n')\n",
    "    #-----\n",
    "    df_outage_OG = pd.read_sql_query(\n",
    "        sql_outage_full, \n",
    "        conn_outages, \n",
    "        dtype={\n",
    "            'CI_NB':np.int32, \n",
    "            'CMI_NB':np.float64, \n",
    "            'OUTG_REC_NB':np.int32\n",
    "        }\n",
    "    )\n",
    "    if save_dfs_to_file:\n",
    "        df_outage_OG.to_pickle(os.path.join(save_dir_base, 'df_outage_OG.pkl'))\n",
    "#-----\n",
    "print(f\"df_outage_OG.shape = {df_outage_OG.shape}\")\n",
    "print(f\"# OUTG_REC_NBs     = {df_outage_OG['OUTG_REC_NB'].nunique()}\")\n",
    "print(f'\\ntime = {time.time()-start}\\n'+'-----'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761e131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if read_dfs_from_file:\n",
    "    # No real reason to read in df_mp_outg_OG, as it's not used after df_mp_outg is built\n",
    "    # df_mp_outg_OG = pd.read_pickle(os.path.join(save_dir_base, 'df_mp_outg_b4_dupl_rmvl.pkl'))\n",
    "    df_mp_outg = pd.read_pickle(os.path.join(save_dir_base, 'df_mp_outg_full.pkl'))\n",
    "else:\n",
    "    df_mp_outg_OG = build_active_MP_for_outages_df(\n",
    "        df_outage=df_outage_OG, \n",
    "        prem_nb_col='PREMISE_NB', \n",
    "        is_slim=False, \n",
    "        assert_all_PNs_found=False\n",
    "    )\n",
    "    #-----\n",
    "    df_mp_outg_OG['inst_ts'] = pd.to_datetime(df_mp_outg_OG['inst_ts'])\n",
    "    df_mp_outg_OG['rmvl_ts'] = pd.to_datetime(df_mp_outg_OG['rmvl_ts'])\n",
    "    #-----\n",
    "    if save_dfs_to_file:\n",
    "        df_mp_outg_OG.to_pickle(os.path.join(save_dir_base, 'df_mp_outg_b4_dupl_rmvl.pkl'))\n",
    "    #-------------------------\n",
    "    df_mp_outg = MeterPremise.drop_approx_mp_duplicates(\n",
    "        mp_df = df_mp_outg_OG.copy(), \n",
    "        fuzziness=pd.Timedelta('1 hour'), \n",
    "        assert_single_overlap=True, \n",
    "        addtnl_groupby_cols=['OUTG_REC_NB'], \n",
    "        gpby_dropna=False\n",
    "    )\n",
    "    #-----\n",
    "    if save_dfs_to_file:\n",
    "        df_mp_outg.to_pickle(os.path.join(save_dir_base, 'df_mp_outg_full.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage = DOVSOutages.merge_df_outage_with_mp(\n",
    "    df_outage=df_outage_OG.copy(), \n",
    "    df_mp=df_mp_outg, \n",
    "    merge_on_outg=['OUTG_REC_NB', 'PREMISE_NB'], \n",
    "    merge_on_mp=['OUTG_REC_NB', 'prem_nb'], \n",
    "    cols_to_include_mp=None, \n",
    "    drop_cols = None, \n",
    "    rename_cols=None, \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73fb792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc43f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, 'prim' stands for primary, and means the meters are connected directly to a transformer pole causing an outage\n",
    "df_outage_prim = df_outage[df_outage['LOCATION_ID']==df_outage['trsf_pole_nb']].copy()\n",
    "\n",
    "# Below, 'prim_strict' stands for primary strict, and means the  meters are connected directly \n",
    "# to a transformer pole causing an outage, AND the equipment type causing the outage is a transformer\n",
    "# (exact EQUIP_TYP_NMs given in xfmr_equip_typ_nms_of_interest)\n",
    "xfmr_equip_typ_nms_of_interest = ['TRANSFORMER, OH', 'TRANSFORMER, UG']\n",
    "df_outage_prim_strict = df_outage[(df_outage['LOCATION_ID']==df_outage['trsf_pole_nb']) & \n",
    "                                  (df_outage['EQUIP_TYP_NM'].isin(xfmr_equip_typ_nms_of_interest))].copy()\n",
    "\n",
    "print(df_outage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2eacd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35bfbcd0",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de465de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_slim_OLD             = DOVSOutages.consolidate_df_outage_OLD(df_outage)\n",
    "df_outage_prim_slim_OLD        = DOVSOutages.consolidate_df_outage_OLD(df_outage_prim)\n",
    "df_outage_prim_strict_slim_OLD = DOVSOutages.consolidate_df_outage_OLD(df_outage_prim_strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outage_slim_OLD             = DOVSOutages.consolidate_df_outage_OLD(df_outage)\n",
    "# df_outage_prim_slim_OLD        = DOVSOutages.consolidate_df_outage_OLD(df_outage_prim)\n",
    "# df_outage_prim_strict_slim_OLD = DOVSOutages.consolidate_df_outage_OLD(df_outage_prim_strict)\n",
    "\n",
    "\n",
    "df_outage_slim             = DOVSOutages.consolidate_df_outage(\n",
    "    df_outage, \n",
    "    addtnl_grpby_cols=['trsf_pole_nb'], \n",
    "    set_outg_rec_nb_as_index=False\n",
    ")\n",
    "df_outage_prim_slim        = DOVSOutages.consolidate_df_outage(\n",
    "    df_outage_prim, \n",
    "    addtnl_grpby_cols=['trsf_pole_nb'], \n",
    "    set_outg_rec_nb_as_index=False\n",
    ")\n",
    "df_outage_prim_strict_slim = DOVSOutages.consolidate_df_outage(\n",
    "    df_outage_prim_strict, \n",
    "    addtnl_grpby_cols=['trsf_pole_nb'], \n",
    "    set_outg_rec_nb_as_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53145cf6",
   "metadata": {},
   "source": [
    "# Save CSVs if save_dfs_to_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_dfs_to_file:\n",
    "    df_outage.to_pickle(os.path.join(save_dir_base, 'df_outage.pkl'))\n",
    "    df_outage_prim.to_pickle(os.path.join(save_dir_base, 'df_outage_prim.pkl'))\n",
    "    df_outage_prim_strict.to_pickle(os.path.join(save_dir_base, 'df_outage_prim_strict.pkl'))\n",
    "    #-----\n",
    "    df_outage_slim.to_pickle(os.path.join(save_dir_base, 'df_outage_slim.pkl'))\n",
    "    df_outage_prim_slim.to_pickle(os.path.join(save_dir_base, 'df_outage_prim_slim.pkl'))\n",
    "    df_outage_prim_strict_slim.to_pickle(os.path.join(save_dir_base, 'df_outage_prim_strict_slim.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1523fa",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6296f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outage_slim = pd.read_pickle(os.path.join(save_dir_base, 'df_outage_slim.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4d9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_slim = DOVSOutages.set_search_time_in_outage_df(\n",
    "    df_outage=df_outage_slim, \n",
    "    search_time_half_window=search_time_half_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d465100",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_outage_slim['OUTG_REC_NB'].nunique())\n",
    "print(len(DOVSOutages.get_prem_nbs_from_consolidated_df_outage(df_outage_slim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_outage_OG\n",
    "del df_outage\n",
    "del df_outage_prim\n",
    "del df_outage_prim_strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677894c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "usg_split_to_CTEs=True\n",
    "df_construct_type=DFConstructType.kRunSqlQuery\n",
    "contstruct_df_args_end_events=None\n",
    "addtnl_groupby_cols=['OUTG_REC_NB', 'trsf_pole_nb']\n",
    "\n",
    "cols_of_interest_end_dev_event = ['*']\n",
    "# batch_size=10\n",
    "batch_size=30\n",
    "verbose=True\n",
    "n_update=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d85fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest_end_dev_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf874cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "end_events_sql_function_kwargs = dict(\n",
    "    cols_of_interest=cols_of_interest_end_dev_event, \n",
    "    df_outage=df_outage_slim, \n",
    "    date_only=True, \n",
    "    split_to_CTEs=usg_split_to_CTEs, \n",
    "    join_mp_args=False, \n",
    "    df_args = dict(\n",
    "        addtnl_groupby_cols=addtnl_groupby_cols, \n",
    "        mapping_to_ami={'PREMISE_NBS':'premise_nbs'}, \n",
    "        is_df_consolidated=True\n",
    "    ), \n",
    "    field_to_split='df_outage', \n",
    "    field_to_split_location_in_kwargs=['df_outage'], \n",
    "    save_and_dump=True, \n",
    "    sort_coll_to_split=True,\n",
    "    batch_size=batch_size, verbose=verbose, n_update=n_update\n",
    ")\n",
    "addtnl_end_events_sql_function_kwargs = dict(\n",
    "    build_sql_function_kwargs=dict(\n",
    "        schema_name='meter_events', \n",
    "        table_name='events_summary_vw', \n",
    "        opco=opcos\n",
    "    )\n",
    ")\n",
    "end_events_sql_function_kwargs = {**end_events_sql_function_kwargs, \n",
    "                                  **addtnl_end_events_sql_function_kwargs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0121a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start=time.time()\n",
    "# end_events = AMIEndEvents(\n",
    "#     df_construct_type=df_construct_type, \n",
    "#     contstruct_df_args = contstruct_df_args_end_events, \n",
    "#     build_sql_function=AMIEndEvents_SQL.build_sql_end_events_for_outages, \n",
    "#     build_sql_function_kwargs=end_events_sql_function_kwargs, \n",
    "#     init_df_in_constructor=True, \n",
    "#     save_args=end_events_save_args\n",
    "# )\n",
    "# end_events_build_time = time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2fe0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        end_events = AMIEndEvents(\n",
    "            df_construct_type=df_construct_type, \n",
    "            contstruct_df_args = contstruct_df_args_end_events, \n",
    "            build_sql_function=AMIEndEvents_SQL.build_sql_end_events_for_outages, \n",
    "            build_sql_function_kwargs=end_events_sql_function_kwargs, \n",
    "            init_df_in_constructor=True, \n",
    "            save_args=end_events_save_args\n",
    "        )\n",
    "        break # stop the loop if the function completes sucessfully\n",
    "    except Exception as e:\n",
    "        print(\"Function errored out!\", e)\n",
    "        print(\"Retrying ... \")\n",
    "        \n",
    "end_events_build_time = time.time()-start\n",
    "print(end_events_build_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c09188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66faae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
