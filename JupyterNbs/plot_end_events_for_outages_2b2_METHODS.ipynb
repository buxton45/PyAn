{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5755605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "# NOTE: To reload a class imported as, e.g., \n",
    "# from module import class\n",
    "# One must call:\n",
    "#   1. import module\n",
    "#   2. reload module\n",
    "#   3. from module import class\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns, natsort_keygen\n",
    "from packaging import version\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm #e.g. for cmap=cm.jet\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from MECPODf import MECPODf\n",
    "from MECPOAn import MECPOAn\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "#sys.path.insert(0, os.path.join(os.path.realpath('..'), 'Utilities'))\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "from Utilities_df import DFConstructType\n",
    "import Utilities_dt\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import Plot_Hist\n",
    "import Plot_Bar\n",
    "import GrubbsTest\n",
    "import DataFrameSubsetSlicer\n",
    "from DataFrameSubsetSlicer import DataFrameSubsetSlicer as DFSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb80e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2accdb4",
   "metadata": {},
   "source": [
    "# BEGIN NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e322f6",
   "metadata": {},
   "source": [
    "### OLD FUNCTIONS TO BE UPDATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9500d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO OutageMdlrPrep\n",
    "def get_active_SNs_for_xfmrs_OLD(\n",
    "    trsf_pole_nbs, \n",
    "    df_mp_curr, \n",
    "    df_mp_hist,\n",
    "    no_outg_time_infos_df=None, \n",
    "    addtnl_mp_df_curr_cols=None, \n",
    "    addtnl_mp_df_hist_cols=None, \n",
    "    files_dir_no_outg=r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\EndEvents_NoOutg', \n",
    "    file_path_glob_no_outg = r'end_events_[0-9]*.csv', \n",
    "    return_SNs_col='SNs', \n",
    "    return_prem_nbs_col='prem_nbs', \n",
    "    assert_all_trsf_pole_nbs_found=True, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb', \n",
    "    t_min_col='t_min', \n",
    "    t_max_col='t_max'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Difficulty is that default.meter_premise_hist does not have trsf_pole_nb field.\n",
    "    Therefore, one must use default.meter_premise to find the premise numbers for xfrms in trsf_pole_nbs,\n",
    "      then use those PNs to select the correct entries from default.meter_premise_hist.\n",
    "    \n",
    "    If df_mp_curr OR df_mp_hist is not supplied, both will be built!\n",
    "    \n",
    "    addtnl_mp_df_curr_cols/addtnl_mp_df_hist_cols:\n",
    "      Only used when df_mp_curr/df_mp_hist not supplied and therefore need to be built\n",
    "      \n",
    "    If no_outg_time_infos_df is not supplied, it will be built.\n",
    "      files_dir_no_outg and file_path_glob_no_outg are only used when no_outg_time_infos_df needs built\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    necessary_mp_cols = [df_mp_serial_number_col, df_mp_prem_nb_col, df_mp_install_time_col, df_mp_removal_time_col]\n",
    "    #-------------------------\n",
    "    if df_mp_curr is None or df_mp_hist is None:\n",
    "        mp_df_curr_hist = MeterPremise.build_mp_df_curr_hist_for_xfmrs(\n",
    "            trsf_pole_nbs, \n",
    "            join_curr_hist=False, \n",
    "            addtnl_mp_df_curr_cols=addtnl_mp_df_curr_cols, \n",
    "            addtnl_mp_df_hist_cols=addtnl_mp_df_hist_cols, \n",
    "            df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "            df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "            df_mp_install_time_col=df_mp_install_time_col, \n",
    "            df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "            df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "        )\n",
    "        df_mp_curr = mp_df_curr_hist['mp_df_curr']\n",
    "        df_mp_hist = mp_df_curr_hist['mp_df_hist']\n",
    "    #-------------------------\n",
    "    # At a bare minimum, df_mp_curr and df_mp_hist must both have the following columns:\n",
    "    #   necessary_mp_cols = ['mfr_devc_ser_nbr', 'prem_nb', 'inst_ts', 'rmvl_ts']\n",
    "    assert(all([x in df_mp_curr.columns for x in necessary_mp_cols+[df_mp_trsf_pole_nb_col]]))\n",
    "    assert(all([x in df_mp_hist.columns for x in necessary_mp_cols]))\n",
    "    #-------------------------\n",
    "    # PNs_for_xfmrs is a DF with trsf_pole_nbs indices and elements which are lists of PNs for each xfmr\n",
    "    PNs_for_xfmrs = MeterPremise.get_SNs_andor_PNs_for_xfmrs(\n",
    "        trsf_pole_nbs=trsf_pole_nbs, \n",
    "        include_SNs=False,\n",
    "        include_PNs=True,\n",
    "        trsf_pole_nb_col=df_mp_trsf_pole_nb_col, \n",
    "        serial_number_col=df_mp_serial_number_col, \n",
    "        prem_nb_col=df_mp_prem_nb_col, \n",
    "        return_SNs_col=None, #Not grabbing SNs\n",
    "        return_PNs_col=return_prem_nbs_col, \n",
    "        assert_all_trsf_pole_nbs_found=assert_all_trsf_pole_nbs_found, \n",
    "        mp_df=df_mp_curr, \n",
    "        return_mp_df_also=False\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Instead of a DF with trsf_pole_nb index and prem_nb column, we want opposite\n",
    "    xfmr_for_PNs_df = PNs_for_xfmrs.explode(return_prem_nbs_col)\n",
    "    xfmr_for_PNs_df[xfmr_for_PNs_df.index.name] = xfmr_for_PNs_df.index\n",
    "    xfmr_for_PNs_df = xfmr_for_PNs_df.set_index(return_prem_nbs_col)  \n",
    "    #-------------------------\n",
    "    # If no_outg_time_infos_df is None, build it.  \n",
    "    #   no_outg_time_infos_df has prem_nbs indices and t_min, t_max columns\n",
    "    #   This is where the time information for each premise number comes from\n",
    "    if no_outg_time_infos_df is None:\n",
    "        paths_no_outg = Utilities.find_all_paths(base_dir=files_dir_no_outg, glob_pattern=file_path_glob_no_outg)\n",
    "        no_outg_time_infos_df = MECPOAn.get_bsln_time_interval_infos_df_from_summary_files(\n",
    "            summary_paths=[AMIEndEvents.find_summary_file_from_csv(x) for x in paths_no_outg], \n",
    "            output_prem_nbs_col=return_prem_nbs_col, \n",
    "            output_t_min_col=t_min_col, \n",
    "            output_t_max_col=t_max_col, \n",
    "            make_prem_nbs_idx=True, \n",
    "            include_summary_paths=False\n",
    "        )    \n",
    "    #-------------------------\n",
    "    # Merge xfmr_for_PNs_df with no_outg_time_infos_df to append the time data to the former\n",
    "    # NOTE: It is possible for t_min/t_max to be NaT (NaN) for some entries after the merge, meaning that the \n",
    "    #       premise numbers were not found in no_outg_time_infos_df\n",
    "    #       This happens because these premise numbers must not have had any meter events, and thus were not included \n",
    "    #         in the SQL query (as it takes a long time to find empty results, so I weed these out before running the \n",
    "    #         query), and therefore the premise numbers were not found in the summary files/no_outg_time_infos_df.    \n",
    "    xfmr_for_PNs_df = pd.merge(xfmr_for_PNs_df, no_outg_time_infos_df, how='left', left_index=True, right_index=True)\n",
    "    #-------------------------\n",
    "    # Want to consolidate xfmr_for_PNs_df, grouping by trsf_pole_nb and collecting t_min,t_max, and a list\n",
    "    # of the premise numbers.  Therefore, first the index must be reset to make a PNs columns\n",
    "    xfmr_for_PNs_df=xfmr_for_PNs_df.reset_index()\n",
    "    #-----\n",
    "    # Consolidate xfmr_for_PNs_df\n",
    "    # NOTE: If t_min/t_max is NaT (NaN) for all premise numbers in a given trsf_pole_nb (see NOTE above before merge\n",
    "    #       with no_outg_time_infos_df), then Utilities_df.consolidate_df will return an empty list (technically, an\n",
    "    #       empty np.ndarray) for that trsf_pole_nb\n",
    "    xfmr_for_PNs_df=Utilities_df.consolidate_df_OLD(\n",
    "        df=xfmr_for_PNs_df, \n",
    "        groupby_col=df_mp_trsf_pole_nb_col, \n",
    "        cols_shared_by_group=[t_min_col, t_max_col], \n",
    "        cols_to_collect_in_lists=[return_prem_nbs_col]\n",
    "    )    \n",
    "    #--------------------------------------------------\n",
    "    # Only reason for making dict is to ensure trsf_pole_nbs are not repeated \n",
    "    active_SNs_in_xfmrs_dfs_dict = {}\n",
    "\n",
    "    for trsf_pole_nb_i, row_i in xfmr_for_PNs_df.iterrows():\n",
    "        # active_SNs_df_i will have indices equal to premise numbers and value equal to lists\n",
    "        #   of active SNs for each PN\n",
    "        PNs_i=row_i[return_prem_nbs_col]\n",
    "        dt_0_i=row_i[t_min_col]\n",
    "        dt_1_i=row_i[t_max_col]\n",
    "        # See NOTEs above regarding t_min/t_max being empty\n",
    "        # In such a case, it is simply impossibe (with the summary files currently generated) to access\n",
    "        #   the date over which the data would have been run, if any events existed.\n",
    "        #   In future versions, this information will be included in the summary files!\n",
    "        # I don't want to completely exclude these (by e.g., setting dt_0_i=pd.Timestamp.min and \n",
    "        #   dt_1_i=pd.Timestamp.max), so I will simply include the meters which are active TODAY.\n",
    "        # This obviously is not correct, but this occurrence is rare (only happening when every single meter\n",
    "        #   on a transformer had no events during the time period) and this crude approximation will be fine.\n",
    "        if Utilities.is_object_one_of_types(dt_0_i, [list, np.ndarray]):\n",
    "            assert(len(dt_0_i)==0)\n",
    "            # I believe if this happens for one it should happen for both...\n",
    "            assert(Utilities.is_object_one_of_types(dt_1_i, [list, np.ndarray]) and len(dt_1_i)==0)\n",
    "            dt_0_i=pd.Timestamp.today()\n",
    "        if Utilities.is_object_one_of_types(dt_1_i, [list, np.ndarray]):\n",
    "            assert(len(dt_1_i)==0)\n",
    "            # I believe if this happens for one it should happen for both...\n",
    "            # But, dt_0_i changed already above, so much check row_i[t_min_col] instead!\n",
    "            assert(Utilities.is_object_one_of_types(row_i[t_min_col], [list, np.ndarray]) and len(row_i[t_min_col])==0)\n",
    "            dt_1_i=pd.Timestamp.today()\n",
    "        if((not isinstance(PNs_i, list) and pd.isna(PNs_i)) or \n",
    "           len(PNs_i)==0):\n",
    "            active_SNs_df_i = pd.DataFrame()\n",
    "        else:\n",
    "            active_SNs_df_i = MeterPremise.get_active_SNs_for_PNs_at_datetime_interval(\n",
    "                PNs=PNs_i,\n",
    "                df_mp_curr=df_mp_curr, \n",
    "                df_mp_hist=df_mp_hist, \n",
    "                dt_0=dt_0_i,\n",
    "                dt_1=dt_1_i,\n",
    "                output_index=None,\n",
    "                output_groupby=[df_mp_prem_nb_col], \n",
    "                include_prems_wo_active_SNs_when_groupby=True, \n",
    "                assert_all_PNs_found=False\n",
    "            )\n",
    "            active_SNs_df_i=active_SNs_df_i.reset_index()\n",
    "        if active_SNs_df_i.shape[0]==0:\n",
    "            active_SNs_df_i[df_mp_prem_nb_col] = np.nan\n",
    "            active_SNs_df_i[df_mp_serial_number_col] = [[]] \n",
    "        active_SNs_df_i[df_mp_trsf_pole_nb_col] = trsf_pole_nb_i\n",
    "        active_SNs_df_i = active_SNs_df_i.explode(df_mp_serial_number_col)\n",
    "        assert(trsf_pole_nb_i not in active_SNs_in_xfmrs_dfs_dict)\n",
    "        active_SNs_in_xfmrs_dfs_dict[trsf_pole_nb_i] = active_SNs_df_i\n",
    "    #-------------------------\n",
    "    active_SNs_df = pd.concat(list(active_SNs_in_xfmrs_dfs_dict.values()))\n",
    "    #-------------------------\n",
    "    active_SNs_df = Utilities_df.consolidate_df_OLD(\n",
    "        df=active_SNs_df, \n",
    "        groupby_col=df_mp_trsf_pole_nb_col, \n",
    "        cols_shared_by_group=None, \n",
    "        cols_to_collect_in_lists=[df_mp_serial_number_col, df_mp_prem_nb_col], \n",
    "        include_groupby_col_in_output_cols=False, \n",
    "        allow_duplicates_in_lists=False, \n",
    "        recover_uniqueness_violators=True, \n",
    "        rename_cols=None, \n",
    "        verbose=False\n",
    "    )\n",
    "    #-----\n",
    "    # Change [nan] entries to []\n",
    "    active_SNs_df.loc[active_SNs_df[df_mp_serial_number_col].apply(lambda x: len([ix for ix in x if not pd.isna(ix)]))==0, df_mp_serial_number_col] = [[]]\n",
    "    active_SNs_df.loc[active_SNs_df[df_mp_prem_nb_col].apply(lambda x: len([ix for ix in x if not pd.isna(ix)]))==0, df_mp_prem_nb_col] = [[]]\n",
    "    #-------------------------\n",
    "    active_SNs_df = active_SNs_df.rename(columns={\n",
    "        df_mp_prem_nb_col:return_prem_nbs_col, \n",
    "        df_mp_serial_number_col:return_SNs_col\n",
    "    })\n",
    "    #-------------------------\n",
    "    return active_SNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def add_xfmr_active_SNs_to_rcpo_df_OLD(\n",
    "    rcpo_df, \n",
    "    set_xfmr_nSNs=True, \n",
    "    include_active_xfmr_PNs=False, #Should be equal to the PNs already in rcpo_df!\n",
    "    df_mp_curr=None,\n",
    "    df_mp_hist=None, \n",
    "    no_outg_time_infos_df=None, \n",
    "    addtnl_get_active_SNs_for_xfmrs_kwargs=None, \n",
    "    xfmr_SNs_col='_xfmr_SNs', \n",
    "    xfmr_nSNs_col='_xfmr_nSNs', \n",
    "    xfmr_PNs_col='_xfmr_PNs', \n",
    "    xfmr_nPNs_col='_xfmr_nPNs', \n",
    "):\n",
    "    r\"\"\"\n",
    "    NOTE: If include_active_xfmr_PNs is True, this column (named xfmr_PNs_col='_xfmr_SNs') should be \n",
    "          equal to the PNs already in rcpo_df!\n",
    "    NOTE: xfmr_SNs_col, xfmr_nSNs_col, xfmr_PNs_col, and xfmr_nPNs_col should all be strings, not tuples.\n",
    "          If column is multiindex, the level_0 value will be handled below.\n",
    "          \n",
    "    NOTE: If any of xfmr_SNs_col, xfmr_nSNs_col, xfmr_PNs_col, and xfmr_nPNs_col are already contained in \n",
    "          rcpo_df, they will be replaced.  This is needed so that the merge operation does not come back with _x and _y\n",
    "          values.  So, one should make sure this function call is truly needed, as grabbing the serial numbers for the\n",
    "          outages typically takes a couple/few minutes.\n",
    "          \n",
    "    NOTE: To make things run faster, the user can supply df_mp_curr and df_mp_hist.  These will be included in \n",
    "          get_active_SNs_for_xfmrs_kwargs.\n",
    "          NOTE: If df_mp_curr/df_mp_hist is also supplied in addtnl_get_active_SNs_for_xfmrs_kwargs,\n",
    "                that/those in addtnl_get_active_SNs_for_xfmrs_kwargs will ultimately be used (not the\n",
    "                explicity df_mp_hist/curr in the function arguments!)\n",
    "          CAREFUL: If one does supple df_mp_curr/hist, one must be certain these DFs contain all necessary elements!\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    get_active_SNs_for_xfmrs_kwargs = dict(\n",
    "        trsf_pole_nbs=rcpo_df.index.unique().tolist(), \n",
    "        df_mp_curr=df_mp_curr, \n",
    "        df_mp_hist=df_mp_hist, \n",
    "        no_outg_time_infos_df=no_outg_time_infos_df, \n",
    "        return_prem_nbs_col=xfmr_PNs_col, \n",
    "        return_SNs_col=xfmr_SNs_col\n",
    "    )\n",
    "    if addtnl_get_active_SNs_for_xfmrs_kwargs is not None:\n",
    "        get_active_SNs_for_xfmrs_kwargs = {**get_active_SNs_for_xfmrs_kwargs, \n",
    "                                           **addtnl_get_active_SNs_for_xfmrs_kwargs}\n",
    "    active_SNs_df = get_active_SNs_for_xfmrs_OLD(**get_active_SNs_for_xfmrs_kwargs)\n",
    "    assert(isinstance(active_SNs_df, pd.DataFrame))\n",
    "    #-------------------------\n",
    "    # Assert below might be too strong here...\n",
    "    assert(sorted(rcpo_df.index.unique().tolist())==sorted(active_SNs_df.index.unique().tolist()))\n",
    "    assert(rcpo_df.columns.nlevels<=2)\n",
    "    if rcpo_df.columns.nlevels==1:\n",
    "        #----------\n",
    "        # See note above about columns being replaced/dropped\n",
    "        cols_to_drop = [x for x in rcpo_df.columns if x in active_SNs_df.columns]\n",
    "        if len(cols_to_drop)>0:\n",
    "            rcpo_df = rcpo_df.drop(columns=cols_to_drop)\n",
    "        #----------\n",
    "        rcpo_df = rcpo_df.merge(active_SNs_df, left_index=True, right_index=True)\n",
    "        #----------\n",
    "        if set_xfmr_nSNs:\n",
    "            rcpo_df = MECPODf.set_nSNs_from_SNs_in_rcpo_df(rcpo_df, xfmr_SNs_col, xfmr_nSNs_col)\n",
    "            if include_active_xfmr_PNs:\n",
    "                rcpo_df = MECPODf.set_nSNs_from_SNs_in_rcpo_df(rcpo_df, xfmr_PNs_col, xfmr_nPNs_col)\n",
    "    else:\n",
    "        # Currently, only expecting raw and/or norm.  No problem to allow more, but for now keep this to alert \n",
    "        # of anything unexpected\n",
    "        assert(rcpo_df.columns.get_level_values(0).nunique()<=2)\n",
    "        for i,level_0_val in enumerate(rcpo_df.columns.get_level_values(0).unique()):\n",
    "            if i==0:\n",
    "                active_SNs_df.columns = pd.MultiIndex.from_product([[level_0_val], active_SNs_df.columns])\n",
    "            else:\n",
    "                active_SNs_df.columns = active_SNs_df.columns.set_levels([level_0_val], level=0)\n",
    "            #----------\n",
    "            # See note above about columns being replaced/dropped\n",
    "            cols_to_drop = [x for x in rcpo_df.columns if x in active_SNs_df.columns]\n",
    "            if len(cols_to_drop)>0:\n",
    "                rcpo_df = rcpo_df.drop(columns=cols_to_drop)\n",
    "            #----------\n",
    "            rcpo_df = rcpo_df.merge(active_SNs_df, left_index=True, right_index=True)\n",
    "            #----------\n",
    "            if set_xfmr_nSNs:\n",
    "                rcpo_df = MECPODf.set_nSNs_from_SNs_in_rcpo_df(rcpo_df, (level_0_val, xfmr_SNs_col), (level_0_val, xfmr_nSNs_col))\n",
    "                if include_active_xfmr_PNs:\n",
    "                    rcpo_df = MECPODf.set_nSNs_from_SNs_in_rcpo_df(rcpo_df, (level_0_val, xfmr_PNs_col), (level_0_val, xfmr_nPNs_col))\n",
    "    #-------------------------\n",
    "    rcpo_df = rcpo_df.sort_index(axis=1,level=0)\n",
    "    #-------------------------\n",
    "    return rcpo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a50d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED to OutageMdlrPrep\n",
    "def build_rcpo_df_norm_by_xfmr_active_nSNs_OLD(\n",
    "    rcpo_df_raw, \n",
    "    xfmr_nSNs_col='_xfmr_nSNs', \n",
    "    xfmr_SNs_col='_xfmr_SNs', \n",
    "    other_SNs_col_tags_to_ignore=['_SNs', '_nSNs', '_prem_nbs', '_nprem_nbs', '_xfmr_PNs', '_xfmr_nPNs'], \n",
    "    drop_xfmr_nSNs_eq_0=True, \n",
    "    new_level_0_val='counts_norm_by_xfmr_nSNs', \n",
    "    remove_SNs_cols=False, \n",
    "    df_mp_curr=None,\n",
    "    df_mp_hist=None, \n",
    "    no_outg_time_infos_df=None, \n",
    "    addtnl_get_active_SNs_for_xfmrs_kwargs=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    Build rcpo_df normalized by the number of serial numbers in each outage\n",
    "\n",
    "    drop_xfmr_nSNs_eq_0:\n",
    "      It is possible for the number of serial numbers in an outage to be zero!\n",
    "      Premise numbers are always found, but the meter_premise database does not always \n",
    "        contain the premise numbers.\n",
    "      Dividing by zero will make all counts for such an entry equal to NaN or inf.\n",
    "      When drop_xfmr_nSNs_eq_0 is True, such entries will be removed.\n",
    "\n",
    "    NOTE: xfmr_SNs_col and xfmr_nSNs_col should both be strings, not tuples.\n",
    "          If column is MultiIndex, the level_0 value will be handled below.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    n_counts_col = xfmr_nSNs_col\n",
    "    list_col = xfmr_SNs_col\n",
    "    #-------------------------\n",
    "    # NOTE: MECPODf.add_outage_SNs_to_rcpo_df expects xfmr_SNs_col and xfmr_nSNs_col to be strings, not tuples\n",
    "    #       as it handles the level 0 values if they exist.  So, if tuples, use only highest level values (i.e., level 1)\n",
    "    assert(Utilities.is_object_one_of_types(list_col, [str, tuple]))\n",
    "    assert(Utilities.is_object_one_of_types(n_counts_col, [str, tuple]))\n",
    "    #-----\n",
    "    add_list_col_to_rcpo_df_func = add_xfmr_active_SNs_to_rcpo_df_OLD\n",
    "    add_list_col_to_rcpo_df_kwargs = dict(\n",
    "        set_xfmr_nSNs=True, \n",
    "        include_active_xfmr_PNs=True, \n",
    "        df_mp_curr=df_mp_curr,\n",
    "        df_mp_hist=df_mp_hist, \n",
    "        no_outg_time_infos_df=no_outg_time_infos_df, \n",
    "        addtnl_get_active_SNs_for_xfmrs_kwargs=addtnl_get_active_SNs_for_xfmrs_kwargs, \n",
    "        xfmr_SNs_col='_xfmr_SNs', \n",
    "        xfmr_nSNs_col='_xfmr_nSNs', \n",
    "        xfmr_PNs_col='_xfmr_PNs', \n",
    "        xfmr_nPNs_col='_xfmr_nPNs', \n",
    "    )\n",
    "    #-------------------------\n",
    "    other_col_tags_to_ignore = other_SNs_col_tags_to_ignore\n",
    "    drop_n_counts_eq_0 = drop_xfmr_nSNs_eq_0\n",
    "    new_level_0_val = new_level_0_val\n",
    "    remove_ignored_cols = remove_SNs_cols\n",
    "    #-------------------------\n",
    "    return MECPODf.build_rcpo_df_norm_by_list_counts(\n",
    "        rcpo_df_raw=rcpo_df_raw, \n",
    "        n_counts_col=n_counts_col, \n",
    "        list_col=list_col, \n",
    "        add_list_col_to_rcpo_df_func=add_list_col_to_rcpo_df_func, \n",
    "        add_list_col_to_rcpo_df_kwargs=add_list_col_to_rcpo_df_kwargs, \n",
    "        other_col_tags_to_ignore=other_col_tags_to_ignore, \n",
    "        drop_n_counts_eq_0=drop_n_counts_eq_0, \n",
    "        new_level_0_val=new_level_0_val, \n",
    "        remove_ignored_cols=remove_ignored_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f3a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e370b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "525d34c5",
   "metadata": {},
   "source": [
    "### NEW FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d89781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def reset_index_and_identify_cols_to_merge(\n",
    "    df, \n",
    "    merge_on, \n",
    "    tag_for_idx_names=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    Designed to work with merge_rcpo_and_df (but should definitely be useful elsewhere), which \n",
    "    allows the user to join the DFs by columns, specific index levels or any mixture of the two.\n",
    "    \n",
    "    In order to achieve this type of general merging, it is easiest to call reset_index, making everything\n",
    "      to be merged on a column.\n",
    "    HOWEVER, in order to keep track of the original indices, which likely will be restored after the merge,\n",
    "      it is important for all index levels to have a name.\n",
    "      \n",
    "    merge_on:\n",
    "        This is a list that directs the columns/indices to be used in the join.\n",
    "        Column identifiers:\n",
    "            Single strings for normal DF, lists/tuples for MultiIndex columns\n",
    "        Index identifiers:\n",
    "            f'index_{idx_level}' to specify index level by number\n",
    "            ('index', idx_level_name) to specify index level by name\n",
    "            \n",
    "    RETURNS:\n",
    "        A dict object with keys = ['df', 'df_idx_names_OG', 'df_idx_names', 'reset_merge_on']\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    df=df.copy()\n",
    "    #-------------------------\n",
    "    assert(Utilities.is_object_one_of_types(merge_on, [list, tuple]))\n",
    "    #-------------------------\n",
    "    # Make sure all index levels have names!\n",
    "    # If the level has a name, the code below will leave it unchanged in df_idx_names\n",
    "    # If it does not have a name, it will be names f'index_{idx_level}', where idx_level \n",
    "    #   is the index level number\n",
    "    df_idx_names_OG = list(df.index.names)\n",
    "    df_idx_names = [x if x is not None else f'index_{i}' \n",
    "                    for i,x in enumerate(df_idx_names_OG)]\n",
    "    if tag_for_idx_names is not None:\n",
    "        df_idx_names = [f'{x}_{tag_for_idx_names}' for x in df_idx_names]\n",
    "    df.index.names=df_idx_names\n",
    "    #-------------------------\n",
    "    reset_merge_on = []\n",
    "    for idfr in merge_on:\n",
    "        assert(Utilities.is_object_one_of_types(idfr, [str, list, tuple]))\n",
    "        if idfr in df.columns:\n",
    "            reset_merge_on.append(idfr)\n",
    "        else:\n",
    "            # Must be in indices!\n",
    "            if isinstance(idfr, str):\n",
    "                assert(idfr.startswith('index'))\n",
    "                if idfr=='index':\n",
    "                    idfr_idx_lvl=0\n",
    "                else:\n",
    "                    idfr_idx_lvl = re.findall('index_(\\d*)', idfr)\n",
    "                    assert(len(idfr_idx_lvl)==1)\n",
    "                    idfr_idx_lvl=idfr_idx_lvl[0]\n",
    "                    idfr_idx_lvl=int(idfr_idx_lvl)\n",
    "            else:\n",
    "                assert(len(idfr)==2)\n",
    "                assert(idfr[0]=='index')\n",
    "                idx_level_name = idfr[1]\n",
    "                # If tag_for_idx_names, df.index.names already changed, so idx_level_name must be adjusted\n",
    "                if tag_for_idx_names is not None:\n",
    "                    idx_level_name = f'{idx_level_name}_{tag_for_idx_names}'\n",
    "                assert(idx_level_name in df.index.names)\n",
    "                idfr_idx_lvl = df.index.names.index(idx_level_name)\n",
    "            #---------------\n",
    "            assert(idfr_idx_lvl < df.index.nlevels)\n",
    "            reset_merge_on_i = df.index.names[idfr_idx_lvl]\n",
    "            # NOTE: If df.columns.nlevels>1, then calling df.reset_index() below\n",
    "            #       will make the bottom level reset_merge_on_i and all the rest ''\n",
    "            #       e.g., if nlevels=2, after df.reset_index(), reset_merge_on_i--> (reset_merge_on_i, '')\n",
    "            if df.columns.nlevels>1:\n",
    "                reset_merge_on_i=tuple([reset_merge_on_i] + ['']*(df.columns.nlevels-1))\n",
    "            reset_merge_on.append(reset_merge_on_i)\n",
    "    #-------------------------\n",
    "    # Call reset_index on df, making all indices into columns, and double check that all reset_merge_on\n",
    "    #   are contained in the columns\n",
    "    df = df.reset_index()\n",
    "    assert(len(set(reset_merge_on).difference(set(df.columns)))==0)\n",
    "    #-------------------------\n",
    "    return dict(\n",
    "        df=df, \n",
    "        df_idx_names_OG=df_idx_names_OG, \n",
    "        df_idx_names=df_idx_names, \n",
    "        reset_merge_on=reset_merge_on\n",
    "    )\n",
    "\n",
    "# Moved to OutageMdlrPrep\n",
    "def merge_rcpo_and_df(\n",
    "    rcpo_df, \n",
    "    df_2, \n",
    "    rcpo_df_on,\n",
    "    df_2_on, \n",
    "    how='left'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Merge together rcpo_df and df_2 dfs.\n",
    "    Designed specifically for rcpo_df and time_infos_df.\n",
    "    \n",
    "    rcpo_df_on/df_2_on:\n",
    "        These are lists which direct the columns/indices to be used in the join.\n",
    "        Column identifiers:\n",
    "            Single strings for normal DF, lists/tuples for MultiIndex columns\n",
    "        Index identifiers:\n",
    "            f'index_{idx_level}' to specify index level by number\n",
    "            ('index', idx_level_name) to specify index level by name\n",
    "            \n",
    "        NOTE: Calling reset_index() on both DFs will be the easiest method for this type of general merging, in\n",
    "              which the DFs can be merged by columns, specific index levels or any mixture of the two.\n",
    "              This is done through the use of reset_index_and_identify_cols_to_merge\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    # Only expecting at most 2 levels in columns (e.g., counts or counts_norm as level 0, and reason as level 1)\n",
    "    #     - probably not a necessary assertion, if function to be expanded later\n",
    "    #   Ultimately, the number of levels in df_2 will match that of rcpo_df, so making the same\n",
    "    #     assertion on df_2\n",
    "    assert((rcpo_df.columns.nlevels <= 2) and (df_2.columns.nlevels <= 2))\n",
    "    if rcpo_df.columns.nlevels==2:\n",
    "        # If rcpo_df has two column levels, df_2 must also for the proper merging to occur\n",
    "        #   --Merging with unequal levels will cause all levels to be collapsed down to single dimension\n",
    "        #-----\n",
    "        # Expecting at most 2 unique values for level 0 (definitely not a necessary assertion)\n",
    "        assert(rcpo_df.columns.get_level_values(0).nunique()<=2)\n",
    "        # In this case, likely df_2 has only single level columns.\n",
    "        #   For proper merge, the number of levels should match\n",
    "        if df_2.columns.nlevels==1:\n",
    "            level_0_vals = rcpo_df.columns.get_level_values(0).unique().tolist()\n",
    "            # Add new top level to df_2 with value equal to level_0_vals[0]\n",
    "            df_2=Utilities_df.prepend_level_to_MultiIndex(\n",
    "                df=df_2, level_val=level_0_vals[0], level_name=None, axis=1\n",
    "            )\n",
    "            if len(level_0_vals)>1:\n",
    "                # Grab df without new column level to be copied to other new column level values\n",
    "                # NOTE: If extra [] placed around level_0_vals[0] below, the new column level would be returned\n",
    "                #       (which is not desired here!)\n",
    "                df_0 = df_2[level_0_vals[0]]\n",
    "                df_0_cols = df_0.columns.tolist()\n",
    "                # Reproduce the entries of df_2 for all column level 0 values in rcpo_df\n",
    "                for i_lvl in range(1,len(level_0_vals)):\n",
    "                    new_cols = pd.MultiIndex.from_product([[level_0_vals[i_lvl]], df_0_cols])\n",
    "                    df_2[new_cols] = df_0.copy()\n",
    "        # Now, at this stage, rcpo_df and df_2 should have the same number of column levels\n",
    "        #   and should have overlapping level 0 values if nlevels>1\n",
    "        assert((rcpo_df.columns.nlevels <= 2) and (df_2.columns.nlevels <= 2)) #not needed\n",
    "        assert(rcpo_df.columns.nlevels == df_2.columns.nlevels)\n",
    "        if rcpo_df.columns.nlevels == 2:\n",
    "            # Make sure overlapping 0 values.  \n",
    "            # I suppose user could supply df_2 with only a single level 0 value while rcpo_df has\n",
    "            #   two values.  In this case, only the single value would be merged to rcpo_df\n",
    "            assert(len(set(df_2.columns.get_level_values(0).unique().tolist()).difference(\n",
    "                set(rcpo_df.columns.get_level_values(0).unique().tolist())\n",
    "            ))==0)\n",
    "    else:\n",
    "        assert(rcpo_df.columns.nlevels==df_2.columns.nlevels==1)\n",
    "    #--------------------------------------------------\n",
    "    reset_rcpo_df_dict = reset_index_and_identify_cols_to_merge(\n",
    "        df=rcpo_df, \n",
    "        merge_on=rcpo_df_on, \n",
    "        tag_for_idx_names='from_rcpo_df'\n",
    "    )\n",
    "\n",
    "    reset_df_2_dict = reset_index_and_identify_cols_to_merge(\n",
    "        df=df_2, \n",
    "        merge_on=df_2_on, \n",
    "        tag_for_idx_names='from_df_2'\n",
    "    )\n",
    "    #-------------------------\n",
    "    merged_df = pd.merge(\n",
    "        reset_rcpo_df_dict['df'], \n",
    "        reset_df_2_dict['df'], \n",
    "        left_on=reset_rcpo_df_dict['reset_merge_on'], \n",
    "        right_on = reset_df_2_dict['reset_merge_on'], \n",
    "        how=how\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Want to set index first before changing names back to originals, as the\n",
    "    #   originals could have been None\n",
    "    merged_df = merged_df.set_index(reset_rcpo_df_dict['df_idx_names'])\n",
    "\n",
    "    # Two lines below (instead of calling simply df.index.names=df_idx_names_OG) \n",
    "    #   ensures the order is the same\n",
    "    rcpo_df_rename_dict = dict(zip(reset_rcpo_df_dict['df_idx_names'], reset_rcpo_df_dict['df_idx_names_OG']))\n",
    "    merged_df.index.names = [rcpo_df_rename_dict[x] for x in merged_df.index.names]\n",
    "    #-------------------------\n",
    "    # When merging two columns whose names are different, both columns are kept\n",
    "    #   This is redundant, as these will have identical values, as they were merged, so get rid of\n",
    "    #   Note: If the column names are the same, this is obviously not an issue (hence the need to find\n",
    "    #         cols_to_drop below, instead of simply dropping all of reset_df_2_dict['reset_merge_on']\n",
    "    cols_to_drop = [x for x in reset_df_2_dict['reset_merge_on'] if x in merged_df.columns]\n",
    "    merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Rename columns from df_2 to original values (if original values were not None!)\n",
    "    df_2_rename_dict = dict(zip(reset_df_2_dict['df_idx_names'], reset_df_2_dict['df_idx_names_OG']))\n",
    "    df_2_rename_dict = {k:v for k,v in df_2_rename_dict.items() if k in merged_df.columns and v is not None}\n",
    "    merged_df=merged_df.rename(columns=df_2_rename_dict)\n",
    "    #-------------------------\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO still needs work...\n",
    "# This replaces get_active_SNs_for_xfmrs_OLD (but, typically use get_active_SNs_for_xfmrs_in_rcpo_df)\n",
    "# Moved to OutageMdlrPrep\n",
    "def get_active_SNs_for_xfmrs(\n",
    "    trsf_pole_nbs,     \n",
    "    df_mp_curr, \n",
    "    df_mp_hist,\n",
    "    time_infos_df,     \n",
    "    time_infos_to_PNs = ['index'], \n",
    "    PNs_to_time_infos = ['index'], \n",
    "    how='left',     \n",
    "    output_trsf_pole_nb_col=None, \n",
    "    addtnl_mp_df_curr_cols=None, \n",
    "    addtnl_mp_df_hist_cols=None, \n",
    "    return_SNs_col='SNs', \n",
    "    return_prem_nbs_col='prem_nbs', \n",
    "    assert_all_trsf_pole_nbs_found=True, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb', \n",
    "    t_min_col='t_min', \n",
    "    t_max_col='t_max'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Difficulty is that default.meter_premise_hist does not have trsf_pole_nb field.\n",
    "    Therefore, one must use default.meter_premise to find the premise numbers for xfrms in trsf_pole_nbs,\n",
    "      then use those PNs to select the correct entries from default.meter_premise_hist.\n",
    "      \n",
    "    time_infos_to_PNs:\n",
    "      Defines how time_infos_df and PNs_for_xfmrs will be merged.\n",
    "      NOTE: PNs_for_xfmrs will have indices equal to trsf_pole_nbs and values equal to lists of associated prem_nbs\n",
    "      See merge_rcpo_and_df and reset_index_and_identify_cols_to_merge for more information\n",
    "\n",
    "    If df_mp_curr OR df_mp_hist is not supplied, both will be built!\n",
    "    \n",
    "    addtnl_mp_df_curr_cols/addtnl_mp_df_hist_cols:\n",
    "      Only used when df_mp_curr/df_mp_hist not supplied and therefore need to be built\n",
    "      \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    assert(t_min_col in time_infos_df.columns and \n",
    "           t_max_col in time_infos_df.columns)\n",
    "    time_infos_df = time_infos_df[[t_min_col, t_max_col]]\n",
    "    #-----\n",
    "    # Remove any duplicates from time_infos_df\n",
    "    if time_infos_df.index.nlevels==1:\n",
    "        tmp_col = Utilities.generate_random_string()\n",
    "        time_infos_df[tmp_col] = time_infos_df.index\n",
    "        time_infos_df = time_infos_df.drop_duplicates()\n",
    "        time_infos_df = time_infos_df.drop(columns=[tmp_col])\n",
    "    else:\n",
    "        tmp_cols = [Utilities.generate_random_string() for _ in range(time_infos_df.index.nlevels)]\n",
    "        for i_col, tmp_col in enumerate(tmp_cols):\n",
    "            time_infos_df[tmp_col] = time_infos_df.index.get_level_values(i_col)\n",
    "        time_infos_df = time_infos_df.drop_duplicates()\n",
    "        time_infos_df = time_infos_df.drop(columns=tmp_cols)\n",
    "    #--------------------------------------------------\n",
    "    #-------------------------\n",
    "    necessary_mp_cols = [df_mp_serial_number_col, df_mp_prem_nb_col, df_mp_install_time_col, df_mp_removal_time_col]\n",
    "    #-------------------------\n",
    "    if df_mp_curr is None or df_mp_hist is None:\n",
    "        mp_df_curr_hist = MeterPremise.build_mp_df_curr_hist_for_xfmrs(\n",
    "            trsf_pole_nbs, \n",
    "            join_curr_hist=False, \n",
    "            addtnl_mp_df_curr_cols=addtnl_mp_df_curr_cols, \n",
    "            addtnl_mp_df_hist_cols=addtnl_mp_df_hist_cols, \n",
    "            df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "            df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "            df_mp_install_time_col=df_mp_install_time_col, \n",
    "            df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "            df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "        )\n",
    "        df_mp_curr = mp_df_curr_hist['mp_df_curr']\n",
    "        df_mp_hist = mp_df_curr_hist['mp_df_hist']\n",
    "    #-------------------------\n",
    "    # At a bare minimum, df_mp_curr and df_mp_hist must both have the following columns:\n",
    "    #   necessary_mp_cols = ['mfr_devc_ser_nbr', 'prem_nb', 'inst_ts', 'rmvl_ts']\n",
    "    assert(all([x in df_mp_curr.columns for x in necessary_mp_cols+[df_mp_trsf_pole_nb_col]]))\n",
    "    assert(all([x in df_mp_hist.columns for x in necessary_mp_cols]))\n",
    "    #-------------------------\n",
    "    # PNs_for_xfmrs is a DF with trsf_pole_nbs indices and elements which are lists of PNs for each xfmr\n",
    "    PNs_for_xfmrs = MeterPremise.get_SNs_andor_PNs_for_xfmrs(\n",
    "        trsf_pole_nbs=trsf_pole_nbs, \n",
    "        include_SNs=False,\n",
    "        include_PNs=True,\n",
    "        trsf_pole_nb_col=df_mp_trsf_pole_nb_col, \n",
    "        serial_number_col=df_mp_serial_number_col, \n",
    "        prem_nb_col=df_mp_prem_nb_col, \n",
    "        return_SNs_col=None, #Not grabbing SNs\n",
    "        return_PNs_col=return_prem_nbs_col, \n",
    "        assert_all_trsf_pole_nbs_found=assert_all_trsf_pole_nbs_found, \n",
    "        mp_df=df_mp_curr, \n",
    "        return_mp_df_also=False\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Join together time_infos_df and PNs_for_xfmrs\n",
    "    #-----\n",
    "    time_infos_df = merge_rcpo_and_df(\n",
    "        rcpo_df=time_infos_df, \n",
    "        df_2=PNs_for_xfmrs, \n",
    "        rcpo_df_on=time_infos_to_PNs,\n",
    "        df_2_on=PNs_to_time_infos, \n",
    "        how=how\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    # Only reason for making dict is to ensure trsf_pole_nbs are not repeated \n",
    "    active_SNs_in_xfmrs_dfs_dict = {}\n",
    "    if output_trsf_pole_nb_col is None:\n",
    "        output_trsf_pole_nb_col='trsf_pole_nb'\n",
    "    for trsf_pole_nb in trsf_pole_nbs:\n",
    "        # active_SNs_df_i will have indices equal to premise numbers and value equal to lists\n",
    "        #   of active SNs for each PN\n",
    "        PNs_i=time_infos_df.loc[trsf_pole_nb, return_prem_nbs_col]\n",
    "        dt_0_i=time_infos_df.loc[trsf_pole_nb, t_min_col]\n",
    "        dt_1_i=time_infos_df.loc[trsf_pole_nb, t_max_col]\n",
    "        #-----\n",
    "        # See NOTEs above regarding t_min/t_max being empty\n",
    "        # In such a case, it is simply impossibe (with the summary files currently generated) to access\n",
    "        #   the date over which the data would have been run, if any events existed.\n",
    "        #   In future versions, this information will be included in the summary files!\n",
    "        # I don't want to completely exclude these (by e.g., setting dt_0_i=pd.Timestamp.min and \n",
    "        #   dt_1_i=pd.Timestamp.max), so I will simply include the meters which are active TODAY.\n",
    "        # This obviously is not correct, but this occurrence is rare (only happening when every single meter\n",
    "        #   on a transformer had no events during the time period) and this crude approximation will be fine.\n",
    "        if Utilities.is_object_one_of_types(dt_0_i, [list, np.ndarray]):\n",
    "            assert(len(dt_0_i)==0)\n",
    "            # I believe if this happens for one it should happen for both...\n",
    "            assert(Utilities.is_object_one_of_types(dt_1_i, [list, np.ndarray]) and len(dt_1_i)==0)\n",
    "            dt_0_i=pd.Timestamp.today()\n",
    "        if Utilities.is_object_one_of_types(dt_1_i, [list, np.ndarray]):\n",
    "            assert(len(dt_1_i)==0)\n",
    "            # I believe if this happens for one it should happen for both...\n",
    "            # But, dt_0_i changed already above, so must check time_infos_df.loc[trsf_pole_nb, t_min_col] instead!\n",
    "            assert(Utilities.is_object_one_of_types(time_infos_df.loc[trsf_pole_nb, t_min_col], [list, np.ndarray]) and \n",
    "                   len(time_infos_df.loc[trsf_pole_nb, t_min_col])==0)\n",
    "            dt_1_i=pd.Timestamp.today()\n",
    "        if((not isinstance(PNs_i, list) and pd.isna(PNs_i)) or \n",
    "           len(PNs_i)==0):\n",
    "            active_SNs_df_i = pd.DataFrame()\n",
    "        else:\n",
    "            active_SNs_df_i = MeterPremise.get_active_SNs_for_PNs_at_datetime_interval(\n",
    "                PNs=PNs_i,\n",
    "                df_mp_curr=df_mp_curr, \n",
    "                df_mp_hist=df_mp_hist, \n",
    "                dt_0=dt_0_i,\n",
    "                dt_1=dt_1_i,\n",
    "                output_index=None,\n",
    "                output_groupby=[df_mp_prem_nb_col], \n",
    "                include_prems_wo_active_SNs_when_groupby=True, \n",
    "                assert_all_PNs_found=False\n",
    "            )\n",
    "            active_SNs_df_i=active_SNs_df_i.reset_index()\n",
    "        if active_SNs_df_i.shape[0]==0:\n",
    "            active_SNs_df_i[df_mp_prem_nb_col] = np.nan\n",
    "            active_SNs_df_i[df_mp_serial_number_col] = [[]] \n",
    "            active_SNs_df_i[output_trsf_pole_nb_col] = trsf_pole_nb\n",
    "            active_SNs_df_i = active_SNs_df_i.set_index(output_trsf_pole_nb_col)\n",
    "        else:\n",
    "            active_SNs_df_i[output_trsf_pole_nb_col] = trsf_pole_nb\n",
    "            active_SNs_df_i = active_SNs_df_i.explode(df_mp_serial_number_col)\n",
    "            active_SNs_df_i = Utilities_df.consolidate_df(\n",
    "                df=active_SNs_df_i, \n",
    "                groupby_cols=[output_trsf_pole_nb_col], \n",
    "                cols_shared_by_group=None, \n",
    "                cols_to_collect_in_lists=[df_mp_serial_number_col, df_mp_prem_nb_col], \n",
    "                include_groupby_cols_in_output_cols=False, \n",
    "                allow_duplicates_in_lists=False, \n",
    "                recover_uniqueness_violators=True, \n",
    "                rename_cols=None, \n",
    "                verbose=False\n",
    "            )\n",
    "        assert(trsf_pole_nb not in active_SNs_in_xfmrs_dfs_dict)\n",
    "        active_SNs_in_xfmrs_dfs_dict[trsf_pole_nb] = active_SNs_df_i\n",
    "    #-------------------------\n",
    "    active_SNs_df = pd.concat(list(active_SNs_in_xfmrs_dfs_dict.values()))\n",
    "    #-------------------------\n",
    "    # Change [nan] entries to []\n",
    "    #-----\n",
    "    # First, if any entries equal NaN, change to []\n",
    "    active_SNs_df.loc[active_SNs_df[df_mp_serial_number_col].isna(), df_mp_serial_number_col] = active_SNs_df.loc[active_SNs_df[df_mp_serial_number_col].isna(), df_mp_serial_number_col].apply(lambda x: [])\n",
    "    # Now, change any entries equal to [] or [NaN] to []\n",
    "    found_nans_srs = active_SNs_df[df_mp_serial_number_col].apply(lambda x: len([ix for ix in x if not pd.isna(ix)]))==0\n",
    "    if found_nans_srs.sum()>0:\n",
    "        active_SNs_df.loc[found_nans_srs, df_mp_serial_number_col] = active_SNs_df.loc[found_nans_srs, df_mp_serial_number_col].apply(lambda x: [])\n",
    "    #-----\n",
    "    # First, if any entries equal NaN, change to []\n",
    "    active_SNs_df.loc[active_SNs_df[df_mp_prem_nb_col].isna(), df_mp_prem_nb_col] = active_SNs_df.loc[active_SNs_df[df_mp_prem_nb_col].isna(), df_mp_prem_nb_col].apply(lambda x: [])\n",
    "    # Now, change any entries equal to [] or [NaN] to []\n",
    "    found_nans_srs = active_SNs_df[df_mp_prem_nb_col].apply(lambda x: len([ix for ix in x if not pd.isna(ix)]))==0\n",
    "    if found_nans_srs.sum()>0:\n",
    "        active_SNs_df.loc[found_nans_srs, df_mp_prem_nb_col] = active_SNs_df.loc[found_nans_srs, df_mp_prem_nb_col].apply(lambda x: [])\n",
    "    #-------------------------\n",
    "    active_SNs_df = active_SNs_df.rename(columns={\n",
    "        df_mp_prem_nb_col:return_prem_nbs_col, \n",
    "        df_mp_serial_number_col:return_SNs_col\n",
    "    })\n",
    "    #-------------------------\n",
    "    return active_SNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces get_active_SNs_for_xfmrs_OLD, but should probably build get_active_SNs_for_xfmrs\n",
    "#  which accepts a list of trsf_pole_nbs instead of rcpo_df, which this function can use\n",
    "# Moved to OutageMdlrPrep\n",
    "def get_active_SNs_for_xfmrs_in_rcpo_df(\n",
    "    rcpo_df, \n",
    "    trsf_pole_nbs_loc, \n",
    "    df_mp_curr, \n",
    "    df_mp_hist,\n",
    "    time_infos_df, \n",
    "    rcpo_df_to_time_infos_on = [('index', 'outg_rec_nb')], \n",
    "    time_infos_to_rcpo_df_on = ['index'], \n",
    "    how='left', \n",
    "    rcpo_df_to_PNs_on = [('index', 'trsf_pole_nb')], \n",
    "    PNs_to_rcpo_df_on = ['index'], \n",
    "    addtnl_mp_df_curr_cols=None, \n",
    "    addtnl_mp_df_hist_cols=None, \n",
    "    return_SNs_col='SNs', \n",
    "    return_prem_nbs_col='prem_nbs', \n",
    "    assert_all_trsf_pole_nbs_found=True, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb', \n",
    "    t_min_col='t_min', \n",
    "    t_max_col='t_max'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Difficulty is that default.meter_premise_hist does not have trsf_pole_nb field.\n",
    "    Therefore, one must use default.meter_premise to find the premise numbers for xfrms in trsf_pole_nbs,\n",
    "      then use those PNs to select the correct entries from default.meter_premise_hist.\n",
    "    The trsf_pole_nbs should be contained in rcpo_df, and will be found using the trsf_pole_nbs_loc\n",
    "      parameter described below.\n",
    "      \n",
    "    trsf_pole_nbs_loc:\n",
    "        Directs where the transformer pole numbers are located\n",
    "        This should identify an index (w/ level)\n",
    "        Set equal to 'index' for normal DFs, or when trsf_pole_nbs are in level 0 of index.\n",
    "        For a DF with MultiIndex index, there are two options:\n",
    "            i.  Set equal to f'index_{idx_level}' for a DF with MutliIndex index, where idx_level\n",
    "                is an int identifying the level in which the trsf_pole_nbs reside\n",
    "            ii. Set equal to the tuple ('index', trsf_pole_nbs_idx_name), where trsf_pole_nbs_idx_name is\n",
    "            the name of the index level in which the trsf_pole_nbs reside.\n",
    "\n",
    "    If df_mp_curr OR df_mp_hist is not supplied, both will be built!\n",
    "    \n",
    "    addtnl_mp_df_curr_cols/addtnl_mp_df_hist_cols:\n",
    "      Only used when df_mp_curr/df_mp_hist not supplied and therefore need to be built\n",
    "      \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    assert(t_min_col in time_infos_df.columns and \n",
    "           t_max_col in time_infos_df.columns)\n",
    "    time_infos_df = time_infos_df[[t_min_col, t_max_col]]\n",
    "    #-----\n",
    "    # Remove any duplicates from time_infos_df\n",
    "    if time_infos_df.index.nlevels==1:\n",
    "        tmp_col = Utilities.generate_random_string()\n",
    "        time_infos_df[tmp_col] = time_infos_df.index\n",
    "        time_infos_df = time_infos_df.drop_duplicates()\n",
    "        time_infos_df = time_infos_df.drop(columns=[tmp_col])\n",
    "    else:\n",
    "        tmp_cols = [Utilities.generate_random_string() for _ in range(time_infos_df.index.nlevels)]\n",
    "        for i_col, tmp_col in enumerate(tmp_cols):\n",
    "            time_infos_df[tmp_col] = time_infos_df.index.get_level_values(i_col)\n",
    "        time_infos_df = time_infos_df.drop_duplicates()\n",
    "        time_infos_df = time_infos_df.drop(columns=tmp_cols)\n",
    "    #--------------------------------------------------\n",
    "    # trsf_pole_nbs_loc can be a string or tuple/list\n",
    "    # First, find trsf_pole_nbs and trsf_pole_nbs_idx_lvl\n",
    "    assert(Utilities.is_object_one_of_types(trsf_pole_nbs_loc, [str, list, tuple]))\n",
    "    if isinstance(trsf_pole_nbs_loc, str):\n",
    "        assert(trsf_pole_nbs_loc.startswith('index'))\n",
    "        if trsf_pole_nbs_loc=='index':\n",
    "            trsf_pole_nbs_idx_lvl = 0\n",
    "        else:\n",
    "            trsf_pole_nbs_idx_lvl = re.findall('index_(\\d*)', trsf_pole_nbs_loc)\n",
    "            assert(len(trsf_pole_nbs_idx_lvl)==1)\n",
    "            trsf_pole_nbs_idx_lvl=trsf_pole_nbs_idx_lvl[0]\n",
    "            trsf_pole_nbs_idx_lvl=int(trsf_pole_nbs_idx_lvl)\n",
    "    else:\n",
    "        assert(len(trsf_pole_nbs_loc)==2)\n",
    "        assert(trsf_pole_nbs_loc[0]=='index')\n",
    "        assert(trsf_pole_nbs_loc[1] in rcpo_df.index.names)\n",
    "        trsf_pole_nbs_idx_lvl = rcpo_df.index.names.index(trsf_pole_nbs_loc[1])\n",
    "        #---------------\n",
    "        assert(trsf_pole_nbs_idx_lvl < rcpo_df.index.nlevels)\n",
    "        trsf_pole_nbs = rcpo_df.index.get_level_values(trsf_pole_nbs_idx_lvl).tolist()\n",
    "    #--------------------------------------------------\n",
    "    #-------------------------\n",
    "    necessary_mp_cols = [df_mp_serial_number_col, df_mp_prem_nb_col, df_mp_install_time_col, df_mp_removal_time_col]\n",
    "    #-------------------------\n",
    "    if df_mp_curr is None or df_mp_hist is None:\n",
    "        mp_df_curr_hist = MeterPremise.build_mp_df_curr_hist_for_xfmrs(\n",
    "            trsf_pole_nbs, \n",
    "            join_curr_hist=False, \n",
    "            addtnl_mp_df_curr_cols=addtnl_mp_df_curr_cols, \n",
    "            addtnl_mp_df_hist_cols=addtnl_mp_df_hist_cols, \n",
    "            df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "            df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "            df_mp_install_time_col=df_mp_install_time_col, \n",
    "            df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "            df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "        )\n",
    "        df_mp_curr = mp_df_curr_hist['mp_df_curr']\n",
    "        df_mp_hist = mp_df_curr_hist['mp_df_hist']\n",
    "    #-------------------------\n",
    "    # At a bare minimum, df_mp_curr and df_mp_hist must both have the following columns:\n",
    "    #   necessary_mp_cols = ['mfr_devc_ser_nbr', 'prem_nb', 'inst_ts', 'rmvl_ts']\n",
    "    assert(all([x in df_mp_curr.columns for x in necessary_mp_cols+[df_mp_trsf_pole_nb_col]]))\n",
    "    assert(all([x in df_mp_hist.columns for x in necessary_mp_cols]))\n",
    "    #-------------------------\n",
    "    # PNs_for_xfmrs is a DF with trsf_pole_nbs indices and elements which are lists of PNs for each xfmr\n",
    "    PNs_for_xfmrs = MeterPremise.get_SNs_andor_PNs_for_xfmrs(\n",
    "        trsf_pole_nbs=trsf_pole_nbs, \n",
    "        include_SNs=False,\n",
    "        include_PNs=True,\n",
    "        trsf_pole_nb_col=df_mp_trsf_pole_nb_col, \n",
    "        serial_number_col=df_mp_serial_number_col, \n",
    "        prem_nb_col=df_mp_prem_nb_col, \n",
    "        return_SNs_col=None, #Not grabbing SNs\n",
    "        return_PNs_col=return_prem_nbs_col, \n",
    "        assert_all_trsf_pole_nbs_found=assert_all_trsf_pole_nbs_found, \n",
    "        mp_df=df_mp_curr, \n",
    "        return_mp_df_also=False\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Join together rcpo_df, time_infos_df and PNs_for_xfmrs\n",
    "    rcpo_df = merge_rcpo_and_df(\n",
    "        rcpo_df=rcpo_df, \n",
    "        df_2=time_infos_df, \n",
    "        rcpo_df_on=rcpo_df_to_time_infos_on,\n",
    "        df_2_on=time_infos_to_rcpo_df_on, \n",
    "        how=how\n",
    "    )\n",
    "    #-----\n",
    "    rcpo_df = merge_rcpo_and_df(\n",
    "        rcpo_df=rcpo_df, \n",
    "        df_2=PNs_for_xfmrs, \n",
    "        rcpo_df_on=rcpo_df_to_PNs_on,\n",
    "        df_2_on=PNs_to_rcpo_df_on, \n",
    "        how=how\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    # Only reason for making dict is to ensure trsf_pole_nbs are not repeated \n",
    "    active_SNs_in_xfmrs_dfs_dict = {}\n",
    "\n",
    "    rcpo_idx_names = list(rcpo_df.index.names)\n",
    "    assert(not any([x is None for x in rcpo_idx_names]))\n",
    "    for idx_i, row_i in rcpo_df.iterrows():\n",
    "        # active_SNs_df_i will have indices equal to premise numbers and value equal to lists\n",
    "        #   of active SNs for each PN\n",
    "        # Purpose of making idx_names_w_vals a list of tuples, instead of a dict, is to ensure the correct order is maintained\n",
    "        #   Dicts usually return the correct order, but this is not guaranteed\n",
    "        if len(rcpo_idx_names)==1:\n",
    "            assert(rcpo_df.index.nlevels==1)\n",
    "            idx_names_w_vals = [(rcpo_idx_names[0], idx_i)]\n",
    "        else:\n",
    "            idx_names_w_vals = [((rcpo_idx_names[i] if i!=trsf_pole_nbs_idx_lvl else df_mp_trsf_pole_nb_col), idx_i[i]) \n",
    "                                for i in range(len(idx_i))]\n",
    "        PNs_i=row_i[return_prem_nbs_col]\n",
    "        dt_0_i=row_i[t_min_col]\n",
    "        dt_1_i=row_i[t_max_col]\n",
    "        #-----\n",
    "        # See NOTEs above regarding t_min/t_max being empty\n",
    "        # In such a case, it is simply impossibe (with the summary files currently generated) to access\n",
    "        #   the date over which the data would have been run, if any events existed.\n",
    "        #   In future versions, this information will be included in the summary files!\n",
    "        # I don't want to completely exclude these (by e.g., setting dt_0_i=pd.Timestamp.min and \n",
    "        #   dt_1_i=pd.Timestamp.max), so I will simply include the meters which are active TODAY.\n",
    "        # This obviously is not correct, but this occurrence is rare (only happening when every single meter\n",
    "        #   on a transformer had no events during the time period) and this crude approximation will be fine.\n",
    "        if Utilities.is_object_one_of_types(dt_0_i, [list, np.ndarray]):\n",
    "            assert(len(dt_0_i)==0)\n",
    "            # I believe if this happens for one it should happen for both...\n",
    "            assert(Utilities.is_object_one_of_types(dt_1_i, [list, np.ndarray]) and len(dt_1_i)==0)\n",
    "            dt_0_i=pd.Timestamp.today()\n",
    "        if Utilities.is_object_one_of_types(dt_1_i, [list, np.ndarray]):\n",
    "            assert(len(dt_1_i)==0)\n",
    "            # I believe if this happens for one it should happen for both...\n",
    "            # But, dt_0_i changed already above, so must check row_i[t_min_col] instead!\n",
    "            assert(Utilities.is_object_one_of_types(row_i[t_min_col], [list, np.ndarray]) and len(row_i[t_min_col])==0)\n",
    "            dt_1_i=pd.Timestamp.today()\n",
    "        if((not isinstance(PNs_i, list) and pd.isna(PNs_i)) or \n",
    "           len(PNs_i)==0):\n",
    "            active_SNs_df_i = pd.DataFrame()\n",
    "        else:\n",
    "            active_SNs_df_i = MeterPremise.get_active_SNs_for_PNs_at_datetime_interval(\n",
    "                PNs=PNs_i,\n",
    "                df_mp_curr=df_mp_curr, \n",
    "                df_mp_hist=df_mp_hist, \n",
    "                dt_0=dt_0_i,\n",
    "                dt_1=dt_1_i,\n",
    "                output_index=None,\n",
    "                output_groupby=[df_mp_prem_nb_col], \n",
    "                include_prems_wo_active_SNs_when_groupby=True, \n",
    "                assert_all_PNs_found=False\n",
    "            )\n",
    "            active_SNs_df_i=active_SNs_df_i.reset_index()\n",
    "        if active_SNs_df_i.shape[0]==0:\n",
    "            active_SNs_df_i[df_mp_prem_nb_col] = np.nan\n",
    "            active_SNs_df_i[df_mp_serial_number_col] = [[]] \n",
    "            for name,val in idx_names_w_vals:\n",
    "                active_SNs_df_i[name] = val\n",
    "            active_SNs_df_i = active_SNs_df_i.set_index([x[0] for x in idx_names_w_vals])\n",
    "        else:\n",
    "            for name,val in idx_names_w_vals:\n",
    "                active_SNs_df_i[name] = val\n",
    "            active_SNs_df_i = active_SNs_df_i.explode(df_mp_serial_number_col)\n",
    "            active_SNs_df_i = Utilities_df.consolidate_df(\n",
    "                df=active_SNs_df_i, \n",
    "                groupby_cols=[x[0] for x in idx_names_w_vals], \n",
    "                cols_shared_by_group=None, \n",
    "                cols_to_collect_in_lists=[df_mp_serial_number_col, df_mp_prem_nb_col], \n",
    "                include_groupby_cols_in_output_cols=False, \n",
    "                allow_duplicates_in_lists=False, \n",
    "                recover_uniqueness_violators=True, \n",
    "                rename_cols=None, \n",
    "                verbose=False\n",
    "            )\n",
    "        assert(idx_i not in active_SNs_in_xfmrs_dfs_dict)\n",
    "        active_SNs_in_xfmrs_dfs_dict[idx_i] = active_SNs_df_i\n",
    "    #-------------------------\n",
    "    active_SNs_df = pd.concat(list(active_SNs_in_xfmrs_dfs_dict.values()))\n",
    "    #-------------------------\n",
    "    # Change [nan] entries to []\n",
    "    #-----\n",
    "    # First, if any entries equal NaN, change to []\n",
    "    active_SNs_df.loc[active_SNs_df[df_mp_serial_number_col].isna(), df_mp_serial_number_col] = active_SNs_df.loc[active_SNs_df[df_mp_serial_number_col].isna(), df_mp_serial_number_col].apply(lambda x: [])\n",
    "    # Now, change any entries equal to [] or [NaN] to []\n",
    "    found_nans_srs = active_SNs_df[df_mp_serial_number_col].apply(lambda x: len([ix for ix in x if not pd.isna(ix)]))==0\n",
    "    if found_nans_srs.sum()>0:\n",
    "        active_SNs_df.loc[found_nans_srs, df_mp_serial_number_col] = active_SNs_df.loc[found_nans_srs, df_mp_serial_number_col].apply(lambda x: [])\n",
    "    #-----\n",
    "    # First, if any entries equal NaN, change to []\n",
    "    active_SNs_df.loc[active_SNs_df[df_mp_prem_nb_col].isna(), df_mp_prem_nb_col] = active_SNs_df.loc[active_SNs_df[df_mp_prem_nb_col].isna(), df_mp_prem_nb_col].apply(lambda x: [])\n",
    "    # Now, change any entries equal to [] or [NaN] to []\n",
    "    found_nans_srs = active_SNs_df[df_mp_prem_nb_col].apply(lambda x: len([ix for ix in x if not pd.isna(ix)]))==0\n",
    "    if found_nans_srs.sum()>0:\n",
    "        active_SNs_df.loc[found_nans_srs, df_mp_prem_nb_col] = active_SNs_df.loc[found_nans_srs, df_mp_prem_nb_col].apply(lambda x: [])\n",
    "    #-------------------------\n",
    "    active_SNs_df = active_SNs_df.rename(columns={\n",
    "        df_mp_prem_nb_col:return_prem_nbs_col, \n",
    "        df_mp_serial_number_col:return_SNs_col\n",
    "    })\n",
    "    #-------------------------\n",
    "    return active_SNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_mfs = get_active_SNs_for_xfmrs_in_rcpo_df(\n",
    "#     rcpo_df=rcpo_df_raw, \n",
    "#     trsf_pole_nbs_loc=trsf_pole_nbs_loc, \n",
    "#     df_mp_curr=mp_df_curr_hist['mp_df_curr'],\n",
    "#     df_mp_hist=mp_df_curr_hist['mp_df_hist'], \n",
    "#     time_infos_df=time_infos_df, \n",
    "#     rcpo_df_to_time_infos_on=rcpo_df_to_time_infos_on, \n",
    "#     time_infos_to_rcpo_df_on=time_infos_to_rcpo_df_on, \n",
    "#     how=how, \n",
    "#     rcpo_df_to_PNs_on=rcpo_df_to_PNs_on, \n",
    "#     PNs_to_rcpo_df_on=PNs_to_rcpo_df_on, \n",
    "#     assert_all_trsf_pole_nbs_found=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces get_active_SNs_for_xfmrs_OLD, but should probably build get_active_SNs_for_xfmrs\n",
    "#  which accepts a list of trsf_pole_nbs instead of rcpo_df, which this function can use\n",
    "# Moved to OutageMdlrPrep\n",
    "def get_active_SNs_for_xfmrs_in_rcpo_df_v2(\n",
    "    rcpo_df, \n",
    "    trsf_pole_nbs_loc, \n",
    "    df_mp_curr, \n",
    "    df_mp_hist,\n",
    "    time_infos_df, \n",
    "    rcpo_df_to_time_infos_on = [('index', 'outg_rec_nb')], \n",
    "    time_infos_to_rcpo_df_on = ['index'], \n",
    "    how='left', \n",
    "    rcpo_df_to_PNs_on = [('index', 'trsf_pole_nb')], \n",
    "    PNs_to_rcpo_df_on = ['index'], \n",
    "    addtnl_mp_df_curr_cols=None, \n",
    "    addtnl_mp_df_hist_cols=None, \n",
    "    return_SNs_col='SNs', \n",
    "    return_prem_nbs_col='prem_nbs', \n",
    "    assert_all_trsf_pole_nbs_found=True, \n",
    "    df_mp_serial_number_col='mfr_devc_ser_nbr', \n",
    "    df_mp_prem_nb_col='prem_nb', \n",
    "    df_mp_install_time_col='inst_ts', \n",
    "    df_mp_removal_time_col='rmvl_ts', \n",
    "    df_mp_trsf_pole_nb_col='trsf_pole_nb', \n",
    "    t_min_col='t_min', \n",
    "    t_max_col='t_max'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Difficulty is that default.meter_premise_hist does not have trsf_pole_nb field.\n",
    "    Therefore, one must use default.meter_premise to find the premise numbers for xfrms in trsf_pole_nbs,\n",
    "      then use those PNs to select the correct entries from default.meter_premise_hist.\n",
    "    The trsf_pole_nbs should be contained in rcpo_df, and will be found using the trsf_pole_nbs_loc\n",
    "      parameter described below.\n",
    "      \n",
    "    trsf_pole_nbs_loc:\n",
    "        Directs where the transformer pole numbers are located\n",
    "        This should identify an index (w/ level)\n",
    "        Set equal to 'index' for normal DFs, or when trsf_pole_nbs are in level 0 of index.\n",
    "        For a DF with MultiIndex index, there are two options:\n",
    "            i.  Set equal to f'index_{idx_level}' for a DF with MutliIndex index, where idx_level\n",
    "                is an int identifying the level in which the trsf_pole_nbs reside\n",
    "            ii. Set equal to the tuple ('index', trsf_pole_nbs_idx_name), where trsf_pole_nbs_idx_name is\n",
    "            the name of the index level in which the trsf_pole_nbs reside.\n",
    "\n",
    "    If df_mp_curr OR df_mp_hist is not supplied, both will be built!\n",
    "    \n",
    "    addtnl_mp_df_curr_cols/addtnl_mp_df_hist_cols:\n",
    "      Only used when df_mp_curr/df_mp_hist not supplied and therefore need to be built\n",
    "      \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    assert(t_min_col in time_infos_df.columns and \n",
    "           t_max_col in time_infos_df.columns)\n",
    "    time_infos_df = time_infos_df[[t_min_col, t_max_col]]\n",
    "    #-----\n",
    "    # Remove any duplicates from time_infos_df\n",
    "    if time_infos_df.index.nlevels==1:\n",
    "        tmp_col = Utilities.generate_random_string()\n",
    "        time_infos_df[tmp_col] = time_infos_df.index\n",
    "        time_infos_df = time_infos_df.drop_duplicates()\n",
    "        time_infos_df = time_infos_df.drop(columns=[tmp_col])\n",
    "    else:\n",
    "        tmp_cols = [Utilities.generate_random_string() for _ in range(time_infos_df.index.nlevels)]\n",
    "        for i_col, tmp_col in enumerate(tmp_cols):\n",
    "            time_infos_df[tmp_col] = time_infos_df.index.get_level_values(i_col)\n",
    "        time_infos_df = time_infos_df.drop_duplicates()\n",
    "        time_infos_df = time_infos_df.drop(columns=tmp_cols)\n",
    "    #--------------------------------------------------\n",
    "    # trsf_pole_nbs_loc can be a string or tuple/list\n",
    "    # First, find trsf_pole_nbs and trsf_pole_nbs_idx_lvl\n",
    "    assert(Utilities.is_object_one_of_types(trsf_pole_nbs_loc, [str, list, tuple]))\n",
    "    if isinstance(trsf_pole_nbs_loc, str):\n",
    "        assert(trsf_pole_nbs_loc.startswith('index'))\n",
    "        if trsf_pole_nbs_loc=='index':\n",
    "            trsf_pole_nbs_idx_lvl = 0\n",
    "        else:\n",
    "            trsf_pole_nbs_idx_lvl = re.findall('index_(\\d*)', trsf_pole_nbs_loc)\n",
    "            assert(len(trsf_pole_nbs_idx_lvl)==1)\n",
    "            trsf_pole_nbs_idx_lvl=trsf_pole_nbs_idx_lvl[0]\n",
    "            trsf_pole_nbs_idx_lvl=int(trsf_pole_nbs_idx_lvl)\n",
    "    else:\n",
    "        assert(len(trsf_pole_nbs_loc)==2)\n",
    "        assert(trsf_pole_nbs_loc[0]=='index')\n",
    "        assert(trsf_pole_nbs_loc[1] in rcpo_df.index.names)\n",
    "        trsf_pole_nbs_idx_lvl = rcpo_df.index.names.index(trsf_pole_nbs_loc[1])\n",
    "        #---------------\n",
    "        assert(trsf_pole_nbs_idx_lvl < rcpo_df.index.nlevels)\n",
    "        trsf_pole_nbs = rcpo_df.index.get_level_values(trsf_pole_nbs_idx_lvl).tolist()\n",
    "    #--------------------------------------------------\n",
    "    #-------------------------\n",
    "    necessary_mp_cols = [df_mp_serial_number_col, df_mp_prem_nb_col, df_mp_install_time_col, df_mp_removal_time_col]\n",
    "    #-------------------------\n",
    "    if df_mp_curr is None or df_mp_hist is None:\n",
    "        mp_df_curr_hist = MeterPremise.build_mp_df_curr_hist_for_xfmrs(\n",
    "            trsf_pole_nbs, \n",
    "            join_curr_hist=False, \n",
    "            addtnl_mp_df_curr_cols=addtnl_mp_df_curr_cols, \n",
    "            addtnl_mp_df_hist_cols=addtnl_mp_df_hist_cols, \n",
    "            df_mp_serial_number_col=df_mp_serial_number_col, \n",
    "            df_mp_prem_nb_col=df_mp_prem_nb_col, \n",
    "            df_mp_install_time_col=df_mp_install_time_col, \n",
    "            df_mp_removal_time_col=df_mp_removal_time_col, \n",
    "            df_mp_trsf_pole_nb_col=df_mp_trsf_pole_nb_col\n",
    "        )\n",
    "        df_mp_curr = mp_df_curr_hist['mp_df_curr']\n",
    "        df_mp_hist = mp_df_curr_hist['mp_df_hist']\n",
    "    #-------------------------\n",
    "    # At a bare minimum, df_mp_curr and df_mp_hist must both have the following columns:\n",
    "    #   necessary_mp_cols = ['mfr_devc_ser_nbr', 'prem_nb', 'inst_ts', 'rmvl_ts']\n",
    "    assert(all([x in df_mp_curr.columns for x in necessary_mp_cols+[df_mp_trsf_pole_nb_col]]))\n",
    "    assert(all([x in df_mp_hist.columns for x in necessary_mp_cols]))\n",
    "    #-------------------------\n",
    "    # PNs_for_xfmrs is a DF with trsf_pole_nbs indices and elements which are lists of PNs for each xfmr\n",
    "    PNs_for_xfmrs = MeterPremise.get_SNs_andor_PNs_for_xfmrs(\n",
    "        trsf_pole_nbs=trsf_pole_nbs, \n",
    "        include_SNs=False,\n",
    "        include_PNs=True,\n",
    "        trsf_pole_nb_col=df_mp_trsf_pole_nb_col, \n",
    "        serial_number_col=df_mp_serial_number_col, \n",
    "        prem_nb_col=df_mp_prem_nb_col, \n",
    "        return_SNs_col=None, #Not grabbing SNs\n",
    "        return_PNs_col=return_prem_nbs_col, \n",
    "        assert_all_trsf_pole_nbs_found=assert_all_trsf_pole_nbs_found, \n",
    "        mp_df=df_mp_curr, \n",
    "        return_mp_df_also=False\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Join together rcpo_df, time_infos_df and PNs_for_xfmrs\n",
    "    rcpo_df = merge_rcpo_and_df(\n",
    "        rcpo_df=rcpo_df, \n",
    "        df_2=time_infos_df, \n",
    "        rcpo_df_on=rcpo_df_to_time_infos_on,\n",
    "        df_2_on=time_infos_to_rcpo_df_on, \n",
    "        how=how\n",
    "    )\n",
    "    #-----\n",
    "    rcpo_df = merge_rcpo_and_df(\n",
    "        rcpo_df=rcpo_df, \n",
    "        df_2=PNs_for_xfmrs, \n",
    "        rcpo_df_on=rcpo_df_to_PNs_on,\n",
    "        df_2_on=PNs_to_rcpo_df_on, \n",
    "        how=how\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    # Only reason for making dict is to ensure trsf_pole_nbs are not repeated \n",
    "    active_SNs_in_xfmrs_dfs_dict = {}\n",
    "\n",
    "    rcpo_idx_names = list(rcpo_df.index.names)\n",
    "    assert(not any([x is None for x in rcpo_idx_names]))\n",
    "    for idx_i, row_i in rcpo_df.iterrows():\n",
    "        # active_SNs_df_i will have indices equal to premise numbers and value equal to lists\n",
    "        #   of active SNs for each PN\n",
    "        # Purpose of making idx_names_w_vals a list of tuples, instead of a dict, is to ensure the correct order is maintained\n",
    "        #   Dicts usually return the correct order, but this is not guaranteed\n",
    "        if len(rcpo_idx_names)==1:\n",
    "            assert(rcpo_df.index.nlevels==1)\n",
    "            idx_names_w_vals = [(rcpo_idx_names[0], idx_i)]\n",
    "        else:\n",
    "            idx_names_w_vals = [((rcpo_idx_names[i] if i!=trsf_pole_nbs_idx_lvl else df_mp_trsf_pole_nb_col), idx_i[i]) \n",
    "                                for i in range(len(idx_i))]\n",
    "        PNs_i=row_i[return_prem_nbs_col]\n",
    "        dt_0_i=row_i[t_min_col]\n",
    "        dt_1_i=row_i[t_max_col]\n",
    "        #-----\n",
    "        # See NOTEs above regarding t_min/t_max being empty\n",
    "        # In such a case, it is simply impossibe (with the summary files currently generated) to access\n",
    "        #   the date over which the data would have been run, if any events existed.\n",
    "        #   In future versions, this information will be included in the summary files!\n",
    "        # I don't want to completely exclude these (by e.g., setting dt_0_i=pd.Timestamp.min and \n",
    "        #   dt_1_i=pd.Timestamp.max), so I will simply include the meters which are active TODAY.\n",
    "        # This obviously is not correct, but this occurrence is rare (only happening when every single meter\n",
    "        #   on a transformer had no events during the time period) and this crude approximation will be fine.\n",
    "        if Utilities.is_object_one_of_types(dt_0_i, [list, np.ndarray]):\n",
    "            assert(len(dt_0_i)==0)\n",
    "            # I believe if this happens for one it should happen for both...\n",
    "            assert(Utilities.is_object_one_of_types(dt_1_i, [list, np.ndarray]) and len(dt_1_i)==0)\n",
    "            dt_0_i=pd.Timestamp.today()\n",
    "        if Utilities.is_object_one_of_types(dt_1_i, [list, np.ndarray]):\n",
    "            assert(len(dt_1_i)==0)\n",
    "            # I believe if this happens for one it should happen for both...\n",
    "            # But, dt_0_i changed already above, so must check row_i[t_min_col] instead!\n",
    "            assert(Utilities.is_object_one_of_types(row_i[t_min_col], [list, np.ndarray]) and len(row_i[t_min_col])==0)\n",
    "            dt_1_i=pd.Timestamp.today()\n",
    "        if((not isinstance(PNs_i, list) and pd.isna(PNs_i)) or \n",
    "           len(PNs_i)==0):\n",
    "            active_SNs_df_i = pd.DataFrame()\n",
    "        else:\n",
    "            active_SNs_df_i = MeterPremise.get_active_SNs_for_PNs_at_datetime_interval(\n",
    "                PNs=PNs_i,\n",
    "                df_mp_curr=df_mp_curr, \n",
    "                df_mp_hist=df_mp_hist, \n",
    "                dt_0=dt_0_i,\n",
    "                dt_1=dt_1_i,\n",
    "                output_index=None,\n",
    "                output_groupby=[df_mp_prem_nb_col], \n",
    "                include_prems_wo_active_SNs_when_groupby=True, \n",
    "                assert_all_PNs_found=False\n",
    "            )\n",
    "            active_SNs_df_i=active_SNs_df_i.reset_index()\n",
    "        if active_SNs_df_i.shape[0]==0:\n",
    "            active_SNs_df_i[df_mp_prem_nb_col] = np.nan\n",
    "            active_SNs_df_i[df_mp_serial_number_col] = [[]] \n",
    "        for name,val in idx_names_w_vals:\n",
    "            active_SNs_df_i[name] = val\n",
    "        active_SNs_df_i = active_SNs_df_i.explode(df_mp_serial_number_col)\n",
    "        assert(idx_i not in active_SNs_in_xfmrs_dfs_dict)\n",
    "        active_SNs_in_xfmrs_dfs_dict[idx_i] = active_SNs_df_i\n",
    "    #-------------------------\n",
    "    active_SNs_df = pd.concat(list(active_SNs_in_xfmrs_dfs_dict.values()))\n",
    "    #-------------------------\n",
    "    active_SNs_df = Utilities_df.consolidate_df(\n",
    "        df=active_SNs_df, \n",
    "        groupby_cols=[x[0] for x in idx_names_w_vals], \n",
    "        cols_shared_by_group=None, \n",
    "        cols_to_collect_in_lists=[df_mp_serial_number_col, df_mp_prem_nb_col], \n",
    "        include_groupby_cols_in_output_cols=False, \n",
    "        allow_duplicates_in_lists=False, \n",
    "        recover_uniqueness_violators=True, \n",
    "        rename_cols=None, \n",
    "        verbose=False\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Change [nan] entries to []\n",
    "    #-----\n",
    "    # First, if any entries equal NaN, change to []\n",
    "    active_SNs_df.loc[active_SNs_df[df_mp_serial_number_col].isna(), df_mp_serial_number_col] = active_SNs_df.loc[active_SNs_df[df_mp_serial_number_col].isna(), df_mp_serial_number_col].apply(lambda x: [])\n",
    "    # Now, change any entries equal to [] or [NaN] to []\n",
    "    found_nans_srs = active_SNs_df[df_mp_serial_number_col].apply(lambda x: len([ix for ix in x if not pd.isna(ix)]))==0\n",
    "    if found_nans_srs.sum()>0:\n",
    "        active_SNs_df.loc[found_nans_srs, df_mp_serial_number_col] = active_SNs_df.loc[found_nans_srs, df_mp_serial_number_col].apply(lambda x: [])\n",
    "    #-----\n",
    "    # First, if any entries equal NaN, change to []\n",
    "    active_SNs_df.loc[active_SNs_df[df_mp_prem_nb_col].isna(), df_mp_prem_nb_col] = active_SNs_df.loc[active_SNs_df[df_mp_prem_nb_col].isna(), df_mp_prem_nb_col].apply(lambda x: [])\n",
    "    # Now, change any entries equal to [] or [NaN] to []\n",
    "    found_nans_srs = active_SNs_df[df_mp_prem_nb_col].apply(lambda x: len([ix for ix in x if not pd.isna(ix)]))==0\n",
    "    if found_nans_srs.sum()>0:\n",
    "        active_SNs_df.loc[found_nans_srs, df_mp_prem_nb_col] = active_SNs_df.loc[found_nans_srs, df_mp_prem_nb_col].apply(lambda x: [])\n",
    "    #------------------------- \n",
    "    active_SNs_df = active_SNs_df.rename(columns={\n",
    "        df_mp_prem_nb_col:return_prem_nbs_col, \n",
    "        df_mp_serial_number_col:return_SNs_col\n",
    "    })\n",
    "    #-------------------------\n",
    "    return active_SNs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147fe8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces add_xfmr_active_SNs_to_rcpo_df_OLD\n",
    "# Moved to OutageMdlrPrep\n",
    "def add_xfmr_active_SNs_to_rcpo_df(\n",
    "    rcpo_df, \n",
    "    trsf_pole_nbs_loc, \n",
    "    set_xfmr_nSNs=True, \n",
    "    include_active_xfmr_PNs=False, \n",
    "    df_mp_curr=None,\n",
    "    df_mp_hist=None, \n",
    "    time_infos_df=None, \n",
    "    rcpo_df_to_time_infos_on = [('index', 'outg_rec_nb')], \n",
    "    time_infos_to_rcpo_df_on = ['index'], \n",
    "    how='left', \n",
    "    rcpo_df_to_PNs_on = [('index', 'trsf_pole_nb')], \n",
    "    PNs_to_rcpo_df_on = ['index'], \n",
    "    addtnl_get_active_SNs_for_xfmrs_kwargs=None, \n",
    "    xfmr_SNs_col='_xfmr_SNs', \n",
    "    xfmr_nSNs_col='_xfmr_nSNs', \n",
    "    xfmr_PNs_col='_xfmr_PNs', \n",
    "    xfmr_nPNs_col='_xfmr_nPNs', \n",
    "):\n",
    "    r\"\"\"\n",
    "    NOTE: If include_active_xfmr_PNs is True, this column (named xfmr_PNs_col='_xfmr_SNs') should be \n",
    "          equal to the PNs already in rcpo_df!\n",
    "    NOTE: xfmr_SNs_col, xfmr_nSNs_col, xfmr_PNs_col, and xfmr_nPNs_col should all be strings, not tuples.\n",
    "          If column is multiindex, the level_0 value will be handled below.\n",
    "          \n",
    "    NOTE: If any of xfmr_SNs_col, xfmr_nSNs_col, xfmr_PNs_col, and xfmr_nPNs_col are already contained in \n",
    "          rcpo_df, they will be replaced.  This is needed so that the merge operation does not come back with _x and _y\n",
    "          values.  So, one should make sure this function call is truly needed, as grabbing the serial numbers for the\n",
    "          outages typically takes a couple/few minutes.\n",
    "          \n",
    "    NOTE: To make things run faster, the user can supply df_mp_curr and df_mp_hist.  These will be included in \n",
    "          get_active_SNs_for_xfmrs_kwargs.\n",
    "          NOTE: If df_mp_curr/df_mp_hist is also supplied in addtnl_get_active_SNs_for_xfmrs_kwargs,\n",
    "                that/those in addtnl_get_active_SNs_for_xfmrs_kwargs will ultimately be used (not the\n",
    "                explicity df_mp_hist/curr in the function arguments!)\n",
    "          CAREFUL: If one does supple df_mp_curr/hist, one must be certain these DFs contain all necessary elements!\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    get_active_SNs_for_xfmrs_kwargs = dict(\n",
    "        rcpo_df=rcpo_df, \n",
    "        trsf_pole_nbs_loc=trsf_pole_nbs_loc, \n",
    "        df_mp_curr=df_mp_curr, \n",
    "        df_mp_hist=df_mp_hist, \n",
    "        time_infos_df=time_infos_df, \n",
    "        rcpo_df_to_time_infos_on=rcpo_df_to_time_infos_on, \n",
    "        time_infos_to_rcpo_df_on=time_infos_to_rcpo_df_on, \n",
    "        how=how, \n",
    "        rcpo_df_to_PNs_on=rcpo_df_to_PNs_on, \n",
    "        PNs_to_rcpo_df_on=PNs_to_rcpo_df_on, \n",
    "        return_prem_nbs_col=xfmr_PNs_col, \n",
    "        return_SNs_col=xfmr_SNs_col\n",
    "    )\n",
    "    if addtnl_get_active_SNs_for_xfmrs_kwargs is not None:\n",
    "        get_active_SNs_for_xfmrs_kwargs = {**get_active_SNs_for_xfmrs_kwargs, \n",
    "                                           **addtnl_get_active_SNs_for_xfmrs_kwargs}\n",
    "    active_SNs_df = get_active_SNs_for_xfmrs_in_rcpo_df(**get_active_SNs_for_xfmrs_kwargs)\n",
    "    assert(isinstance(active_SNs_df, pd.DataFrame))\n",
    "    #-------------------------\n",
    "    # Assert below might be too strong here...\n",
    "    assert(sorted(rcpo_df.index.unique().tolist())==sorted(active_SNs_df.index.unique().tolist()))\n",
    "    assert(rcpo_df.columns.nlevels<=2)\n",
    "    if rcpo_df.columns.nlevels==1:\n",
    "        #----------\n",
    "        # See note above about columns being replaced/dropped\n",
    "        cols_to_drop = [x for x in rcpo_df.columns if x in active_SNs_df.columns]\n",
    "        if len(cols_to_drop)>0:\n",
    "            rcpo_df = rcpo_df.drop(columns=cols_to_drop)\n",
    "        #----------\n",
    "        rcpo_df = rcpo_df.merge(active_SNs_df, left_index=True, right_index=True)\n",
    "        #----------\n",
    "        if set_xfmr_nSNs:\n",
    "            rcpo_df = MECPODf.set_nSNs_from_SNs_in_rcpo_df(rcpo_df, xfmr_SNs_col, xfmr_nSNs_col)\n",
    "            if include_active_xfmr_PNs:\n",
    "                rcpo_df = MECPODf.set_nSNs_from_SNs_in_rcpo_df(rcpo_df, xfmr_PNs_col, xfmr_nPNs_col)\n",
    "    else:\n",
    "        # Currently, only expecting raw and/or norm.  No problem to allow more, but for now keep this to alert \n",
    "        # of anything unexpected\n",
    "        assert(rcpo_df.columns.get_level_values(0).nunique()<=2)\n",
    "        for i,level_0_val in enumerate(rcpo_df.columns.get_level_values(0).unique()):\n",
    "            if i==0:\n",
    "                active_SNs_df.columns = pd.MultiIndex.from_product([[level_0_val], active_SNs_df.columns])\n",
    "            else:\n",
    "                active_SNs_df.columns = active_SNs_df.columns.set_levels([level_0_val], level=0)\n",
    "            #----------\n",
    "            # See note above about columns being replaced/dropped\n",
    "            cols_to_drop = [x for x in rcpo_df.columns if x in active_SNs_df.columns]\n",
    "            if len(cols_to_drop)>0:\n",
    "                rcpo_df = rcpo_df.drop(columns=cols_to_drop)\n",
    "            #----------\n",
    "            rcpo_df = rcpo_df.merge(active_SNs_df, left_index=True, right_index=True)\n",
    "            #----------\n",
    "            if set_xfmr_nSNs:\n",
    "                rcpo_df = MECPODf.set_nSNs_from_SNs_in_rcpo_df(rcpo_df, (level_0_val, xfmr_SNs_col), (level_0_val, xfmr_nSNs_col))\n",
    "                if include_active_xfmr_PNs:\n",
    "                    rcpo_df = MECPODf.set_nSNs_from_SNs_in_rcpo_df(rcpo_df, (level_0_val, xfmr_PNs_col), (level_0_val, xfmr_nPNs_col))\n",
    "    #-------------------------\n",
    "    rcpo_df = rcpo_df.sort_index(axis=1,level=0)\n",
    "    #-------------------------\n",
    "    return rcpo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces build_rcpo_df_norm_by_xfmr_active_nSNs_OLD\n",
    "# Moved to OutageMdlrPrep\n",
    "def build_rcpo_df_norm_by_xfmr_active_nSNs(\n",
    "    rcpo_df_raw, \n",
    "    trsf_pole_nbs_loc, \n",
    "    xfmr_nSNs_col='_xfmr_nSNs', \n",
    "    xfmr_SNs_col='_xfmr_SNs', \n",
    "    other_SNs_col_tags_to_ignore=['_SNs', '_nSNs', '_prem_nbs', '_nprem_nbs', '_xfmr_PNs', '_xfmr_nPNs'], \n",
    "    drop_xfmr_nSNs_eq_0=True, \n",
    "    new_level_0_val='counts_norm_by_xfmr_nSNs', \n",
    "    remove_SNs_cols=False, \n",
    "    df_mp_curr=None,\n",
    "    df_mp_hist=None, \n",
    "    time_infos_df=None,\n",
    "    rcpo_df_to_time_infos_on = [('index', 'outg_rec_nb')], \n",
    "    time_infos_to_rcpo_df_on = ['index'], \n",
    "    how='left', \n",
    "    rcpo_df_to_PNs_on = [('index', 'trsf_pole_nb')], \n",
    "    PNs_to_rcpo_df_on = ['index'], \n",
    "    addtnl_get_active_SNs_for_xfmrs_kwargs=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    Build rcpo_df normalized by the number of serial numbers in each outage\n",
    "\n",
    "    drop_xfmr_nSNs_eq_0:\n",
    "      It is possible for the number of serial numbers in an outage to be zero!\n",
    "      Premise numbers are always found, but the meter_premise database does not always \n",
    "        contain the premise numbers.\n",
    "      Dividing by zero will make all counts for such an entry equal to NaN or inf.\n",
    "      When drop_xfmr_nSNs_eq_0 is True, such entries will be removed.\n",
    "\n",
    "    NOTE: xfmr_SNs_col and xfmr_nSNs_col should both be strings, not tuples.\n",
    "          If column is MultiIndex, the level_0 value will be handled below.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    n_counts_col = xfmr_nSNs_col\n",
    "    list_col = xfmr_SNs_col\n",
    "    #-------------------------\n",
    "    # NOTE: MECPODf.add_outage_SNs_to_rcpo_df expects xfmr_SNs_col and xfmr_nSNs_col to be strings, not tuples\n",
    "    #       as it handles the level 0 values if they exist.  So, if tuples, use only highest level values (i.e., level 1)\n",
    "    assert(Utilities.is_object_one_of_types(list_col, [str, tuple]))\n",
    "    assert(Utilities.is_object_one_of_types(n_counts_col, [str, tuple]))\n",
    "    #-----\n",
    "    add_list_col_to_rcpo_df_func = add_xfmr_active_SNs_to_rcpo_df\n",
    "    add_list_col_to_rcpo_df_kwargs = dict(\n",
    "        trsf_pole_nbs_loc=trsf_pole_nbs_loc, \n",
    "        set_xfmr_nSNs=True, \n",
    "        include_active_xfmr_PNs=True, \n",
    "        df_mp_curr=df_mp_curr,\n",
    "        df_mp_hist=df_mp_hist, \n",
    "        time_infos_df=time_infos_df, \n",
    "        rcpo_df_to_time_infos_on=rcpo_df_to_time_infos_on, \n",
    "        time_infos_to_rcpo_df_on=time_infos_to_rcpo_df_on, \n",
    "        how=how, \n",
    "        rcpo_df_to_PNs_on=rcpo_df_to_PNs_on, \n",
    "        PNs_to_rcpo_df_on=PNs_to_rcpo_df_on, \n",
    "        addtnl_get_active_SNs_for_xfmrs_kwargs=addtnl_get_active_SNs_for_xfmrs_kwargs, \n",
    "        xfmr_SNs_col='_xfmr_SNs', \n",
    "        xfmr_nSNs_col='_xfmr_nSNs', \n",
    "        xfmr_PNs_col='_xfmr_PNs', \n",
    "        xfmr_nPNs_col='_xfmr_nPNs', \n",
    "    )\n",
    "    #-------------------------\n",
    "    other_col_tags_to_ignore = other_SNs_col_tags_to_ignore\n",
    "    drop_n_counts_eq_0 = drop_xfmr_nSNs_eq_0\n",
    "    new_level_0_val = new_level_0_val\n",
    "    remove_ignored_cols = remove_SNs_cols\n",
    "    #-------------------------\n",
    "    return MECPODf.build_rcpo_df_norm_by_list_counts(\n",
    "        rcpo_df_raw=rcpo_df_raw, \n",
    "        n_counts_col=n_counts_col, \n",
    "        list_col=list_col, \n",
    "        add_list_col_to_rcpo_df_func=add_list_col_to_rcpo_df_func, \n",
    "        add_list_col_to_rcpo_df_kwargs=add_list_col_to_rcpo_df_kwargs, \n",
    "        other_col_tags_to_ignore=other_col_tags_to_ignore, \n",
    "        drop_n_counts_eq_0=drop_n_counts_eq_0, \n",
    "        new_level_0_val=new_level_0_val, \n",
    "        remove_ignored_cols=remove_ignored_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb9d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dce56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def get_outg_time_infos_df(\n",
    "    rcpo_df, \n",
    "    outg_rec_nb_idx_lvl, \n",
    "    times_relative_to_off_ts_only=True, \n",
    "    td_for_left=None, \n",
    "    td_for_right=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a rcpo_df, with outg_rec_nbs stored in the index (located by outg_rec_nb_idx_lvl), construct the\n",
    "    time_infos_df whose indices are outg_rec_nbs and columns, ['t_min', 't_max'], give the search time windows\n",
    "    for each outage.\n",
    "    \n",
    "    This was designed, and is generally used, for finding the active meters for each outage.\n",
    "    \n",
    "    td_for_left / td_for_right: \n",
    "        datetime.timedelta objects used to define the left and right edges of the time window.\n",
    "        These can be positive or negative.\n",
    "        If input value is None, set equal to 0 (datetime.timedelta(days=0)), meaning the returned t_min values\n",
    "          will be the time the power went out, and the returned t_max values will either be the time the power went out\n",
    "          (times_relative_to_off_ts_only==True) or the time power was restored (times_relative_to_off_ts_only==False), \n",
    "          depending on the value of times_relative_to_off_ts_only.\n",
    "        The return t_min column is calculated by:\n",
    "            adding td_for_left to the time the outage began, \n",
    "                time_infos_df['t_min'] = pd.to_datetime(time_infos_df['DT_OFF_TS_FULL'])+td_for_left\n",
    "            Thus, for t_min to be before the outage, td_for_left must be negative.\n",
    "        The return t_max column is calculated by:\n",
    "            If times_relative_to_off_ts_only==True:  adding td_for_right to the time the outage began (DT_OFF_TS_FULL)\n",
    "            If times_relative_to_off_ts_only==False: adding td_for_right to the time the outage ended (DT_ON_TS)\n",
    "        RESTRICTIONS:    \n",
    "            When times_relative_to_off_ts_only==True, td_for_left must be less than td_for_right, so that t_min is less\n",
    "              than t_max.\n",
    "            However, when times_relative_to_off_ts_only==False, this is not strictly true (although, in most cases, should be true)\n",
    "            At the end of the day, all that really matters is that the application of td_for_left and td_for_right to\n",
    "              the off_ts/on_ts columns results in time_infos_df['t_min'] being less than time_infos_df['t_max']\n",
    "            ----------\n",
    "            When can td_for_left be greater than td_for_right?\n",
    "            -----\n",
    "                Take, for example, an outage that lasts an entire day.  \n",
    "                  Assume the outage begins off_ts=2022-10-05 12:00:00 and ends on_ts=2022-10-06 12:00:00.\n",
    "                  Further assume td_for_left = datetime.timedelta(hours=1) and td_for_right = datetime.timedelta(hours=-1), so\n",
    "                    clearly td_for_left > td_for_right.\n",
    "                  ==> left  = off_ts+td_for_left = 2022-10-05 13:00:00\n",
    "                      right = on_ts+td_for_right = 2022-10-06 11:00:00\n",
    "                  Thus, left<right, and the window makes sense!\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if td_for_left is None:\n",
    "        td_for_left = datetime.timedelta(days=0)\n",
    "    if td_for_right is None:\n",
    "        td_for_right = datetime.timedelta(days=0)\n",
    "    assert(isinstance(td_for_left, datetime.timedelta))\n",
    "    assert(isinstance(td_for_right, datetime.timedelta))\n",
    "    #-------------------------\n",
    "    dovs_outgs = DOVSOutages(                 \n",
    "        df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "        contstruct_df_args=None, \n",
    "        init_df_in_constructor=True, \n",
    "        build_sql_function=DOVSOutages_SQL.build_sql_outage, \n",
    "        build_sql_function_kwargs=dict(\n",
    "            outg_rec_nbs=rcpo_df.index.get_level_values(outg_rec_nb_idx_lvl).tolist(), \n",
    "            from_table_alias='DOV', \n",
    "            datetime_col='DT_OFF_TS_FULL', \n",
    "            cols_of_interest=[\n",
    "                'OUTG_REC_NB', \n",
    "                dict(field_desc=f\"DOV.DT_ON_TS - DOV.STEP_DRTN_NB/(60*24)\", \n",
    "                     alias='DT_OFF_TS_FULL', table_alias_prefix=None), \n",
    "                'DT_ON_TS'\n",
    "            ], \n",
    "            field_to_split='outg_rec_nbs'\n",
    "        )\n",
    "    )\n",
    "    #-------------------------\n",
    "    time_infos_df = dovs_outgs.df\n",
    "    time_infos_df = Utilities_df.convert_col_type(df=time_infos_df, column='OUTG_REC_NB', to_type=str)\n",
    "    time_infos_df=time_infos_df.set_index('OUTG_REC_NB')\n",
    "    #-------------------------\n",
    "    time_infos_df['t_min'] = pd.to_datetime(time_infos_df['DT_OFF_TS_FULL'])+td_for_left\n",
    "    if times_relative_to_off_ts_only:\n",
    "        assert(td_for_left<=td_for_right)\n",
    "        time_infos_df['t_max'] = pd.to_datetime(time_infos_df['DT_OFF_TS_FULL'])+td_for_right\n",
    "    else:\n",
    "        time_infos_df['t_max'] = pd.to_datetime(time_infos_df['DT_ON_TS'])+td_for_right\n",
    "        assert(all(time_infos_df['t_min'] <= time_infos_df['t_max']))\n",
    "    #-------------------------\n",
    "    time_infos_df = time_infos_df.drop(columns=['DT_OFF_TS_FULL', 'DT_ON_TS'])\n",
    "    #-------------------------\n",
    "    # Remove any duplicates from time_infos_df\n",
    "    if time_infos_df.index.nlevels==1:\n",
    "        tmp_col = Utilities.generate_random_string()\n",
    "        time_infos_df[tmp_col] = time_infos_df.index\n",
    "        time_infos_df = time_infos_df.drop_duplicates()\n",
    "        time_infos_df = time_infos_df.drop(columns=[tmp_col])\n",
    "    else:\n",
    "        tmp_cols = [Utilities.generate_random_string() for _ in range(time_infos_df.index.nlevels)]\n",
    "        for i_col, tmp_col in enumerate(tmp_cols):\n",
    "            time_infos_df[tmp_col] = time_infos_df.index.get_level_values(i_col)\n",
    "        time_infos_df = time_infos_df.drop_duplicates()\n",
    "        time_infos_df = time_infos_df.drop(columns=tmp_cols)\n",
    "    #-------------------------\n",
    "    return time_infos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852892a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ce456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def build_reason_counts_per_outage_from_csvs_NEW_v0(    \n",
    "    files_dir, \n",
    "    file_path_glob, \n",
    "    file_path_regex, \n",
    "    patterns_to_replace, \n",
    "    mp_df, \n",
    "    min_outg_td_window=datetime.timedelta(days=1),\n",
    "    max_outg_td_window=datetime.timedelta(days=30),\n",
    "    build_ede_typeid_to_reason_df=False, \n",
    "    batch_size=50, \n",
    "    cols_and_types_to_convert_dict=None, \n",
    "    to_numeric_errors='coerce', \n",
    "    assert_all_cols_equal=True, \n",
    "    include_normalize_by_nSNs=True, \n",
    "    inclue_zero_counts=True, \n",
    "    return_multiindex_outg_reason=False, \n",
    "    return_normalized_separately=False, \n",
    "    verbose=True, \n",
    "    n_update=1, \n",
    "    grp_by_cols='outg_rec_nb', \n",
    "    outg_rec_nb_col='outg_rec_nb',\n",
    "    trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "    addtnl_dropna_subset_cols=None, \n",
    "    is_no_outage=False, \n",
    "    prem_nb_col='aep_premise_nb', \n",
    "    serial_number_col='serialnumber', \n",
    "    include_prem_nbs=False, \n",
    "    trust_sql_grouping=True, \n",
    "    mp_df_cols = dict(\n",
    "        serial_number_col='mfr_devc_ser_nbr', \n",
    "        prem_nb_col='prem_nb', \n",
    "        outg_rec_nb_col='OUTG_REC_NB'\n",
    "    )\n",
    "):\n",
    "    r\"\"\"\n",
    "    Note: The larger the batch_size, the more memory that will be consumed during building\n",
    "\n",
    "    Any rows with NaNs in outg_rec_nb_col+addtnl_dropna_subset_cols will be dropped\n",
    "\n",
    "    #NOTE: Currently, only set up for the case return_normalized_separately==False\n",
    "    \"\"\"\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # TODO Allow method for reading DOVS from CSV (instead of only from SQL query)\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    #-------------------------\n",
    "    assert(not return_normalized_separately)\n",
    "    #-------------------------\n",
    "    assert(Utilities.is_object_one_of_types(grp_by_cols, [str, list, tuple]))\n",
    "    if isinstance(grp_by_cols, str):\n",
    "        grp_by_cols = [grp_by_cols]        \n",
    "    #-------------------------\n",
    "    # If including prem numbers, also include n_prem numbers\n",
    "    include_nprem_nbs = include_prem_nbs\n",
    "    #-------------------------\n",
    "    # normalize_by_nSNs_included needed when return_multiindex_outg_reason is True\n",
    "    if include_normalize_by_nSNs and not return_normalized_separately:\n",
    "        normalize_by_nSNs_included=True\n",
    "    else:\n",
    "        normalize_by_nSNs_included=False\n",
    "\n",
    "    are_dfs_wide_form = not return_multiindex_outg_reason\n",
    "    is_norm=False #TODO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    #-------------------------\n",
    "    paths = Utilities.find_all_paths(\n",
    "        base_dir=files_dir, \n",
    "        glob_pattern=file_path_glob, \n",
    "        regex_pattern=file_path_regex\n",
    "    )\n",
    "    if len(paths)==0:\n",
    "        print(f'No paths found in files_dir = {files_dir}')\n",
    "        return None\n",
    "    paths=natsorted(paths)\n",
    "    #-------------------------\n",
    "    rcpo_full = pd.DataFrame()\n",
    "    #-------------------------\n",
    "    batch_idxs = Utilities.get_batch_idx_pairs(len(paths), batch_size)\n",
    "    n_batches = len(batch_idxs)    \n",
    "    if verbose:\n",
    "        print(f'n_paths = {len(paths)}')\n",
    "        print(f'batch_size = {batch_size}')\n",
    "        print(f'n_batches = {n_batches}')    \n",
    "    #-------------------------\n",
    "    for i, batch_i in enumerate(batch_idxs):\n",
    "        start = time.time()\n",
    "        if verbose and (i+1)%n_update==0:\n",
    "            print(f'{i+1}/{n_batches}')\n",
    "        i_beg = batch_i[0]\n",
    "        i_end = batch_i[1]\n",
    "        #-----\n",
    "        # NOTE: make_all_columns_lowercase=True because...\n",
    "        #   EMR would return lowercase outg_rec_nb or outg_rec_nb_gpd_for_sql\n",
    "        #   Athena maintains the original case, and does not conver to lower case,\n",
    "        #     so it returns OUTG_REC_NB or OUTG_REC_NB_GPD_FOR_SQL\n",
    "        end_events_df_i = GenAn.read_df_from_csv_batch(\n",
    "            paths=paths[i_beg:i_end], \n",
    "            cols_and_types_to_convert_dict=cols_and_types_to_convert_dict, \n",
    "            to_numeric_errors=to_numeric_errors, \n",
    "            make_all_columns_lowercase=True, \n",
    "            assert_all_cols_equal=assert_all_cols_equal\n",
    "        )\n",
    "        print(f'0: {time.time()-start}')\n",
    "        #-------------------------\n",
    "        if end_events_df_i.shape[0]==0:\n",
    "            continue\n",
    "        #-------------------------\n",
    "        start = time.time()\n",
    "        for grp_by_col in grp_by_cols:\n",
    "            if f'{grp_by_col}_gpd_for_sql' in end_events_df_i.columns:\n",
    "                if grp_by_col in end_events_df_i.columns:\n",
    "                    if trust_sql_grouping:\n",
    "                        end_events_df_i = end_events_df_i.drop(columns=[grp_by_col])\n",
    "                    else:\n",
    "                        end_events_df_i = end_events_df_i.drop(columns=[f'{grp_by_col}_gpd_for_sql'])\n",
    "                end_events_df_i = end_events_df_i.rename(columns={f'{grp_by_col}_gpd_for_sql':grp_by_col})\n",
    "#             assert(grp_by_col in end_events_df_i.columns)\n",
    "        #-----\n",
    "        if not is_no_outage:\n",
    "            if f'{outg_rec_nb_col}_gpd_for_sql' in end_events_df_i.columns:\n",
    "                if outg_rec_nb_col in end_events_df_i.columns:\n",
    "                    if trust_sql_grouping:\n",
    "                        end_events_df_i = end_events_df_i.drop(columns=[outg_rec_nb_col])\n",
    "                    else:\n",
    "                        end_events_df_i = end_events_df_i.drop(columns=[f'{outg_rec_nb_col}_gpd_for_sql'])\n",
    "                end_events_df_i = end_events_df_i.rename(columns={f'{outg_rec_nb_col}_gpd_for_sql':outg_rec_nb_col})\n",
    "            assert(outg_rec_nb_col in end_events_df_i.columns)\n",
    "        #-------------------------\n",
    "        merge_on_mp = [mp_df_cols['serial_number_col'], mp_df_cols['prem_nb_col']]\n",
    "        merge_on_ede = [serial_number_col, prem_nb_col]\n",
    "        if not is_no_outage:\n",
    "            merge_on_mp.append(mp_df_cols['outg_rec_nb_col'])\n",
    "            merge_on_ede.append(outg_rec_nb_col)\n",
    "        if trsf_pole_nb_col in end_events_df_i.columns:\n",
    "            assert(trsf_pole_nb_col in mp_df.columns)\n",
    "            merge_on_mp.append(trsf_pole_nb_col)\n",
    "            merge_on_ede.append(trsf_pole_nb_col)\n",
    "        # Below ensures there is only one entry per 'meter' (meter here is defined by a unique grouping of merge_on_mp)\n",
    "        assert(not any(mp_df.groupby(merge_on_mp).size()>1))\n",
    "        #-----\n",
    "        # FROM WHAT I CAN TELL, the meters which are 'missing' from meter premise but are present in end events\n",
    "        #   are caused by meters which were removed or installed close to (a few days/weeks before) an outage.\n",
    "        # Therefore, although the meter may not have been present at the time of the outage (and therefore was exluced from\n",
    "        #   meter premise), it could have registered events leading up to/following the outage.\n",
    "        # e.g., if a meter was removed in the days before an outage, end events are still found for this meter in the days leading up to the outage\n",
    "        # e.g., if a meter was installed in the days after an outage, end events are still found for this meter in the days following an outage.\n",
    "        # How should these be handled?\n",
    "        # The simplest method, which I will implement for now, is to simply ONLY consider those meters which were present\n",
    "        #   at the time of the outage.  THEREFORE, the two DFs should be joined with an inner merge!\n",
    "        end_events_df_i = AMIEndEvents.merge_end_events_df_with_mp(\n",
    "            end_events_df=end_events_df_i, \n",
    "            df_mp=mp_df, \n",
    "            merge_on_ede=merge_on_ede, \n",
    "            merge_on_mp=merge_on_mp, \n",
    "            cols_to_include_mp=None, \n",
    "            drop_cols = None, \n",
    "            rename_cols=None, \n",
    "            how='inner', \n",
    "            inplace=True\n",
    "        )\n",
    "        for grp_by_col in grp_by_cols:\n",
    "            assert(grp_by_col in end_events_df_i.columns)\n",
    "        #-------------------------\n",
    "        dropna_subset_cols = grp_by_cols\n",
    "        if addtnl_dropna_subset_cols is not None:\n",
    "            dropna_subset_cols.extend(addtnl_dropna_subset_cols)\n",
    "        end_events_df_i = end_events_df_i.dropna(subset=dropna_subset_cols)\n",
    "        #-------------------------\n",
    "        end_events_df_i=Utilities_dt.strip_tz_info_and_convert_to_dt(\n",
    "            df=end_events_df_i, \n",
    "            time_col='valuesinterval', \n",
    "            placement_col='valuesinterval_local', \n",
    "            run_quick=True, \n",
    "            n_strip=6, \n",
    "            inplace=False\n",
    "        )\n",
    "        print(f'1: {time.time()-start}')\n",
    "        start = time.time()\n",
    "        #---------------------------------------------------------------------------\n",
    "        # If min_outg_td_window or max_outg_td_window is not None, enforce time restrictions around outages\n",
    "        if min_outg_td_window is not None or max_outg_td_window is not None:\n",
    "            #----------\n",
    "            if not is_no_outage:\n",
    "                dovs_outgs = DOVSOutages(                 \n",
    "                    df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "                    contstruct_df_args=None, \n",
    "                    init_df_in_constructor=True, \n",
    "                    build_sql_function=DOVSOutages_SQL.build_sql_outage, \n",
    "                    build_sql_function_kwargs=dict(\n",
    "                        outg_rec_nbs=end_events_df_i[outg_rec_nb_col].unique().tolist(), \n",
    "                        from_table_alias='DOV', \n",
    "                        datetime_col='DT_OFF_TS_FULL', \n",
    "                        cols_of_interest=[\n",
    "                            'OUTG_REC_NB', \n",
    "                            dict(field_desc=f\"DOV.DT_ON_TS - DOV.STEP_DRTN_NB/(60*24)\", \n",
    "                                 alias='DT_OFF_TS_FULL', table_alias_prefix=None)\n",
    "                        ], \n",
    "                        field_to_split='outg_rec_nbs'\n",
    "                    ),\n",
    "                )\n",
    "                outg_dt_off_df = dovs_outgs.df\n",
    "                outg_dt_off_df = Utilities_df.convert_col_type(df=outg_dt_off_df, column='OUTG_REC_NB', to_type=str)\n",
    "                outg_dt_off_df=outg_dt_off_df.set_index('OUTG_REC_NB')\n",
    "                outg_dt_off_series = outg_dt_off_df['DT_OFF_TS_FULL']\n",
    "\n",
    "                end_events_df_i = AMIEndEvents.enforce_end_events_within_interval_of_outage(\n",
    "                    end_events_df=end_events_df_i, \n",
    "                    outg_times_series=outg_dt_off_series, \n",
    "                    min_timedelta=min_outg_td_window, \n",
    "                    max_timedelta=max_outg_td_window, \n",
    "                    outg_rec_nb_col = outg_rec_nb_col, \n",
    "                    datetime_col='valuesinterval_local', \n",
    "                    assert_one_time_per_group=True\n",
    "                )\n",
    "            else:\n",
    "                no_outg_time_infos_df = MECPOAn.get_bsln_time_interval_infos_df_from_summary_files(\n",
    "                    summary_paths=[AMIEndEvents.find_summary_file_from_csv(x) for x in paths[i_beg:i_end]], \n",
    "                    output_prem_nbs_col='prem_nbs', \n",
    "                    output_t_min_col='t_min', \n",
    "                    output_t_max_col='DT_OFF_TS_FULL', \n",
    "                    make_addtnl_groupby_idx=True, \n",
    "                    include_summary_paths=False\n",
    "                )\n",
    "                assert(no_outg_time_infos_df.index.name==trsf_pole_nb_col)\n",
    "                assert(end_events_df_i[trsf_pole_nb_col].dtype==no_outg_time_infos_df.index.dtype)\n",
    "                no_outg_time_infos_series = no_outg_time_infos_df['DT_OFF_TS_FULL']    \n",
    "\n",
    "                end_events_df_i = AMIEndEvents.enforce_end_events_within_interval_of_outage(\n",
    "                    end_events_df=end_events_df_i, \n",
    "                    outg_times_series=no_outg_time_infos_series, \n",
    "                    min_timedelta=min_outg_td_window, \n",
    "                    max_timedelta=max_outg_td_window, \n",
    "                    outg_rec_nb_col = trsf_pole_nb_col, \n",
    "                    datetime_col='valuesinterval_local', \n",
    "                    assert_one_time_per_group=False\n",
    "                )\n",
    "        print(f'2: {time.time()-start}')\n",
    "        #---------------------------------------------------------------------------\n",
    "        # After enforcing events within specific time frame, it is possible end_events_df_i is empty.\n",
    "        # If so, continue\n",
    "        if end_events_df_i.shape[0]==0:\n",
    "            continue\n",
    "        #-------------------------\n",
    "#         end_events_df_i = AMIEndEvents.extract_reboot_counts_from_reasons_in_df(end_events_df_i)\n",
    "#         end_events_df_i = AMIEndEvents.extract_fail_reasons_from_reasons_in_df(end_events_df_i)\n",
    "        #-------------------------\n",
    "        start = time.time()\n",
    "        end_events_df_i = AMIEndEvents.reduce_end_event_reasons_in_df(\n",
    "            df=end_events_df_i, \n",
    "#             patterns_to_replace=patterns_to_replace\n",
    "        )\n",
    "        print(f'3: {time.time()-start}')\n",
    "        #-------------------------\n",
    "        start = time.time()\n",
    "        if build_ede_typeid_to_reason_df:\n",
    "            ede_typeid_to_reason_df_i = AMIEndEvents.build_ede_typeid_to_reason_df(\n",
    "                end_events_df=end_events_df_i, \n",
    "                reason_col='reason', \n",
    "                ede_typeid_col='enddeviceeventtypeid'\n",
    "            )\n",
    "            if i==0:\n",
    "                ede_typeid_to_reason_df = ede_typeid_to_reason_df_i.copy()\n",
    "            else:\n",
    "                ede_typeid_to_reason_df = AMIEndEvents.combine_two_ede_typeid_to_reason_dfs(\n",
    "                    ede_typeid_to_reason_df1=ede_typeid_to_reason_df, \n",
    "                    ede_typeid_to_reason_df2=ede_typeid_to_reason_df_i,\n",
    "                    sort=True\n",
    "                )\n",
    "        print(f'4: {time.time()-start}')\n",
    "        #-------------------------\n",
    "#         ordinal_encoder = OrdinalEncoder()\n",
    "#         end_events_df_i['reason_enc'] = ordinal_encoder.fit_transform(end_events_df_i[['reason']])\n",
    "        #-------------------------\n",
    "        start = time.time()\n",
    "        rcpo_i = AMIEndEvents.get_reason_counts_per_group(\n",
    "            end_events_df = end_events_df_i, \n",
    "            group_cols=grp_by_cols, \n",
    "            group_freq=None, \n",
    "            serial_number_col='serialnumber', \n",
    "            reason_col='reason', \n",
    "            include_normalize_by_nSNs=include_normalize_by_nSNs, \n",
    "            inclue_zero_counts=inclue_zero_counts,\n",
    "            possible_reasons=None, \n",
    "            include_nSNs=True, \n",
    "            include_SNs=True, \n",
    "            prem_nb_col=prem_nb_col, \n",
    "            include_nprem_nbs=include_nprem_nbs,\n",
    "            include_prem_nbs=include_prem_nbs,   \n",
    "            return_form = dict(return_multiindex_outg_reason = return_multiindex_outg_reason, \n",
    "                               return_normalized_separately  = return_normalized_separately)\n",
    "        )\n",
    "        print(f'5: {time.time()-start}')\n",
    "        #-------------------------\n",
    "        if rcpo_i.shape[0]==0:\n",
    "            continue\n",
    "        #-------------------------\n",
    "        start = time.time()\n",
    "        # Include or below in case i=0 rcpo_i comes back empty, causing continue to be called above\n",
    "        if i==0 or rcpo_full.shape[0]==0:\n",
    "            rcpo_full = rcpo_i.copy()\n",
    "        else:\n",
    "            list_cols=['_SNs']\n",
    "            list_counts_cols=['_nSNs']\n",
    "            w_col = '_nSNs'\n",
    "            if include_prem_nbs:\n",
    "                list_cols.append('_prem_nbs')\n",
    "                list_counts_cols.append('_nprem_nbs')\n",
    "            #-----\n",
    "            rcpo_full = AMIEndEvents.combine_two_reason_counts_per_outage_dfs(\n",
    "                rcpo_df_1=rcpo_full, \n",
    "                rcpo_df_2=rcpo_i, \n",
    "                are_dfs_wide_form=are_dfs_wide_form, \n",
    "                normalize_by_nSNs_included=normalize_by_nSNs_included, \n",
    "                is_norm=is_norm, \n",
    "                list_cols=list_cols, \n",
    "                list_counts_cols=list_counts_cols,\n",
    "                w_col = w_col, \n",
    "                level_0_raw_col = 'counts', \n",
    "                level_0_nrm_col = 'counts_norm', \n",
    "                convert_rcpo_wide_to_long_col_args=None\n",
    "            )\n",
    "        print(f'6: {time.time()-start}')\n",
    "    if not build_ede_typeid_to_reason_df:\n",
    "        return rcpo_full\n",
    "    else:\n",
    "        return rcpo_full, ede_typeid_to_reason_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966cb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f866058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cd646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def check_end_events_merge_with_mp(\n",
    "    ede_file_paths,\n",
    "    mp_df, \n",
    "    threshold_pct=1.0, \n",
    "    outg_rec_nb_col='outg_rec_nb',\n",
    "    trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "    prem_nb_col='aep_premise_nb', \n",
    "    serial_number_col='serialnumber',\n",
    "    mp_df_cols = dict(\n",
    "        serial_number_col='mfr_devc_ser_nbr', \n",
    "        prem_nb_col='prem_nb', \n",
    "        trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "        outg_rec_nb_col='OUTG_REC_NB'\n",
    "    ), \n",
    "    is_no_outage=False, \n",
    "    assert_all_cols_equal=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    When building the RCPX dfs, oftentimes the end events data need to be joined with meter premise data (e.g., when one\n",
    "      wants to group by transformer, this information typically needs to be brought in from MP).\n",
    "    The meter premise data are supplied by the user.  Mistakes can be made, so this function serves to check whether the \n",
    "      alignment between end events and meter premise is as expected.\n",
    "      \n",
    "    NOTE: This also runs some other checks on the end events collection as a whole (e.g., if assert_all_cols_equal, it\n",
    "            checks the assertion).\n",
    "          Thus, some of the operations can be replaced from the for loop in build_reason_counts_per_outage_from_csv\n",
    "            (This will help performance, but I don't think gains will be significant)\n",
    "            \n",
    "    In order to reduce some of the load on build_reason_counts_per_outage_from_csv, this function returns merge_on_ede and merge_on_mp\n",
    "      \n",
    "    End events and meter premise are merged on serial number and premise number.\n",
    "      If data are for outages, also merged on outg_rec_nb.\n",
    "      If trsf_pole_nb is present, also merged on it.\n",
    "      \n",
    "    NOTE:  I have found there are meters which are 'missing' from meter premise but are present in end events.\n",
    "           These seem to be caused by meters which were removed or installed close to (a few days/weeks before) an outage.\n",
    "           Therefore, although the meter may not have been present at the time of the outage (and therefore was exluced from\n",
    "             meter premise), it could have registered events leading up to/following the outage.\n",
    "           e.g., if a meter was removed in the days before an outage, end events are still found for this meter in the days leading up to the outage\n",
    "           e.g., if a meter was installed in the days after an outage, end events are still found for this meter in the days following an outage.\n",
    "           How should these be handled?\n",
    "           The simplest method, which I will implement for now, is to simply ONLY consider those meters which were present\n",
    "             at the time of the outage.  THEREFORE, the two DFs should be joined with an inner merge! \n",
    "             \n",
    "    NOTE: Utilities_df.make_all_column_names_lowercase utilized when reading in end events dfs because...\n",
    "      EMR would return lowercase outg_rec_nb or outg_rec_nb_gpd_for_sql\n",
    "      Athena maintains the original case, and does not conver to lower case,\n",
    "        so it returns OUTG_REC_NB or OUTG_REC_NB_GPD_FOR_SQL\n",
    "             \n",
    "    threshold_pct:\n",
    "        If the percentage of entries present in end events but absent in meter premise exceeds this value, the program will crash.\n",
    "        This is to safeguard the user from inputting incorrect meter premise data.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    merge_on_mp = [mp_df_cols['serial_number_col'], mp_df_cols['prem_nb_col']]\n",
    "    merge_on_ede = [serial_number_col, prem_nb_col]\n",
    "    if not is_no_outage:\n",
    "        merge_on_mp.append(mp_df_cols['outg_rec_nb_col'])\n",
    "        merge_on_ede.append(outg_rec_nb_col)\n",
    "    #-------------------------\n",
    "    # Grab the first end events df and check for trsf_pole_nb\n",
    "    #  Actually, just need to grab first entry of first df\n",
    "    end_events_df_0 = GenAn.read_df_from_csv(\n",
    "        read_path=ede_file_paths[0], \n",
    "        cols_and_types_to_convert_dict=None, \n",
    "        to_numeric_errors='coerce', \n",
    "        drop_na_rows_when_exception=True, \n",
    "        drop_unnamed0_col=True, \n",
    "        pd_read_csv_kwargs = dict(nrows=1)\n",
    "    )\n",
    "    end_events_df_0 = Utilities_df.make_all_column_names_lowercase(end_events_df_0)\n",
    "    #-----\n",
    "    if trsf_pole_nb_col in end_events_df_0.columns:\n",
    "        assert(mp_df_cols['trsf_pole_nb_col'] in mp_df.columns)\n",
    "        merge_on_mp.append(mp_df_cols['trsf_pole_nb_col'])\n",
    "        merge_on_ede.append(trsf_pole_nb_col)\n",
    "    #-------------------------\n",
    "    # Below ensures there is only one entry per 'meter' (meter here is defined by a unique grouping of merge_on_mp)\n",
    "    assert(not any(mp_df.groupby(merge_on_mp).size()>1))\n",
    "    #--------------------------------------------------\n",
    "    # Build DF with all end events to be checked with mp_df\n",
    "    # The only columns which matter for the check are those in merge_on_ede, so those are the only kept.\n",
    "    #   This keeps the overall size of the end_events_df smaller, allowing all to be loaded at once in most cases.\n",
    "    dfs = []\n",
    "    for path in ede_file_paths:\n",
    "        df_i = GenAn.read_df_from_csv(\n",
    "            read_path=path, \n",
    "            cols_and_types_to_convert_dict=None, \n",
    "            to_numeric_errors='coerce', \n",
    "            drop_na_rows_when_exception=True, \n",
    "            drop_unnamed0_col=True\n",
    "        )\n",
    "        if df_i.shape[0]==0:\n",
    "            continue\n",
    "        #-----\n",
    "        df_i = Utilities_df.make_all_column_names_lowercase(df_i) \n",
    "        #-----\n",
    "        dfs.append(df_i[merge_on_ede])\n",
    "    #-------------------------                \n",
    "    df_cols = Utilities_df.get_shared_columns(dfs, maintain_df0_order=True)\n",
    "    for i in range(len(dfs)):\n",
    "        if assert_all_cols_equal:\n",
    "            # In order to account for case where columns are the same but in different order\n",
    "            # one must compare the length of dfs[i].columns to that of df_cols (found by utilizing\n",
    "            # the Utilities_df.get_shared_columns(dfs) functionality)\n",
    "            assert(dfs[i].shape[1]==len(df_cols))\n",
    "        dfs[i] = dfs[i][df_cols]\n",
    "    end_events_df = pd.concat(dfs)\n",
    "    #-------------------------\n",
    "    # Alter dtypes in mp_df if needed for proper merging\n",
    "    end_events_df, mp_df = Utilities_df.make_df_col_dtypes_equal(\n",
    "        df_1              = end_events_df, \n",
    "        col_1             = merge_on_ede, \n",
    "        df_2              = mp_df, \n",
    "        col_2             = merge_on_mp, \n",
    "        allow_reverse_set = False, \n",
    "        assert_success    = True\n",
    "    )\n",
    "    #-------------------------\n",
    "    # Now, compare the unique combinations of merge_on_ede in end_events_df to those of merge_on_mp in mp_df\n",
    "    gps_ede = list(end_events_df.groupby(merge_on_ede).groups.keys())\n",
    "    gps_mp  = list(mp_df.groupby(merge_on_mp).groups.keys())\n",
    "    # Finally, find the percent of meters in end events missing from meter premise (where a meter is defined as a unique\n",
    "    #   combination of merge_on_ede values (e.g., serial number and premise number))\n",
    "    n_missing = len(set(gps_ede).difference(set(gps_mp)))\n",
    "    pct_missing = 100*(n_missing/len(gps_ede))\n",
    "    print(f'% meters in end events missing from mp_df: {pct_missing}')\n",
    "    if pct_missing>threshold_pct:\n",
    "        assert(0)\n",
    "    #-------------------------\n",
    "    return mp_df, merge_on_ede, merge_on_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def perform_build_RCPX_from_csvs_prereqs_v0(\n",
    "    files_dir, \n",
    "    file_path_glob, \n",
    "    file_path_regex,\n",
    "    mp_df, \n",
    "    ede_mp_mismatch_threshold_pct=1.0, \n",
    "    grp_by_cols='outg_rec_nb', \n",
    "    outg_rec_nb_col='outg_rec_nb',\n",
    "    trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "    prem_nb_col='aep_premise_nb', \n",
    "    serial_number_col='serialnumber',\n",
    "    mp_df_cols = dict(\n",
    "        serial_number_col='mfr_devc_ser_nbr', \n",
    "        prem_nb_col='prem_nb', \n",
    "        trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "        outg_rec_nb_col='OUTG_REC_NB'\n",
    "    ), \n",
    "    is_no_outage=False, \n",
    "    assert_all_cols_equal=True, \n",
    "    trust_sql_grouping=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Prepares for running build_reason_counts_per_outage_from_csvs.\n",
    "    Many of these items were done for each iteration of the main for loop in build_reason_counts_per_outage_from_csvs, which\n",
    "      was really unnecessary.  \n",
    "      This will improve performance (although, likely not much as none of the operations are super heavy) and simplify the code.\n",
    "    \n",
    "    1. Adjust grp_by_cols and outg_rec_nb_col to handle cases where _gpd_for_sql appendix was added during data acquisition.\n",
    "       If there is an instance where, e.g., outg_rec_nb_col and f'{outg_rec_nb_col}_gpd_for_sql' are both present, it settles\n",
    "         the discrepancy (according to the trust_sql_grouping parameter) and compiles a list of columns which will need \n",
    "         to be dropped.\n",
    "    2. Determine merge_on_ede and merge_on_mp columns (done within check_end_events_merge_with_mp).\n",
    "    3. Checks that the user supplied mp_df aligns well with the end events data.\n",
    "    4. If assert_all_cols_equal==True, enforce the assertion.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(Utilities.is_object_one_of_types(grp_by_cols, [str, list, tuple]))\n",
    "    if isinstance(grp_by_cols, str):\n",
    "        grp_by_cols = [grp_by_cols]    \n",
    "    #-------------------------\n",
    "    paths = Utilities.find_all_paths(\n",
    "        base_dir=files_dir, \n",
    "        glob_pattern=file_path_glob, \n",
    "        regex_pattern=file_path_regex\n",
    "    )\n",
    "    if len(paths)==0:\n",
    "        print(f'No paths found in files_dir = {files_dir}')\n",
    "        return None\n",
    "    paths=natsorted(paths)\n",
    "    #-------------------------\n",
    "    #--------------------------------------------------\n",
    "    # Grab first row from each CSV file to be used to check columns\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df_i = GenAn.read_df_from_csv(\n",
    "            read_path=path, \n",
    "            cols_and_types_to_convert_dict=None, \n",
    "            to_numeric_errors='coerce', \n",
    "            drop_na_rows_when_exception=True, \n",
    "            drop_unnamed0_col=True, \n",
    "            pd_read_csv_kwargs = dict(nrows=1)\n",
    "        )\n",
    "        if df_i.shape[0]==0:\n",
    "            continue\n",
    "        #-----\n",
    "        # NOTE: make_all_columns_lowercase because...\n",
    "        #   EMR would return lowercase outg_rec_nb or outg_rec_nb_gpd_for_sql\n",
    "        #   Athena maintains the original case, and does not conver to lower case,\n",
    "        #     so it returns OUTG_REC_NB or OUTG_REC_NB_GPD_FOR_SQL\n",
    "        df_i = Utilities_df.make_all_column_names_lowercase(df_i) \n",
    "        #-----\n",
    "        dfs.append(df_i)\n",
    "    #-------------------------\n",
    "    if len(dfs)==0:\n",
    "        print(f'No DFs found in files_dir={files_dir}')\n",
    "        assert(0)\n",
    "    #-------------------------                \n",
    "    df_cols = Utilities_df.get_shared_columns(dfs, maintain_df0_order=True)\n",
    "    for i in range(len(dfs)):\n",
    "        if assert_all_cols_equal:\n",
    "            # In order to account for case where columns are the same but in different order\n",
    "            # one must compare the length of dfs[i].columns to that of df_cols (found by utilizing\n",
    "            # the Utilities_df.get_shared_columns(dfs) functionality)\n",
    "            assert(dfs[i].shape[1]==len(df_cols))\n",
    "        dfs[i] = dfs[i][df_cols]\n",
    "    end_events_df = pd.concat(dfs)\n",
    "    #--------------------------------------------------\n",
    "    #--------------------------------------------------\n",
    "    # The columns in grp_by_cols may sometimes be appended with _gpd_for_sql (e.g., typically one will see outg_rec_nb_gpd_for_sql \n",
    "    #   not outg_rec_nb in the raw CSV files).\n",
    "    # The _gpd_for_sql was appended during data acquisition.\n",
    "    # However, the user typically does not remember this, and will usually input, e.g., outg_rec_nb_col='outg_rec_nb'\n",
    "    #   Such a scenario will obviously lead to an error, as 'outg_rec_nb' is not found in the columns\n",
    "    # The below methods serve to remedy that issue.\n",
    "    # NOTE: In the case that column_i and column_i_gpd_for_sql are for whatever reason BOTH found in the DF, the parameter\n",
    "    #       trust_sql_grouping directs the code on which to use (the other will be dropped)\n",
    "    cols_to_drop = []\n",
    "    #-------------------------\n",
    "    grp_by_cols_final = []\n",
    "    for grp_by_col in grp_by_cols:\n",
    "        if f'{grp_by_col}_gpd_for_sql' in end_events_df.columns:\n",
    "            if grp_by_col in end_events_df.columns:\n",
    "                # Both f'{grp_by_col}_gpd_for_sql' and grp_by_col.\n",
    "                # One must be kept (by inserting into grp_by_cols_final), and one dropped (by inserting into cols_to_drop)\n",
    "                if trust_sql_grouping:\n",
    "                    grp_by_cols_final.append(f'{grp_by_col}_gpd_for_sql')\n",
    "                    cols_to_drop.append(grp_by_col)\n",
    "                else:\n",
    "                    grp_by_cols_final.append(grp_by_col)\n",
    "                    cols_to_drop.append(f'{grp_by_col}_gpd_for_sql')\n",
    "            else:\n",
    "                # Only f'{grp_by_col}_gpd_for_sql', not grp_by_col, so no need to drop anything\n",
    "                grp_by_cols_final.append(f'{grp_by_col}_gpd_for_sql')\n",
    "        else:\n",
    "            grp_by_cols_final.append(grp_by_col)\n",
    "    grp_by_cols = grp_by_cols_final\n",
    "    #-------------------------\n",
    "    if not is_no_outage:\n",
    "        if f'{outg_rec_nb_col}_gpd_for_sql' in end_events_df.columns:\n",
    "            if outg_rec_nb_col in end_events_df.columns:\n",
    "                # Both f'{outg_rec_nb_col}_gpd_for_sql' and outg_rec_nb_col.\n",
    "                # One must be kept (by setting equal to outg_rec_nb_col), and one dropped (by inserting into cols_to_drop)\n",
    "                if trust_sql_grouping:\n",
    "                    cols_to_drop.append(outg_rec_nb_col)\n",
    "                    outg_rec_nb_col = f'{outg_rec_nb_col}_gpd_for_sql'\n",
    "                else:\n",
    "                    cols_to_drop.append(f'{outg_rec_nb_col}_gpd_for_sql')\n",
    "                    outg_rec_nb_col = outg_rec_nb_col\n",
    "            else:\n",
    "                # Only f'{outg_rec_nb_col}_gpd_for_sql', not outg_rec_nb_col, so no need to drop anything\n",
    "                outg_rec_nb_col = f'{outg_rec_nb_col}_gpd_for_sql'\n",
    "        assert(outg_rec_nb_col in end_events_df.columns)\n",
    "    #-------------------------\n",
    "    if f'{trsf_pole_nb_col}_gpd_for_sql' in end_events_df.columns:\n",
    "        if trsf_pole_nb_col in end_events_df.columns:\n",
    "            # Both f'{trsf_pole_nb_col}_gpd_for_sql' and trsf_pole_nb_col.\n",
    "            # One must be kept (by setting equal to trsf_pole_nb_col), and one dropped (by inserting into cols_to_drop)\n",
    "            if trust_sql_grouping:\n",
    "                cols_to_drop.append(trsf_pole_nb_col)\n",
    "                trsf_pole_nb_col = f'{trsf_pole_nb_col}_gpd_for_sql'\n",
    "            else:\n",
    "                cols_to_drop.append(f'{trsf_pole_nb_col}_gpd_for_sql')\n",
    "                trsf_pole_nb_col = trsf_pole_nb_col\n",
    "        else:\n",
    "            # Only f'{trsf_pole_nb_col}_gpd_for_sql', not trsf_pole_nb_col, so no need to drop anything\n",
    "            trsf_pole_nb_col = f'{trsf_pole_nb_col}_gpd_for_sql'        \n",
    "    #-------------------------\n",
    "    # NOTE: Many times outg_rec_nb_col is in grp_by_cols, hence the need for the set operation (to eliminate duplicates!)\n",
    "    cols_to_drop = list(set(cols_to_drop))\n",
    "    #--------------------------------------------------\n",
    "    #--------------------------------------------------\n",
    "    mp_df, merge_on_ede, merge_on_mp = check_end_events_merge_with_mp(\n",
    "        ede_file_paths=paths,\n",
    "        mp_df=mp_df, \n",
    "        threshold_pct=ede_mp_mismatch_threshold_pct, \n",
    "        outg_rec_nb_col=outg_rec_nb_col,\n",
    "        trsf_pole_nb_col=trsf_pole_nb_col, \n",
    "        prem_nb_col=prem_nb_col, \n",
    "        serial_number_col=serial_number_col,\n",
    "        mp_df_cols = mp_df_cols, \n",
    "        is_no_outage=is_no_outage, \n",
    "        assert_all_cols_equal=assert_all_cols_equal\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    #--------------------------------------------------\n",
    "    return_dict = dict(\n",
    "        paths=paths, \n",
    "        grp_by_cols=grp_by_cols, \n",
    "        outg_rec_nb_col=outg_rec_nb_col, \n",
    "        trsf_pole_nb_col=trsf_pole_nb_col, \n",
    "        cols_to_drop=cols_to_drop, \n",
    "        mp_df=mp_df, \n",
    "        merge_on_ede=merge_on_ede, \n",
    "        merge_on_mp=merge_on_mp\n",
    "    )\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def find_all_cols_with_gpd_for_sql_appendix(df):\n",
    "    r\"\"\"\n",
    "    The _gpd_for_sql appendix is added in the end events data acquisition stage, but is typically no longer needed after.\n",
    "    NOTE: Method is case insensitive!! (i.e., col_gpd_for_sql --> col, col_GPD_FOR_SQL --> col, etc.)\n",
    "    Returns a dict with key values equal to the found columns containing _gpd_for_sql and values w\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    found_cols_dict = {}\n",
    "    for col in df.columns.tolist():\n",
    "        if col.lower().endswith('_gpd_for_sql'):\n",
    "            assert(col not in found_cols_dict.keys())\n",
    "            found_cols_dict[col] = col[:-12]\n",
    "    return found_cols_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea76f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def identify_ede_cols_of_interest_to_update_andor_drop(\n",
    "    end_events_df,  \n",
    "    grp_by_cols        = 'outg_rec_nb', \n",
    "    outg_rec_nb_col    = 'outg_rec_nb',\n",
    "    trsf_pole_nb_col   = 'trsf_pole_nb', \n",
    "    prem_nb_col        = 'aep_premise_nb', \n",
    "    serial_number_col  = 'serialnumber',\n",
    "    trust_sql_grouping = True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Find any entries in ede_cols_of_interest (see below) which contain _gpd_for_sql appendix and update accordingly.\n",
    "      The columns in end events may sometimes be appended with _gpd_for_sql (e.g., typically one will see outg_rec_nb_gpd_for_sql \n",
    "        not outg_rec_nb in the raw CSV files).\n",
    "      The _gpd_for_sql was appended during data acquisition.\n",
    "      However, the user typically does not remember this, and will usually input, e.g., outg_rec_nb_col='outg_rec_nb'\n",
    "        Such a scenario will obviously lead to an error, as 'outg_rec_nb' is not found in the columns\n",
    "      The below methods serve to remedy that issue.\n",
    "      \n",
    "    ede_cols_of_interest consist of grp_by_cols+[outg_rec_nb_col, trsf_pole_nb_col, prem_nb_col, serial_number_col]\n",
    "      \n",
    "    NOTE: In the case that column_i and column_i_gpd_for_sql are for whatever reason BOTH found in the DF, the parameter\n",
    "        trust_sql_grouping directs the code on which to use (the other will be dropped)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(Utilities.is_object_one_of_types(grp_by_cols, [str, list, tuple]))\n",
    "    if isinstance(grp_by_cols, str):\n",
    "        grp_by_cols = [grp_by_cols]    \n",
    "    #-------------------------\n",
    "    # Below called ede_gpd_coi_dict because the columns of interest are grouped by their input parameter in build_reason_counts_per_outage_from_csvs.\n",
    "    #   e.g., key value grp_by_cols should contain a list of columns (strings)\n",
    "    #         key value outg_rec_nb_col contains a single string for the outg_rec_nb_col\n",
    "    # The values, coi, can stand for column of interest (e.g., outg_rec_nb_col) or columns of interest (e.g., grp_by_cols)\n",
    "    ede_gpd_coi_dict = dict(\n",
    "        grp_by_cols=grp_by_cols, \n",
    "        outg_rec_nb_col=outg_rec_nb_col, \n",
    "        trsf_pole_nb_col=trsf_pole_nb_col, \n",
    "        prem_nb_col=prem_nb_col, \n",
    "        serial_number_col=serial_number_col\n",
    "    )\n",
    "    #-----\n",
    "    # Below, ede_cols_of_interest is essentially the flattened values from ede_gpd_coi_dict with duplicates removed\n",
    "    ede_cols_of_interest = []\n",
    "    for ede_coi_grp, coi in ede_gpd_coi_dict.items():\n",
    "        assert(Utilities.is_object_one_of_types(coi, [str,list,tuple]))\n",
    "        if isinstance(coi, str):\n",
    "            ede_cols_of_interest.append(coi)\n",
    "        else:\n",
    "            ede_cols_of_interest.extend(coi)\n",
    "    # NOTE: grp_by_cols can contain the others (e.g., outg_rec_nb_col), so set operation needed to removed duplicates\n",
    "    ede_cols_of_interest = list(set(ede_cols_of_interest))\n",
    "    #--------------------------------------------------\n",
    "    # Below, found_cols_w_gpd_for_sql_appendix has keys equal to any columns found containing _gpd_for_sql appendix and\n",
    "    #   value equal to the column without the appendix.\n",
    "    # found_cols_w_gpd_for_sql_appendix_inv is the inverse (i.e., keys and values switched)\n",
    "    found_cols_w_gpd_for_sql_appendix = find_all_cols_with_gpd_for_sql_appendix(end_events_df)\n",
    "    found_cols_w_gpd_for_sql_appendix_inv = Utilities.invert_dict(found_cols_w_gpd_for_sql_appendix)\n",
    "    #--------------------------------------------------\n",
    "    # NOTE: No harm below if ede_coi not actually found in end_events_df (e.g., when running over baseline data, outg_rec_nb_col \n",
    "    #       will not be found), as updates are only made to those whose values change (and can only change if found)\n",
    "    ede_cols_of_interest_updates = dict()\n",
    "    cols_to_drop = []\n",
    "    #-----\n",
    "    for ede_coi in ede_cols_of_interest:\n",
    "        assert(ede_coi not in ede_cols_of_interest_updates)\n",
    "        #-----\n",
    "        if ede_coi in found_cols_w_gpd_for_sql_appendix_inv.keys():\n",
    "            # f'{ede_coi}_gpd_for_sql' found in end_events_df, so an updadte to ede_coi in ede_cols_of_interest \n",
    "            #   and/or column drop will be needed\n",
    "            if ede_coi in end_events_df.columns:\n",
    "                # Both f'{ede_coi}_gpd_for_sql' and ede_coi were found in end_events_df!\n",
    "                # One must be kept (by inserting into grp_by_cols_final), and one dropped (by inserting into cols_to_drop)\n",
    "                if trust_sql_grouping:\n",
    "                    # Keep/update f'{ede_coi}_gpd_for_sql', and drop ede_coi\n",
    "                    ede_cols_of_interest_updates[ede_coi] = found_cols_w_gpd_for_sql_appendix_inv[ede_coi]\n",
    "                    cols_to_drop.append(ede_coi)\n",
    "                else:\n",
    "                    # Keep ede_coi, drop f'{ede_coi}_gpd_for_sql'\n",
    "                    ede_cols_of_interest_updates[ede_coi] = ede_coi\n",
    "                    cols_to_drop.append(found_cols_w_gpd_for_sql_appendix_inv[ede_coi])\n",
    "            else:\n",
    "                # Only f'{ede_coi}_gpd_for_sql' found in end_events_df, not grp_by_col, so no need to drop anything\n",
    "                ede_cols_of_interest_updates[ede_coi] = found_cols_w_gpd_for_sql_appendix_inv[ede_coi]\n",
    "        else:\n",
    "            ede_cols_of_interest_updates[ede_coi] = ede_coi\n",
    "    #-----\n",
    "    # Make sure no repeats in cols_to_drop\n",
    "    cols_to_drop = list(set(cols_to_drop))\n",
    "    #--------------------------------------------------\n",
    "    # Build ede_gpd_coi_dict_updates, which has the same structure as ede_gpd_coi_dict (see above)\n",
    "    ede_gpd_coi_dict_updates = dict()\n",
    "    for ede_coi_grp, coi in ede_gpd_coi_dict.items():\n",
    "        assert(ede_coi_grp not in ede_gpd_coi_dict_updates.keys())\n",
    "        assert(Utilities.is_object_one_of_types(coi, [str,list,tuple]))\n",
    "        if isinstance(coi, str):\n",
    "            assert(coi in ede_cols_of_interest_updates.keys())\n",
    "            ede_gpd_coi_dict_updates[ede_coi_grp] = ede_cols_of_interest_updates[coi]\n",
    "        else:\n",
    "            ede_gpd_coi_dict_updates[ede_coi_grp] = []\n",
    "            for coi_i in coi:\n",
    "                assert(coi_i in ede_cols_of_interest_updates.keys())\n",
    "                ede_gpd_coi_dict_updates[ede_coi_grp].append(ede_cols_of_interest_updates[coi_i])\n",
    "    assert(len(set(ede_gpd_coi_dict.keys()).symmetric_difference(set(ede_gpd_coi_dict_updates.keys())))==0)\n",
    "    #-----\n",
    "    # Sanity check\n",
    "    assert(len(set(ede_gpd_coi_dict.keys()).symmetric_difference(set(ede_gpd_coi_dict_updates.keys())))==0)\n",
    "    for ede_coi_grp, coi in ede_gpd_coi_dict.items():\n",
    "        if Utilities.is_object_one_of_types(coi, [list,tuple]):\n",
    "            assert(len(coi)==len(ede_gpd_coi_dict_updates[ede_coi_grp]))\n",
    "    #--------------------------------------------------\n",
    "    return ede_gpd_coi_dict_updates, cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3300df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def perform_build_RCPX_from_csvs_prereqs(\n",
    "    files_dir, \n",
    "    file_path_glob, \n",
    "    file_path_regex,\n",
    "    mp_df, \n",
    "    ede_mp_mismatch_threshold_pct=1.0, \n",
    "    grp_by_cols='outg_rec_nb', \n",
    "    outg_rec_nb_col='outg_rec_nb',\n",
    "    trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "    prem_nb_col='aep_premise_nb', \n",
    "    serial_number_col='serialnumber',\n",
    "    mp_df_cols = dict(\n",
    "        serial_number_col='mfr_devc_ser_nbr', \n",
    "        prem_nb_col='prem_nb', \n",
    "        trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "        outg_rec_nb_col='OUTG_REC_NB'\n",
    "    ), \n",
    "    is_no_outage=False, \n",
    "    assert_all_cols_equal=True, \n",
    "    trust_sql_grouping=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Prepares for running build_reason_counts_per_outage_from_csvs.\n",
    "    Many of these items were done for each iteration of the main for loop in build_reason_counts_per_outage_from_csvs, which\n",
    "      was really unnecessary.  \n",
    "      This will improve performance (although, likely not much as none of the operations are super heavy) and simplify the code.\n",
    "    \n",
    "    1. Adjust grp_by_cols and outg_rec_nb_col to handle cases where _gpd_for_sql appendix was added during data acquisition.\n",
    "       If there is an instance where, e.g., outg_rec_nb_col and f'{outg_rec_nb_col}_gpd_for_sql' are both present, it settles\n",
    "         the discrepancy (according to the trust_sql_grouping parameter) and compiles a list of columns which will need \n",
    "         to be dropped.\n",
    "    2. Determine merge_on_ede and merge_on_mp columns (done within check_end_events_merge_with_mp).\n",
    "    3. Checks that the user supplied mp_df aligns well with the end events data.\n",
    "    4. If assert_all_cols_equal==True, enforce the assertion.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(Utilities.is_object_one_of_types(grp_by_cols, [str, list, tuple]))\n",
    "    if isinstance(grp_by_cols, str):\n",
    "        grp_by_cols = [grp_by_cols]    \n",
    "    #-------------------------\n",
    "    paths = Utilities.find_all_paths(\n",
    "        base_dir=files_dir, \n",
    "        glob_pattern=file_path_glob, \n",
    "        regex_pattern=file_path_regex\n",
    "    )\n",
    "    if len(paths)==0:\n",
    "        print(f'No paths found in files_dir = {files_dir}')\n",
    "        return None\n",
    "    paths=natsorted(paths)\n",
    "    #-------------------------\n",
    "    #--------------------------------------------------\n",
    "    # Grab first row from each CSV file to be used to check columns\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df_i = GenAn.read_df_from_csv(\n",
    "            read_path=path, \n",
    "            cols_and_types_to_convert_dict=None, \n",
    "            to_numeric_errors='coerce', \n",
    "            drop_na_rows_when_exception=True, \n",
    "            drop_unnamed0_col=True, \n",
    "            pd_read_csv_kwargs = dict(nrows=1)\n",
    "        )\n",
    "        if df_i.shape[0]==0:\n",
    "            continue\n",
    "        #-----\n",
    "        # NOTE: make_all_columns_lowercase because...\n",
    "        #   EMR would return lowercase outg_rec_nb or outg_rec_nb_gpd_for_sql\n",
    "        #   Athena maintains the original case, and does not conver to lower case,\n",
    "        #     so it returns OUTG_REC_NB or OUTG_REC_NB_GPD_FOR_SQL\n",
    "        df_i = Utilities_df.make_all_column_names_lowercase(df_i) \n",
    "        #-----\n",
    "        dfs.append(df_i)\n",
    "    #-------------------------\n",
    "    if len(dfs)==0:\n",
    "        print(f'No DFs found in files_dir={files_dir}')\n",
    "        assert(0)\n",
    "    #-------------------------                \n",
    "    df_cols = Utilities_df.get_shared_columns(dfs, maintain_df0_order=True)\n",
    "    for i in range(len(dfs)):\n",
    "        if assert_all_cols_equal:\n",
    "            # In order to account for case where columns are the same but in different order\n",
    "            # one must compare the length of dfs[i].columns to that of df_cols (found by utilizing\n",
    "            # the Utilities_df.get_shared_columns(dfs) functionality)\n",
    "            assert(dfs[i].shape[1]==len(df_cols))\n",
    "        dfs[i] = dfs[i][df_cols]\n",
    "    end_events_df = pd.concat(dfs)\n",
    "    #--------------------------------------------------\n",
    "    #--------------------------------------------------\n",
    "    ede_cols_of_interest_updates, cols_to_drop = identify_ede_cols_of_interest_to_update_andor_drop(\n",
    "        end_events_df      = end_events_df,  \n",
    "        grp_by_cols        = grp_by_cols, \n",
    "        outg_rec_nb_col    = outg_rec_nb_col,\n",
    "        trsf_pole_nb_col   = trsf_pole_nb_col, \n",
    "        prem_nb_col        = prem_nb_col, \n",
    "        serial_number_col  = serial_number_col,\n",
    "        trust_sql_grouping = trust_sql_grouping\n",
    "    )\n",
    "    grp_by_cols       = ede_cols_of_interest_updates['grp_by_cols']\n",
    "    outg_rec_nb_col   = ede_cols_of_interest_updates['outg_rec_nb_col']\n",
    "    trsf_pole_nb_col  = ede_cols_of_interest_updates['trsf_pole_nb_col']\n",
    "    prem_nb_col       = ede_cols_of_interest_updates['prem_nb_col']\n",
    "    serial_number_col = ede_cols_of_interest_updates['serial_number_col']\n",
    "    #--------------------------------------------------\n",
    "    #--------------------------------------------------\n",
    "    mp_df, merge_on_ede, merge_on_mp = check_end_events_merge_with_mp(\n",
    "        ede_file_paths        = paths,\n",
    "        mp_df                 = mp_df, \n",
    "        threshold_pct         = ede_mp_mismatch_threshold_pct, \n",
    "        outg_rec_nb_col       = outg_rec_nb_col,\n",
    "        trsf_pole_nb_col      = trsf_pole_nb_col, \n",
    "        prem_nb_col           = prem_nb_col, \n",
    "        serial_number_col     = serial_number_col,\n",
    "        mp_df_cols            = mp_df_cols, \n",
    "        is_no_outage          = is_no_outage, \n",
    "        assert_all_cols_equal = assert_all_cols_equal\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    #--------------------------------------------------\n",
    "    return_dict = dict(\n",
    "        paths=paths, \n",
    "        grp_by_cols=grp_by_cols, \n",
    "        outg_rec_nb_col=outg_rec_nb_col, \n",
    "        trsf_pole_nb_col=trsf_pole_nb_col, \n",
    "        cols_to_drop=cols_to_drop, \n",
    "        mp_df=mp_df, \n",
    "        merge_on_ede=merge_on_ede, \n",
    "        merge_on_mp=merge_on_mp\n",
    "    )\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c94d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def drop_gpd_for_sql_appendix_from_all_cols(df):\n",
    "    r\"\"\"\n",
    "    The _gpd_for_sql appendix is added in the end events data acquisition stage, but is typically no longer needed after.\n",
    "    This function removes the appendix.\n",
    "    \n",
    "    NOTE: Method is case insensitive!! (i.e., col_gpd_for_sql --> col, col_GPD_FOR_SQL --> col, etc.)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    for col in df.columns.tolist():\n",
    "        if col.lower().endswith('_gpd_for_sql'):\n",
    "            df = df.rename(columns={col:col[:-12]})\n",
    "    return df\n",
    "\n",
    "\n",
    "# Moved to OutageMdlrPrep\n",
    "def drop_gpd_for_sql_appendix_from_all_index_names(df):\n",
    "    r\"\"\"\n",
    "    The _gpd_for_sql appendix is added in the end events data acquisition stage, but is typically no longer needed after.\n",
    "    This function removes the appendix.\n",
    "    \n",
    "    NOTE: Method is case insensitive!! (i.e., col_gpd_for_sql --> col, col_GPD_FOR_SQL --> col, etc.)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    idx_names_new = []\n",
    "    for name in df.index.names:\n",
    "        if name.lower().endswith('_gpd_for_sql'):\n",
    "            idx_names_new.append(name[:-12])\n",
    "        else:\n",
    "            idx_names_new.append(name)\n",
    "    #-----\n",
    "    assert(len(df.index.names)==len(idx_names_new))\n",
    "    df.index.names = idx_names_new\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to OutageMdlrPrep\n",
    "def build_reason_counts_per_outage_from_csvs_NEW(    \n",
    "    files_dir, \n",
    "    file_path_glob, \n",
    "    file_path_regex, \n",
    "    patterns_to_replace, \n",
    "    mp_df, \n",
    "    min_outg_td_window=datetime.timedelta(days=1),\n",
    "    max_outg_td_window=datetime.timedelta(days=30),\n",
    "    build_ede_typeid_to_reason_df=False, \n",
    "    batch_size=50, \n",
    "    cols_and_types_to_convert_dict=None, \n",
    "    to_numeric_errors='coerce', \n",
    "    assert_all_cols_equal=True, \n",
    "    include_normalize_by_nSNs=True, \n",
    "    inclue_zero_counts=True, \n",
    "    return_multiindex_outg_reason=False, \n",
    "    return_normalized_separately=False, \n",
    "    verbose=True, \n",
    "    n_update=1, \n",
    "    grp_by_cols='outg_rec_nb', \n",
    "    outg_rec_nb_col='outg_rec_nb',\n",
    "    trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "    addtnl_dropna_subset_cols=None, \n",
    "    is_no_outage=False, \n",
    "    prem_nb_col='aep_premise_nb', \n",
    "    serial_number_col='serialnumber', \n",
    "    include_prem_nbs=False, \n",
    "    trust_sql_grouping=True, \n",
    "    mp_df_cols = dict(\n",
    "        serial_number_col='mfr_devc_ser_nbr', \n",
    "        prem_nb_col='prem_nb', \n",
    "        trsf_pole_nb_col = 'trsf_pole_nb', \n",
    "        outg_rec_nb_col='OUTG_REC_NB'\n",
    "    )\n",
    "):\n",
    "    r\"\"\"\n",
    "    Note: The larger the batch_size, the more memory that will be consumed during building\n",
    "\n",
    "    Any rows with NaNs in outg_rec_nb_col+addtnl_dropna_subset_cols will be dropped\n",
    "\n",
    "    #NOTE: Currently, only set up for the case return_normalized_separately==False\n",
    "    \"\"\"\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # TODO Allow method for reading DOVS from CSV (instead of only from SQL query)\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    #-------------------------\n",
    "    assert(not return_normalized_separately)\n",
    "    #-------------------------\n",
    "    assert(Utilities.is_object_one_of_types(grp_by_cols, [str, list, tuple]))\n",
    "    if isinstance(grp_by_cols, str):\n",
    "        grp_by_cols = [grp_by_cols]        \n",
    "    #-------------------------\n",
    "    # If including prem numbers, also include n_prem numbers\n",
    "    include_nprem_nbs = include_prem_nbs\n",
    "    #-------------------------\n",
    "    # normalize_by_nSNs_included needed when return_multiindex_outg_reason is True\n",
    "    if include_normalize_by_nSNs and not return_normalized_separately:\n",
    "        normalize_by_nSNs_included=True\n",
    "    else:\n",
    "        normalize_by_nSNs_included=False\n",
    "\n",
    "    are_dfs_wide_form = not return_multiindex_outg_reason\n",
    "    is_norm=False #TODO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    #-------------------------\n",
    "    prereq_dict = perform_build_RCPX_from_csvs_prereqs(\n",
    "        files_dir=files_dir, \n",
    "        file_path_glob=file_path_glob, \n",
    "        file_path_regex=file_path_regex,\n",
    "        mp_df=mp_df, \n",
    "        ede_mp_mismatch_threshold_pct=1.0, \n",
    "        grp_by_cols=grp_by_cols, \n",
    "        outg_rec_nb_col=outg_rec_nb_col,\n",
    "        trsf_pole_nb_col=trsf_pole_nb_col, \n",
    "        prem_nb_col=prem_nb_col, \n",
    "        serial_number_col=serial_number_col,\n",
    "        mp_df_cols=mp_df_cols, \n",
    "        is_no_outage=is_no_outage, \n",
    "        assert_all_cols_equal=assert_all_cols_equal\n",
    "    )\n",
    "    paths            = prereq_dict['paths']\n",
    "    grp_by_cols      = prereq_dict['grp_by_cols']\n",
    "    outg_rec_nb_col  = prereq_dict['outg_rec_nb_col']\n",
    "    trsf_pole_nb_col = prereq_dict['trsf_pole_nb_col']\n",
    "    cols_to_drop     = prereq_dict['cols_to_drop']\n",
    "    mp_df            = prereq_dict['mp_df']\n",
    "    merge_on_ede     = prereq_dict['merge_on_ede']\n",
    "    merge_on_mp      = prereq_dict['merge_on_mp']\n",
    "    #-------------------------\n",
    "    rcpo_full = pd.DataFrame()\n",
    "    #-------------------------\n",
    "    batch_idxs = Utilities.get_batch_idx_pairs(len(paths), batch_size)\n",
    "    n_batches = len(batch_idxs)    \n",
    "    if verbose:\n",
    "        print(f'n_paths = {len(paths)}')\n",
    "        print(f'batch_size = {batch_size}')\n",
    "        print(f'n_batches = {n_batches}')    \n",
    "    #-------------------------\n",
    "    for i, batch_i in enumerate(batch_idxs):\n",
    "        start = time.time()\n",
    "        if verbose and (i+1)%n_update==0:\n",
    "            print(f'{i+1}/{n_batches}')\n",
    "        i_beg = batch_i[0]\n",
    "        i_end = batch_i[1]\n",
    "        #-----\n",
    "        # NOTE: make_all_columns_lowercase=True because...\n",
    "        #   EMR would return lowercase outg_rec_nb or outg_rec_nb_gpd_for_sql\n",
    "        #   Athena maintains the original case, and does not conver to lower case,\n",
    "        #     so it returns OUTG_REC_NB or OUTG_REC_NB_GPD_FOR_SQL\n",
    "        end_events_df_i = GenAn.read_df_from_csv_batch(\n",
    "            paths=paths[i_beg:i_end], \n",
    "            cols_and_types_to_convert_dict=cols_and_types_to_convert_dict, \n",
    "            to_numeric_errors=to_numeric_errors, \n",
    "            make_all_columns_lowercase=True, \n",
    "            assert_all_cols_equal=assert_all_cols_equal\n",
    "        )\n",
    "        print(f'0: {time.time()-start}')\n",
    "        #-------------------------\n",
    "        if end_events_df_i.shape[0]==0:\n",
    "            continue\n",
    "        #-------------------------\n",
    "        start = time.time()\n",
    "        # Drop any columns from cols_to_drop\n",
    "        if len(cols_to_drop)>0:\n",
    "            end_events_df_i = end_events_df_i.drop(columns=cols_to_drop)\n",
    "        #-----\n",
    "        # FROM WHAT I CAN TELL, the meters which are 'missing' from meter premise but are present in end events\n",
    "        #   are caused by meters which were removed or installed close to (a few days/weeks before) an outage.\n",
    "        # Therefore, although the meter may not have been present at the time of the outage (and therefore was exluced from\n",
    "        #   meter premise), it could have registered events leading up to/following the outage.\n",
    "        # e.g., if a meter was removed in the days before an outage, end events are still found for this meter in the days leading up to the outage\n",
    "        # e.g., if a meter was installed in the days after an outage, end events are still found for this meter in the days following an outage.\n",
    "        # How should these be handled?\n",
    "        # The simplest method, which I will implement for now, is to simply ONLY consider those meters which were present\n",
    "        #   at the time of the outage.  THEREFORE, the two DFs should be joined with an inner merge!\n",
    "        end_events_df_i = AMIEndEvents.merge_end_events_df_with_mp(\n",
    "            end_events_df=end_events_df_i, \n",
    "            df_mp=mp_df, \n",
    "            merge_on_ede=merge_on_ede, \n",
    "            merge_on_mp=merge_on_mp, \n",
    "            cols_to_include_mp=None, \n",
    "            drop_cols = None, \n",
    "            rename_cols=None, \n",
    "            how='inner', \n",
    "            inplace=True\n",
    "        )\n",
    "        for grp_by_col in grp_by_cols:\n",
    "            assert(grp_by_col in end_events_df_i.columns)\n",
    "        #-------------------------\n",
    "        dropna_subset_cols = grp_by_cols\n",
    "        if addtnl_dropna_subset_cols is not None:\n",
    "            dropna_subset_cols.extend(addtnl_dropna_subset_cols)\n",
    "        end_events_df_i = end_events_df_i.dropna(subset=dropna_subset_cols)\n",
    "        #-------------------------\n",
    "        end_events_df_i=Utilities_dt.strip_tz_info_and_convert_to_dt(\n",
    "            df=end_events_df_i, \n",
    "            time_col='valuesinterval', \n",
    "            placement_col='valuesinterval_local', \n",
    "            run_quick=True, \n",
    "            n_strip=6, \n",
    "            inplace=False\n",
    "        )\n",
    "        print(f'1: {time.time()-start}')\n",
    "        start = time.time()\n",
    "        #---------------------------------------------------------------------------\n",
    "        # If min_outg_td_window or max_outg_td_window is not None, enforce time restrictions around outages\n",
    "        if min_outg_td_window is not None or max_outg_td_window is not None:\n",
    "            #----------\n",
    "            if not is_no_outage:\n",
    "                dovs_outgs = DOVSOutages(                 \n",
    "                    df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "                    contstruct_df_args=None, \n",
    "                    init_df_in_constructor=True, \n",
    "                    build_sql_function=DOVSOutages_SQL.build_sql_outage, \n",
    "                    build_sql_function_kwargs=dict(\n",
    "                        outg_rec_nbs=end_events_df_i[outg_rec_nb_col].unique().tolist(), \n",
    "                        from_table_alias='DOV', \n",
    "                        datetime_col='DT_OFF_TS_FULL', \n",
    "                        cols_of_interest=[\n",
    "                            'OUTG_REC_NB', \n",
    "                            dict(field_desc=f\"DOV.DT_ON_TS - DOV.STEP_DRTN_NB/(60*24)\", \n",
    "                                 alias='DT_OFF_TS_FULL', table_alias_prefix=None)\n",
    "                        ], \n",
    "                        field_to_split='outg_rec_nbs'\n",
    "                    ),\n",
    "                )\n",
    "                outg_dt_off_df = dovs_outgs.df\n",
    "                outg_dt_off_df = Utilities_df.convert_col_type(df=outg_dt_off_df, column='OUTG_REC_NB', to_type=str)\n",
    "                outg_dt_off_df=outg_dt_off_df.set_index('OUTG_REC_NB')\n",
    "                outg_dt_off_series = outg_dt_off_df['DT_OFF_TS_FULL']\n",
    "\n",
    "                end_events_df_i = AMIEndEvents.enforce_end_events_within_interval_of_outage(\n",
    "                    end_events_df=end_events_df_i, \n",
    "                    outg_times_series=outg_dt_off_series, \n",
    "                    min_timedelta=min_outg_td_window, \n",
    "                    max_timedelta=max_outg_td_window, \n",
    "                    outg_rec_nb_col = outg_rec_nb_col, \n",
    "                    datetime_col='valuesinterval_local', \n",
    "                    assert_one_time_per_group=True\n",
    "                )\n",
    "            else:\n",
    "                no_outg_time_infos_df = MECPOAn.get_bsln_time_interval_infos_df_from_summary_files(\n",
    "                    summary_paths=[AMIEndEvents.find_summary_file_from_csv(x) for x in paths[i_beg:i_end]], \n",
    "                    output_prem_nbs_col='prem_nbs', \n",
    "                    output_t_min_col='t_min', \n",
    "                    output_t_max_col='DT_OFF_TS_FULL', \n",
    "                    make_addtnl_groupby_idx=True, \n",
    "                    include_summary_paths=False\n",
    "                )\n",
    "                assert(no_outg_time_infos_df.index.name==trsf_pole_nb_col)\n",
    "                assert(end_events_df_i[trsf_pole_nb_col].dtype==no_outg_time_infos_df.index.dtype)\n",
    "                no_outg_time_infos_series = no_outg_time_infos_df['DT_OFF_TS_FULL']    \n",
    "\n",
    "                end_events_df_i = AMIEndEvents.enforce_end_events_within_interval_of_outage(\n",
    "                    end_events_df=end_events_df_i, \n",
    "                    outg_times_series=no_outg_time_infos_series, \n",
    "                    min_timedelta=min_outg_td_window, \n",
    "                    max_timedelta=max_outg_td_window, \n",
    "                    outg_rec_nb_col = trsf_pole_nb_col, \n",
    "                    datetime_col='valuesinterval_local', \n",
    "                    assert_one_time_per_group=False\n",
    "                )\n",
    "        print(f'2: {time.time()-start}')\n",
    "        #---------------------------------------------------------------------------\n",
    "        # After enforcing events within specific time frame, it is possible end_events_df_i is empty.\n",
    "        # If so, continue\n",
    "        if end_events_df_i.shape[0]==0:\n",
    "            continue\n",
    "        #-------------------------\n",
    "#         end_events_df_i = AMIEndEvents.extract_reboot_counts_from_reasons_in_df(end_events_df_i)\n",
    "#         end_events_df_i = AMIEndEvents.extract_fail_reasons_from_reasons_in_df(end_events_df_i)\n",
    "        #-------------------------\n",
    "        start = time.time()\n",
    "        end_events_df_i = AMIEndEvents.reduce_end_event_reasons_in_df(\n",
    "            df=end_events_df_i, \n",
    "        )\n",
    "        print(f'3: {time.time()-start}')\n",
    "        #-------------------------\n",
    "        start = time.time()\n",
    "        if build_ede_typeid_to_reason_df:\n",
    "            ede_typeid_to_reason_df_i = AMIEndEvents.build_ede_typeid_to_reason_df(\n",
    "                end_events_df=end_events_df_i, \n",
    "                reason_col='reason', \n",
    "                ede_typeid_col='enddeviceeventtypeid'\n",
    "            )\n",
    "            if i==0:\n",
    "                ede_typeid_to_reason_df = ede_typeid_to_reason_df_i.copy()\n",
    "            else:\n",
    "                ede_typeid_to_reason_df = AMIEndEvents.combine_two_ede_typeid_to_reason_dfs(\n",
    "                    ede_typeid_to_reason_df1=ede_typeid_to_reason_df, \n",
    "                    ede_typeid_to_reason_df2=ede_typeid_to_reason_df_i,\n",
    "                    sort=True\n",
    "                )\n",
    "        print(f'4: {time.time()-start}')\n",
    "        #-------------------------\n",
    "        start = time.time()\n",
    "        rcpo_i = AMIEndEvents.get_reason_counts_per_group(\n",
    "            end_events_df = end_events_df_i, \n",
    "            group_cols=grp_by_cols, \n",
    "            group_freq=None, \n",
    "            serial_number_col='serialnumber', \n",
    "            reason_col='reason', \n",
    "            include_normalize_by_nSNs=include_normalize_by_nSNs, \n",
    "            inclue_zero_counts=inclue_zero_counts,\n",
    "            possible_reasons=None, \n",
    "            include_nSNs=True, \n",
    "            include_SNs=True, \n",
    "            prem_nb_col=prem_nb_col, \n",
    "            include_nprem_nbs=include_nprem_nbs,\n",
    "            include_prem_nbs=include_prem_nbs,   \n",
    "            return_form = dict(return_multiindex_outg_reason = return_multiindex_outg_reason, \n",
    "                               return_normalized_separately  = return_normalized_separately)\n",
    "        )\n",
    "        print(f'5: {time.time()-start}')\n",
    "        #-------------------------\n",
    "        if rcpo_i.shape[0]==0:\n",
    "            continue\n",
    "        #-------------------------\n",
    "        start = time.time()\n",
    "        # Include or below in case i=0 rcpo_i comes back empty, causing continue to be called above\n",
    "        if i==0 or rcpo_full.shape[0]==0:\n",
    "            rcpo_full = rcpo_i.copy()\n",
    "        else:\n",
    "            list_cols=['_SNs']\n",
    "            list_counts_cols=['_nSNs']\n",
    "            w_col = '_nSNs'\n",
    "            if include_prem_nbs:\n",
    "                list_cols.append('_prem_nbs')\n",
    "                list_counts_cols.append('_nprem_nbs')\n",
    "            #-----\n",
    "            rcpo_full = AMIEndEvents.combine_two_reason_counts_per_outage_dfs(\n",
    "                rcpo_df_1=rcpo_full, \n",
    "                rcpo_df_2=rcpo_i, \n",
    "                are_dfs_wide_form=are_dfs_wide_form, \n",
    "                normalize_by_nSNs_included=normalize_by_nSNs_included, \n",
    "                is_norm=is_norm, \n",
    "                list_cols=list_cols, \n",
    "                list_counts_cols=list_counts_cols,\n",
    "                w_col = w_col, \n",
    "                level_0_raw_col = 'counts', \n",
    "                level_0_nrm_col = 'counts_norm', \n",
    "                convert_rcpo_wide_to_long_col_args=None\n",
    "            )\n",
    "        print(f'6: {time.time()-start}')\n",
    "    #-------------------------\n",
    "    # Drop _gpd_for_sql appendix from any index names \n",
    "    rcpo_full = drop_gpd_for_sql_appendix_from_all_index_names(rcpo_full)\n",
    "    #-------------------------\n",
    "    if not build_ede_typeid_to_reason_df:\n",
    "        return rcpo_full\n",
    "    else:\n",
    "        return rcpo_full, ede_typeid_to_reason_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497158fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1121db3",
   "metadata": {},
   "source": [
    "# END NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca1f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4f444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
