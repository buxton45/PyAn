{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./check_DOVS_METHODS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns\n",
    "from packaging import version\n",
    "\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "import Utilities_dt\n",
    "from Utilities_df import DFConstructType\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import GrubbsTest\n",
    "import DataFrameSubsetSlicer\n",
    "from DataFrameSubsetSlicer import DataFrameSubsetSlicer as DFSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025651d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e019c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4352390",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_outages = Utilities.get_utldb01p_oracle_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24595a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outgs_file_from_mico = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_check\\forMico2\\2023-04-08 to 04-15 Reviews (1).xlsx'\n",
    "expand_time = pd.Timedelta('1 day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "mico_df_raw = pd.read_excel(outgs_file_from_mico, sheet_name='Scorecard')\n",
    "mico_df = mico_df_raw.copy()\n",
    "#-----\n",
    "# For now, keep only the following columns:\n",
    "mico_cols_to_keep = [\n",
    "    'Outage #', \n",
    "    'Outage Start DT', \n",
    "    'Adj Outage Start DT', \n",
    "    'Outage End DT',\n",
    "    'Adj Outage End DT', \n",
    "    'Circuit Name',\n",
    "    'Step CMI'\n",
    "]\n",
    "mico_df = mico_df[mico_cols_to_keep]\n",
    "\n",
    "#-------------------------\n",
    "# Currently, outage numbers have -1, -2, etc. appended.\n",
    "# I believe an outage number will have such multiple rows when the outage affects more than one circuit.\n",
    "# In the DOVS database, these will be split iunto separate outg_rec_nbs\n",
    "#-----\n",
    "# I will instead merge the data via the outage number and circuit name, so remove the -1, -2, etc. from \n",
    "#   the 'Outage #', store the result in 'OUTAGE_NB' (to be consistent with DOVS), and drop 'Outage #'\n",
    "mico_df['OUTAGE_NB'] = mico_df['Outage #'].apply(lambda x: re.sub('(\\d*)-\\d*', r'\\1', x))\n",
    "mico_df=mico_df.drop(columns=['Outage #'])\n",
    "\n",
    "#-------------------------\n",
    "# Each outage can also have multiple rows corresponding to the power recover steps\n",
    "# Aggregate the steps into a single row for each outage\n",
    "mico_df = mico_df.groupby(\n",
    "    ['OUTAGE_NB', 'Circuit Name'], \n",
    "    dropna=False, \n",
    "    as_index=False, \n",
    "    group_keys=False\n",
    ").agg({\n",
    "    'Outage Start DT':     'min', \n",
    "    'Adj Outage Start DT': 'min', \n",
    "    'Outage End DT':       'max', \n",
    "    'Adj Outage End DT':   'max', \n",
    "    'Step CMI':            'sum'\n",
    "})\n",
    "\n",
    "# At this point, each outage (unique combinatino of 'OUTAGE_NB' and 'Circuit Name') should\n",
    "#   correspond to a single row\n",
    "assert(mico_df.shape[0] == mico_df.groupby(['OUTAGE_NB', 'Circuit Name']).ngroups)\n",
    "\n",
    "#-------------------------\n",
    "mico_df['Min Start Date'] = mico_df[['Outage Start DT', 'Adj Outage Start DT']].min(axis=1).dt.date - expand_time\n",
    "mico_df['Max End Date']   = mico_df[['Outage End DT',   'Adj Outage End DT'  ]].max(axis=1).dt.date + expand_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50dee67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# NOTE: A single OUTAGE_NB can correspond to more than one OUTG_REC_NBs!\n",
    "#       It appears this is the case when the outage affects multiple GIS_CRCT_NBs, in which case,\n",
    "#         each GIS_CRCT_NB gets its own OUTG_REC_NB\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# First, grab DF containing all OUTAGE_NBs\n",
    "# Note, the OUTAGE_NB is not unique, so there will generally be multiple entries\n",
    "#   Here, I'm talking about the same OUTAGE_NB being used for different outages throughout the years, \n",
    "#     not a single OUTAGE_NB corresponding to multiple OUTG_REC_NBs, as described above.\n",
    "#   Determine which is correct entry using the times from mico_df\n",
    "sql_using_outage_nbs = DOVSOutages_SQL.build_sql_std_outage(\n",
    "    mjr_mnr_cause=None, \n",
    "    include_premise=True, \n",
    "    outage_nbs=mico_df['OUTAGE_NB'].unique().tolist(), \n",
    "    date_range=[mico_df['Min Start Date'].min(), mico_df['Max End Date'].max()], \n",
    "    MJR_CAUSE_CD=None, \n",
    "    DEVICE_CD=None, \n",
    "    INTRPTN_TYP_CD=None, \n",
    "    CURR_REC_STAT_CD=None, \n",
    "    select_cols_DOVS_PREMISE_DIM=['CIRCT_NM']\n",
    ").get_sql_statement()\n",
    "#-----\n",
    "df_using_outage_nbs = pd.read_sql_query(\n",
    "    sql_using_outage_nbs, \n",
    "    conn_outages, \n",
    "    dtype={\n",
    "        'CI_NB':np.int32, \n",
    "        'CMI_NB':np.float64, \n",
    "        'OUTG_REC_NB':np.int32\n",
    "    }\n",
    ")\n",
    "#-------------------------\n",
    "# Determine appropriate OUTG_REC_NBs by using 'Min Start Date', 'Max End Date'\n",
    "df_using_outage_nbs = pd.merge(\n",
    "    df_using_outage_nbs, \n",
    "    mico_df[['OUTAGE_NB', 'Circuit Name', 'Min Start Date', 'Max End Date']], \n",
    "    left_on=['OUTAGE_NB', 'CIRCT_NM'], \n",
    "    right_on=['OUTAGE_NB', 'Circuit Name'], \n",
    "    how='inner'\n",
    ")\n",
    "df_using_outage_nbs= df_using_outage_nbs[\n",
    "    (df_using_outage_nbs['DT_OFF_TS_FULL'].dt.date >= df_using_outage_nbs['Min Start Date']) & \n",
    "    (df_using_outage_nbs['DT_ON_TS'].dt.date       <= df_using_outage_nbs['Max End Date'])\n",
    "]\n",
    "df_using_outage_nbs = df_using_outage_nbs.drop(columns=['Min Start Date', 'Max End Date', 'Circuit Name'])\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81163a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ae054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_using_outage_nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c4553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage = df_using_outage_nbs\n",
    "outg_rec_nbs_all = df_outage['OUTG_REC_NB'].unique().tolist()\n",
    "#-----\n",
    "print(f\"df_outage.shape = {df_outage.shape}\")\n",
    "print(f\"# OUTG_REC_NBs  = {df_outage['OUTG_REC_NB'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23b330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c44c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "#-----\n",
    "# df_mp_outg_OG = build_active_MP_for_xfmrs_in_outages_df(\n",
    "#     df_outage=df_outage, \n",
    "#     prem_nb_col='PREMISE_NB', \n",
    "#     is_slim=False, \n",
    "#     addtnl_mp_df_curr_cols=['technology_tx', 'state_cd'], \n",
    "#     addtnl_mp_df_hist_cols=['technology_tx', 'state_cd']\n",
    "# )\n",
    "# #-----\n",
    "# print(f'Time for build_active_MP_for_xfmrs_in_outages_df: {time.time()-start}')\n",
    "df_mp_outg_OG = build_active_MP_for_outages_df(\n",
    "    df_outage=df_outage, \n",
    "    prem_nb_col='PREMISE_NB', \n",
    "    is_slim=False, \n",
    "    addtnl_mp_df_curr_cols=['technology_tx', 'state_cd'], \n",
    "    addtnl_mp_df_hist_cols=['technology_tx', 'state_cd'], \n",
    "    assert_all_PNs_found=False\n",
    ")\n",
    "#-----\n",
    "print(f'Time for build_active_MP_for_outages_df: {time.time()-start}')\n",
    "start=time.time()\n",
    "#-----\n",
    "df_mp_outg_OG['inst_ts'] = pd.to_datetime(df_mp_outg_OG['inst_ts'])\n",
    "df_mp_outg_OG['rmvl_ts'] = pd.to_datetime(df_mp_outg_OG['rmvl_ts'])\n",
    "#-------------------------\n",
    "df_mp_outg = MeterPremise.drop_approx_mp_duplicates(\n",
    "    mp_df = df_mp_outg_OG.copy(), \n",
    "    fuzziness=pd.Timedelta('1 hour'), \n",
    "    assert_single_overlap=True, \n",
    "    addtnl_groupby_cols=['OUTG_REC_NB', 'technology_tx', 'state_cd'], \n",
    "    gpby_dropna=False\n",
    ")\n",
    "#-----\n",
    "print(f'Time for drop_approx_mp_duplicates: {time.time()-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some premises are listed in DOVS are simply not found in AMI\n",
    "print(f\"#PNs DOVS: {df_outage['PREMISE_NB'].nunique()}\")\n",
    "print(f\"#PNs AMI:  {df_mp_outg['prem_nb'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_mp_outg['prem_nb'].unique().tolist()).difference(set(df_outage['PREMISE_NB'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40309074",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_outage['PREMISE_NB'].unique().tolist()).difference(set(df_mp_outg['prem_nb'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a07592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0dfe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really only want one entry per meter (here, meter being a mfr_devc_ser_nbr/prem_nb combination)\n",
    "# ALthough drop_duplicates was used, multiple entries could still exist if, e.g., a meter has two\n",
    "#   non-fuzzy-overlapping intervals\n",
    "assert(all(df_mp_outg[['mfr_devc_ser_nbr', 'prem_nb', 'OUTG_REC_NB']].value_counts()==1))\n",
    "\n",
    "# # Simple-minded (if assertion fails): Let's just keep the one with the most recent install date\n",
    "# df_mp_outg = df_mp_outg.iloc[df_mp_outg.reset_index().groupby(['mfr_devc_ser_nbr', 'prem_nb', 'OUTG_REC_NB'])['inst_ts'].idxmax()]\n",
    "# assert(all(df_mp_outg[['mfr_devc_ser_nbr', 'prem_nb', 'OUTG_REC_NB']].value_counts()==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3de7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp_outg.groupby(['OUTG_REC_NB']).apply(lambda x: 100*(x[x['technology_tx']=='AMI'].shape[0]/x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp_outg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53936be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae139945",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = DOVSOutages.merge_df_outage_with_mp(\n",
    "    df_outage=df_outage.copy(), \n",
    "    df_mp=df_mp_outg.copy(), \n",
    "    merge_on_outg=['OUTG_REC_NB', 'PREMISE_NB'], \n",
    "    merge_on_mp=['OUTG_REC_NB', 'prem_nb'], \n",
    "    cols_to_include_mp=None, \n",
    "    drop_cols = None, \n",
    "    rename_cols=None, \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e55553",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_outage.shape[0])\n",
    "print(df_mp_outg.shape[0])\n",
    "print(check_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508adca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df['STATE_ABBR_TX'].equals(check_df['state_cd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc13758",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"STATE_ABBR_TX==state_cd: {(check_df['STATE_ABBR_TX']==check_df['state_cd']).sum()}\")\n",
    "print(f\"STATE_ABBR_TX!=state_cd: {(check_df['STATE_ABBR_TX']!=check_df['state_cd']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bea6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df[check_df['STATE_ABBR_TX']!=check_df['state_cd']][['STATE_ABBR_TX', 'state_cd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be79e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846a9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf21a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = check_df.groupby(['OUTG_REC_NB', 'OUTAGE_NB']).apply(lambda x: 100*(x[x['technology_tx']=='AMI'].shape[0]/x.shape[0])).to_frame(name='%AMI')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some premises are listed in DOVS are simply not found in AMI\n",
    "print(f\"#PNs DOVS:            {df_outage['PREMISE_NB'].nunique()}\")\n",
    "print(f\"#PNs AMI using xfmrs: {df_mp_outg['prem_nb'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a029d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = check_df.groupby(['OUTG_REC_NB', 'OUTAGE_NB']).apply(\n",
    "    lambda x: len(set(df_outage[df_outage['OUTG_REC_NB']==x.name[0]]['PREMISE_NB'].unique().tolist()).difference(set(x['prem_nb'].unique().tolist())))\n",
    ").to_frame(name='# PNs missing')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8fe0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = check_df.groupby(['OUTG_REC_NB', 'OUTAGE_NB']).apply(\n",
    "    lambda x: df_outage[df_outage['OUTG_REC_NB']==x.name[0]]['PREMISE_NB'].nunique()\n",
    ").to_frame(name='# PNs total')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3346d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = pd.merge(df1, df2, how='inner', left_index=True, right_index=True)\n",
    "df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afa588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df123 = pd.merge(df12, df3, how='inner', left_index=True, right_index=True)\n",
    "df123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df123['%AMI']==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df123.sort_values(by='%AMI', ascending=False)['%AMI'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab80c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pct_ami = 0\n",
    "outg_rec_nbs_to_keep = df123[df123['%AMI']>=min_pct_ami].reset_index()['OUTG_REC_NB'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df_outage['PREMISE_NB'].unique().tolist()).difference(set(df_mp_outg['prem_nb'].unique().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_outage.shape[0])\n",
    "print(df_outage[df_outage['OUTG_REC_NB'].isin(outg_rec_nbs_to_keep)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2344a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_w_mp = DOVSOutages.merge_df_outage_with_mp(\n",
    "    df_outage=df_outage[df_outage['OUTG_REC_NB'].isin(outg_rec_nbs_to_keep)].copy(), \n",
    "    df_mp=df_mp_outg, \n",
    "    merge_on_outg=['OUTG_REC_NB', 'PREMISE_NB'], \n",
    "    merge_on_mp=['OUTG_REC_NB', 'prem_nb'], \n",
    "    cols_to_include_mp=None, \n",
    "    drop_cols = None, \n",
    "    rename_cols=None, \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3add73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7abe362",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_w_mp['OUTG_REC_NB'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_w_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d8520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_w_mp_slim = DOVSOutages.consolidate_df_outage(df_outage_w_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f47e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_w_mp_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdad16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_w_mp_slim = DOVSOutages.set_search_time_in_outage_df(\n",
    "    df_outage=df_outage_w_mp_slim, \n",
    "    search_time_half_window=pd.Timedelta('24 hours')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d557855",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_w_mp_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe5017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e909bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage_w_mp_slim['OUTG_REC_NB'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc1e2bf",
   "metadata": {},
   "source": [
    "## AMI NonVee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd290f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "usg_split_to_CTEs=True\n",
    "df_construct_type=DFConstructType.kRunSqlQuery\n",
    "contstruct_df_args_ami=None\n",
    "addtnl_groupby_cols=['OUTG_REC_NB', 'trsf_pole_nb']\n",
    "\n",
    "cols_of_interest_ami = TableInfos.AMINonVee_TI.std_columns_of_interest\n",
    "batch_size=10\n",
    "verbose=True\n",
    "n_update=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e2509e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267377c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_sql_function_kwargs = dict(\n",
    "    cols_of_interest=cols_of_interest_ami, \n",
    "    df_outage=df_outage_w_mp_slim, \n",
    "    split_to_CTEs=usg_split_to_CTEs, \n",
    "    join_mp_args=False, \n",
    "    df_args = dict(\n",
    "        addtnl_groupby_cols=addtnl_groupby_cols, \n",
    "        mapping_to_ami={'PREMISE_NBS':'premise_nbs'}, \n",
    "        is_df_consolidated=True\n",
    "    ), \n",
    "    field_to_split='df_outage', \n",
    "    field_to_split_location_in_kwargs=['df_outage'], \n",
    "    save_and_dump=True,  \n",
    "    sort_coll_to_split=True,\n",
    "    batch_size=batch_size, verbose=verbose, n_update=n_update\n",
    ")\n",
    "# addtnl_ami_sql_function_kwargs = dict(\n",
    "#     build_sql_function_kwargs=dict(opco=opcos)\n",
    "# )\n",
    "# ami_sql_function_kwargs = {**ami_sql_function_kwargs, \n",
    "#                            **addtnl_ami_sql_function_kwargs}\n",
    "\n",
    "\n",
    "save_args = dict(\n",
    "    save_to_file=True, \n",
    "    save_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_check\\forMico2\\AMINonVee', \n",
    "    save_name=r'ami_nonvee.csv', \n",
    "    index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "ami_nonvee = AMINonVee(\n",
    "    df_construct_type=df_construct_type, \n",
    "    contstruct_df_args = contstruct_df_args_ami, \n",
    "    build_sql_function=AMINonVee_SQL.build_sql_usg_for_outages, \n",
    "    build_sql_function_kwargs=ami_sql_function_kwargs, \n",
    "    init_df_in_constructor=True, \n",
    "    save_args=save_args\n",
    ")\n",
    "build_time = time.time()-start\n",
    "print(build_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf28c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1784e320",
   "metadata": {},
   "source": [
    "# AMI End Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f997866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "usg_split_to_CTEs=True\n",
    "df_construct_type=DFConstructType.kRunSqlQuery\n",
    "contstruct_df_args_end_events=None\n",
    "addtnl_groupby_cols=['OUTG_REC_NB', 'trsf_pole_nb']\n",
    "\n",
    "cols_of_interest_end_dev_event = TableInfos.AMIEndEvents_TI.std_columns_of_interest\n",
    "batch_size=10\n",
    "verbose=True\n",
    "n_update=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_events_sql_function_kwargs = dict(\n",
    "    cols_of_interest=cols_of_interest_end_dev_event, \n",
    "    df_outage=df_outage_w_mp_slim, \n",
    "    split_to_CTEs=usg_split_to_CTEs, \n",
    "    join_mp_args=False, \n",
    "    df_args = dict(\n",
    "        addtnl_groupby_cols=addtnl_groupby_cols, \n",
    "        mapping_to_ami={'PREMISE_NBS':'premise_nbs'}, \n",
    "        is_df_consolidated=True\n",
    "    ), \n",
    "    field_to_split='df_outage', \n",
    "    field_to_split_location_in_kwargs=['df_outage'], \n",
    "    save_and_dump=True, \n",
    "    sort_coll_to_split=True,\n",
    "    batch_size=batch_size, verbose=verbose, n_update=n_update\n",
    ")\n",
    "# addtnl_end_events_sql_function_kwargs = dict(\n",
    "#     build_sql_function_kwargs=dict(opco=opcos)\n",
    "# )\n",
    "# end_events_sql_function_kwargs = {**end_events_sql_function_kwargs, \n",
    "#                                   **addtnl_end_events_sql_function_kwargs}\n",
    "\n",
    "end_events_save_args = dict(\n",
    "    save_to_file=True, \n",
    "    save_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_check\\forMico2\\EndEvents', \n",
    "    save_name=r'end_events.csv', \n",
    "    index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "end_events = AMIEndEvents(\n",
    "    df_construct_type=df_construct_type, \n",
    "    contstruct_df_args = contstruct_df_args_end_events, \n",
    "    build_sql_function=AMIEndEvents_SQL.build_sql_end_events_for_outages, \n",
    "    build_sql_function_kwargs=end_events_sql_function_kwargs, \n",
    "    init_df_in_constructor=True, \n",
    "    save_args=end_events_save_args\n",
    ")\n",
    "end_events_build_time = time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24979c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d188e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f26ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e4802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674acabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccd516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlap_intervals_in_df(\n",
    "    df, \n",
    "    ovrlp_intrvl_0_col, \n",
    "    ovrlp_intrvl_1_col, \n",
    "    fuzziness, \n",
    "    int_idxs=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a pd.DataFrame df with intervals defined by starting values and ending values in columns ovrlp_intrvl_0_col\n",
    "      and ovrlp_intrvl_1_col, respectively, find the reduced set of overlap intervals.\n",
    "    The fuzziness argument sets how close two intervals must be to be considered overlapping (see Utilities.get_fuzzy_overlap_intervals \n",
    "      for more information.)\n",
    "    Returns a list of dict objects, one for each overlap interval.\n",
    "        The keys for each dict object are 'min_val', 'max_val', and 'idxs',  \n",
    "        By default (when int_idxs==True) 'idxs' correspond to the integer index locations of the rows included in the overlap.\n",
    "        If int_idxs==False, the index labels are instead used.\n",
    "          \n",
    "    NOTE: The function/operation calling this method should ensure all values are appropriate, meaning ensure that for each row\n",
    "            row_i[ovrlp_intrvl_1_col] > row_i[ovrlp_intrvl_0_col]\n",
    "          This method will not attempt to remedy any incorrect values, but will simply assert this is true\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure fuzziness is compatible with ovrlp_intrvl_0(1)_col\n",
    "    try:\n",
    "        test_0 = df.iloc[0][ovrlp_intrvl_0_col]+fuzziness\n",
    "        test_1 = df.iloc[0][ovrlp_intrvl_1_col]+fuzziness\n",
    "    except:\n",
    "        print(f'''\n",
    "        In consolidate_df_according_to_fuzzy_overlap_intervals: Incompatible fuzziness type\n",
    "            type(fuzziness) = {type(fuzziness)}\n",
    "            df[ovrlp_intrvl_0_col].dtype = {df[ovrlp_intrvl_0_col].dtype}\n",
    "            df[ovrlp_intrvl_1_col].dtype = {df[ovrlp_intrvl_1_col].dtype}\n",
    "        CRASH IMMINENT!\n",
    "        ''')\n",
    "    #-------------------------\n",
    "    # First, make sure the second element in each tuple should be greater than the first\n",
    "    # NOTE: The second element can be NaN (either NaT for times, or NaN otherwise)\n",
    "    #         Apparently, NaN evaluates as False when compared in any manner to anything else\n",
    "    #         (i.e., anything>NaN = False, anything<NaN = False, anything==NaN = False)\n",
    "    #       Thus, df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col] will result in False whenever\n",
    "    #         df[ovrlp_intrvl_1_col] is NaN, and therefore the assertion:\n",
    "    #           assert(all(df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]))\n",
    "    #         would fail in in unwanted circumstances, since NaN value means and open ended interval\n",
    "    #         and therefore any beginning value should be considered less than a NaN value.\n",
    "    #       Therefore, instead of the single-line assertion above, I will include two assertions,\n",
    "    #         one to ensure df[ovrlp_intrvl_0_col] doesn't contain any NaNs, and one to ensure\n",
    "    #         either df[ovrlp_intrvl_1_col] is a Nan or df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]\n",
    "    assert(all(df[ovrlp_intrvl_0_col].notna()))\n",
    "    assert(all((df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]) | (df[ovrlp_intrvl_1_col].isna())))\n",
    "    # For above assertion, probably could have isntead used: \n",
    "    #   assert(all(df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col].fillna(pd.Timestamp.max)))\n",
    "    #-------------------------\n",
    "    # Sort ranges, as will be necessary for this procedure\n",
    "    df = df.sort_values(by=[ovrlp_intrvl_0_col, ovrlp_intrvl_1_col])\n",
    "    #-------------------------\n",
    "    # Set the first range in overlaps simply as from the first entry in df\n",
    "    overlaps = []\n",
    "    current_beg, current_end = df.iloc[0][[ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]]\n",
    "    overlaps.append(\n",
    "        dict(min_val=current_beg, max_val=current_end)\n",
    "    )\n",
    "    if int_idxs:\n",
    "        overlaps[0]['idxs'] = [0]\n",
    "    else:\n",
    "        overlaps[0]['idxs'] = [df.index[0]]\n",
    "    #-------------------------\n",
    "    # Iterate through and create the overlaps items, each of which will be a dict with key\n",
    "    #   equal to min_val, max_val, and idxs (where idxs identifies which indices from df\n",
    "    #   belong to each overlap group)\n",
    "    if int_idxs:\n",
    "        df_to_itr = df.reset_index()\n",
    "    else:\n",
    "        df_to_itr = df\n",
    "    #-----\n",
    "    for i, (idx, row) in enumerate(df_to_itr.iterrows()):\n",
    "        if i==0:\n",
    "            continue\n",
    "        #---------------\n",
    "        beg = row[ovrlp_intrvl_0_col]\n",
    "        end = row[ovrlp_intrvl_1_col]\n",
    "        if beg > current_end+fuzziness:\n",
    "            # beg after current end (with fuzziness buffer), so new interval needed\n",
    "            # NOTE: beg > current_end+fuzziness will evaluate to False whenever current_end\n",
    "            #       is NaN, which is the desired functionality.\n",
    "            overlaps.append(dict(min_val=beg, max_val=end, idxs=[idx]))\n",
    "            current_beg, current_end = beg, end\n",
    "        else:\n",
    "            # beg <= current_end+fuzziness, so overlap\n",
    "            # The beg of overlaps[-1] remains the same, but the end of overlaps[-1] should be changed to\n",
    "            #   the max of current_end and end.\n",
    "            # Also, idx needs to be added to the overlap\n",
    "            # NOTE: max(any_non_NaT, NaT) = any_non_NaT (remember, NaN evaluates as False when compared in any manner to anything else), \n",
    "            #       which is not the funcionality I want, as an end of NaT essentially means no end (e.g., a meter which is still in service), \n",
    "            #       and should therefore be treated as Inf.  Thus, cannot simply use the one-liner 'current_end = max(current_end, end)'\n",
    "            # NOTE 2: Cannot simply do 'if pd.isna(current_end) or pd.isna(end)' in single line because this function is\n",
    "            #         designed to work with various data types, so in such a scenario it would be unclear what to set\n",
    "            #         current_end to (e.g., should it be pd.NaT, pd.NaN, etc?).\n",
    "            #         Thus, instead of if-else, need if-elif-else\n",
    "            #current_end = max(current_end, end)\n",
    "            if pd.isna(current_end):\n",
    "                current_end=current_end\n",
    "            elif pd.isna(end):\n",
    "                current_end = end\n",
    "            else:\n",
    "                current_end = max(current_end, end)\n",
    "            overlaps[-1]['max_val'] = current_end\n",
    "            overlaps[-1]['idxs'].append(idx)\n",
    "    #-------------------------\n",
    "    return overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88215048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_df_group_according_to_fuzzy_overlap_intervals(\n",
    "    df_i, \n",
    "    ovrlp_intrvl_0_col, \n",
    "    ovrlp_intrvl_1_col, \n",
    "    gpd_cols, \n",
    "    fuzziness, \n",
    "    assert_single_overlap=False, \n",
    "    maintain_original_cols=False, \n",
    "    enforce_list_cols_for_1d_df=True, \n",
    "    allow_duplicates_in_lists=False, \n",
    "    allow_NaNs_in_lists=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    This is for the specific case of a df group.\n",
    "    It is expected, and enforced, that there exists a single unique value for each column in df_i outside of\n",
    "      ovrlp_intrvl_0_col and ovrlp_intrvl_1_col\n",
    "      \n",
    "    gpd_cols:\n",
    "        For the typical case where this function is used inside of a groupby().apply(lambda x:) function, the\n",
    "          gpd_cols should match those input into groupby (for a typical use case, see \n",
    "          Utilities_df.consolidate_df_according_to_fuzzy_overlap_intervals)\n",
    "        Each of these columns should contain a single unique value.\n",
    "        This input is needed so the function knows which columns to collect in lists (those outside of gpd_cols+\n",
    "          [ovrlp_intrvl_0_col, ovrlp_intrvl_1_col])\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if not isinstance(gpd_cols, list):\n",
    "        gpd_cols = [gpd_cols]\n",
    "    #-------------------------\n",
    "    assert(len(set(gpd_cols).intersection(set([ovrlp_intrvl_0_col, ovrlp_intrvl_1_col])))==0)\n",
    "    assert((df_i[gpd_cols].nunique()<=1).all())\n",
    "    cols_to_collect_in_lists = [x for x in df_i.columns.tolist() if x not in gpd_cols+[ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]]\n",
    "    #-------------------------\n",
    "    og_cols = df_i.columns\n",
    "    #-------------------------\n",
    "    if df_i.shape[0]<=1 and maintain_original_cols and len(cols_to_collect_in_lists)>0: \n",
    "        # Reason for maintain_original_cols:\n",
    "        #   If this is used in .groupby, it is important for all to have same shape/labelling.\n",
    "        #   So, if maintain_original_cols is False, in order for a df_i with df_i.shape[0]==1 to fit\n",
    "        #     into the .groupby procedure, it must go through the steps below to ensure it has the same\n",
    "        #     form as the other groups.\n",
    "        if enforce_list_cols_for_1d_df:\n",
    "        # Make any cols_to_collect_in_lists into lists EXCEPT ovrlp_intrvl_0_col and ovrlp_intrvl_1_col...\n",
    "            lst_cols_1d = [x for x in cols_to_collect_in_lists if x not in [ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]] #1d for 1-dimensional DF\n",
    "            df_i[lst_cols_1d] = df_i[lst_cols_1d].apply(lambda x: [x.tolist()])\n",
    "        return df_i\n",
    "    #-------------------------\n",
    "    if len(cols_to_collect_in_lists)>0:\n",
    "        # Collect the values from cols_to_collect_in_lists, and remove from df_i\n",
    "        if allow_NaNs_in_lists:\n",
    "            if allow_duplicates_in_lists:\n",
    "                agg_func = Utilities_df.agg_func_list\n",
    "            else:\n",
    "                agg_func = Utilities_df.agg_func_unq_list\n",
    "        else:\n",
    "            if allow_duplicates_in_lists:\n",
    "                agg_func = Utilities_df.agg_func_list_dropna\n",
    "            else:\n",
    "                agg_func = Utilities_df.agg_func_unq_list_dropna\n",
    "        list_cols_df = df_i.groupby(gpd_cols, as_index=False, group_keys=False)[cols_to_collect_in_lists].agg(lambda x: agg_func(x))\n",
    "        assert(list_cols_df.shape[0]==1)\n",
    "        #-----\n",
    "        df_i = df_i.drop(columns=cols_to_collect_in_lists)\n",
    "    #-------------------------\n",
    "    overlaps = find_overlap_intervals_in_df(\n",
    "        df=df_i, \n",
    "        ovrlp_intrvl_0_col=ovrlp_intrvl_0_col, \n",
    "        ovrlp_intrvl_1_col=ovrlp_intrvl_1_col, \n",
    "        fuzziness=fuzziness, \n",
    "        int_idxs=True\n",
    "    )\n",
    "    #-------------------------\n",
    "    if assert_single_overlap and len(overlaps)!=1:\n",
    "        print(f'assert_single_overlap and len(overlaps)={len(overlaps)}')\n",
    "        print(overlaps)\n",
    "        print(f'df_i.head():\\n{df_i.head()}')\n",
    "        assert(0)\n",
    "    #-------------------------\n",
    "    # The number of overlaps must be less than or equal to the shape of df_i\n",
    "    assert(len(overlaps) <= df_i.shape[0])\n",
    "\n",
    "    # Already know (assertion at top of function) that all columns are uniform except for [ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]\n",
    "    # Therefore, for return df, just grab as many rows as needed from df_i, and replace the [ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]\n",
    "    #   values with those from overlaps\n",
    "    return_df = df_i.iloc[0:len(overlaps)].copy()\n",
    "    assert(return_df.shape[0]==len(overlaps))\n",
    "    #-------------------------\n",
    "    # Determine the index positions of ovrlp_intrvl_0_col and ovrlp_intrvl_1_col, as these will be needed\n",
    "    #   below to set to values in the consolidated DF using iloc\n",
    "    ovrlp_intrvl_0_col_idx = Utilities_df.find_idxs_in_highest_order_of_columns(return_df, ovrlp_intrvl_0_col)\n",
    "    assert(len(ovrlp_intrvl_0_col_idx)==1)\n",
    "    ovrlp_intrvl_0_col_idx=ovrlp_intrvl_0_col_idx[0]\n",
    "    #-----\n",
    "    ovrlp_intrvl_1_col_idx = Utilities_df.find_idxs_in_highest_order_of_columns(return_df, ovrlp_intrvl_1_col)\n",
    "    assert(len(ovrlp_intrvl_1_col_idx)==1)\n",
    "    ovrlp_intrvl_1_col_idx=ovrlp_intrvl_1_col_idx[0]    \n",
    "    #----------\n",
    "    for i_row in range(return_df.shape[0]):\n",
    "        return_df.iloc[i_row, ovrlp_intrvl_0_col_idx] = overlaps[i_row]['min_val']\n",
    "        return_df.iloc[i_row, ovrlp_intrvl_1_col_idx] = overlaps[i_row]['max_val']\n",
    "    #-------------------------\n",
    "    if len(cols_to_collect_in_lists)>0:\n",
    "        # Add back on the list_cols_df\n",
    "        return_df = return_df.merge(\n",
    "            pd.concat([list_cols_df]*return_df.shape[0]), \n",
    "            left_on=gpd_cols, right_on=gpd_cols, how='left'\n",
    "        )\n",
    "    #-------------------------\n",
    "    if maintain_original_cols:\n",
    "        assert(len(set(og_cols).difference(set(return_df.columns)))==0)\n",
    "        return_df = return_df[og_cols]\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2595def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e497f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidate_df_group_according_to_fuzzy_overlap_intervals(\n",
    "    df_i.drop(columns=['technology_tx', 'state_cd']), \n",
    "    ovrlp_intrvl_0_col, \n",
    "    ovrlp_intrvl_1_col, \n",
    "    gpd_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb'], \n",
    "    fuzziness=pd.Timedelta('1hour'), \n",
    "    assert_single_overlap=False, \n",
    "    maintain_original_cols=False, \n",
    "    enforce_list_cols_for_1d_df=True, \n",
    "    allow_duplicates_in_lists=False, \n",
    "    allow_NaNs_in_lists=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135acdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4bad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def consolidate_df_according_to_fuzzy_overlap_intervals_NEW(\n",
    "    df, \n",
    "    ovrlp_intrvl_0_col, \n",
    "    ovrlp_intrvl_1_col, \n",
    "    fuzziness, \n",
    "    groupby_cols, \n",
    "    assert_single_overlap=True, \n",
    "    cols_to_collect_in_lists=None, \n",
    "    recover_uniqueness_violators=False, \n",
    "    gpby_dropna=True, \n",
    "    allow_duplicates_in_lists=False, \n",
    "    allow_NaNs_in_lists=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Don't want to alter df itself\n",
    "    df = df.copy()\n",
    "    og_cols = df.columns\n",
    "    #-------------------------\n",
    "    if groupby_cols is None:\n",
    "        tmp_col = Utilities.generate_random_string()\n",
    "        df[tmp_col] = 1\n",
    "        groupby_cols = [tmp_col]\n",
    "    else:\n",
    "        tmp_col = None\n",
    "    #-------------------------\n",
    "    if cols_to_collect_in_lists is None:\n",
    "        cols_to_collect_in_lists=[x for x in df.columns if x not in groupby_cols]\n",
    "    #-------------------------\n",
    "    assert(len(set(groupby_cols).intersection(set([ovrlp_intrvl_0_col, ovrlp_intrvl_1_col])))==0)\n",
    "    #-------------------------\n",
    "    # Make sure fuzziness is compatible with ovrlp_intrvl_0(1)_col\n",
    "    try:\n",
    "        test_0 = df.iloc[0][ovrlp_intrvl_0_col]+fuzziness\n",
    "        test_1 = df.iloc[0][ovrlp_intrvl_1_col]+fuzziness\n",
    "    except:\n",
    "        print(f'''\n",
    "        In consolidate_df_according_to_fuzzy_overlap_intervals: Incompatible fuzziness type\n",
    "            type(fuzziness) = {type(fuzziness)}\n",
    "            df[ovrlp_intrvl_0_col].dtype = {df[ovrlp_intrvl_0_col].dtype}\n",
    "            df[ovrlp_intrvl_1_col].dtype = {df[ovrlp_intrvl_1_col].dtype}\n",
    "        CRASH IMMINENT!\n",
    "        ''')\n",
    "        \n",
    "    #-------------------------\n",
    "    # First, make sure the second element in each tuple should be greater than the first\n",
    "    # NOTE: The second element can be NaN (either NaT for times, or NaN otherwise)\n",
    "    #         Apparently, NaN evaluates as False when compared in any manner to anything else\n",
    "    #         (i.e., anything>NaN = False, anything<NaN = False, anything==NaN = False)\n",
    "    #       Thus, df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col] will result in False whenever\n",
    "    #         df[ovrlp_intrvl_1_col] is NaN, and therefore the assertion:\n",
    "    #           assert(all(df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]))\n",
    "    #         would fail in in unwanted circumstances, since NaN value means and open ended interval\n",
    "    #         and therefore any beginning value should be considered less than a NaN value.\n",
    "    #       Therefore, instead of the single-line assertion above, I will include two assertions,\n",
    "    #         one to ensure df[ovrlp_intrvl_0_col] doesn't contain any NaNs, and one to ensure\n",
    "    #         either df[ovrlp_intrvl_1_col] is a Nan or df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]\n",
    "    # NOTE: I actually have found at least one case where ovrlp_intrvl_1_col<ovrlp_intrvl_0_col (by one second)\n",
    "    #       Instead of throwing an assertion error, I will instead print a warning and simply remove the offending entry(ies)\n",
    "    if any(df[ovrlp_intrvl_0_col].isna()):\n",
    "        print(f'''\n",
    "            !!!!! WARNING !!!!! In consolidate_df_according_to_fuzzy_overlap_intervals:\n",
    "            df[ovrlp_intrvl_0_col] has NaN values!\n",
    "            df[ovrlp_intrvl_0_col].isna().sum() = {df[ovrlp_intrvl_0_col].isna().sum()}\n",
    "            df.shape[0]                         = {df.shape[0]}\n",
    "            Row containing these NaNs will be omitted!\n",
    "        ''')\n",
    "        df = df[df[ovrlp_intrvl_0_col].notna()]\n",
    "    #-----\n",
    "    # NOTE: In printing the output below, the parentheses in (~scnd_gt_frst_srs).sum() are important!\n",
    "    #       (~scnd_gt_frst_srs).sum() != ~scnd_gt_frst_srs.sum() (the latter essentially equals -1*scnd_gt_frst_srs.sum())\n",
    "    scnd_gt_frst_srs = ((df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]) | (df[ovrlp_intrvl_1_col].isna()))\n",
    "    if any(~scnd_gt_frst_srs):\n",
    "        print(f'''\n",
    "            !!!!! WARNING !!!!! In consolidate_df_according_to_fuzzy_overlap_intervals:\n",
    "            df has values for which df[ovrlp_intrvl_0_col]>=df[ovrlp_intrvl_1_col]!\n",
    "            Number of violators = {(~scnd_gt_frst_srs).sum()}\n",
    "            df.shape[0]         = {df.shape[0]}\n",
    "            Rows containing these violators will be omitted!\n",
    "        ''')\n",
    "        df = df[scnd_gt_frst_srs]\n",
    "    # Now, at this stage assertions should both pass    \n",
    "    assert(all(df[ovrlp_intrvl_0_col].notna()))\n",
    "    assert(all((df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]) | (df[ovrlp_intrvl_1_col].isna())))\n",
    "    # Also, sort ranges, as will be necessary for this procedure\n",
    "    df = df.sort_values(by=[ovrlp_intrvl_0_col, ovrlp_intrvl_1_col])\n",
    "    #-------------------------\n",
    "    return_df = df.groupby(groupby_cols, dropna=gpby_dropna, as_index=False, group_keys=False).apply(\n",
    "        lambda x: consolidate_df_group_according_to_fuzzy_overlap_intervals(\n",
    "            df_i=x, \n",
    "            ovrlp_intrvl_0_col=ovrlp_intrvl_0_col, \n",
    "            ovrlp_intrvl_1_col=ovrlp_intrvl_1_col, \n",
    "            gpd_cols=groupby_cols, \n",
    "            fuzziness=fuzziness, \n",
    "            assert_single_overlap=assert_single_overlap, \n",
    "            maintain_original_cols=True, \n",
    "            enforce_list_cols_for_1d_df=True, \n",
    "            allow_duplicates_in_lists=allow_duplicates_in_lists,\n",
    "            allow_NaNs_in_lists=allow_NaNs_in_lists\n",
    "        )\n",
    "    )\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8325e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Utilities_df.consolidate_df_according_to_fuzzy_overlap_intervals(\n",
    "                df=df_i.drop(columns=['technology_tx', 'state_cd']), \n",
    "                ovrlp_intrvl_0_col='inst_ts', \n",
    "                ovrlp_intrvl_1_col='rmvl_ts', \n",
    "                fuzziness=pd.Timedelta('1 hour'), \n",
    "                groupby_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb'], \n",
    "                assert_single_overlap=False, \n",
    "#                 cols_to_collect_in_lists=None, \n",
    "#                 recover_uniqueness_violators=False, \n",
    "#                 gpby_dropna=gpby_dropna, \n",
    "                drop_idx_cols=True, \n",
    "#                 maintain_original_cols=True, \n",
    "#                 enforce_list_cols_for_1d_df=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consolidate_df_according_to_fuzzy_overlap_intervals_NEW(\n",
    "                df=df_i.drop(columns=['technology_tx', 'state_cd']), \n",
    "                ovrlp_intrvl_0_col='inst_ts', \n",
    "                ovrlp_intrvl_1_col='rmvl_ts', \n",
    "                fuzziness=pd.Timedelta('1 hour'), \n",
    "                groupby_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb'], \n",
    "                assert_single_overlap=False, \n",
    "#                 cols_to_collect_in_lists=None, \n",
    "#                 recover_uniqueness_violators=False, \n",
    "#                 gpby_dropna=gpby_dropna, \n",
    "#                 drop_idx_cols=True, \n",
    "#                 maintain_original_cols=True, \n",
    "#                 enforce_list_cols_for_1d_df=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d3e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa4e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mp_hist_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb0886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac683de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Utilities_df.consolidate_df_according_to_fuzzy_overlap_intervals(\n",
    "                df=df_i, \n",
    "                ovrlp_intrvl_0_col='inst_ts', \n",
    "                ovrlp_intrvl_1_col='rmvl_ts', \n",
    "                fuzziness=pd.Timedelta('1 hour'), \n",
    "                groupby_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb'], \n",
    "                assert_single_overlap=False, \n",
    "#                 cols_to_collect_in_lists=None, \n",
    "#                 recover_uniqueness_violators=False, \n",
    "#                 gpby_dropna=gpby_dropna, \n",
    "                drop_idx_cols=True, \n",
    "#                 maintain_original_cols=True, \n",
    "#                 enforce_list_cols_for_1d_df=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694a810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consolidate_df_according_to_fuzzy_overlap_intervals_NEW(\n",
    "                df=df_i, \n",
    "                ovrlp_intrvl_0_col='inst_ts', \n",
    "                ovrlp_intrvl_1_col='rmvl_ts', \n",
    "                fuzziness=pd.Timedelta('1 hour'), \n",
    "                groupby_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb'], \n",
    "                assert_single_overlap=False, \n",
    "#                 cols_to_collect_in_lists=None, \n",
    "#                 recover_uniqueness_violators=False, \n",
    "#                 gpby_dropna=gpby_dropna, \n",
    "#                 drop_idx_cols=True, \n",
    "#                 maintain_original_cols=True, \n",
    "#                 enforce_list_cols_for_1d_df=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd327a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884eb818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e27bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345cdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d52cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3c531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovrlp_intrvl_0_col='inst_ts'\n",
    "ovrlp_intrvl_1_col='rmvl_ts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i[\n",
    "    [x for x in df_i.columns.tolist() \n",
    "     if x not in [ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]]\n",
    "].nunique()<=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Utilities_df.consolidate_df_according_to_fuzzy_overlap_intervals(\n",
    "                df=df_i, \n",
    "                ovrlp_intrvl_0_col='inst_ts', \n",
    "                ovrlp_intrvl_1_col='rmvl_ts', \n",
    "                fuzziness=pd.Timedelta('1 hour'), \n",
    "                groupby_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb', 'technology_tx'], \n",
    "                assert_single_overlap=False, \n",
    "#                 cols_to_collect_in_lists=None, \n",
    "#                 recover_uniqueness_violators=False, \n",
    "#                 gpby_dropna=gpby_dropna, \n",
    "#                 drop_idx_cols=True, \n",
    "#                 maintain_original_cols=True, \n",
    "#                 enforce_list_cols_for_1d_df=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consolidate_df_according_to_fuzzy_overlap_intervals_NEW(\n",
    "                df=df_i, \n",
    "                ovrlp_intrvl_0_col='inst_ts', \n",
    "                ovrlp_intrvl_1_col='rmvl_ts', \n",
    "                fuzziness=pd.Timedelta('1 hour'), \n",
    "                groupby_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb', 'technology_tx'], \n",
    "                assert_single_overlap=False, \n",
    "#                 cols_to_collect_in_lists=None, \n",
    "#                 recover_uniqueness_violators=False, \n",
    "#                 gpby_dropna=gpby_dropna, \n",
    "#                 drop_idx_cols=True, \n",
    "#                 maintain_original_cols=True, \n",
    "#                 enforce_list_cols_for_1d_df=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f97af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consolidate_df_according_to_fuzzy_overlap_intervals_NEW(\n",
    "                df=df_i, \n",
    "                ovrlp_intrvl_0_col='inst_ts', \n",
    "                ovrlp_intrvl_1_col='rmvl_ts', \n",
    "                fuzziness=pd.Timedelta('1 hour'), \n",
    "                groupby_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb', 'technology_tx', 'state_cd'], \n",
    "                assert_single_overlap=False, \n",
    "#                 cols_to_collect_in_lists=None, \n",
    "#                 recover_uniqueness_violators=False, \n",
    "#                 gpby_dropna=gpby_dropna, \n",
    "#                 drop_idx_cols=True, \n",
    "#                 maintain_original_cols=True, \n",
    "#                 enforce_list_cols_for_1d_df=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbeba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49809d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a0458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp_hist_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Utilities_df.consolidate_df_according_to_fuzzy_overlap_intervals(\n",
    "                df=df_mp_hist_exp.drop(columns=['technology_tx', 'state_cd']), \n",
    "                ovrlp_intrvl_0_col='inst_ts', \n",
    "                ovrlp_intrvl_1_col='rmvl_ts', \n",
    "                fuzziness=pd.Timedelta('1 hour'), \n",
    "                groupby_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb'], \n",
    "                assert_single_overlap=False, \n",
    "#                 cols_to_collect_in_lists=None, \n",
    "#                 recover_uniqueness_violators=False, \n",
    "#                 gpby_dropna=gpby_dropna, \n",
    "                drop_idx_cols=True, \n",
    "#                 maintain_original_cols=True, \n",
    "#                 enforce_list_cols_for_1d_df=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f367146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ab932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consolidate_df_according_to_fuzzy_overlap_intervals_NEW(\n",
    "                df=df_mp_hist_exp.drop(columns=['technology_tx', 'state_cd']), \n",
    "                ovrlp_intrvl_0_col='inst_ts', \n",
    "                ovrlp_intrvl_1_col='rmvl_ts', \n",
    "                fuzziness=pd.Timedelta('1 hour'), \n",
    "                groupby_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb'], \n",
    "                assert_single_overlap=False, \n",
    "#                 cols_to_collect_in_lists=None, \n",
    "#                 recover_uniqueness_violators=False, \n",
    "#                 gpby_dropna=gpby_dropna, \n",
    "#                 drop_idx_cols=True, \n",
    "#                 maintain_original_cols=True, \n",
    "#                 enforce_list_cols_for_1d_df=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33663b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp_hist_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a04de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "consol2 = consolidate_df_according_to_fuzzy_overlap_intervals_NEW(\n",
    "                df=df_mp_hist_exp, \n",
    "                ovrlp_intrvl_0_col='inst_ts', \n",
    "                ovrlp_intrvl_1_col='rmvl_ts', \n",
    "                fuzziness=pd.Timedelta('1 hour'), \n",
    "                groupby_cols=['mfr_devc_ser_nbr', 'prem_nb', 'trsf_pole_nb'], \n",
    "                assert_single_overlap=False, \n",
    "#                 cols_to_collect_in_lists=None, \n",
    "#                 recover_uniqueness_violators=False, \n",
    "#                 gpby_dropna=gpby_dropna, \n",
    "#                 drop_idx_cols=True, \n",
    "#                 maintain_original_cols=True, \n",
    "#                 enforce_list_cols_for_1d_df=True\n",
    "            )\n",
    "print(time.time()-start)\n",
    "consol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8149cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        return_df_w_dups = Utilities_df.consolidate_df_according_to_fuzzy_overlap_intervals(\n",
    "            df=return_df_w_dups, \n",
    "            ovrlp_intrvl_0_col=df_mp_install_time_col, \n",
    "            ovrlp_intrvl_1_col=df_mp_removal_time_col, \n",
    "            fuzziness=fuzziness, \n",
    "            groupby_cols=groupby_cols, \n",
    "            assert_single_overlap=assert_single_overlap, \n",
    "            cols_to_collect_in_lists=cols_to_collect_in_lists, \n",
    "            recover_uniqueness_violators=False, \n",
    "            gpby_dropna=gpby_dropna, \n",
    "            allow_duplicates_in_lists=False, \n",
    "            allow_NaNs_in_lists=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bab3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1091625a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1d88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d217a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349eddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af282bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e64366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee2f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39706b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ead22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f76df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7697eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549a36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec630e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlap_intervals_in_df(\n",
    "    df, \n",
    "    ovrlp_intrvl_0_col, \n",
    "    ovrlp_intrvl_1_col, \n",
    "    fuzziness, \n",
    "    int_idxs=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a pd.DataFrame df with intervals defined by starting values and ending values in columns ovrlp_intrvl_0_col\n",
    "      and ovrlp_intrvl_1_col, respectively, find the reduced set of overlap intervals.\n",
    "    The fuzziness argument sets how close two intervals must be to be considered overlapping (see Utilities.get_fuzzy_overlap_intervals \n",
    "      for more information.)\n",
    "    Returns a list of dict objects, one for each overlap interval.\n",
    "        The keys for each dict object are 'min_val', 'max_val', and 'idxs',  \n",
    "        By default (when int_idxs==True) 'idxs' correspond to the integer index locations of the rows included in the overlap.\n",
    "        If int_idxs==False, the index labels are instead used.\n",
    "          \n",
    "    NOTE: The function/operation calling this method should ensure all values are appropriate, meaning ensure that for each row\n",
    "            row_i[ovrlp_intrvl_1_col] > row_i[ovrlp_intrvl_0_col]\n",
    "          This method will not attempt to remedy any incorrect values, but will simply assert this is true\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure fuzziness is compatible with ovrlp_intrvl_0(1)_col\n",
    "    try:\n",
    "        test_0 = df.iloc[0][ovrlp_intrvl_0_col]+fuzziness\n",
    "        test_1 = df.iloc[0][ovrlp_intrvl_1_col]+fuzziness\n",
    "    except:\n",
    "        print(f'''\n",
    "        In consolidate_df_according_to_fuzzy_overlap_intervals: Incompatible fuzziness type\n",
    "            type(fuzziness) = {type(fuzziness)}\n",
    "            df[ovrlp_intrvl_0_col].dtype = {df[ovrlp_intrvl_0_col].dtype}\n",
    "            df[ovrlp_intrvl_1_col].dtype = {df[ovrlp_intrvl_1_col].dtype}\n",
    "        CRASH IMMINENT!\n",
    "        ''')\n",
    "    #-------------------------\n",
    "    # First, make sure the second element in each tuple should be greater than the first\n",
    "    # NOTE: The second element can be NaN (either NaT for times, or NaN otherwise)\n",
    "    #         Apparently, NaN evaluates as False when compared in any manner to anything else\n",
    "    #         (i.e., anything>NaN = False, anything<NaN = False, anything==NaN = False)\n",
    "    #       Thus, df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col] will result in False whenever\n",
    "    #         df[ovrlp_intrvl_1_col] is NaN, and therefore the assertion:\n",
    "    #           assert(all(df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]))\n",
    "    #         would fail in in unwanted circumstances, since NaN value means and open ended interval\n",
    "    #         and therefore any beginning value should be considered less than a NaN value.\n",
    "    #       Therefore, instead of the single-line assertion above, I will include two assertions,\n",
    "    #         one to ensure df[ovrlp_intrvl_0_col] doesn't contain any NaNs, and one to ensure\n",
    "    #         either df[ovrlp_intrvl_1_col] is a Nan or df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]\n",
    "    assert(all(df[ovrlp_intrvl_0_col].notna()))\n",
    "    assert(all((df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]) | (df[ovrlp_intrvl_1_col].isna())))\n",
    "    # For above assertion, probably could have isntead used: \n",
    "    #   assert(all(df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col].fillna(pd.Timestamp.max)))\n",
    "    #-------------------------\n",
    "    # Sort ranges, as will be necessary for this procedure\n",
    "    df = df.sort_values(by=[ovrlp_intrvl_0_col, ovrlp_intrvl_1_col])\n",
    "    #-------------------------\n",
    "    # Set the first range in overlaps simply as from the first entry in df\n",
    "    overlaps = []\n",
    "    current_beg, current_end = df.iloc[0][[ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]]\n",
    "    overlaps.append(\n",
    "        dict(min_val=current_beg, max_val=current_end)\n",
    "    )\n",
    "    if int_idxs:\n",
    "        overlaps[0]['idxs'] = [0]\n",
    "    else:\n",
    "        overlaps[0]['idxs'] = [df.index[0]]\n",
    "    #-------------------------\n",
    "    # Iterate through and create the overlaps items, each of which will be a dict with key\n",
    "    #   equal to min_val, max_val, and idxs (where idxs identifies which indices from df\n",
    "    #   belong to each overlap group)\n",
    "    if int_idxs:\n",
    "        df_to_itr = df.reset_index()\n",
    "    else:\n",
    "        df_to_itr = df\n",
    "    #-----\n",
    "    for i, (idx, row) in enumerate(df_to_itr.iterrows()):\n",
    "        if i==0:\n",
    "            continue\n",
    "        #---------------\n",
    "        beg = row[ovrlp_intrvl_0_col]\n",
    "        end = row[ovrlp_intrvl_1_col]\n",
    "        if beg > current_end+fuzziness:\n",
    "            # beg after current end (with fuzziness buffer), so new interval needed\n",
    "            # NOTE: beg > current_end+fuzziness will evaluate to False whenever current_end\n",
    "            #       is NaN, which is the desired functionality.\n",
    "            overlaps.append(dict(min_val=beg, max_val=end, idxs=[idx]))\n",
    "            current_beg, current_end = beg, end\n",
    "        else:\n",
    "            # beg <= current_end+fuzziness, so overlap\n",
    "            # The beg of overlaps[-1] remains the same, but the end of overlaps[-1] should be changed to\n",
    "            #   the max of current_end and end.\n",
    "            # Also, idx needs to be added to the overlap\n",
    "            # NOTE: max(any_non_NaT, NaT) = any_non_NaT (remember, NaN evaluates as False when compared in any manner to anything else), \n",
    "            #       which is not the funcionality I want, as an end of NaT essentially means no end (e.g., a meter which is still in service), \n",
    "            #       and should therefore be treated as Inf.  Thus, cannot simply use the one-liner 'current_end = max(current_end, end)'\n",
    "            # NOTE 2: Cannot simply do 'if pd.isna(current_end) or pd.isna(end)' in single line because this function is\n",
    "            #         designed to work with various data types, so in such a scenario it would be unclear what to set\n",
    "            #         current_end to (e.g., should it be pd.NaT, pd.NaN, etc?).\n",
    "            #         Thus, instead of if-else, need if-elif-else\n",
    "            #current_end = max(current_end, end)\n",
    "            if pd.isna(current_end):\n",
    "                current_end=current_end\n",
    "            elif pd.isna(end):\n",
    "                current_end = end\n",
    "            else:\n",
    "                current_end = max(current_end, end)\n",
    "            overlaps[-1]['max_val'] = current_end\n",
    "            overlaps[-1]['idxs'].append(idx)\n",
    "    #-------------------------\n",
    "    return overlaps\n",
    "\n",
    "def consolidate_df_group_according_to_fuzzy_overlap_intervals(\n",
    "    df_i, \n",
    "    ovrlp_intrvl_0_col, \n",
    "    ovrlp_intrvl_1_col, \n",
    "    gpd_cols, \n",
    "    fuzziness, \n",
    "    assert_single_overlap=False, \n",
    "    maintain_original_cols=False, \n",
    "    enforce_list_cols_for_1d_df=True, \n",
    "    allow_duplicates_in_lists=False, \n",
    "    allow_NaNs_in_lists=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    This is for the specific case of a df group.\n",
    "    It is expected, and enforced, that there exists a single unique value for each column in df_i outside of\n",
    "      ovrlp_intrvl_0_col and ovrlp_intrvl_1_col\n",
    "      \n",
    "    gpd_cols:\n",
    "        For the typical case where this function is used inside of a groupby().apply(lambda x:) function, the\n",
    "          gpd_cols should match those input into groupby (for a typical use case, see \n",
    "          Utilities_df.consolidate_df_according_to_fuzzy_overlap_intervals)\n",
    "        Each of these columns should contain a single unique value.\n",
    "        This input is needed so the function knows which columns to collect in lists (those outside of gpd_cols+\n",
    "          [ovrlp_intrvl_0_col, ovrlp_intrvl_1_col])\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if not isinstance(gpd_cols, list):\n",
    "        gpd_cols = [gpd_cols]\n",
    "    #-------------------------\n",
    "    assert(len(set(gpd_cols).intersection(set([ovrlp_intrvl_0_col, ovrlp_intrvl_1_col])))==0)\n",
    "    assert((df_i[gpd_cols].nunique()<=1).all())\n",
    "    cols_to_collect_in_lists = [x for x in df_i.columns.tolist() if x not in gpd_cols+[ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]]\n",
    "    #-------------------------\n",
    "    og_cols = df_i.columns\n",
    "    #-------------------------\n",
    "    if df_i.shape[0]<=1 and maintain_original_cols and len(cols_to_collect_in_lists)>0: \n",
    "        # Reason for maintain_original_cols:\n",
    "        #   If this is used in .groupby, it is important for all to have same shape/labelling.\n",
    "        #   So, if maintain_original_cols is False, in order for a df_i with df_i.shape[0]==1 to fit\n",
    "        #     into the .groupby procedure, it must go through the steps below to ensure it has the same\n",
    "        #     form as the other groups.\n",
    "        if enforce_list_cols_for_1d_df:\n",
    "        # Make any cols_to_collect_in_lists into lists EXCEPT ovrlp_intrvl_0_col and ovrlp_intrvl_1_col...\n",
    "            lst_cols_1d = [x for x in cols_to_collect_in_lists if x not in [ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]] #1d for 1-dimensional DF\n",
    "            df_i[lst_cols_1d] = df_i[lst_cols_1d].apply(lambda x: [x.tolist()])\n",
    "        return df_i\n",
    "    #-------------------------\n",
    "    if len(cols_to_collect_in_lists)>0:\n",
    "        # Collect the values from cols_to_collect_in_lists, and remove from df_i\n",
    "        if allow_NaNs_in_lists:\n",
    "            if allow_duplicates_in_lists:\n",
    "                agg_func = agg_func_list\n",
    "            else:\n",
    "                agg_func = agg_func_unq_list\n",
    "        else:\n",
    "            if allow_duplicates_in_lists:\n",
    "                agg_func = agg_func_list_dropna\n",
    "            else:\n",
    "                agg_func = agg_func_unq_list_dropna\n",
    "        list_cols_df = df_i.groupby(gpd_cols, as_index=False, group_keys=False)[cols_to_collect_in_lists].agg(lambda x: agg_func(x))\n",
    "        assert(list_cols_df.shape[0]==1)\n",
    "        #-----\n",
    "        df_i = df_i.drop(columns=cols_to_collect_in_lists)\n",
    "    #-------------------------\n",
    "    overlaps = find_overlap_intervals_in_df(\n",
    "        df=df_i, \n",
    "        ovrlp_intrvl_0_col=ovrlp_intrvl_0_col, \n",
    "        ovrlp_intrvl_1_col=ovrlp_intrvl_1_col, \n",
    "        fuzziness=fuzziness, \n",
    "        int_idxs=True\n",
    "    )\n",
    "    #-------------------------\n",
    "    if assert_single_overlap and len(overlaps)!=1:\n",
    "        print(f'assert_single_overlap and len(overlaps)={len(overlaps)}')\n",
    "        print(overlaps)\n",
    "        print(f'df_i.head():\\n{df_i.head()}')\n",
    "        assert(0)\n",
    "    #-------------------------\n",
    "    # The number of overlaps must be less than or equal to the shape of df_i\n",
    "    assert(len(overlaps) <= df_i.shape[0])\n",
    "\n",
    "    # Already know (assertion at top of function) that all columns are uniform except for [ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]\n",
    "    # Therefore, for return df, just grab as many rows as needed from df_i, and replace the [ovrlp_intrvl_0_col, ovrlp_intrvl_1_col]\n",
    "    #   values with those from overlaps\n",
    "    return_df = df_i.iloc[0:len(overlaps)].copy()\n",
    "    assert(return_df.shape[0]==len(overlaps))\n",
    "    #-------------------------\n",
    "    # Determine the index positions of ovrlp_intrvl_0_col and ovrlp_intrvl_1_col, as these will be needed\n",
    "    #   below to set to values in the consolidated DF using iloc\n",
    "    ovrlp_intrvl_0_col_idx = find_idxs_in_highest_order_of_columns(return_df, ovrlp_intrvl_0_col)\n",
    "    assert(len(ovrlp_intrvl_0_col_idx)==1)\n",
    "    ovrlp_intrvl_0_col_idx=ovrlp_intrvl_0_col_idx[0]\n",
    "    #-----\n",
    "    ovrlp_intrvl_1_col_idx = find_idxs_in_highest_order_of_columns(return_df, ovrlp_intrvl_1_col)\n",
    "    assert(len(ovrlp_intrvl_1_col_idx)==1)\n",
    "    ovrlp_intrvl_1_col_idx=ovrlp_intrvl_1_col_idx[0]    \n",
    "    #----------\n",
    "    for i_row in range(return_df.shape[0]):\n",
    "        return_df.iloc[i_row, ovrlp_intrvl_0_col_idx] = overlaps[i_row]['min_val']\n",
    "        return_df.iloc[i_row, ovrlp_intrvl_1_col_idx] = overlaps[i_row]['max_val']\n",
    "    #-------------------------\n",
    "    if len(cols_to_collect_in_lists)>0:\n",
    "        # Add back on the list_cols_df\n",
    "        return_df = return_df.merge(\n",
    "            pd.concat([list_cols_df]*return_df.shape[0]), \n",
    "            left_on=gpd_cols, right_on=gpd_cols, how='left'\n",
    "        )\n",
    "    #-------------------------\n",
    "    if maintain_original_cols:\n",
    "        assert(len(set(og_cols).difference(set(return_df.columns)))==0)\n",
    "        return_df = return_df[og_cols]\n",
    "    #-------------------------\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def consolidate_df_according_to_fuzzy_overlap_intervals(\n",
    "    df, \n",
    "    ovrlp_intrvl_0_col, \n",
    "    ovrlp_intrvl_1_col, \n",
    "    fuzziness, \n",
    "    groupby_cols, \n",
    "    assert_single_overlap=True, \n",
    "    cols_to_collect_in_lists=None, \n",
    "    recover_uniqueness_violators=False, \n",
    "    gpby_dropna=True, \n",
    "    allow_duplicates_in_lists=False, \n",
    "    allow_NaNs_in_lists=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Don't want to alter df itself\n",
    "    df = df.copy()\n",
    "    og_cols = df.columns\n",
    "    #-------------------------\n",
    "    if groupby_cols is None:\n",
    "        tmp_col = Utilities.generate_random_string()\n",
    "        df[tmp_col] = 1\n",
    "        groupby_cols = [tmp_col]\n",
    "    else:\n",
    "        tmp_col = None\n",
    "    #-------------------------\n",
    "    if cols_to_collect_in_lists is None:\n",
    "        cols_to_collect_in_lists=[x for x in df.columns if x not in groupby_cols]\n",
    "    #-------------------------\n",
    "    assert(len(set(groupby_cols).intersection(set([ovrlp_intrvl_0_col, ovrlp_intrvl_1_col])))==0)\n",
    "    #-------------------------\n",
    "    # Make sure fuzziness is compatible with ovrlp_intrvl_0(1)_col\n",
    "    try:\n",
    "        test_0 = df.iloc[0][ovrlp_intrvl_0_col]+fuzziness\n",
    "        test_1 = df.iloc[0][ovrlp_intrvl_1_col]+fuzziness\n",
    "    except:\n",
    "        print(f'''\n",
    "        In consolidate_df_according_to_fuzzy_overlap_intervals: Incompatible fuzziness type\n",
    "            type(fuzziness) = {type(fuzziness)}\n",
    "            df[ovrlp_intrvl_0_col].dtype = {df[ovrlp_intrvl_0_col].dtype}\n",
    "            df[ovrlp_intrvl_1_col].dtype = {df[ovrlp_intrvl_1_col].dtype}\n",
    "        CRASH IMMINENT!\n",
    "        ''')\n",
    "        \n",
    "    #-------------------------\n",
    "    # First, make sure the second element in each tuple should be greater than the first\n",
    "    # NOTE: The second element can be NaN (either NaT for times, or NaN otherwise)\n",
    "    #         Apparently, NaN evaluates as False when compared in any manner to anything else\n",
    "    #         (i.e., anything>NaN = False, anything<NaN = False, anything==NaN = False)\n",
    "    #       Thus, df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col] will result in False whenever\n",
    "    #         df[ovrlp_intrvl_1_col] is NaN, and therefore the assertion:\n",
    "    #           assert(all(df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]))\n",
    "    #         would fail in in unwanted circumstances, since NaN value means and open ended interval\n",
    "    #         and therefore any beginning value should be considered less than a NaN value.\n",
    "    #       Therefore, instead of the single-line assertion above, I will include two assertions,\n",
    "    #         one to ensure df[ovrlp_intrvl_0_col] doesn't contain any NaNs, and one to ensure\n",
    "    #         either df[ovrlp_intrvl_1_col] is a Nan or df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]\n",
    "    # NOTE: I actually have found at least one case where ovrlp_intrvl_1_col<ovrlp_intrvl_0_col (by one second)\n",
    "    #       Instead of throwing an assertion error, I will instead print a warning and simply remove the offending entry(ies)\n",
    "    if any(df[ovrlp_intrvl_0_col].isna()):\n",
    "        print(f'''\n",
    "            !!!!! WARNING !!!!! In consolidate_df_according_to_fuzzy_overlap_intervals:\n",
    "            df[ovrlp_intrvl_0_col] has NaN values!\n",
    "            df[ovrlp_intrvl_0_col].isna().sum() = {df[ovrlp_intrvl_0_col].isna().sum()}\n",
    "            df.shape[0]                         = {df.shape[0]}\n",
    "            Row containing these NaNs will be omitted!\n",
    "        ''')\n",
    "        df = df[df[ovrlp_intrvl_0_col].notna()]\n",
    "    #-----\n",
    "    # NOTE: In printing the output below, the parentheses in (~scnd_gt_frst_srs).sum() are important!\n",
    "    #       (~scnd_gt_frst_srs).sum() != ~scnd_gt_frst_srs.sum() (the latter essentially equals -1*scnd_gt_frst_srs.sum())\n",
    "    scnd_gt_frst_srs = ((df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]) | (df[ovrlp_intrvl_1_col].isna()))\n",
    "    if any(~scnd_gt_frst_srs):\n",
    "        print(f'''\n",
    "            !!!!! WARNING !!!!! In consolidate_df_according_to_fuzzy_overlap_intervals:\n",
    "            df has values for which df[ovrlp_intrvl_0_col]>=df[ovrlp_intrvl_1_col]!\n",
    "            Number of violators = {(~scnd_gt_frst_srs).sum()}\n",
    "            df.shape[0]         = {df.shape[0]}\n",
    "            Rows containing these violators will be omitted!\n",
    "        ''')\n",
    "        df = df[scnd_gt_frst_srs]\n",
    "    # Now, at this stage assertions should both pass    \n",
    "    assert(all(df[ovrlp_intrvl_0_col].notna()))\n",
    "    assert(all((df[ovrlp_intrvl_0_col]<df[ovrlp_intrvl_1_col]) | (df[ovrlp_intrvl_1_col].isna())))\n",
    "    # Also, sort ranges, as will be necessary for this procedure\n",
    "    df = df.sort_values(by=[ovrlp_intrvl_0_col, ovrlp_intrvl_1_col])\n",
    "    #-------------------------\n",
    "    return_df = df.groupby(groupby_cols, dropna=gpby_dropna, as_index=False, group_keys=False).apply(\n",
    "        lambda x: consolidate_df_group_according_to_fuzzy_overlap_intervals(\n",
    "            df_i=x, \n",
    "            ovrlp_intrvl_0_col=ovrlp_intrvl_0_col, \n",
    "            ovrlp_intrvl_1_col=ovrlp_intrvl_1_col, \n",
    "            gpd_cols=groupby_cols, \n",
    "            fuzziness=fuzziness, \n",
    "            assert_single_overlap=assert_single_overlap, \n",
    "            maintain_original_cols=True, \n",
    "            enforce_list_cols_for_1d_df=True, \n",
    "            allow_duplicates_in_lists=allow_duplicates_in_lists,\n",
    "            allow_NaNs_in_lists=allow_NaNs_in_lists\n",
    "        )\n",
    "    )\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3091e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c3287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mico_df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mico_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83098bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mico_df['OUTAGE_NB'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00209111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6486b75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d97525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
