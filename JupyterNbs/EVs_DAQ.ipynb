{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444718d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns\n",
    "from packaging import version\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "import Utilities_dt\n",
    "from Utilities_df import DFConstructType\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import GrubbsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c07d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "660dd359",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "- TARF_CD==59\n",
    "    - submetered\n",
    "    - the main meter is accounting for the EV meter as well\n",
    "    - Both meters are fed from the same drop. \n",
    "    - If you wanted just the house load, you would need to subtract the EV meter. \n",
    "    - If the tariff says separately metered (donâ€™t think I included any) they are fed off separate drops and the main meter does not account the EV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98697a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ea885",
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_trff_df = pd.read_csv(r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Tariff\\apc_tarf_EV.csv')\n",
    "imp_trff_df = pd.read_csv(r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Tariff\\imp_tarf_EV.csv')\n",
    "oh_trff_df = pd.read_csv(r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Tariff\\oh_tarf_EV.csv')\n",
    "pso_trff_df = pd.read_csv(r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Tariff\\pso_tarf_EV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4caeeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(apc_trff_df.shape[0])\n",
    "print(imp_trff_df.shape[0])\n",
    "print(oh_trff_df.shape[0])\n",
    "print(pso_trff_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf407cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "opco_trff_dfs = dict(\n",
    "    apc=apc_trff_df, \n",
    "    imp=imp_trff_df, \n",
    "    oh=oh_trff_df, \n",
    "    pso=pso_trff_df, \n",
    ")\n",
    "#-------------------------\n",
    "# Add 'opco' column\n",
    "# Make sure all PREM_NB values have 9 characters\n",
    "for opco_i in opco_trff_dfs.keys():\n",
    "    trff_df_i = opco_trff_dfs[opco_i]\n",
    "    #-----\n",
    "    trff_df_i['opco'] = opco_i\n",
    "    #-----\n",
    "    trff_df_i['PREM_NB'] = trff_df_i['PREM_NB'].astype(str)\n",
    "    trff_df_i['PREM_NB'] = trff_df_i['PREM_NB'].str.zfill(9)\n",
    "    #-----\n",
    "    opco_trff_dfs[opco_i] = trff_df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89032261",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_base = r'C:\\Users\\s346557\\Documents\\LocalData\\EVs'\n",
    "# save_dir_base = r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Recent_Data'\n",
    "expand_time = pd.Timedelta('60 days')\n",
    "\n",
    "if not os.path.exists(save_dir_base):\n",
    "    os.makedirs(save_dir_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a15857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trff_dfs = pd.concat(list(opco_trff_dfs.values()))\n",
    "#--------------------------------------------------\n",
    "print('Before removing mult. EV vals')\n",
    "print(f'\\tall_trff_dfs.shape[0] = {all_trff_dfs.shape[0]}')\n",
    "print(f\"\\tnPNs = {all_trff_dfs['PREM_NB'].nunique()}\")\n",
    "#-------------------------\n",
    "# Some premises have multiple entries, as expected for various tariffs coming into effect.\n",
    "#   More importantly, some premises have multiple values in the EV column.\n",
    "#   In this case, we want to treat the premise as an EV prem, and keep only the EV entry\n",
    "# Find the PNs with multiple EV values (0 and 1), and separate out from all_trff_dfs\n",
    "n_EV_vals_per_PN = all_trff_dfs.groupby('PREM_NB')['EV'].nunique()\n",
    "assert(not (n_EV_vals_per_PN>2).any())\n",
    "PNs_w_mult_EV_vals = n_EV_vals_per_PN[n_EV_vals_per_PN==2].index.tolist()\n",
    "#-----\n",
    "trff_df_PNs_w_mult_EV_vals = all_trff_dfs[all_trff_dfs['PREM_NB'].isin(PNs_w_mult_EV_vals)]\n",
    "trff_df_PNs_w_singl_EV_val = all_trff_dfs[~all_trff_dfs['PREM_NB'].isin(PNs_w_mult_EV_vals)]\n",
    "assert(trff_df_PNs_w_mult_EV_vals.shape[0]+trff_df_PNs_w_singl_EV_val.shape[0]==all_trff_dfs.shape[0])\n",
    "#-------------------------\n",
    "# In trff_df_PNs_w_mult_EV_vals, keep only the EV==1 values\n",
    "#-----\n",
    "# tmp_nPNs for sanity check\n",
    "tmp_nPNs = trff_df_PNs_w_mult_EV_vals['PREM_NB'].nunique()\n",
    "trff_df_PNs_w_mult_EV_vals=trff_df_PNs_w_mult_EV_vals[trff_df_PNs_w_mult_EV_vals['EV']==1]\n",
    "assert(trff_df_PNs_w_mult_EV_vals['PREM_NB'].nunique()==tmp_nPNs)\n",
    "#-----\n",
    "# If a premise had multiple EV==1 values, the assertion below would fail\n",
    "# In which case, I would probably want to keep the most recent entry and remove the other(s)\n",
    "# However, I have not observed such a situation in the current data, so leave as assertion\n",
    "assert(trff_df_PNs_w_mult_EV_vals['PREM_NB'].nunique()==trff_df_PNs_w_mult_EV_vals.shape[0])\n",
    "#-------------------------\n",
    "# Join trff_df_PNs_w_mult_EV_vals and trff_df_PNs_w_singl_EV_val to restore all_trff_dfs\n",
    "all_trff_dfs = pd.concat([trff_df_PNs_w_mult_EV_vals, trff_df_PNs_w_singl_EV_val])\n",
    "#-------------------------\n",
    "print('After removing mult. EV vals')\n",
    "print(f'\\tall_trff_dfs.shape[0] = {all_trff_dfs.shape[0]}')\n",
    "print(f\"\\tnPNs = {all_trff_dfs['PREM_NB'].nunique()}\")\n",
    "#--------------------------------------------------\n",
    "all_trff_dfs_evs = all_trff_dfs[all_trff_dfs['EV']==1].copy()\n",
    "all_trff_dfs_non = all_trff_dfs[all_trff_dfs['EV']==0].copy()\n",
    "assert(all_trff_dfs.shape[0]==(all_trff_dfs_evs.shape[0]+all_trff_dfs_non.shape[0]))\n",
    "#-------------------------\n",
    "# PNs in all_trff_dfs_evs should all have a single row, due to the procedure above\n",
    "assert(all_trff_dfs_evs['PREM_NB'].nunique()==all_trff_dfs_evs['PREM_NB'].shape[0])\n",
    "#-----\n",
    "# PNs in all_trff_dfs_non can have multiple rows corresponding to multiple tariffs\n",
    "# If this is the case, keep the most recent\n",
    "if all_trff_dfs_non['PREM_NB'].nunique()!=all_trff_dfs_non['PREM_NB'].shape[0]:\n",
    "    all_trff_dfs_non = all_trff_dfs_non.sort_values(by=['TARF_EFCT_TS'], ascending=False).groupby(['PREM_NB'], as_index=False, group_keys=False).first()\n",
    "assert(all_trff_dfs_non['PREM_NB'].nunique()==all_trff_dfs_non['PREM_NB'].shape[0])\n",
    "\n",
    "#--------------------------------------------------\n",
    "all_trff_dfs_evs['t_search_min'] = pd.to_datetime(all_trff_dfs_evs['TARF_EFCT_TS'])\n",
    "all_trff_dfs_evs['t_search_max'] = all_trff_dfs_evs['t_search_min'] + pd.Timedelta(expand_time)\n",
    "#-------------------------\n",
    "# !!!!!!!!!!! cds_ds_db.ev_ami_final only contains data from 2022-04-01 to 2023-04-01!\n",
    "# So, any entries in all_trff_dfs_evs with search windows outside of this will need to be adjusted!\n",
    "available_data_min = pd.to_datetime('2022-04-01')\n",
    "available_data_max = pd.to_datetime('2023-04-01')\n",
    "tmp_shape = all_trff_dfs_evs.shape[0]\n",
    "all_trff_dfs_evs_to_adjst = all_trff_dfs_evs[\n",
    "    (all_trff_dfs_evs['t_search_min']<available_data_min) |\n",
    "    (all_trff_dfs_evs['t_search_max']>available_data_max)\n",
    "].copy()\n",
    "all_trff_dfs_evs_no_adjst = all_trff_dfs_evs[\n",
    "    (all_trff_dfs_evs['t_search_min']>=available_data_min) &\n",
    "    (all_trff_dfs_evs['t_search_max']<=available_data_max)\n",
    "].copy()\n",
    "assert(all_trff_dfs_evs_to_adjst.shape[0]+all_trff_dfs_evs_no_adjst.shape[0]==all_trff_dfs_evs.shape[0])\n",
    "#-----\n",
    "all_trff_dfs_evs_to_adjst[['t_search_min', 't_search_max']] = [\n",
    "    Utilities_dt.get_random_datetime_interval_between(available_data_min, available_data_max, pd.Timedelta(expand_time)) \n",
    "    for _ in range(all_trff_dfs_evs_to_adjst.shape[0])\n",
    "]\n",
    "#-----\n",
    "all_trff_dfs_evs = pd.concat([all_trff_dfs_evs_to_adjst, all_trff_dfs_evs_no_adjst])\n",
    "assert(all_trff_dfs_evs.shape[0]==tmp_shape)\n",
    "#--------------------------------------------------\n",
    "# For non-EV entries, collect events from the general time period as EVs\n",
    "#   NOTE: Due to the restrictions of cds_ds_db.ev_ami_final (available_data_min/max above), instead of \n",
    "#         grabbing EVS_t_min/max from TARF_EFCT_TS, the values should be grabbed from t_search_min/max\n",
    "EVS_t_min = pd.to_datetime(all_trff_dfs_evs['t_search_min']).min()\n",
    "EVS_t_max = pd.to_datetime(all_trff_dfs_evs['t_search_max']).max()\n",
    "#-----\n",
    "all_trff_dfs_non[['t_search_min', 't_search_max']] = [\n",
    "    Utilities_dt.get_random_datetime_interval_between(EVS_t_min, EVS_t_max, pd.Timedelta(expand_time)) \n",
    "    for _ in range(all_trff_dfs_non.shape[0])\n",
    "]\n",
    "#--------------------------------------------------\n",
    "all_trff_dfs = pd.concat([all_trff_dfs_evs, all_trff_dfs_non])\n",
    "assert(all_trff_dfs['PREM_NB'].nunique()==all_trff_dfs['PREM_NB'].shape[0])\n",
    "#--------------------------------------------------\n",
    "print(f\"all_trff_dfs['t_search_min'].min() = {all_trff_dfs['t_search_min'].min()}\")\n",
    "print(f\"all_trff_dfs['t_search_max'].max() = {all_trff_dfs['t_search_max'].max()}\")\n",
    "#-----\n",
    "assert(\n",
    "    all_trff_dfs[\n",
    "        (all_trff_dfs['t_search_min']<available_data_min) | \n",
    "        (all_trff_dfs['t_search_max']>available_data_max)\n",
    "    ].shape[0]==0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d299b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a23480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'#PNs:     {all_trff_dfs.shape[0]}')\n",
    "print(f'#EVs:     {all_trff_dfs[\"EV\"].sum()}')\n",
    "print(f'#not EVs: {(all_trff_dfs[\"EV\"]==0).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa63505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df59f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trff_dfs.to_pickle(os.path.join(save_dir_base, 'all_trff_dfs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd655cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_trff_dfs['t_search_min'] = pd.to_datetime('2023-02-01')\n",
    "# all_trff_dfs['t_search_max'] = pd.to_datetime('2023-04-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(save_dir_base, 'Data')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2731f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "n_update=10\n",
    "for i, (idx_i, row_i) in enumerate(all_trff_dfs.iterrows()):\n",
    "    if i%n_update==0:\n",
    "        print(f\"{i}/{all_trff_dfs.shape[0]}\")\n",
    "    ami_i = AMINonVee(\n",
    "        df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "        build_sql_function_kwargs= dict(\n",
    "            cols_of_interest = ['*'], \n",
    "            premise_nbs = row_i['PREM_NB'],  \n",
    "            date_range = [\n",
    "                row_i['t_search_min'], \n",
    "                row_i['t_search_max']\n",
    "            ], \n",
    "            schema_name='cds_ds_db', \n",
    "            table_name='ev_ami_final'\n",
    "        ), \n",
    "        init_df_in_constructor=True\n",
    "    )\n",
    "    df_i = ami_i.df.copy()\n",
    "    df_i['opco'] = row_i['opco']\n",
    "    save_name_i = f\"{row_i['opco']}_{row_i['PREM_NB']}.csv\"\n",
    "    df_i.to_csv(os.path.join(save_dir, save_name_i), index=False)\n",
    "    \n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597c74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f455a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
