{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444718d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns\n",
    "from packaging import version\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "import Utilities_dt\n",
    "from Utilities_df import DFConstructType\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import GrubbsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve\n",
    "#-----\n",
    "from sklearn import svm\n",
    "#-----\n",
    "import scipy\n",
    "from scipy import signal\n",
    "#-----\n",
    "from tensorflow import keras\n",
    "#-----\n",
    "# from tslearn.shapelets import LearningShapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a2b136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137db911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def get_outg_confusion_matrix_text_colors(\n",
    "    cmd\n",
    "):\n",
    "    r\"\"\"\n",
    "    Basically taken from sklearn.metrics.ConfusionMatrixDisplay.plot\n",
    "    Colors are needed so my additional text matches what's provided by sklearn method.\n",
    "    \n",
    "    SHOULD BE CALLED AFTER sklearn.metrics.ConfusionMatrixDisplay.plot!\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    cm = cmd.confusion_matrix\n",
    "    assert(cm.shape[0]==2)\n",
    "    n_classes = cm.shape[0]\n",
    "    #-------------------------\n",
    "    cmap_min, cmap_max = cmd.im_.cmap(0), cmd.im_.cmap(1.0)\n",
    "    thresh = (cm.max() + cm.min()) / 2.0\n",
    "    #-------------------------\n",
    "    colors = np.empty((2,2), dtype=object)\n",
    "    for i,j in product(range(2), range(2)):\n",
    "        color = cmap_max if cm[i, j] < thresh else cmap_min\n",
    "        colors[i,j] = color\n",
    "    #-------------------------\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401acaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(\n",
    "    y, \n",
    "    y_pred, \n",
    "    title=None, \n",
    "    normalize=None, \n",
    "    scientific=True, \n",
    "    ax=None, \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='Outage', \n",
    "    target_eq_0_name='Baseline'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Visualize confusion matrix for outages using sklearn.metrics.ConfusionMatrixDisplay\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    cmd = ConfusionMatrixDisplay(\n",
    "        confusion_matrix(y, y_pred, normalize=normalize), \n",
    "        display_labels=[target_eq_0_name, target_eq_1_name]\n",
    "    )\n",
    "    #-----\n",
    "    if scientific:\n",
    "        cmd.plot(values_format='.3e', ax=ax, text_kw=text_kw)\n",
    "    else:\n",
    "        cmd.plot(values_format='', ax=ax, text_kw=text_kw)\n",
    "    #-------------------------\n",
    "    cmd.ax_.set_xlabel('Predicted', fontsize='xx-large')\n",
    "    cmd.ax_.set_ylabel('True', fontsize='xx-large')\n",
    "    cmd.ax_.set_title(title, fontsize='xx-large', fontweight='semibold')\n",
    "    #ax.set_size_inches(12, 10)\n",
    "    #-------------------------\n",
    "    if scientific:\n",
    "        txt_fmt     = '{:.4e}'\n",
    "        pct_txt_fmt = txt_fmt\n",
    "    else:\n",
    "        txt_fmt     = '{}'\n",
    "        pct_txt_fmt = '{:.4f}'\n",
    "    #-----\n",
    "    ax = cmd.ax_\n",
    "    ax.text(1.5, 0.9, \"# Entries:  {}\".format(txt_fmt).format(y.shape[0]), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.8, \"# {}:  {}\".format(target_eq_1_name, txt_fmt).format(y.sum()), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.7, \"# {}: {}\".format(target_eq_0_name, txt_fmt).format(y.shape[0]-y.sum()), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.6, \"% {}:  {}\".format(target_eq_1_name, pct_txt_fmt).format(100*y.sum()/y.shape[0]), fontsize=14, transform=ax.transAxes)\n",
    "    #-------------------------\n",
    "    ax.text(1.5, 0.4, \"ACCURACY:  {:.4f}\".format(accuracy_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.3, \"PRECISION:  {:.4f}\".format(precision_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.2, \"RECALL:       {:.4f}\".format(recall_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.1, \"F1:               {:.4f}\".format(f1_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    #--------------------------------------------------\n",
    "    # Include TN, FP, FN, TP labels\n",
    "    # NOTES:\n",
    "    #   Text stored in cmd.text_ is stored in row-major fashion\n",
    "    #   Therefore, when plotting the value, the y-value corresponds to the 0th\n",
    "    #     index and the x-value corresponds to the 1st index\n",
    "    #\n",
    "    #   Axes are defined with limits:\n",
    "    #     x_lim = (-0.5, 1.5)\n",
    "    #     y_lim = (1.5, -0.5)\n",
    "    #   The axes are defined such that: \n",
    "    #     top-left corner     = (-0.5, -0.5)\n",
    "    #     bottom-right corner = (1.5, 1.5) \n",
    "    #-----\n",
    "    colors = get_outg_confusion_matrix_text_colors(cmd)\n",
    "    cat_fontsize = text_kw.get('fontsize', 'xx-large')\n",
    "    #-----\n",
    "    cmd.ax_.text(0,   -0.25, 'TN', ha='center', va='center', color=colors[0,0], fontweight='bold', fontsize=cat_fontsize)\n",
    "    cmd.ax_.text(1.0, -0.25, 'FP', ha='center', va='center', color=colors[0,1], fontweight='bold', fontsize=cat_fontsize)\n",
    "\n",
    "    cmd.ax_.text(0,    0.75, 'FN', ha='center', va='center', color=colors[1,0], fontweight='bold', fontsize=cat_fontsize)\n",
    "    cmd.ax_.text(1.0,  0.75, 'TP', ha='center', va='center', color=colors[1,1], fontweight='bold', fontsize=cat_fontsize)    \n",
    "    #--------------------------------------------------\n",
    "    #return ax, cmd.ax_\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3640b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5d890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ev_submeter_in_pair(\n",
    "    ami_df_i, \n",
    "    pct_0_thresh_main=0.1, \n",
    "    pct_0_thresh_subm=0.6, \n",
    "    enforce_corr=True, \n",
    "    corr_thresh=0.5, \n",
    "    #remove_undetermined=True, \n",
    "    value_col='value', \n",
    "    time_col='index', \n",
    "    PN_col='aep_premise_nb', \n",
    "    SN_col='serialnumber'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a pair of meter connected to a single premise, this tries to determine which is the EV submeter in the pair, \n",
    "      and keeps that which is not.\n",
    "    Basically, the submeter only monitors the EV, whereas the other monitors the usage of the entire premise.\n",
    "    Thus, the EV signal should be 0 for a majority of the time, and the two meters should be correlated.\n",
    "    \n",
    "    This is a somewhat specialized function for use with engineering the EV dataset\n",
    "    \n",
    "    ami_df_i:\n",
    "        An AMI DF containing data for a single premise which has two meters\n",
    "        \n",
    "    pct_0_thresh_main:\n",
    "        The maximum allowed percentage of 0 values for a meter to be considered the main meter\n",
    "    pct_0_thresh_subm:\n",
    "        The minimum allowed percentage of 0 values for a meter to be considered the submeter\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(ami_df_i[PN_col].nunique()==1)\n",
    "    assert(pct_0_thresh_main < pct_0_thresh_subm)\n",
    "    #-------------------------\n",
    "    SNs_i = ami_df_i[SN_col].unique().tolist()\n",
    "    assert(len(SNs_i)<=2)\n",
    "    #-------------------------\n",
    "    if len(SNs_i)==1:\n",
    "        return ami_df_i\n",
    "    #-------------------------\n",
    "    ami_df_i_1 = ami_df_i[ami_df_i[SN_col]==SNs_i[0]]\n",
    "    ami_df_i_2 = ami_df_i[ami_df_i[SN_col]==SNs_i[1]]\n",
    "    if time_col=='index':\n",
    "        ami_df_i_1 = ami_df_i_1.sort_index()\n",
    "        ami_df_i_2 = ami_df_i_2.sort_index()\n",
    "    else:\n",
    "        ami_df_i_1 = ami_df_i_1.sort_values(by=[time_col])\n",
    "        ami_df_i_2 = ami_df_i_2.sort_values(by=[time_col])\n",
    "    #--------------------------------------------------\n",
    "    # NOTE: I have seen some SNs which have a bunch of 0.002 values, which, for the purposes here should be treated as 0s\n",
    "    #       This is why I use <0.005 instead of ==0 and >=0.005 instead of !=0 below\n",
    "    #--------------------------------------------------\n",
    "    pct_1 = (ami_df_i_1[value_col]<0.005).sum()/ami_df_i_1.shape[0]\n",
    "    pct_2 = (ami_df_i_2[value_col]<0.005).sum()/ami_df_i_2.shape[0]\n",
    "    #-------------------------\n",
    "    if pct_1 <= pct_0_thresh_main:\n",
    "        defn_1 = 1\n",
    "    elif pct_1 >= pct_0_thresh_subm:\n",
    "        defn_1 = 0\n",
    "    else:\n",
    "        defn_1 = -1\n",
    "    #-----\n",
    "    if pct_2 <= pct_0_thresh_main:\n",
    "        defn_2 = 1\n",
    "    elif pct_2 >= pct_0_thresh_subm:\n",
    "        defn_2 = 0\n",
    "    else:\n",
    "        defn_2 = -1\n",
    "    #-------------------------\n",
    "    if defn_1+defn_2==1:\n",
    "        pass_pcts=True\n",
    "    else:\n",
    "        pass_pcts=False\n",
    "    #-------------------------\n",
    "    if not pass_pcts:\n",
    "        return\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    pass_corr=True\n",
    "    if enforce_corr:\n",
    "        if time_col=='index':\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_index=True, right_index=True, how='outer')\n",
    "        else:\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_on=time_col, right_on=time_col, how='outer')\n",
    "        # For the correlation only using values which are non-zero (and aligning, meaning non-NaN)\n",
    "        ami_df_i_12 = ami_df_i_12[(ami_df_i_12[f'{value_col}_x']>=0.005) & (ami_df_i_12[f'{value_col}_y']>=0.005)]\n",
    "        corr_val = ami_df_i_12[f'{value_col}_x'].corr(ami_df_i_12[f'{value_col}_y'])\n",
    "        if corr_val >= corr_thresh:\n",
    "            pass_corr = True\n",
    "        else:\n",
    "            pass_corr = False\n",
    "            \n",
    "    #--------------------------------------------------\n",
    "    if not (pass_pcts and pass_corr):\n",
    "        return\n",
    "    #-------------------------\n",
    "    if defn_1==1:\n",
    "        assert(defn_2==0) # Sanity check, not needed\n",
    "        return ami_df_i[ami_df_i[SN_col]==SNs_i[0]]\n",
    "    else:\n",
    "        assert(defn_1==0 and defn_2==1) # Sanity check, not needed\n",
    "        return ami_df_i[ami_df_i[SN_col]==SNs_i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_pairs_w_submeter(\n",
    "    ami_df_i, \n",
    "    pct_0_thresh_main=0.1, \n",
    "    pct_0_thresh_subm=0.6, \n",
    "    enforce_corr=True, \n",
    "    corr_thresh=0.5, \n",
    "    #remove_undetermined=True, \n",
    "    value_col='value', \n",
    "    time_col='index', \n",
    "    PN_col='aep_premise_nb', \n",
    "    SN_col='serialnumber'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a pair of meter connected to a single premise, this tries to determine which is the EV submeter in the pair, \n",
    "      and keeps that which is not.\n",
    "    Basically, the submeter only monitors the EV, whereas the other monitors the usage of the entire premise.\n",
    "    Thus, the EV signal should be 0 for a majority of the time, and the two meters should be correlated.\n",
    "    \n",
    "    This is a somewhat specialized function for use with engineering the EV dataset\n",
    "    \n",
    "    ami_df_i:\n",
    "        An AMI DF containing data for a single premise which has two meters\n",
    "        \n",
    "    pct_0_thresh_main:\n",
    "        The maximum allowed percentage of 0 values for a meter to be considered the main meter\n",
    "    pct_0_thresh_subm:\n",
    "        The minimum allowed percentage of 0 values for a meter to be considered the submeter\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(ami_df_i[PN_col].nunique()==1)\n",
    "    assert(pct_0_thresh_main < pct_0_thresh_subm)\n",
    "    #-------------------------\n",
    "    SNs_i = ami_df_i[SN_col].unique().tolist()\n",
    "    assert(len(SNs_i)<=2)\n",
    "    #-------------------------\n",
    "    if len(SNs_i)==1:\n",
    "        return \n",
    "    #-------------------------\n",
    "    ami_df_i_1 = ami_df_i[ami_df_i[SN_col]==SNs_i[0]]\n",
    "    ami_df_i_2 = ami_df_i[ami_df_i[SN_col]==SNs_i[1]]\n",
    "    if time_col=='index':\n",
    "        ami_df_i_1 = ami_df_i_1.sort_index()\n",
    "        ami_df_i_2 = ami_df_i_2.sort_index()\n",
    "    else:\n",
    "        ami_df_i_1 = ami_df_i_1.sort_values(by=[time_col])\n",
    "        ami_df_i_2 = ami_df_i_2.sort_values(by=[time_col])\n",
    "    #--------------------------------------------------\n",
    "    # NOTE: I have seen some SNs which have a bunch of 0.002 values, which, for the purposes here should be treated as 0s\n",
    "    #       This is why I use <0.005 instead of ==0 and >=0.005 instead of !=0 below\n",
    "    #--------------------------------------------------\n",
    "    pct_1 = (ami_df_i_1[value_col]<0.005).sum()/ami_df_i_1.shape[0]\n",
    "    pct_2 = (ami_df_i_2[value_col]<0.005).sum()/ami_df_i_2.shape[0]\n",
    "    #-------------------------\n",
    "    if pct_1 <= pct_0_thresh_main:\n",
    "        defn_1 = 1\n",
    "    elif pct_1 >= pct_0_thresh_subm:\n",
    "        defn_1 = 0\n",
    "    else:\n",
    "        defn_1 = -1\n",
    "    #-----\n",
    "    if pct_2 <= pct_0_thresh_main:\n",
    "        defn_2 = 1\n",
    "    elif pct_2 >= pct_0_thresh_subm:\n",
    "        defn_2 = 0\n",
    "    else:\n",
    "        defn_2 = -1\n",
    "    #-------------------------\n",
    "    if defn_1+defn_2==1:\n",
    "        pass_pcts=True\n",
    "    else:\n",
    "        pass_pcts=False\n",
    "    #-------------------------\n",
    "    if not pass_pcts:\n",
    "        return\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    pass_corr=True\n",
    "    if enforce_corr:\n",
    "        if time_col=='index':\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_index=True, right_index=True, how='outer')\n",
    "        else:\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_on=time_col, right_on=time_col, how='outer')\n",
    "        # For the correlation only using values which are non-zero (and aligning, meaning non-NaN)\n",
    "        ami_df_i_12 = ami_df_i_12[(ami_df_i_12[f'{value_col}_x']>=0.005) & (ami_df_i_12[f'{value_col}_y']>=0.005)]\n",
    "        corr_val = ami_df_i_12[f'{value_col}_x'].corr(ami_df_i_12[f'{value_col}_y'])\n",
    "        if corr_val >= corr_thresh:\n",
    "            pass_corr = True\n",
    "        else:\n",
    "            pass_corr = False\n",
    "            \n",
    "    #--------------------------------------------------\n",
    "    if not (pass_pcts and pass_corr):\n",
    "        return\n",
    "    #-------------------------\n",
    "    if defn_1==1:\n",
    "        assert(defn_2==0) # Sanity check, not needed\n",
    "        ami_df_i_main = ami_df_i[ami_df_i[SN_col]==SNs_i[0]].copy()\n",
    "        ami_df_i_subm = ami_df_i[ami_df_i[SN_col]==SNs_i[1]].copy()\n",
    "    else:\n",
    "        assert(defn_1==0 and defn_2==1) # Sanity check, not needed\n",
    "        ami_df_i_main =  ami_df_i[ami_df_i[SN_col]==SNs_i[1]].copy()\n",
    "        ami_df_i_subm =  ami_df_i[ami_df_i[SN_col]==SNs_i[0]].copy()\n",
    "    #-------------------------\n",
    "    if time_col=='index':\n",
    "        return_ami_df_i = pd.merge(\n",
    "            ami_df_i_main, \n",
    "            ami_df_i_subm[[value_col, SN_col]], \n",
    "            left_index=True, \n",
    "            right_index=True, \n",
    "            how='inner'\n",
    "        )\n",
    "    else:\n",
    "        return_ami_df_i = pd.merge(\n",
    "            ami_df_i_main, \n",
    "            ami_df_i_subm[[value_col, time_col, SN_col]], \n",
    "            left_on=time_col, \n",
    "            right_on=time_col, \n",
    "            how='inner'\n",
    "        )\n",
    "    #----------\n",
    "    return_ami_df_i = return_ami_df_i.rename(columns={\n",
    "        f'{value_col}_x': f'{value_col}_main', \n",
    "        f'{value_col}_y': f'{value_col}_subm', \n",
    "        f'{SN_col}_x': f'{SN_col}_main', \n",
    "        f'{SN_col}_y': f'{SN_col}_subm', \n",
    "    })\n",
    "    #----------\n",
    "    return_ami_df_i[f'{value_col}_delt'] = return_ami_df_i[f'{value_col}_main']-return_ami_df_i[f'{value_col}_subm']\n",
    "    #-------------------------\n",
    "    return return_ami_df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a43e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ecb9c1",
   "metadata": {},
   "source": [
    "# SEE WEBPAGES\n",
    "https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data\n",
    "https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data/43512887#43512887\n",
    "https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data/56451135#56451135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class real_time_peak_detection():\n",
    "#     def __init__(self, array, lag, threshold, influence):\n",
    "#         self.y = list(array)\n",
    "#         self.length = len(self.y)\n",
    "#         self.lag = lag\n",
    "#         self.threshold = threshold\n",
    "#         self.influence = influence\n",
    "#         self.signals = [0] * len(self.y)\n",
    "#         self.filteredY = np.array(self.y).tolist()\n",
    "#         self.avgFilter = [0] * len(self.y)\n",
    "#         self.stdFilter = [0] * len(self.y)\n",
    "#         self.avgFilter[self.lag - 1] = np.mean(self.y[0:self.lag]).tolist()\n",
    "#         self.stdFilter[self.lag - 1] = np.std(self.y[0:self.lag]).tolist()\n",
    "\n",
    "#     def thresholding_algo(self, new_value):\n",
    "#         self.y.append(new_value)\n",
    "#         i = len(self.y) - 1\n",
    "#         self.length = len(self.y)\n",
    "#         if i < self.lag:\n",
    "#             return 0\n",
    "#         elif i == self.lag:\n",
    "#             self.signals = [0] * len(self.y)\n",
    "#             self.filteredY = np.array(self.y).tolist()\n",
    "#             self.avgFilter = [0] * len(self.y)\n",
    "#             self.stdFilter = [0] * len(self.y)\n",
    "#             self.avgFilter[self.lag] = np.mean(self.y[0:self.lag]).tolist()\n",
    "#             self.stdFilter[self.lag] = np.std(self.y[0:self.lag]).tolist()\n",
    "#             return 0\n",
    "\n",
    "#         self.signals += [0]\n",
    "#         self.filteredY += [0]\n",
    "#         self.avgFilter += [0]\n",
    "#         self.stdFilter += [0]\n",
    "\n",
    "#         if abs(self.y[i] - self.avgFilter[i - 1]) > (self.threshold * self.stdFilter[i - 1]):\n",
    "\n",
    "#             if self.y[i] > self.avgFilter[i - 1]:\n",
    "#                 self.signals[i] = 1\n",
    "#             else:\n",
    "#                 self.signals[i] = -1\n",
    "\n",
    "#             self.filteredY[i] = self.influence * self.y[i] + \\\n",
    "#                 (1 - self.influence) * self.filteredY[i - 1]\n",
    "#             self.avgFilter[i] = np.mean(self.filteredY[(i - self.lag):i])\n",
    "#             self.stdFilter[i] = np.std(self.filteredY[(i - self.lag):i])\n",
    "#         else:\n",
    "#             self.signals[i] = 0\n",
    "#             self.filteredY[i] = self.y[i]\n",
    "#             self.avgFilter[i] = np.mean(self.filteredY[(i - self.lag):i])\n",
    "#             self.stdFilter[i] = np.std(self.filteredY[(i - self.lag):i])\n",
    "\n",
    "#         return self.signals[i]\n",
    "\n",
    "\n",
    "def thresholding_algo_OLD(y, lag, threshold, influence):\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    for i in range(lag, len(y)):\n",
    "        if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter [i-1]:\n",
    "            if y[i] > avgFilter[i-1]:\n",
    "                signals[i] = 1\n",
    "            else:\n",
    "                signals[i] = -1\n",
    "\n",
    "            filteredY[i] = influence * y[i] + (1 - influence) * filteredY[i-1]\n",
    "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "        else:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))\n",
    "\n",
    "\n",
    "def thresholding_algo(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.mean(y)),\n",
    "                    stdFilter = np.asarray(np.std(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.mean(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding_algo_median(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.median(y)),\n",
    "                    stdFilter = np.asarray(np.std(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.median(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.median(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.median(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cddf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(y):\n",
    "    return np.mean(np.abs(y - np.mean(y)))\n",
    "\n",
    "def thresholding_algo_mad(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.mean(y)),\n",
    "                    stdFilter = np.asarray(mad(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = mad(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.mean(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = mad(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))\n",
    "\n",
    "\n",
    "def thresholding_algo_median_mad(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.median(y)),\n",
    "                    stdFilter = np.asarray(mad(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.median(y[0:lag])\n",
    "    stdFilter[lag - 1] = mad(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.median(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.median(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = mad(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666e68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fd6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_signal_groups_in_df_i(\n",
    "    df_i, \n",
    "    SN_col='serialnumber',\n",
    "    signal_col='signals', \n",
    "    return_signal_group_col='signal_grp'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)    \n",
    "    #-------------------------    \n",
    "    drop_idx = False\n",
    "    if df_i.index.name in df_i.columns:\n",
    "        drop_idx = True\n",
    "    signals_df_i = df_i.reset_index(drop=drop_idx)[df_i.reset_index(drop=drop_idx)[signal_col]==1]\n",
    "    #-----\n",
    "    tmp_signal_grp_col = Utilities.generate_random_string()\n",
    "    signals_df_i[tmp_signal_grp_col] = np.nan\n",
    "    #-----\n",
    "    tmp_idx_col = Utilities.generate_random_string()\n",
    "    signals_df_i[tmp_idx_col] = signals_df_i.index\n",
    "    #-----\n",
    "    signals_df_i[tmp_signal_grp_col] = signals_df_i[tmp_idx_col].diff().ne(1).cumsum()\n",
    "    #-------------------------\n",
    "    return_df = df_i.copy()\n",
    "    return_df[return_signal_group_col] = np.nan\n",
    "    return_df.iloc[signals_df_i.index,-1] = signals_df_i[tmp_signal_grp_col]\n",
    "    #-------------------------\n",
    "    return return_df\n",
    "\n",
    "def find_signals_and_set_signal_groups_in_df_i(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber',\n",
    "    signal_col='signals', \n",
    "    return_signal_group_col='signal_grp'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    rtpd_i = thresholding_algo(\n",
    "        y=df_i[value_col].tolist(), \n",
    "        lag=lag,\n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold\n",
    "    )\n",
    "    df_i[signal_col] = rtpd_i['signals']    \n",
    "    #-------------------------\n",
    "    return_df = set_signal_groups_in_df_i(\n",
    "        df_i=df_i, \n",
    "        SN_col=SN_col,\n",
    "        signal_col=signal_col, \n",
    "        return_signal_group_col=return_signal_group_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db7a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signals_and_build_df_i_signals_gpd(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col='signals'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    assert(df_i.index.name == time_col)\n",
    "    df_i = df_i.sort_index()\n",
    "    #-------------------------\n",
    "    signal_group_col='signal_grp'\n",
    "    df_i = find_signals_and_set_signal_groups_in_df_i(\n",
    "        df_i=df_i, \n",
    "        lag=lag, \n",
    "        threshold=threshold,\n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col,\n",
    "        signal_col=signal_col, \n",
    "        return_signal_group_col=signal_group_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    df_i_signals = df_i[df_i[signal_group_col].notna()]\n",
    "    df_i_signals = df_i_signals.drop(columns=[signal_col])\n",
    "    df_i_signals[signal_group_col] = df_i_signals[signal_group_col].astype(int)\n",
    "    df_i_signals = df_i_signals.reset_index()\n",
    "    assert(time_col in df_i_signals.columns.tolist())\n",
    "    #-------------------------\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "\n",
    "    df_i_signals_gpd = df_i_signals.groupby([signal_group_col]).agg({\n",
    "        time_col:['mean', 'std', 'min', 'max'], \n",
    "        value_col:['mean', 'std', 'max']\n",
    "    })\n",
    "    df_i_signals_gpd=df_i_signals_gpd.sort_values(by=[(time_col, 'mean')])\n",
    "    df_i_signals_gpd[(time_col, 'max_m_min')] = df_i_signals_gpd[(time_col, 'max')] - df_i_signals_gpd[(time_col, 'min')]\n",
    "    #-------------------------\n",
    "    return df_i_signals_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f14d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_SN(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col='signals'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "    \n",
    "    df_i_signals_gpd = find_signals_and_build_df_i_signals_gpd(\n",
    "        df_i=df_i, \n",
    "        lag=lag, \n",
    "        threshold=threshold,\n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col, \n",
    "        time_col=time_col, \n",
    "        signal_col=signal_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    peak_max_mean     = df_i_signals_gpd[(value_col, 'max')].mean()\n",
    "    peak_width_mean   = df_i_signals_gpd[(time_col, 'max_m_min')].mean()\n",
    "    peak_spacing_mean = df_i_signals_gpd[(time_col, 'mean')].diff().mean()\n",
    "    # Below, the date being used is of no matter, any random date works, it's simply\n",
    "    #   to make pd.to_datetime happy\n",
    "    if df_i_signals_gpd.shape[0]>0:\n",
    "        peak_hour_mean    = pd.to_datetime(\n",
    "            '2023-01-01 ' + df_i_signals_gpd[(time_col, 'mean')].dt.strftime('%H:%M:%S'), \n",
    "            format=\"%Y-%m-%d %H:%M:%S\"\n",
    "        ).mean().round('H').time().hour\n",
    "    else:\n",
    "        peak_hour_mean = np.nan\n",
    "    #-------------------------\n",
    "    features_srs = pd.Series({\n",
    "        'peak_max_mean':     peak_max_mean, \n",
    "        'peak_width_mean':   peak_width_mean, \n",
    "        'peak_spacing_mean': peak_spacing_mean, \n",
    "        'peak_hour_mean':    peak_hour_mean, \n",
    "    })\n",
    "    features_srs.name = df_i[SN_col].unique()[0]\n",
    "    return features_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_SN_v2(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col='signals'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    assert(df_i.index.name == time_col)\n",
    "    df_i = df_i.sort_index()\n",
    "    #-------------------------\n",
    "    signal_group_col='signal_grp'\n",
    "    df_i = find_signals_and_set_signal_groups_in_df_i(\n",
    "        df_i=df_i, \n",
    "        lag=lag, \n",
    "        threshold=threshold,\n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col,\n",
    "        signal_col=signal_col, \n",
    "        return_signal_group_col=signal_group_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    df_i_signals = df_i[df_i[signal_group_col].notna()]\n",
    "    df_i_signals = df_i_signals.drop(columns=[signal_col])\n",
    "    df_i_signals[signal_group_col] = df_i_signals[signal_group_col].astype(int)\n",
    "    #-----   \n",
    "    drop_idx = False\n",
    "    if df_i_signals.index.name in df_i_signals.columns:\n",
    "        drop_idx = True\n",
    "    df_i_signals = df_i_signals.reset_index(drop=drop_idx)\n",
    "    assert(time_col in df_i_signals.columns.tolist())\n",
    "    #-------------------------\n",
    "    # Features split into:\n",
    "    #   1. Using all data in peaks pooled together\n",
    "    #   2. First grouping by signal group, then extracting features\n",
    "    #-------------------------\n",
    "    # Features (1):\n",
    "    peak_mean = df_i_signals[value_col].mean()\n",
    "    peak_std  = df_i_signals[value_col].std()\n",
    "    #-------------------------\n",
    "    # Features (2):\n",
    "    #-----\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "\n",
    "    df_i_signals_gpd = df_i_signals.groupby([signal_group_col]).agg({\n",
    "        time_col:['mean', 'std', 'min', 'max'], \n",
    "        value_col:['mean', 'std', 'max']\n",
    "    })\n",
    "    df_i_signals_gpd=df_i_signals_gpd.sort_values(by=[(time_col, 'mean')])\n",
    "    df_i_signals_gpd[(time_col, 'max_m_min')] = df_i_signals_gpd[(time_col, 'max')] - df_i_signals_gpd[(time_col, 'min')]\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    peak_max_mean     = df_i_signals_gpd[(value_col, 'max')].mean()\n",
    "    peak_max_std      = df_i_signals_gpd[(value_col, 'max')].std()\n",
    "    #-----\n",
    "    peak_width_mean   = df_i_signals_gpd[(time_col, 'max_m_min')].mean()\n",
    "    peak_width_std    = df_i_signals_gpd[(time_col, 'max_m_min')].std()\n",
    "    #-----\n",
    "    peak_spacing_mean = df_i_signals_gpd[(time_col, 'mean')].diff().mean()\n",
    "    peak_spacing_std  = df_i_signals_gpd[(time_col, 'mean')].diff().std()\n",
    "    #-----\n",
    "    # Below, the date being used is of no matter, any random date works, it's simply\n",
    "    #   to make pd.to_datetime happy\n",
    "    if df_i_signals_gpd.shape[0]>0:\n",
    "        peak_hour_mean    = pd.to_datetime(\n",
    "            '2023-01-01 ' + df_i_signals_gpd[(time_col, 'mean')].dt.strftime('%H:%M:%S'), \n",
    "            format=\"%Y-%m-%d %H:%M:%S\"\n",
    "        ).mean().round('H').time().hour\n",
    "    else:\n",
    "        peak_hour_mean = np.nan\n",
    "    #-------------------------\n",
    "    features_srs = pd.Series({\n",
    "        'peak_mean':         peak_mean, \n",
    "        'peak_std':          peak_std, \n",
    "        #-----\n",
    "        'peak_max_mean':     peak_max_mean, \n",
    "        'peak_max_std':      peak_max_std, \n",
    "        #-----\n",
    "        'peak_width_mean':   peak_width_mean, \n",
    "        'peak_width_std':    peak_width_std, \n",
    "        #-----\n",
    "        'peak_spacing_mean': peak_spacing_mean,\n",
    "        'peak_spacing_std':  peak_spacing_std,\n",
    "        #-----\n",
    "        'peak_hour_mean':    peak_hour_mean, \n",
    "    })\n",
    "    features_srs.name = df_i[SN_col].unique()[0]\n",
    "    return features_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c27092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_it_funky(df_i, n_entries=500, val_col='value', SN_col='serialnumber'):\n",
    "    if df_i.shape[0] < n_entries:\n",
    "        return None\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    vals_i = df_i[val_col].sort_index().iloc[:n_entries].tolist()\n",
    "    return_i = pd.Series(vals_i)\n",
    "    return_i.name = df_i[SN_col].unique()[0]\n",
    "    return return_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b455b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_peaks_OLD(\n",
    "    df, \n",
    "    value_col, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    r\"\"\"\n",
    "    Do not use too small a value for lag!!!!!\n",
    "        This can cause the std values to become very small, causing much to be identified as peaks.\n",
    "        It's not a huge deal when signal_abs_threshold!=0, because this usually gives the algorithm enough\n",
    "          data to work out the rolling mean and std reliably.\n",
    "    \"\"\"\n",
    "    #-----\n",
    "    y = df[value_col].tolist()\n",
    "    #-----\n",
    "    if len(y) <= lag:\n",
    "        return np.asarray(y)\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    \n",
    "\n",
    "    # To avoid possibility of data starting with peak, which can mess up methods (especially if\n",
    "    #   lag is small), start with first value (after lag) less than the mean\n",
    "    # NOTE: The mean here includes all data, so includes peaks as well, so should be larger than the\n",
    "    #         smoothed mean\n",
    "    i_beg = np.argmax((y[lag:] < np.mean(y))) + lag\n",
    "    \n",
    "    non_signal_Y = []\n",
    "    for i in range(i_beg, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = y[i]\n",
    "            filteredY[i] = y[i]\n",
    "        else:\n",
    "            # Only looking for peaks, so don't use abs(y-avg)\n",
    "            if y[i] - avgFilter[i-1] > threshold * stdFilter[i-1]:\n",
    "                #-------------------------\n",
    "                # First, set the signal equal to the rolling average\n",
    "                #-----\n",
    "                # If avgFilter[i-1]==0, try np.mean(avgFilter)\n",
    "                # If both==0, use raw value, y[i]\n",
    "                if avgFilter[i-1]>0:\n",
    "                    signals[i] = avgFilter[i-1]\n",
    "                else:\n",
    "                    if np.mean(avgFilter)>0:\n",
    "                        signals[i] = np.mean(avgFilter)\n",
    "                    else:\n",
    "                        print('Cannot use any averages!')\n",
    "                        signals[i] = y[i]\n",
    "                #-------------------------\n",
    "                # Next, set filteredY\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    tmp_idx = np.random.randint(i-lag, i, 1)[0]\n",
    "                    tmp_mean = filteredY[tmp_idx]\n",
    "                    tmp_std = stdFilter[tmp_idx]\n",
    "                    tmp_val  = np.random.normal(tmp_mean, tmp_std, 1)\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * tmp_val\n",
    "            else:\n",
    "                signals[i] = y[i]\n",
    "                filteredY[i] = y[i]\n",
    "        #-----\n",
    "        # Note: As opposed to other thresholding functions, since any peaks have been reduced to average\n",
    "        #       values, all filteredY are also non_signal_Y (so can be appended here, instead of separately\n",
    "        #       in if/else statements as in other thresholding functions)\n",
    "        non_signal_Y.append(filteredY[i])\n",
    "        #-----\n",
    "        avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "        \n",
    "    #-------------------------\n",
    "    # Now, go back and perform calculation for initial lag entries using the non_signal_Y\n",
    "    #   for average values\n",
    "    for i in range(0,i_beg):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = y[i]\n",
    "        else:\n",
    "            if y[i] - np.mean(non_signal_Y) > threshold * np.std(non_signal_Y):\n",
    "                # Instead of using the mean of non_signal_Y, randomly draw a mean from avgFilter\n",
    "                signals[i] = np.random.choice(a=avgFilter, size=1)\n",
    "            else:\n",
    "                signals[i] = y[i]\n",
    "\n",
    "    return np.asarray(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_peaks(\n",
    "    df, \n",
    "    value_col, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    r\"\"\"\n",
    "    Do not use too small a value for lag!!!!!\n",
    "        This can cause the std values to become very small, causing much to be identified as peaks.\n",
    "        It's not a huge deal when signal_abs_threshold!=0, because this usually gives the algorithm enough\n",
    "          data to work out the rolling mean and std reliably.\n",
    "          \n",
    "    NOTE: While dealing with the first lag entries after i_beg, we are still susceptible to peaks contaminating\n",
    "            the data (causing higher means and, more importantly, higher standard deviations)\n",
    "          THEREFORE, if we are within this regime, instead of using mean and standard devation, I will instead\n",
    "            use the median and interquartile range (which are much more robust against outliers)\n",
    "    \"\"\"\n",
    "    #-----\n",
    "    y = df[value_col].tolist()\n",
    "    #-----\n",
    "    if len(y) <= lag:\n",
    "        return np.asarray(y)\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    \n",
    "\n",
    "    # To avoid possibility of data starting with peak, which can mess up methods (especially if\n",
    "    #   lag is small), start with first value (after lag) less than the mean\n",
    "    # NOTE: The mean here includes all data, so includes peaks as well, so should be larger than the\n",
    "    #         smoothed mean\n",
    "    i_beg = np.argmax((y[lag:] < np.mean(y))) + lag\n",
    "    avgFilter[i_beg - 1] = np.median(y[i_beg-lag:i_beg])\n",
    "    stdFilter[i_beg - 1] = scipy.stats.iqr(y[i_beg-lag:i_beg])\n",
    "    \n",
    "    non_signal_Y = []\n",
    "    for i in range(i_beg, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = y[i]\n",
    "            filteredY[i] = y[i]\n",
    "        else:\n",
    "            # Only looking for peaks, so don't use abs(y-avg)\n",
    "            if y[i] - avgFilter[i-1] > threshold * stdFilter[i-1]:\n",
    "                #-------------------------\n",
    "                # First, set the signal equal to the rolling average\n",
    "                #-----\n",
    "                # If avgFilter[i-1]==0, try np.mean(avgFilter)\n",
    "                # If both==0, use raw value, y[i]\n",
    "                if avgFilter[i-1]>0:\n",
    "                    signals[i] = avgFilter[i-1]\n",
    "                else:\n",
    "                    if np.mean(avgFilter)>0:\n",
    "                        signals[i] = np.mean(avgFilter)\n",
    "                    else:\n",
    "                        print('Cannot use any averages!')\n",
    "                        signals[i] = y[i]\n",
    "                #-------------------------\n",
    "                # Next, set filteredY\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    tmp_idx = np.random.randint(i-lag, i, 1)[0]\n",
    "                    tmp_mean = filteredY[tmp_idx]\n",
    "                    tmp_std = stdFilter[tmp_idx]\n",
    "                    tmp_val  = np.random.normal(tmp_mean, tmp_std, 1)\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * tmp_val\n",
    "            else:\n",
    "                signals[i] = y[i]\n",
    "                filteredY[i] = y[i]\n",
    "        #-----\n",
    "        # Note: As opposed to other thresholding functions, since any peaks have been reduced to average\n",
    "        #       values, all filteredY are also non_signal_Y (so can be appended here, instead of separately\n",
    "        #       in if/else statements as in other thresholding functions)\n",
    "        non_signal_Y.append(filteredY[i])\n",
    "        #-----\n",
    "        if i-i_beg > lag:\n",
    "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "        else:\n",
    "            avgFilter[i] = np.median(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = scipy.stats.iqr(filteredY[(i-lag+1):i+1])            \n",
    "        \n",
    "    #-------------------------\n",
    "    # Now, go back and perform calculation for initial lag entries using the non_signal_Y\n",
    "    #   for average values\n",
    "    for i in range(0,i_beg):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = y[i]\n",
    "        else:\n",
    "            if y[i] - np.mean(non_signal_Y) > threshold * np.std(non_signal_Y):\n",
    "                # Instead of using the mean of non_signal_Y, randomly draw a mean from avgFilter\n",
    "                signals[i] = np.random.choice(a=avgFilter, size=1)\n",
    "            else:\n",
    "                signals[i] = y[i]\n",
    "\n",
    "    return np.asarray(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding_algo_df_i(\n",
    "    df_i, \n",
    "    value_col, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    smooth_rolling_window, \n",
    "    SN_col='serialnumber', \n",
    "    PN_col='aep_premise_nb', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col_out='signal', \n",
    "    signal_pos_col_out='signal_pos', \n",
    "    signal_binary_col_out='signal_binary', \n",
    "    signal_pos_binary_col_out='signal_pos_binary',  \n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    if df_i.index.name != time_col:\n",
    "        assert(time_col in df_i.columns)\n",
    "        df_i = df_i.set_index(time_col, drop=True)\n",
    "    assert(df_i.index.name == time_col)\n",
    "    df_i = df_i.sort_index()\n",
    "    #-------------------------\n",
    "    # First, smooth out the peaks so the un-biased rolling mean and std can be formed\n",
    "    smooth = smooth_peaks(\n",
    "        df=df_i, \n",
    "        value_col=value_col, \n",
    "        lag=lag, \n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold\n",
    "    )\n",
    "    smooth_col = f'{value_col}_smooth'\n",
    "    df_i[smooth_col] = smooth\n",
    "    #-------------------------\n",
    "    # Generate smooth rolling mean and std\n",
    "    smooth_roll_mean_col = f'{value_col}_smooth_roll_mean'\n",
    "    smooth_roll_std_col  = f'{value_col}_smooth_roll_std'\n",
    "    df_i[smooth_roll_mean_col] = df_i[smooth_col].rolling(smooth_rolling_window, center=True).mean()\n",
    "    df_i[smooth_roll_std_col]  = df_i[smooth_col].rolling(smooth_rolling_window, center=True).std()\n",
    "    #-----\n",
    "    # Drop the NaN values at front and back of df_i naturally resulting from rolling window methods\n",
    "    df_i = df_i.dropna(subset=[smooth_roll_mean_col, smooth_roll_std_col]).copy()\n",
    "    #-------------------------\n",
    "    # Generate the signal column, which is defined for each point as the number of (rolling) std deviations \n",
    "    #   away from the (rolling) mean.\n",
    "    # Also generate the binary version of the signal, which is simply whether or not the number of std away from\n",
    "    #   the mean is outside of the threshold.\n",
    "    # Finally, build the positive versions of the above two, which set any values less than the mean equal to 0 (since\n",
    "    #   we are looking for positive peaks)\n",
    "    #-----\n",
    "    df_i[signal_col_out] = (df_i[value_col] - df_i[smooth_roll_mean_col])/df_i[smooth_roll_std_col]\n",
    "\n",
    "    # Probably really only want points which are above the rolling mean, since we're looking for peaks\n",
    "    # For now, keep both signal and signal_pos\n",
    "    df_i[signal_pos_col_out] = df_i[signal_col_out]\n",
    "    df_i.loc[df_i[signal_pos_col_out]<0, signal_pos_col_out] = 0\n",
    "\n",
    "    # Binary results are yes/no of whether peak is found\n",
    "    df_i[signal_binary_col_out]     = np.abs(df_i[signal_col_out]) > threshold\n",
    "    df_i[signal_pos_binary_col_out] = np.abs(df_i[signal_pos_col_out]) > threshold\n",
    "    \n",
    "    #-------------------------\n",
    "    # Generate a couple test series for possible use\n",
    "    df_i['test_final'] = df_i[value_col]\n",
    "    df_i.loc[df_i[signal_binary_col_out]==0, 'test_final'] = df_i.loc[df_i[signal_binary_col_out]==0, smooth_roll_mean_col]\n",
    "    #-----\n",
    "    df_i['test_final2'] = df_i[value_col]\n",
    "    df_i.loc[df_i[signal_binary_col_out]==0, 'test_final2'] = 0\n",
    "    #-----\n",
    "    df_i['test_final_signal'] = df_i[signal_col_out]\n",
    "    df_i.loc[df_i[signal_binary_col_out]==0, 'test_final_signal'] = 0\n",
    "    \n",
    "    #-------------------------\n",
    "    # To save space, don't keep all of columns in df_i, only really keep those build here (plus SN, PN, value)\n",
    "    cols_to_keep = [\n",
    "        SN_col, \n",
    "        PN_col, \n",
    "        value_col, \n",
    "        smooth_col, \n",
    "        smooth_roll_mean_col, \n",
    "        smooth_roll_std_col, \n",
    "        signal_col_out, \n",
    "        signal_pos_col_out, \n",
    "        signal_binary_col_out, \n",
    "        signal_pos_binary_col_out, \n",
    "        'test_final', \n",
    "        'test_final2', \n",
    "        'test_final_signal'\n",
    "    ]\n",
    "    return df_i[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83677667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf36710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_peak_features_for_df_i(\n",
    "    df_i, \n",
    "    signal_group_col='signal_grp', \n",
    "    value_col='value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    if time_col not in df_i.columns:\n",
    "        assert(time_col==df_i.index.name)\n",
    "        df_i[time_col] = df_i.index\n",
    "    assert(time_col in df_i.columns)\n",
    "    #-------------------------\n",
    "    df_i_signals = df_i[df_i[signal_group_col].notna()].copy()\n",
    "    df_i_signals[signal_group_col] = df_i_signals[signal_group_col].astype(int)\n",
    "    df_i_signals = df_i_signals.reset_index(drop=True)\n",
    "    #-------------------------\n",
    "    # Features split into:\n",
    "    #   1. Using all data in peaks pooled together\n",
    "    #   2. First grouping by signal group, then extracting features\n",
    "    #-------------------------\n",
    "    # Features (1):\n",
    "    peak_mean = df_i_signals[value_col].mean()\n",
    "    peak_std  = df_i_signals[value_col].std()\n",
    "    #-------------------------\n",
    "    # Features (2):\n",
    "    #-----\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "\n",
    "    df_i_signals_gpd = df_i_signals.groupby([signal_group_col]).agg({\n",
    "        time_col:['mean', 'std', 'min', 'max'], \n",
    "        value_col:['mean', 'std', 'max']\n",
    "    })\n",
    "    df_i_signals_gpd=df_i_signals_gpd.sort_values(by=[(time_col, 'mean')])\n",
    "    df_i_signals_gpd[(time_col, 'max_m_min')] = df_i_signals_gpd[(time_col, 'max')] - df_i_signals_gpd[(time_col, 'min')]\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    peak_max_mean     = df_i_signals_gpd[(value_col, 'max')].mean()\n",
    "    peak_max_std      = df_i_signals_gpd[(value_col, 'max')].std()\n",
    "    #-----\n",
    "    peak_width_mean   = df_i_signals_gpd[(time_col, 'max_m_min')].mean()\n",
    "    peak_width_std    = df_i_signals_gpd[(time_col, 'max_m_min')].std()\n",
    "    #-----\n",
    "    peak_spacing_mean = df_i_signals_gpd[(time_col, 'mean')].diff().mean()\n",
    "    peak_spacing_std  = df_i_signals_gpd[(time_col, 'mean')].diff().std()\n",
    "    #-----\n",
    "    # Below, the date being used is of no matter, any random date works, it's simply\n",
    "    #   to make pd.to_datetime happy\n",
    "    if df_i_signals_gpd.shape[0]>0:\n",
    "        peak_hour_mean    = pd.to_datetime(\n",
    "            '2023-01-01 ' + df_i_signals_gpd[(time_col, 'mean')].dt.strftime('%H:%M:%S'), \n",
    "            format=\"%Y-%m-%d %H:%M:%S\"\n",
    "        ).mean().round('H').time().hour\n",
    "    else:\n",
    "        peak_hour_mean = np.nan\n",
    "    #-------------------------\n",
    "    features_srs = pd.Series({\n",
    "        'peak_mean':         peak_mean, \n",
    "        'peak_std':          peak_std, \n",
    "        #-----\n",
    "        'peak_max_mean':     peak_max_mean, \n",
    "        'peak_max_std':      peak_max_std, \n",
    "        #-----\n",
    "        'peak_width_mean':   peak_width_mean, \n",
    "        'peak_width_std':    peak_width_std, \n",
    "        #-----\n",
    "        'peak_spacing_mean': peak_spacing_mean,\n",
    "        'peak_spacing_std':  peak_spacing_std,\n",
    "        #-----\n",
    "        'peak_hour_mean':    peak_hour_mean, \n",
    "    })\n",
    "    features_srs.name = df_i[SN_col].unique()[0]\n",
    "    return features_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdcbad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num=0\n",
    "\n",
    "save_figs=True\n",
    "figs_save_dir = r'C:\\Users\\s346557\\Documents\\Presentations\\EVs\\Figures\\Samples_EVs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_ami_df = False\n",
    "files_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Data'\n",
    "pkl_save_dir =  r'C:\\Users\\s346557\\Documents\\LocalData\\EVs'\n",
    "\n",
    "# files_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Recent_Data\\Data'\n",
    "# pkl_save_dir =  r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Recent_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472da5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if build_ami_df:\n",
    "    ami_df = GenAn.read_df_from_csv_dir_batches(\n",
    "        files_dir=files_dir, \n",
    "        file_path_glob=r'*.csv'\n",
    "    )\n",
    "    #-------------------------\n",
    "    ami_df = ami_df[\n",
    "        (ami_df['aep_derived_uom']=='KWH') & \n",
    "        (ami_df['aep_srvc_qlty_idntfr']=='TOTAL')\n",
    "    ].copy()\n",
    "    #-----\n",
    "    ami_df['timezoneoffset'] = ami_df['starttimeperiod'].str[-6:]\n",
    "    #-------------------------\n",
    "    ami_df = AMINonVee.perform_std_initiation_and_cleaning(\n",
    "        ami_df, \n",
    "        timestamp_col=None\n",
    "    )\n",
    "    #-----\n",
    "    ami_df = Utilities_dt.strip_tz_info_and_convert_to_dt(\n",
    "        df=ami_df, \n",
    "        time_col='starttimeperiod', \n",
    "        placement_col='starttimeperiod_local', \n",
    "        run_quick=True, \n",
    "        n_strip=6, \n",
    "        inplace=False\n",
    "    )\n",
    "    ami_df = Utilities_dt.strip_tz_info_and_convert_to_dt(\n",
    "        df=ami_df, \n",
    "        time_col='endtimeperiod', \n",
    "        placement_col='endtimeperiod_local', \n",
    "        run_quick=True, \n",
    "        n_strip=6, \n",
    "        inplace=False\n",
    "    )\n",
    "    ami_df=ami_df.set_index('starttimeperiod_local', drop=False)\n",
    "    #-------------------------\n",
    "    ami_df.to_pickle(os.path.join(pkl_save_dir, 'ami_df.pkl'))\n",
    "else:\n",
    "    ami_df = pd.read_pickle(os.path.join(pkl_save_dir, 'ami_df.pkl'))\n",
    "#-------------------------\n",
    "all_trff_dfs = pd.read_pickle(os.path.join(pkl_save_dir, 'all_trff_dfs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6cf5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc95004",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df['aep_premise_nb'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebed084",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trff_dfs['PREM_NB'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trff_dfs_found     = all_trff_dfs[all_trff_dfs['PREM_NB'].isin(ami_df['aep_premise_nb'].unique().tolist())]\n",
    "all_trff_dfs_not_found = all_trff_dfs[~all_trff_dfs['PREM_NB'].isin(ami_df['aep_premise_nb'].unique().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_trff_dfs_found['PREM_NB'].nunique())\n",
    "print(all_trff_dfs_not_found['PREM_NB'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ddda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ami_df['aep_premise_nb'].nunique())\n",
    "print(ami_df['serialnumber'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ami_df = ami_df.groupby('aep_premise_nb', as_index=False, group_keys=False).apply(\n",
    "#     lambda x: remove_ev_submeter_in_pair(x)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b404b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ami_df['aep_premise_nb'].nunique())\n",
    "print(ami_df['serialnumber'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a186e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_resamples = AMINonVee.build_time_resampled_dfs(\n",
    "    ami_df, \n",
    "    base_freq='15T', \n",
    "    freqs=['H', '2H', '3H', '4H'], \n",
    "    other_grouper_cols=['serialnumber', 'aep_premise_nb']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7450ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df[['starttimeperiod_local', 'starttimeperiod_utc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77292cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c14ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evs_prems = all_trff_dfs[all_trff_dfs['EV']==1]['PREM_NB'].unique().tolist()\n",
    "non_prems = all_trff_dfs[all_trff_dfs['EV']==0]['PREM_NB'].unique().tolist()\n",
    "\n",
    "# aep_premise_nb in ami_df is of type object (i.e., a string), whereas PREM_NB in trff_df is int64\n",
    "evs_prems = [str(x) for x in evs_prems]\n",
    "non_prems = [str(x) for x in non_prems]\n",
    "#-----\n",
    "ami_df_evs = ami_df[ami_df['aep_premise_nb'].isin(evs_prems)].copy()\n",
    "ami_df_non = ami_df[ami_df['aep_premise_nb'].isin(non_prems)].copy()\n",
    "#----\n",
    "assert(ami_df.shape[0]==ami_df_evs.shape[0]+ami_df_non.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd03e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ami_df_evs.shape[0] = {ami_df_evs.shape[0]}')\n",
    "print(f'ami_df_non.shape[0] = {ami_df_non.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a009f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ami_df_evs['aep_premise_nb'].nunique() = {ami_df_evs['aep_premise_nb'].nunique()}\")\n",
    "print(f\"ami_df_non['aep_premise_nb'].nunique() = {ami_df_non['aep_premise_nb'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ee146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54218922",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_resamples_evs = {}\n",
    "ami_df_resamples_non = {}\n",
    "for freq_i, dfs_i in ami_df_resamples.items():\n",
    "    df_i = dfs_i['df']\n",
    "    ami_df_evs_i = df_i[df_i['aep_premise_nb'].isin(evs_prems)].copy()\n",
    "    ami_df_non_i = df_i[df_i['aep_premise_nb'].isin(non_prems)].copy()   \n",
    "    #-----\n",
    "    assert(df_i.shape[0]==ami_df_evs_i.shape[0]+ami_df_non_i.shape[0])\n",
    "    assert(freq_i not in ami_df_resamples_evs.keys())\n",
    "    assert(freq_i not in ami_df_resamples_non.keys())\n",
    "    ami_df_resamples_evs[freq_i] = ami_df_evs_i\n",
    "    ami_df_resamples_non[freq_i] = ami_df_non_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b14fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea781c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e34f37",
   "metadata": {},
   "source": [
    "# =============================================================\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d23d5ec",
   "metadata": {},
   "source": [
    "# Simple peak finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df = ami_df.sort_index()\n",
    "SN = '988181190'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = ami_df[ami_df['serialnumber']==SN].sort_index().drop(columns=['starttimeperiod_local']).copy()\n",
    "value_col = 'value'\n",
    "freq='15T'\n",
    "\n",
    "time_col = 'starttimeperiod_local'\n",
    "lag=24\n",
    "threshold=5\n",
    "influence=0.0\n",
    "signal_abs_threshold=1.5\n",
    "n_zoom=1000\n",
    "\n",
    "# df_i = ami_df_resamples_evs['H'][ami_df_resamples_evs['H']['serialnumber']==SN].sort_index().copy()\n",
    "# value_col = 'mean_TRS value'\n",
    "# freq='H'\n",
    "\n",
    "# time_col = 'starttimeperiod_local'\n",
    "# lag=12\n",
    "# threshold=3\n",
    "# influence=0.0\n",
    "# signal_abs_threshold=1.5\n",
    "# n_zoom=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "rtpd = thresholding_algo(\n",
    "    y=df_i[value_col].tolist(), \n",
    "    lag=lag,\n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold\n",
    ")\n",
    "df_i['signals'] = rtpd['signals']\n",
    "#-------------------------\n",
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "df_i[value_col].plot(ax=axs[0], color='tab:blue')\n",
    "df_i[df_i['signals']==1].reset_index().plot.scatter(\n",
    "    ax=axs[0], x='starttimeperiod_local', y=value_col, color='tab:green'\n",
    ")\n",
    "axs[0].axhline(signal_abs_threshold, color='tab:red')\n",
    "axs[0].grid(True, which='both')\n",
    "# axs[0].legend().set_visible(False)\n",
    "axs[0].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[0].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "df_i_zoom = df_i[:n_zoom].copy()\n",
    "#-----\n",
    "df_i_zoom[value_col].plot(ax=axs[1], color='tab:blue')\n",
    "df_i_zoom[df_i_zoom['signals']==1].reset_index().plot.scatter(\n",
    "    ax=axs[1], x='starttimeperiod_local', y=value_col, color='tab:green'\n",
    ")\n",
    "axs[1].axhline(signal_abs_threshold, color='tab:red')\n",
    "axs[1].grid(True, which='both')\n",
    "# axs[1].legend().set_visible(False)\n",
    "axs[1].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[1].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "if save_figs:\n",
    "    save_name = f\"SimplePeakFinding_{freq}_L{lag}_T{threshold}_ST{str(signal_abs_threshold).replace('.', '_')}.png\"\n",
    "    Plot_General.save_fig(\n",
    "        fig=fig, \n",
    "        save_dir=figs_save_dir, \n",
    "        save_name=save_name, \n",
    "        bbox_inches='tight'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677f02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8c52202",
   "metadata": {},
   "source": [
    "# Simple Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = ami_df[ami_df['serialnumber']==SN].sort_index().drop(columns=['starttimeperiod_local']).copy()\n",
    "value_col = 'value'\n",
    "freq='15T'\n",
    "\n",
    "time_col = 'starttimeperiod_local'\n",
    "lag=24\n",
    "threshold=5\n",
    "influence=0.0\n",
    "signal_abs_threshold=1.5\n",
    "n_zoom=1000\n",
    "\n",
    "# df_i = ami_df_resamples_evs['H'][ami_df_resamples_evs['H']['serialnumber']==SN].sort_index().copy()\n",
    "# value_col = 'mean_TRS value'\n",
    "# freq='H'\n",
    "\n",
    "# time_col = 'starttimeperiod_local'\n",
    "# lag=12\n",
    "# threshold=3\n",
    "# influence=0.0\n",
    "# signal_abs_threshold=1.5\n",
    "# n_zoom=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26415d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df_i = extract_features_for_SN_v2(\n",
    "    df_i=df_i, \n",
    "    lag=lag, \n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold, \n",
    "    value_col=value_col\n",
    ")\n",
    "#--------------------------------------------------\n",
    "rtpd = thresholding_algo(\n",
    "    y=df_i[value_col].tolist(), \n",
    "    lag=lag,\n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold\n",
    ")\n",
    "df_i['signals'] = rtpd['signals']\n",
    "#-------------------------\n",
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "df_i[value_col].plot(ax=axs[0], color='tab:blue')\n",
    "df_i[df_i['signals']==1].reset_index().plot.scatter(\n",
    "    ax=axs[0], x='starttimeperiod_local', y=value_col, color='tab:green'\n",
    ")\n",
    "axs[0].axhline(signal_abs_threshold, color='tab:red')\n",
    "axs[0].grid(True, which='both')\n",
    "# axs[0].legend().set_visible(False)\n",
    "axs[0].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[0].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "df_i_zoom = df_i[:n_zoom].copy()\n",
    "#-----\n",
    "df_i_zoom[value_col].plot(ax=axs[1], color='tab:blue')\n",
    "df_i_zoom[df_i_zoom['signals']==1].reset_index().plot.scatter(\n",
    "    ax=axs[1], x='starttimeperiod_local', y=value_col, color='tab:green'\n",
    ")\n",
    "axs[1].axhline(signal_abs_threshold, color='tab:red')\n",
    "axs[1].grid(True, which='both')\n",
    "# axs[1].legend().set_visible(False)\n",
    "axs[1].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[1].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#--------------------------------------------------\n",
    "left_text_x=0.95\n",
    "text_beg_y = 0.60\n",
    "y_spacing = 0.035\n",
    "#-------------------------\n",
    "# Need to iterate over all strings first to get length of longest\n",
    "# This is needed so I can align all '=' signs\n",
    "max_metric_len = 0\n",
    "max_value_len  = 0\n",
    "for i, (metric_i, value_i) in enumerate(peak_df_i.items()):\n",
    "    if isinstance(value_i, pd.Timedelta):\n",
    "        value_i = value_i.round(\"S\")\n",
    "    else:\n",
    "        value_i = np.round(value_i, decimals=3)\n",
    "    if len(metric_i)>max_metric_len:\n",
    "        max_metric_len = len(metric_i)\n",
    "    if len(str(value_i))>max_value_len:\n",
    "        max_value_len = len(str(value_i))\n",
    "# If max_metric_len==18 and max_value_len==16, the following line will creat: '{:>18}={:<16}'\n",
    "strng_fmt = f'{{:>{max_metric_len+1}}}={{:<{max_value_len+1}}}'\n",
    "#-------------------------\n",
    "# Now, actually print the texts\n",
    "for i, (metric_i, value_i) in enumerate(peak_df_i.items()):\n",
    "    if isinstance(value_i, pd.Timedelta):\n",
    "        value_i = value_i.round(\"S\")\n",
    "    else:\n",
    "        value_i = np.round(value_i, decimals=3)\n",
    "    strng = f\"{metric_i} = {value_i}\"\n",
    "    strng_split = strng.split('=')\n",
    "    final_strng = strng_fmt.format(*strng_split)\n",
    "    fig.text(\n",
    "        left_text_x, \n",
    "        text_beg_y-y_spacing*i, \n",
    "        final_strng, \n",
    "        fontsize=20, \n",
    "        fontfamily='monospace', \n",
    "        ha='left'\n",
    "    )\n",
    "#-------------------------\n",
    "if save_figs:\n",
    "    save_name = f\"SimplePeakMetrics_{freq}_L{lag}_T{threshold}_ST{str(signal_abs_threshold).replace('.', '_')}.png\"\n",
    "    Plot_General.save_fig(\n",
    "        fig=fig, \n",
    "        save_dir=figs_save_dir, \n",
    "        save_name=save_name, \n",
    "        bbox_inches='tight'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcddc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921b77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d2d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "787a41ca",
   "metadata": {},
   "source": [
    "# New Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b99d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185508f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = ami_df[ami_df['serialnumber']==SN].sort_index().copy()\n",
    "value_col = 'value'\n",
    "freq='15T'\n",
    "\n",
    "time_col = 'starttimeperiod_local'\n",
    "lag                   = 24\n",
    "threshold             = 5\n",
    "influence             = 0 \n",
    "signal_abs_threshold  = 1.5\n",
    "smooth_rolling_window = 48\n",
    "n_zoom=1000\n",
    "\n",
    "fit_signal_col_peaks = 'test_final'\n",
    "\n",
    "\n",
    "# df_i = ami_df_resamples_evs['H'][ami_df_resamples_evs['H']['serialnumber']==SN].sort_index().copy()\n",
    "# value_col = 'mean_TRS value'\n",
    "# freq='H'\n",
    "\n",
    "# time_col = 'starttimeperiod_local'\n",
    "# lag                   = 12\n",
    "# threshold             = 3\n",
    "# influence             = 0 \n",
    "# signal_abs_threshold  = 1.5\n",
    "# smooth_rolling_window = 24\n",
    "# n_zoom=250\n",
    "\n",
    "# fit_signal_col_peaks = 'test_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = thresholding_algo_df_i(\n",
    "    df_i=df_i, \n",
    "    value_col=value_col, \n",
    "    lag=lag, \n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold, \n",
    "    smooth_rolling_window=smooth_rolling_window\n",
    ")\n",
    "#-------------------------\n",
    "peak_df_i = set_signal_groups_in_df_i(\n",
    "    df_i=df_i, \n",
    "    SN_col='serialnumber',\n",
    "    signal_col='signal_binary', \n",
    "    return_signal_group_col='signal_grp'     \n",
    ")\n",
    "#-------------------------\n",
    "peak_df_i = build_peak_features_for_df_i(\n",
    "    df_i=peak_df_i, \n",
    "    signal_group_col='signal_grp', \n",
    "    value_col=fit_signal_col_peaks,\n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local',        \n",
    ")\n",
    "#--------------------------------------------------\n",
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.3))\n",
    "\n",
    "df_i[value_col].plot(ax=axs[0], color='tab:blue', label='Raw Data')\n",
    "df_i[df_i['signal_binary']==True].reset_index().plot.scatter(\n",
    "    ax=axs[0], x='starttimeperiod_local', y=value_col, color='tab:green', label='Peaks'\n",
    ")\n",
    "df_i['test_final'].plot(ax=axs[0], color='tab:red', label='Final Data')\n",
    "\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "axs[0].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[0].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "df_i_zoom = df_i[:n_zoom].copy()\n",
    "#-----\n",
    "df_i_zoom[value_col].plot(ax=axs[1], color='tab:blue', label='Raw Data')\n",
    "df_i_zoom[df_i_zoom['signal_binary']==True].reset_index().plot.scatter(\n",
    "    ax=axs[1], x='starttimeperiod_local', y=value_col, color='tab:green', label='Peaks'\n",
    ")\n",
    "df_i_zoom['test_final'].plot(ax=axs[1], color='tab:red', label='Final Data')\n",
    "\n",
    "axs[1].legend(loc=\"upper right\")\n",
    "axs[1].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[1].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#--------------------------------------------------\n",
    "left_text_x=0.95\n",
    "text_beg_y = 0.60\n",
    "y_spacing = 0.035\n",
    "#-------------------------\n",
    "# Need to iterate over all strings first to get length of longest\n",
    "# This is needed so I can align all '=' signs\n",
    "max_metric_len = 0\n",
    "max_value_len  = 0\n",
    "for i, (metric_i, value_i) in enumerate(peak_df_i.items()):\n",
    "    if isinstance(value_i, pd.Timedelta):\n",
    "        value_i = value_i.round(\"S\")\n",
    "    else:\n",
    "        value_i = np.round(value_i, decimals=3)\n",
    "    if len(metric_i)>max_metric_len:\n",
    "        max_metric_len = len(metric_i)\n",
    "    if len(str(value_i))>max_value_len:\n",
    "        max_value_len = len(str(value_i))\n",
    "# If max_metric_len==18 and max_value_len==16, the following line will creat: '{:>18}={:<16}'\n",
    "strng_fmt = f'{{:>{max_metric_len+1}}}={{:<{max_value_len+1}}}'\n",
    "#-------------------------\n",
    "# Now, actually print the texts\n",
    "for i, (metric_i, value_i) in enumerate(peak_df_i.items()):\n",
    "    if isinstance(value_i, pd.Timedelta):\n",
    "        value_i = value_i.round(\"S\")\n",
    "    else:\n",
    "        value_i = np.round(value_i, decimals=3)\n",
    "    strng = f\"{metric_i} = {value_i}\"\n",
    "    strng_split = strng.split('=')\n",
    "    final_strng = strng_fmt.format(*strng_split)\n",
    "    fig.text(\n",
    "        left_text_x, \n",
    "        text_beg_y-y_spacing*i, \n",
    "        final_strng, \n",
    "        fontsize=20, \n",
    "        fontfamily='monospace', \n",
    "        ha='left'\n",
    "    )\n",
    "#-------------------------\n",
    "if save_figs:\n",
    "    save_name = f\"NewPeakMetrics_{freq}_L{lag}_T{threshold}_ST{str(signal_abs_threshold).replace('.', '_')}_SRW{smooth_rolling_window}.png\"\n",
    "    Plot_General.save_fig(\n",
    "        fig=fig, \n",
    "        save_dir=figs_save_dir, \n",
    "        save_name=save_name, \n",
    "        bbox_inches='tight'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe01777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729bb098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38ab8f81",
   "metadata": {},
   "source": [
    "# New Keras (METHODOLOGY PLOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92033ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_i = ami_df_resamples_evs['H'][ami_df_resamples_evs['H']['serialnumber']=='786854657'].sort_index().copy()\n",
    "df_i = ami_df_resamples_evs['H'][ami_df_resamples_evs['H']['serialnumber']==SN].sort_index().copy()\n",
    "value_col = 'mean_TRS value'\n",
    "freq='H'\n",
    "\n",
    "width_to_plot = '4W'\n",
    "\n",
    "lag                   = 24\n",
    "threshold             = 3\n",
    "influence             = 0 \n",
    "signal_abs_threshold  = 1.0\n",
    "smooth_rolling_window = 48\n",
    "\n",
    "smooth_col = f'{value_col}_smooth'\n",
    "smooth_roll_mean_col = f'{value_col}_smooth_roll_mean'\n",
    "smooth_roll_std_col  = f'{value_col}_smooth_roll_std'\n",
    "\n",
    "signal_col_out='signal'\n",
    "signal_pos_col_out='signal_pos'\n",
    "signal_binary_col_out='signal_binary'\n",
    "signal_pos_binary_col_out='signal_pos_binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fd247",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtpd = thresholding_algo(\n",
    "    y=df_i[value_col].tolist(), \n",
    "    lag=lag,\n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold\n",
    ")\n",
    "df_i['simple_signals'] = rtpd['signals']\n",
    "df_i_simple = df_i.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = thresholding_algo_df_i(\n",
    "    df_i=df_i, \n",
    "    value_col=value_col, \n",
    "    lag=lag, \n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold, \n",
    "    smooth_rolling_window=smooth_rolling_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_shape = df_i.shape[0]\n",
    "df_i = pd.merge(df_i, df_i_simple[['simple_signals']], left_index=True, right_index=True, how='inner')\n",
    "assert(df_i.shape[0]==og_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ce5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df_i.loc[df_i.index[0]:df_i.index[0]+pd.Timedelta(width_to_plot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce73438",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i['simple_signals'].astype(int).equals(df_i['signal_binary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d503bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i[df_i['simple_signals'].astype(int)!=df_i['signal_binary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4782a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a202f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.3))\n",
    "\n",
    "df_i[value_col].plot(ax=axs[0], color='tab:blue', label='Raw Data')\n",
    "df_i[df_i['simple_signals']==1].reset_index().plot.scatter(\n",
    "    ax=axs[0], x='starttimeperiod_local', y=value_col, color='tab:green', label='1. Peaks (1)'\n",
    ")\n",
    "df_i[smooth_col].plot(ax=axs[0], color='tab:pink', label='2. Peak Removed')\n",
    "df_i[smooth_roll_mean_col].plot(ax=axs[0], color='tab:orange', label='3. Smoothed Peak Removed')\n",
    "# axs[0].legend()\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "axs[0].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[0].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "df_i[value_col].plot(ax=axs[1], color='tab:blue', label='Raw Data')\n",
    "df_i[df_i['signal_binary']==True].reset_index().plot.scatter(\n",
    "    ax=axs[1], x='starttimeperiod_local', y=value_col, color='tab:green', label='Peaks (2)'\n",
    ")\n",
    "df_i['test_final'].plot(ax=axs[1], color='tab:red', label='4. Final Data')\n",
    "# axs[1].legend()\n",
    "axs[1].legend(loc=\"upper right\")\n",
    "axs[1].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[1].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "if save_figs:\n",
    "    save_name = f\"Smoothing_ex_{freq}_L{lag}_T{threshold}_ST{str(signal_abs_threshold).replace('.', '_')}_SRW{smooth_rolling_window}.png\"\n",
    "    Plot_General.save_fig(\n",
    "        fig=fig, \n",
    "        save_dir=figs_save_dir, \n",
    "        save_name=save_name, \n",
    "        bbox_inches='tight'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d966cb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c25d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01cba6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d927a9d3",
   "metadata": {},
   "source": [
    "# New Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68612edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44296d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_i = ami_df_resamples_evs['H'][ami_df_resamples_evs['H']['serialnumber']=='786854657'].sort_index().copy()\n",
    "df_i = ami_df_resamples_evs['H'][ami_df_resamples_evs['H']['serialnumber']==SN].sort_index().copy()\n",
    "value_col = 'mean_TRS value'\n",
    "freq='H'\n",
    "\n",
    "width_to_plot = '4W'\n",
    "\n",
    "lag                   = 24\n",
    "threshold             = 3\n",
    "influence             = 0 \n",
    "signal_abs_threshold  = 1.0\n",
    "smooth_rolling_window = 48\n",
    "\n",
    "smooth_col = f'{value_col}_smooth'\n",
    "smooth_roll_mean_col = f'{value_col}_smooth_roll_mean'\n",
    "smooth_roll_std_col  = f'{value_col}_smooth_roll_std'\n",
    "\n",
    "signal_col_out='signal'\n",
    "signal_pos_col_out='signal_pos'\n",
    "signal_binary_col_out='signal_binary'\n",
    "signal_pos_binary_col_out='signal_pos_binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = thresholding_algo_df_i(\n",
    "    df_i=df_i, \n",
    "    value_col=value_col, \n",
    "    lag=lag, \n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold, \n",
    "    smooth_rolling_window=smooth_rolling_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d44daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df_i.loc[df_i.index[0]:df_i.index[0]+pd.Timedelta(width_to_plot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf8a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.3))\n",
    "\n",
    "df_i[value_col].plot(ax=axs[0], color='tab:blue', label='Raw Data')\n",
    "df_i[df_i['signal_binary']==True].reset_index().plot.scatter(\n",
    "    ax=axs[0], x='starttimeperiod_local', y=value_col, color='tab:green', label='1. Peaks'\n",
    ")\n",
    "df_i[smooth_col].plot(ax=axs[0], color='tab:pink', label='2. Peak Removed')\n",
    "df_i[smooth_roll_mean_col].plot(ax=axs[0], color='tab:orange', label='3. Smoothed Peak Removed')\n",
    "# axs[0].legend()\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "axs[0].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[0].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "df_i[value_col].plot(ax=axs[1], color='tab:blue', label='Raw Data')\n",
    "df_i[df_i['signal_binary']==True].reset_index().plot.scatter(\n",
    "    ax=axs[1], x='starttimeperiod_local', y=value_col, color='tab:green', label='Peaks'\n",
    ")\n",
    "df_i['test_final'].plot(ax=axs[1], color='tab:red', label='4. Final Data')\n",
    "df_i['test_final2'].plot(ax=axs[1], color='tab:cyan', label='4b. Only Peak Kept', alpha=0.75, linestyle='dashed')\n",
    "# axs[1].legend()\n",
    "axs[1].legend(loc=\"upper right\")\n",
    "axs[1].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[1].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "fig.suptitle('CNN Input: kWh Values', y=0.905, fontsize='xx-large', fontweight='bold')\n",
    "#-------------------------\n",
    "if save_figs:\n",
    "    save_name = f\"CNN_values_{freq}_L{lag}_T{threshold}_ST{str(signal_abs_threshold).replace('.', '_')}_SRW{smooth_rolling_window}.png\"\n",
    "    Plot_General.save_fig(\n",
    "        fig=fig, \n",
    "        save_dir=figs_save_dir, \n",
    "        save_name=save_name, \n",
    "        bbox_inches='tight'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99820d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.3))\n",
    "\n",
    "df_i[value_col].plot(ax=axs[0], color='tab:blue', label='Raw Data')\n",
    "df_i[df_i['signal_binary']==True].reset_index().plot.scatter(\n",
    "    ax=axs[0], x='starttimeperiod_local', y=value_col, color='tab:green', label='1. Peaks'\n",
    ")\n",
    "df_i[smooth_col].plot(ax=axs[0], color='tab:pink', label='2. Peak Removed')\n",
    "df_i[smooth_roll_mean_col].plot(ax=axs[0], color='tab:orange', label='3. Smoothed Peak Removed')\n",
    "# axs[0].legend()\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "axs[0].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[0].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "df_i[df_i[signal_binary_col_out]==1].reset_index().plot.scatter(\n",
    "    ax=axs[1], x='starttimeperiod_local', y=signal_col_out, color='tab:green', label='Peaks'\n",
    ")\n",
    "df_i[signal_col_out].plot(ax=axs[1], color='tab:red', label='4. Final Data')\n",
    "# df_i[signal_pos_col_out].plot(ax=axs[1], color='tab:purple')\n",
    "df_i['test_final_signal'].plot(ax=axs[1], color='tab:cyan', label='4b. Only Peak Kept', alpha=0.75, linestyle='dashed')\n",
    "# axs[1].legend()\n",
    "axs[1].legend(loc=\"upper right\")\n",
    "axs[1].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[1].set_ylabel('$n\\sigma$', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "fig.suptitle('CNN Input: kWh $\\mathbf{n\\sigma}$', y=0.905, fontsize='xx-large', fontweight='bold')\n",
    "#-------------------------\n",
    "if save_figs:\n",
    "    save_name = f\"CNN_nsigma_{freq}_L{lag}_T{threshold}_ST{str(signal_abs_threshold).replace('.', '_')}_SRW{smooth_rolling_window}.png\"\n",
    "    Plot_General.save_fig(\n",
    "        fig=fig, \n",
    "        save_dir=figs_save_dir, \n",
    "        save_name=save_name, \n",
    "        bbox_inches='tight'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58341ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0305c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d1d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "707f0ceb",
   "metadata": {},
   "source": [
    "# EV submeter idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af280cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_w_subm = ami_df_evs.groupby('aep_premise_nb', as_index=False, group_keys=False).apply(\n",
    "    lambda x: select_pairs_w_submeter(\n",
    "        ami_df_i=x, \n",
    "        pct_0_thresh_main=0.1, \n",
    "        pct_0_thresh_subm=0.6, \n",
    "        enforce_corr=True, \n",
    "        corr_thresh=0.5, \n",
    "        value_col='value', \n",
    "        time_col='index', \n",
    "        PN_col='aep_premise_nb', \n",
    "        SN_col='serialnumber'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578774e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_w_subm = ami_df_w_subm.sort_index()\n",
    "ami_df_w_subm['date'] = ami_df_w_subm.index\n",
    "#-------------------------\n",
    "ami_df_main = ami_df_w_subm[['aep_premise_nb', 'serialnumber_main', 'value_main', 'date']].copy()\n",
    "ami_df_delt = ami_df_w_subm[['aep_premise_nb', 'serialnumber_main', 'value_delt', 'date']].copy()\n",
    "ami_df_subm = ami_df_w_subm[['aep_premise_nb', 'serialnumber_subm', 'value_subm', 'date']].copy()\n",
    "#-------------------------\n",
    "ami_df_main = ami_df_main.rename(columns={\n",
    "    'serialnumber_main':'serialnumber', \n",
    "    'value_main':'value'\n",
    "})\n",
    "\n",
    "ami_df_delt = ami_df_delt.rename(columns={\n",
    "    'serialnumber_main':'serialnumber', \n",
    "    'value_delt':'value'\n",
    "})\n",
    "\n",
    "ami_df_subm = ami_df_subm.rename(columns={\n",
    "    'serialnumber_subm':'serialnumber', \n",
    "    'value_subm':'value'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_w_subm['aep_premise_nb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_zoom = 3000\n",
    "PN = '029470841'\n",
    "\n",
    "ami_df_main_i = ami_df_main[ami_df_main['aep_premise_nb']==PN].iloc[:n_zoom]\n",
    "ami_df_delt_i = ami_df_delt[ami_df_main['aep_premise_nb']==PN].iloc[:n_zoom]\n",
    "ami_df_subm_i = ami_df_subm[ami_df_main['aep_premise_nb']==PN].iloc[:n_zoom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204e8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.3))\n",
    "ami_df_main_i['value'].plot(ax=axs[0], color='tab:blue', label='Full Data')\n",
    "ami_df_subm_i['value'].plot(ax=axs[0], color='tab:green', label='EV Submeter')\n",
    "axs[0].grid(True, which='both')\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "axs[0].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[0].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "ami_df_main_i['value'].plot(ax=axs[1], color='tab:blue', label='Full Data (target=1)')\n",
    "ami_df_delt_i['value'].plot(ax=axs[1], color='tab:red', label='Full$-$EV Submeter (target=0)', alpha=0.5, linestyle='dashed')\n",
    "axs[1].grid(True, which='both')\n",
    "axs[1].legend(loc=\"upper right\")\n",
    "axs[1].set_xlabel('Datetime', fontsize='xx-large', loc='right')\n",
    "axs[1].set_ylabel('kWh', fontsize='xx-large', loc='top')\n",
    "#-------------------------\n",
    "if save_figs:\n",
    "    Plot_General.save_fig(\n",
    "        fig=fig, \n",
    "        save_dir=figs_save_dir, \n",
    "        save_name='Using_submeters.png', \n",
    "        bbox_inches='tight'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461b198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ebe88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294578e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
