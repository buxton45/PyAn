{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44fed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "# NOTE: To reload a class imported as, e.g., \n",
    "# from module import class\n",
    "# One must call:\n",
    "#   1. import module\n",
    "#   2. reload module\n",
    "#   3. from module import class\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns, natsort_keygen\n",
    "from packaging import version\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm #e.g. for cmap=cm.jet\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIEDE_DEV import AMIEDE_DEV\n",
    "from MECPODf import MECPODf\n",
    "from MECPOAn import MECPOAn\n",
    "from MECPOCollection import MECPOCollection\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "#sys.path.insert(0, os.path.join(os.path.realpath('..'), 'Utilities'))\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "from Utilities_df import DFConstructType\n",
    "import Utilities_dt\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import Plot_Hist\n",
    "import Plot_Bar\n",
    "import GrubbsTest\n",
    "import DataFrameSubsetSlicer\n",
    "from DataFrameSubsetSlicer import DataFrameSubsetSlicer as DFSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca779dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65809760",
   "metadata": {},
   "source": [
    "# DEVELOPMENT OF finding what still needs to be run to complete XFMR/OUTAGE groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_SNs_w_EDEs_df_from_local_files(\n",
    "    date_0, \n",
    "    date_1, \n",
    "    opco='oh', \n",
    "    files_dir_base=r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\SNs_with_end_events', \n",
    "    allow_duplicate_dates=False, \n",
    "    drop_duplicate_dates=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    drop_duplicate_dates only has effect if allow_duplicate_dates is True, \n",
    "      in which case, any duplicate index entries will be dropped, with the first being kept\n",
    "    Note: Cannot call drop_duplicates on DF because elements are lists\n",
    "    \n",
    "    Currently, FILES EXPECTED TO BE IN DIRECTORY os.path.join(files_dir_base, 'OPCO', opco)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    files_dir = os.path.join(files_dir_base, 'OPCO', opco)\n",
    "    assert(os.path.isdir(files_dir))\n",
    "    #-------------------------\n",
    "    year_0 = pd.to_datetime(date_0).year\n",
    "    year_1 = pd.to_datetime(date_1).year\n",
    "    years_needed = list(range(year_0,year_1+1))\n",
    "    #-------------------------\n",
    "    SNs_w_EDEs_dfs = []\n",
    "    for year in years_needed:\n",
    "        file_name = f'{year}.pkl'\n",
    "        file_path = os.path.join(files_dir, file_name)\n",
    "        assert(os.path.exists(file_path))\n",
    "        #-----\n",
    "        SNs_w_EDEs_df_i = pd.read_pickle(file_path)\n",
    "        SNs_w_EDEs_dfs.append(SNs_w_EDEs_df_i)\n",
    "    assert(len(SNs_w_EDEs_dfs)>0)\n",
    "    if len(SNs_w_EDEs_dfs)==1:\n",
    "        SNs_w_EDEs_df = SNs_w_EDEs_dfs[0]\n",
    "    else:\n",
    "        # Make sure all have same columns\n",
    "        cols = SNs_w_EDEs_dfs[0].columns\n",
    "        for df in SNs_w_EDEs_dfs:\n",
    "            assert(df.columns==cols)\n",
    "        #-----\n",
    "        SNs_w_EDEs_df = pd.concat(SNs_w_EDEs_dfs)\n",
    "        #-----\n",
    "        if not allow_duplicate_dates:\n",
    "            assert(SNs_w_EDEs_df.index.nunique()==SNs_w_EDEs_df.shape[0])\n",
    "        else:\n",
    "            if drop_duplicate_dates and SNs_w_EDEs_df.index.nunique()!=SNs_w_EDEs_df.shape[0]:\n",
    "                SNs_w_EDEs_df = SNs_w_EDEs_df.groupby(SNs_w_EDEs_df.index).first()\n",
    "                assert(SNs_w_EDEs_df.index.nunique()==SNs_w_EDEs_df.shape[0])\n",
    "            else:\n",
    "                # Not really necessary to check this...\n",
    "                assert(SNs_w_EDEs_df.shape[0]==sum([x.shape[0] for x in SNs_w_EDEs_dfs]))\n",
    "    #-------------------------\n",
    "    # Make sure index is datetime with daily frequency\n",
    "    SNs_w_EDEs_df.index = pd.to_datetime(SNs_w_EDEs_df.index)\n",
    "    assert(pd.infer_freq(SNs_w_EDEs_df.index)=='D')\n",
    "    SNs_w_EDEs_df.index.freq = 'D'\n",
    "\n",
    "    # Make sure the index is sorted\n",
    "    SNs_w_EDEs_df = SNs_w_EDEs_df.sort_index()\n",
    "    #-------------------------\n",
    "    return SNs_w_EDEs_df\n",
    "\n",
    "\n",
    "def get_SNs_with_end_events_from_SNs_w_EDEs_df(\n",
    "    SNs_w_EDEs_df, \n",
    "    date_0, \n",
    "    date_1, \n",
    "    SNs_col=None, \n",
    "    batch_size=None, \n",
    "    verbose=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    SNs_col:\n",
    "      typically 'serialnumbers'\n",
    "      If left as None, SNs_w_EDEs_df must have only a single column (which it typically does), and SNs_col\n",
    "        is taken to be that column\n",
    "    \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if SNs_col is None:\n",
    "        assert(SNs_w_EDEs_df.shape[1]==1)\n",
    "        SNs_col = SNs_w_EDEs_df.columns[0]\n",
    "    #-------------------------\n",
    "    # Only date portion is considered here, since data taken daily\n",
    "    # i.e., time is thrown away\n",
    "    date_0 = pd.to_datetime(date_0).date()\n",
    "    date_1 = pd.to_datetime(date_1).date()\n",
    "    #-------------------------\n",
    "    # Make sure index is datetime with daily frequency\n",
    "    SNs_w_EDEs_df.index = pd.to_datetime(SNs_w_EDEs_df.index)\n",
    "    assert(pd.infer_freq(SNs_w_EDEs_df.index)=='D')\n",
    "    SNs_w_EDEs_df.index.freq = 'D'\n",
    "\n",
    "    # Make sure the index is sorted\n",
    "    SNs_w_EDEs_df = SNs_w_EDEs_df.sort_index()\n",
    "\n",
    "    # Make sure date_0 and date_1 in bounds\n",
    "    assert(pd.to_datetime(date_0)>=SNs_w_EDEs_df.index[0])\n",
    "    assert(pd.to_datetime(date_1)<=SNs_w_EDEs_df.index[-1])\n",
    "\n",
    "    sub_SNs_w_EDEs_df = SNs_w_EDEs_df[date_0:date_1]\n",
    "    SNs = Utilities_df.consolidate_column_of_lists(\n",
    "        df=sub_SNs_w_EDEs_df, \n",
    "        col=SNs_col, \n",
    "        sort=True, \n",
    "        include_None=False, \n",
    "        batch_size=batch_size, \n",
    "        verbose=verbose\n",
    "    )\n",
    "    #-------------------------\n",
    "    return SNs\n",
    "\n",
    "def get_SNs_with_end_events_from_local_files(\n",
    "    date_0, \n",
    "    date_1, \n",
    "    opco='oh', \n",
    "    files_dir_base=r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\SNs_with_end_events', \n",
    "    batch_size=None, \n",
    "    verbose=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    SNs_w_EDEs_df = build_SNs_w_EDEs_df_from_local_files(\n",
    "        date_0=date_0, \n",
    "        date_1=date_1, \n",
    "        opco=opco, \n",
    "        files_dir_base=files_dir_base, \n",
    "        allow_duplicate_dates=False, \n",
    "        drop_duplicate_dates=True\n",
    "    )\n",
    "    #-------------------------\n",
    "    SNs = get_SNs_with_end_events_from_SNs_w_EDEs_df(\n",
    "        SNs_w_EDEs_df=SNs_w_EDEs_df, \n",
    "        date_0=date_0, \n",
    "        date_1=date_1,\n",
    "        SNs_col=None, \n",
    "        batch_size=batch_size, \n",
    "        verbose=verbose\n",
    "    )\n",
    "    #-------------------------\n",
    "    return SNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_time_interval_infos_from_summary_file(summary_path):\n",
    "    r\"\"\"\n",
    "    Specialized function.\n",
    "    TODO!!!!!!!!!!!!!!!!!!\n",
    "    In the future, this stuff should probably be output at run-time somewhere\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(os.path.exists(summary_path))\n",
    "    #-------------------------\n",
    "    f = open(summary_path)\n",
    "    summary_json_data = json.load(f)\n",
    "    assert('sql_statement' in summary_json_data)\n",
    "    sql_statement = summary_json_data['sql_statement']\n",
    "    #-------------------------\n",
    "    f.close()\n",
    "    #-------------------------\n",
    "    # Find the last instance of \"SELECT * FROM USG_X\" to extract how many sets of \n",
    "    # t_min,t_max,prem_nbs to expect.\n",
    "    # If not found, expect only one\n",
    "    pattern = r\"SELECT \\* FROM .*_(\\d*)$\"\n",
    "    found_all = re.findall(pattern, sql_statement)\n",
    "    if len(found_all)==0:\n",
    "        n_groups_expected = 1\n",
    "    else:\n",
    "        assert(len(found_all)==1)\n",
    "        n_groups_expected = int(found_all[0])+1\n",
    "    #-------------------------\n",
    "    # So obnoxious...using flags=re.MULTILINE|re.DOTALL with .* was causing the trailing ) and \\n to match in premise numbers\n",
    "    #   This also made it such that only the last occurrence of the match was returned.\n",
    "    #   What I found to work was eliminating the re.DOTALL flag and [\\s\\S] to match a newline or any symbol.\n",
    "    #     Typically, . matches everything BUT newline characters (unless using re.DOTALL).\n",
    "    #     The main idea is that the opposite shorthand classes inside a character class match any symbol there is in the input string.\n",
    "    # NOTE: The new pattern should find both, e.g.:\n",
    "    #       (a) un_rin.aep_premise_nb IN ('102186833','102252463','106876833','108452463')\n",
    "    #       (b) un_rin.aep_premise_nb = '072759453'\n",
    "    #       However, now need the if prem_nbs[0]=='(' block below\n",
    "    #       ALSO: (?: TIMESTAMP){0,1} needed to be included (twice) after switch to Athena\n",
    "    #             See, e.g., is_timestamp in SQLWhere class    \n",
    "    pattern = r\"SELECT[\\s\\S]+?\"\\\n",
    "              r\"(?:\\'(\\d*)\\' AS OUTG_REC_NB[\\s\\S]+?)\"\\\n",
    "              r\"CAST.* BETWEEN(?: TIMESTAMP){0,1} '(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})' AND(?: TIMESTAMP){0,1} '(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})'[\\s\\S]+?\"\\\n",
    "              r\"un_rin.aep_premise_nb\\s*(?:IN|=)?\\s*(\\((?:.*)\\)|(?:\\'.*\\'))[\\s\\S]+?\"\n",
    "    \n",
    "    found_all = re.findall(pattern, sql_statement, flags=re.MULTILINE)\n",
    "    assert(len(found_all)>0)\n",
    "    #-------------------------\n",
    "    return_coll=[]\n",
    "    for found in found_all:\n",
    "        assert(len(found)==4)\n",
    "        outg_rec_nb,t_min,t_max,prem_nbs = found\n",
    "        if prem_nbs[0]=='(':\n",
    "            assert(prem_nbs[-1]==')')\n",
    "            prem_nbs=prem_nbs[1:-1]\n",
    "        prem_nbs = prem_nbs.replace('\\'', '')\n",
    "        prem_nbs = prem_nbs.split(',')\n",
    "        return_dict_i = {\n",
    "            'outg_rec_nb':outg_rec_nb, \n",
    "            'prem_nbs':prem_nbs, \n",
    "            't_min':t_min, \n",
    "            't_max':t_max\n",
    "        }\n",
    "        return_coll.append(return_dict_i)\n",
    "    #-------------------------\n",
    "    return return_coll\n",
    "\n",
    "\n",
    "def get_search_time_interval_infos_df_from_summary_file(\n",
    "    summary_path, \n",
    "    output_index_name='outg_rec_nb', \n",
    "    output_prem_nbs_col='prem_nbs', \n",
    "    output_t_min_col='t_min', \n",
    "    output_t_max_col='t_max', \n",
    "    include_summary_path=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Returns a pd.DataFrame version of get_search_time_interval_infos_from_summary_file\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    return_df = pd.DataFrame()\n",
    "    no_outg_time_infos = get_search_time_interval_infos_from_summary_file(summary_path)\n",
    "    for i,no_outg_time_info_i in enumerate(no_outg_time_infos):\n",
    "        no_outg_time_info_df_i = pd.DataFrame(\n",
    "            data=[[no_outg_time_info_i['t_min'], no_outg_time_info_i['t_max']]], \n",
    "            columns=[output_t_min_col, output_t_max_col], \n",
    "            index=[no_outg_time_info_i['outg_rec_nb']]\n",
    "        )\n",
    "        no_outg_time_info_df_i.index.name=output_index_name\n",
    "        no_outg_time_info_df_i[output_prem_nbs_col] = [no_outg_time_info_i['prem_nbs']]\n",
    "        #-------------------------\n",
    "        return_df = pd.concat([return_df, no_outg_time_info_df_i], ignore_index=False)\n",
    "    return_df[output_t_min_col] = pd.to_datetime(return_df[output_t_min_col])\n",
    "    return_df[output_t_max_col] = pd.to_datetime(return_df[output_t_max_col])\n",
    "    if include_summary_path:\n",
    "        return_df['summary_path'] = summary_path\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def combine_search_time_interval_infos_dfs_w_like_indices(\n",
    "    search_time_interval_infos_df_i, \n",
    "    index_name='outg_rec_nb', \n",
    "    prem_nbs_col='prem_nbs', \n",
    "    t_min_col='t_min', \n",
    "    t_max_col='t_max', \n",
    "    summary_path_col='summary_path'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Small helper function for use in get_search_time_interval_infos_df_from_summary_files\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # If there is only one row in df, then no combining to be done, in which\n",
    "    # case, simply return series version of df\n",
    "    if search_time_interval_infos_df_i.shape[0]==1:\n",
    "        return search_time_interval_infos_df_i.squeeze()\n",
    "    #-------------------------\n",
    "    assert(search_time_interval_infos_df_i.index.nunique()==1)\n",
    "    assert(search_time_interval_infos_df_i[t_min_col].nunique()==1)\n",
    "    assert(search_time_interval_infos_df_i[t_max_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    return_series = search_time_interval_infos_df_i.iloc[0].copy()\n",
    "    #-------------------------\n",
    "    prem_nbs = Utilities_df.consolidate_column_of_lists(\n",
    "        df=search_time_interval_infos_df_i, \n",
    "        col=prem_nbs_col, \n",
    "        sort=True, \n",
    "        include_None=True, \n",
    "        batch_size=None, \n",
    "        verbose=False\n",
    "    )\n",
    "    return_series[prem_nbs_col] = prem_nbs\n",
    "    #-------------------------\n",
    "    if summary_path_col in search_time_interval_infos_df_i.columns:\n",
    "        summary_paths = Utilities_df.consolidate_column_of_lists(\n",
    "            df=search_time_interval_infos_df_i, \n",
    "            col=summary_path_col, \n",
    "            sort=True, \n",
    "            include_None=True, \n",
    "            batch_size=None, \n",
    "            verbose=False\n",
    "        )\n",
    "        return_series[summary_path_col] = summary_paths\n",
    "    #-------------------------\n",
    "    return return_series\n",
    "\n",
    "def get_search_time_interval_infos_df_from_summary_files(\n",
    "    summary_paths, \n",
    "    output_index_name='outg_rec_nb', \n",
    "    output_prem_nbs_col='prem_nbs', \n",
    "    output_t_min_col='t_min', \n",
    "    output_t_max_col='t_max', \n",
    "    include_summary_paths=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Handles multiple summary files\n",
    "    \n",
    "    Note: drop_duplicates will remove rows if indices are different (but all columns equal)\n",
    "          Therefore, if make_prem_nbs_idx==True, this should only be done AFTER drop duplicates\n",
    "          This explains why make_prem_nbs_idx=False in the call to get_search_time_interval_infos_df_from_summary_file\n",
    "    Note: The reason for drop duplicates if for the case where a collection is split over mulitple\n",
    "          files/runs (i.e., the asynchronous case)\n",
    "    \"\"\"\n",
    "    return_df = pd.DataFrame()\n",
    "    for summary_path in summary_paths:\n",
    "        df_i = get_search_time_interval_infos_df_from_summary_file(\n",
    "            summary_path=summary_path, \n",
    "            output_index_name=output_index_name, \n",
    "            output_prem_nbs_col=output_prem_nbs_col, \n",
    "            output_t_min_col=output_t_min_col, \n",
    "            output_t_max_col=output_t_max_col, \n",
    "            include_summary_path=include_summary_paths\n",
    "        )\n",
    "        return_df = pd.concat([return_df, df_i], ignore_index=False)\n",
    "    #-------------------------\n",
    "    # Now, take care of grouping together any repeated index values.\n",
    "    # This would happen if, e.g., an outage were split acrosse multiple files, or even if an \n",
    "    #   outage were split across multiple CTEs in a single file.\n",
    "    return_df = return_df.groupby(return_df.index).apply(\n",
    "        lambda x: combine_search_time_interval_infos_dfs_w_like_indices(\n",
    "            search_time_interval_infos_df_i=x, \n",
    "            index_name=output_index_name, \n",
    "            prem_nbs_col=output_prem_nbs_col, \n",
    "            t_min_col=output_t_min_col, \n",
    "            t_max_col=output_t_max_col, \n",
    "            summary_path_col='summary_path'\n",
    "        )\n",
    "    )    \n",
    "    #-------------------------\n",
    "    return return_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecpo_dict_01_30_1 = AMIEDE_DEV.build_mecpo_dict_from_pkls(\n",
    "    r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\rcpo_dfs\\NEW_w_prems', \n",
    "    days_min_max_outg_td_window=[1,30], normalize_by_time_interval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir_outg             = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\EndEvents'\n",
    "\n",
    "is_no_outg = False\n",
    "paths = Utilities.find_all_paths(base_dir=files_dir_outg, glob_pattern=file_path_glob)\n",
    "# i_beg = 1000\n",
    "# i_end = 2000\n",
    "i_beg = 0\n",
    "i_end = 1000\n",
    "cols_and_types_to_convert_dict=None\n",
    "to_numeric_errors='coerce'\n",
    "assert_all_cols_equal=True\n",
    "outg_rec_nb_col='outg_rec_nb'\n",
    "addtnl_dropna_subset_cols=None\n",
    "\n",
    "days_min_outg_td_window=1\n",
    "days_max_outg_td_window=30\n",
    "\n",
    "min_outg_td_window=datetime.timedelta(days=days_min_outg_td_window)\n",
    "max_outg_td_window=datetime.timedelta(days=days_max_outg_td_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b638669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rcpo_df         = mecpo_dict_01_30_1['full'].cpo_dfs['rcpo_df_raw'].copy()\n",
    "dev_rcpo_no_outg_df = mecpo_dict_01_30_1['no_outg'].cpo_dfs['rcpo_df_raw'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cb3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3957b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS ONLY USED TO DOUBLE CHECK TIMES\n",
    "dovs_outgs = DOVSOutages(\n",
    "    df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "    contstruct_df_args=None, \n",
    "    init_df_in_constructor=True, \n",
    "    build_sql_function=DOVSOutages_SQL.build_sql_outage, \n",
    "    build_sql_function_kwargs=dict(\n",
    "        outg_rec_nbs=dev_rcpo_df.index.tolist(), \n",
    "        from_table_alias='DOV', \n",
    "        datetime_col='DT_OFF_TS_FULL', \n",
    "        cols_of_interest=[\n",
    "            'OUTG_REC_NB', 'DT_ON_TS', \n",
    "            dict(field_desc=f\"DOV.DT_ON_TS - DOV.STEP_DRTN_NB/(60*24)\", \n",
    "                 alias='DT_OFF_TS_FULL', table_alias_prefix=None)\n",
    "        ], \n",
    "        field_to_split='outg_rec_nbs'\n",
    "    ),\n",
    ")\n",
    "outg_dt_off_df = dovs_outgs.df\n",
    "outg_dt_off_df = Utilities_df.convert_col_type(df=outg_dt_off_df, column='OUTG_REC_NB', to_type=str)\n",
    "outg_dt_off_df=outg_dt_off_df.set_index('OUTG_REC_NB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86e67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "# prem_nbs_with_end_events:\n",
    "#   Simply a list of premise numbers which have at least one end event between date_0 and date_1\n",
    "#   It is quicker to build this to encompass all needed dates first, as opposed to building it on-the-fly\n",
    "#     for each individual outage.\n",
    "prem_nbs_with_end_events = get_SNs_with_end_events_from_local_files(\n",
    "    date_0='2017-01-01', \n",
    "    date_1='2022-06-30', \n",
    "    files_dir_base=r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\prem_nbs_with_end_events', \n",
    "    batch_size=100,\n",
    "    verbose=True\n",
    ")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f64ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir_outg             = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\EndEvents'\n",
    "paths_outg = Utilities.find_all_paths(base_dir=files_dir_outg, glob_pattern=file_path_glob)\n",
    "\n",
    "# run_infos_df:\n",
    "#   This is where t_min and t_max come from for each outage.  For the case of outages, run_infos_df will have indices\n",
    "#     equal to outg_rec_nbs, and will have columns [t_min, t_max, prem_nbs, (and possibly) summary path]\n",
    "#   NOTE: Unfortunately, since the original code was run with INNER join with MeterPremise, instead of LEFT join,\n",
    "#         although run_infos_df has a prem_nbs column, it shows the full set of premise numbers in the outage, not \n",
    "#         the set for which data were actually acquired.  \n",
    "#         If each premise in run_infos_df['prem_nbs'] has a match in MeterPremise, then these would be\n",
    "#         the same, but in general they are not.\n",
    "run_infos_df = get_search_time_interval_infos_df_from_summary_files(\n",
    "    summary_paths=[AMIEDE_DEV.find_summary_file_from_csv(x) for x in paths_outg], \n",
    "    output_index_name='outg_rec_nb', \n",
    "    output_prem_nbs_col='prem_nbs', \n",
    "    output_t_min_col='t_min', \n",
    "    output_t_max_col='t_max', \n",
    "    include_summary_paths=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prem_nbs_for_outages:\n",
    "#   A series object with indices equal to OUTG_REC_NBs and values equal to the premise numbers\n",
    "#     contained in the outage.\n",
    "#   If one wants to use serial numbers instead of premise numbers, one should use \n",
    "#     DOVSOutages.get_serial_numbers_for_outages\n",
    "prem_nbs_for_outages = DOVSOutages.get_premise_nbs_for_outages(\n",
    "    outg_rec_nbs=dev_rcpo_df.index.tolist(), \n",
    "    return_type=pd.Series, \n",
    "    col_type_outg_rec_nb=str, \n",
    "    col_type_premise_nb=None, \n",
    "    to_numeric_errors='coerce', \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c29a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prem_nbs_col_rcpo = '_prem_nbs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcpo_i = dev_rcpo_df.iloc[2]  #pd.Series object\n",
    "# outg_rec_nb_i = rcpo_i.name\n",
    "# prem_nbs_acquired_i = rcpo_i[prem_nbs_col_rcpo]\n",
    "# #-----\n",
    "# if outg_rec_nb_i not in prem_nbs_for_outages.index:\n",
    "#     print(f'outg_rec_nb_i={outg_rec_nb_i} not in prem_nbs_for_outages!!!!!')\n",
    "#     #TODO UNCOMMENT CONTINUE\n",
    "#     #continue\n",
    "    \n",
    "# #UNCOMMENT ELSE\n",
    "# #else\n",
    "# prem_nbs_in_outg_i = prem_nbs_for_outages.loc[outg_rec_nb_i]\n",
    "# #-----\n",
    "# run_info_i = run_infos_df.loc[outg_rec_nb_i]\n",
    "# t_min_i = run_info_i['t_min']\n",
    "# t_max_i = run_info_i['t_max']\n",
    "# #-----\n",
    "# # prem_nbs_with_end_events = get_SNs_with_end_events_from_local_files(\n",
    "# #     date_0=t_min_i, \n",
    "# #     date_1=t_max_i, \n",
    "# #     files_dir_base=r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\prem_nbs_with_end_events'\n",
    "# # )\n",
    "# #-----\n",
    "# # Premise numbers needed are the difference between prem_nbs_in_outg_i and prem_nbs_acquired_i, but only if the premise numbers\n",
    "# # have an end event\n",
    "# # NOTE: Using set operations MUCH faster than list comprehension\n",
    "# prem_nbs_w_events_needed_i = list((set(prem_nbs_in_outg_i).difference(set(prem_nbs_acquired_i)))\n",
    "#                                   .intersection(prem_nbs_with_end_events))\n",
    "# print(len(prem_nbs_w_events_needed_i))\n",
    "\n",
    "\n",
    "# outgs_w_prem_nbs_and_search_times = []\n",
    "# outgs_w_prem_nbs_and_search_times.append(\n",
    "#     dict(\n",
    "#         outg_rec_nb=outg_rec_nb_i, \n",
    "#         prem_nbs=prem_nbs_w_events_needed_i, \n",
    "#         t_min=t_min_i, \n",
    "#         t_max=t_max_i\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd6b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbf476",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dev_rcpo_df.shape[0])\n",
    "outgs_w_prem_nbs_and_search_times = []\n",
    "for idx in range(dev_rcpo_df.shape[0]):\n",
    "    #print(idx)\n",
    "    if idx%50==0:\n",
    "        print(idx)\n",
    "    rcpo_i = dev_rcpo_df.iloc[idx]  #pd.Series object\n",
    "    outg_rec_nb_i = rcpo_i.name\n",
    "    prem_nbs_acquired_i = rcpo_i[prem_nbs_col_rcpo]\n",
    "    #-----\n",
    "    if outg_rec_nb_i not in prem_nbs_for_outages.index:\n",
    "        print(f'outg_rec_nb_i={outg_rec_nb_i} not in prem_nbs_for_outages!!!!!')\n",
    "        continue\n",
    "    #-----\n",
    "    prem_nbs_in_outg_i = prem_nbs_for_outages.loc[outg_rec_nb_i]\n",
    "    #-----\n",
    "    run_info_i = run_infos_df.loc[outg_rec_nb_i]\n",
    "    t_min_i = run_info_i['t_min']\n",
    "    t_max_i = run_info_i['t_max']\n",
    "    #-----\n",
    "    # prem_nbs_with_end_events = get_SNs_with_end_events_from_local_files(\n",
    "    #     date_0=t_min_i, \n",
    "    #     date_1=t_max_i, \n",
    "    #     files_dir_base=r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\prem_nbs_with_end_events'\n",
    "    # )\n",
    "    #-----\n",
    "    # Premise numbers needed are the difference between prem_nbs_in_outg_i and prem_nbs_acquired_i, but only if the premise numbers\n",
    "    # have an end event\n",
    "    # NOTE: Using set operations MUCH faster than list comprehension\n",
    "    prem_nbs_w_events_needed_i = list((set(prem_nbs_in_outg_i).difference(set(prem_nbs_acquired_i)))\n",
    "                                      .intersection(prem_nbs_with_end_events))\n",
    "    #-----\n",
    "    outgs_w_prem_nbs_and_search_times.append(\n",
    "        dict(\n",
    "            outg_rec_nb=outg_rec_nb_i, \n",
    "            prem_nbs=prem_nbs_w_events_needed_i, \n",
    "            t_min=t_min_i, \n",
    "            t_max=t_max_i\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a3a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52588317",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outgs_w_prem_nbs_and_search_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90296939",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(outgs_w_prem_nbs_and_search_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest_end_dev_event = TableInfos.AMIEndEvents_TI.std_columns_of_interest\n",
    "\n",
    "cols_of_interest_met_prem = TableInfos.MeterPremise_TI.std_columns_of_interest\n",
    "cols_of_interest_met_prem.append('curr_acct_cls_cd')\n",
    "\n",
    "df_construct_type=DFConstructType.kRunSqlQuery\n",
    "contstruct_df_args_end_events=None\n",
    "\n",
    "build_sql_function = AMIEndEvents_SQL.build_sql_end_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outgs_w_prem_nbs_and_search_times))\n",
    "for i in range(len(outgs_w_prem_nbs_and_search_times)):\n",
    "    print(i)\n",
    "    entry_i = outgs_w_prem_nbs_and_search_times[i]\n",
    "    outg_rec_nb_i = entry_i['outg_rec_nb']\n",
    "    prem_nbs_i = entry_i['prem_nbs']\n",
    "    t_min_i = entry_i['t_min']\n",
    "    t_max_i = entry_i['t_max']\n",
    "    if len(prem_nbs_i)==0:\n",
    "        continue #all already recorded!\n",
    "    \n",
    "    build_sql_function_kwargs = dict(\n",
    "        cols_of_interest=cols_of_interest_end_dev_event, \n",
    "        premise_nbs=prem_nbs_i, \n",
    "        date_range=[str(t_min_i.date()), str(t_max_i.date())],\n",
    "        datetime_range=[str(t_min_i), str(t_max_i)], \n",
    "        serialnumber_col='serialnumber', \n",
    "        from_table_alias='un_rin', \n",
    "        schema_name='meter_events', \n",
    "        table_name='end_device_event', \n",
    "        datetime_col = 'valuesinterval', \n",
    "        datetime_pattern = r\"([0-9]{4}-[0-9]{2}-[0-9]{2})T([0-9]{2}:[0-9]{2}:[0-9]{2}).*\", \n",
    "        date_col     = 'aep_event_dt', \n",
    "        join_mp_args=dict(\n",
    "            join_with_CTE=True, \n",
    "            build_mp_kwargs=dict(cols_of_interest=cols_of_interest_met_prem), \n",
    "            join_type='LEFT'\n",
    "        ), \n",
    "    #     field_to_split='df_mp_no_outg', \n",
    "    #     field_to_split_location_in_kwargs=['df_mp_no_outg'], \n",
    "    #     sort_coll_to_split=False,\n",
    "    #     batch_size=10, verbose=True, n_update=1\n",
    "        addtnl_select_elements = [dict(field_desc=f\"'{outg_rec_nb_i}'\", alias='OUTG_REC_NB_GPD_FOR_SQL')]\n",
    "    )\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    save_args = dict(save_to_file=True, \n",
    "                     save_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\EndEvents', \n",
    "                     save_name=r'end_events.csv', \n",
    "                     index=True)\n",
    "\n",
    "    save_args['offset_int'] = GenAn.get_next_summary_file_tag_int(save_args)\n",
    "    save_args = GenAn.prepare_save_args(save_args, make_save_dir_if_dne=False)\n",
    "    save_args['save_name'] = Utilities.append_to_path(save_args['save_name'], appendix=f'_{save_args[\"offset_int\"]}', \n",
    "                                                        ext_to_find=save_args['save_ext'], append_to_end_if_ext_no_found=True)\n",
    "    save_args = GenAn.prepare_save_args(save_args, make_save_dir_if_dne=False)\n",
    "\n",
    "    end_events = AMIEndEvents(\n",
    "        df_construct_type=df_construct_type, \n",
    "        contstruct_df_args = contstruct_df_args_end_events, \n",
    "        build_sql_function=build_sql_function, \n",
    "        build_sql_function_kwargs=build_sql_function_kwargs, \n",
    "        init_df_in_constructor=True, \n",
    "        save_args=save_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197ef3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b77759d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
