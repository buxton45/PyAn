{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb520489",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./model_end_events_for_outages_METHODS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5755605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "# NOTE: To reload a class imported as, e.g., \n",
    "# from module import class\n",
    "# One must call:\n",
    "#   1. import module\n",
    "#   2. reload module\n",
    "#   3. from module import class\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns, natsort_keygen\n",
    "from packaging import version\n",
    "import copy\n",
    "from functools import reduce\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm #e.g. for cmap=cm.jet\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "from EEMSP import EEMSP\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIEDE_DEV import AMIEDE_DEV\n",
    "from MECPODf import MECPODf\n",
    "from MECPOAn import MECPOAn\n",
    "from MECPOCollection import MECPOCollection\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "#sys.path.insert(0, os.path.join(os.path.realpath('..'), 'Utilities'))\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "from Utilities_df import DFConstructType\n",
    "import Utilities_dt\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import Plot_Hist\n",
    "import Plot_Bar\n",
    "import GrubbsTest\n",
    "import DataFrameSubsetSlicer\n",
    "from DataFrameSubsetSlicer import DataFrameSubsetSlicer as DFSlicer\n",
    "from CustomJSON import CustomEncoder, CustomWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import scipy\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33459065",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d471c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num=0\n",
    "\n",
    "# save_dir_model_base = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20230615\\Models_00_05'\n",
    "save_dir_model_base = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20230615\\Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d97a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = False\n",
    "save_dir_model = None\n",
    "if save_dir_model is None:\n",
    "    save_dir_model = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_dir_model = os.path.join(save_dir_model_base, save_dir_model)\n",
    "#-----\n",
    "if not os.path.exists(save_dir_model) and save_results:\n",
    "    os.makedirs(save_dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d2a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92933a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_others=True\n",
    "\n",
    "merged_df_full=pd.read_pickle(os.path.join(save_dir_model_base, 'merged_df_full.pkl'))\n",
    "merged_df_no_outg=pd.read_pickle(os.path.join(save_dir_model_base, 'merged_df_no_outg.pkl'))\n",
    "merged_df_no_outg_prstn=pd.read_pickle(os.path.join(save_dir_model_base, 'merged_df_no_outg_prstn.pkl'))\n",
    "#-------------------------\n",
    "# with open(os.path.join(save_dir_model_base, 'mecpo_coll_full.pkl'), 'rb') as handle:\n",
    "#     mecpo_coll_full = pickle.load(handle)\n",
    "# with open(os.path.join(save_dir_model_base, 'mecpo_coll_no_outg.pkl'), 'rb') as handle:\n",
    "#     mecpo_coll_no_outg = pickle.load(handle)\n",
    "# with open(os.path.join(save_dir_model_base, 'mecpo_coll_no_outg_prstn.pkl'), 'rb') as handle:\n",
    "#     mecpo_coll_no_outg_prstn = pickle.load(handle)\n",
    "#-------------------------\n",
    "with open(os.path.join(save_dir_model_base, 'counts_series_full.pkl'), 'rb') as handle:\n",
    "    counts_series_full = pickle.load(handle)\n",
    "with open(os.path.join(save_dir_model_base, 'counts_series_no_outg.pkl'), 'rb') as handle:\n",
    "    counts_series_no_outg = pickle.load(handle)\n",
    "with open(os.path.join(save_dir_model_base, 'counts_series_no_outg_prstn.pkl'), 'rb') as handle:\n",
    "    counts_series_no_outg_prstn = pickle.load(handle)\n",
    "    \n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "merged_df_no_outg_prstn.index.names     = ['trsf_pole_nb', 'no_outg_rec_nb']\n",
    "counts_series_no_outg_prstn.index.names = ['trsf_pole_nb', 'no_outg_rec_nb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30565e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df_full.shape)\n",
    "print(merged_df_no_outg.shape)\n",
    "print(merged_df_no_outg_prstn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "natsorted(merged_df_full.columns.get_level_values(1).unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1658dcfe",
   "metadata": {},
   "source": [
    "### Initiate summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd574252",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict = dict()\n",
    "#-------------------------\n",
    "an_keys = natsorted(merged_df_full.columns.get_level_values(0).unique().tolist())\n",
    "assert(an_keys==natsorted(merged_df_no_outg.columns.get_level_values(0).unique().tolist()))\n",
    "assert(an_keys==natsorted(merged_df_no_outg_prstn.columns.get_level_values(0).unique().tolist()))\n",
    "#-----\n",
    "summary_dict['an_keys'] = an_keys\n",
    "#-------------------------\n",
    "n_top_reasons_to_inclue = 10\n",
    "# n_top_reasons_to_inclue = None\n",
    "#-----\n",
    "summary_dict['n_top_reasons_to_inclue'] = n_top_reasons_to_inclue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81e311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368cb77d",
   "metadata": {},
   "source": [
    "# !!!!!!! No outage data have indices backwards from outages!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b21959",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_no_outg = merged_df_no_outg.reset_index().set_index(['no_outg_rec_nb', 'trsf_pole_nb'])\n",
    "counts_series_no_outg = counts_series_no_outg.reset_index().set_index(['no_outg_rec_nb', 'trsf_pole_nb']).squeeze()\n",
    "#-----\n",
    "merged_df_no_outg_prstn = merged_df_no_outg_prstn.reset_index().set_index(['no_outg_rec_nb', 'trsf_pole_nb'])\n",
    "counts_series_no_outg_prstn = counts_series_no_outg_prstn.reset_index().set_index(['no_outg_rec_nb', 'trsf_pole_nb']).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d90550",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_full.columns.get_level_values(1).unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf77a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c08e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(save_dir_model_base).name=='Models' or Path(save_dir_model_base).name=='Models_00_05':\n",
    "    is_norm=True\n",
    "elif Path(save_dir_model_base).name=='Models_raw' or Path(save_dir_model_base).name=='Models_00_05_raw':\n",
    "    is_norm=False\n",
    "else:\n",
    "    assert(0)\n",
    "\n",
    "if n_top_reasons_to_inclue is not None:\n",
    "    merged_df_full, [merged_df_no_outg, merged_df_no_outg_prstn] = MECPOCollection.get_top_reasons_subset_from_merged_cpo_df_and_project_from_others(\n",
    "        merged_cpo_df=merged_df_full,\n",
    "        other_dfs_w_counts_series=[ \n",
    "            [merged_df_no_outg, counts_series_no_outg], \n",
    "            [merged_df_no_outg_prstn, counts_series_no_outg_prstn]\n",
    "        ], \n",
    "        how='per_mecpo_an', \n",
    "        n_reasons_to_include=n_top_reasons_to_inclue,\n",
    "        combine_others=combine_others,\n",
    "        output_combine_others_col='Other Reasons',\n",
    "        SNs_tags=None, \n",
    "        is_norm=is_norm, \n",
    "        counts_series=counts_series_full\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe263b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df_full.shape)\n",
    "print(merged_df_no_outg.shape)\n",
    "print(merged_df_no_outg_prstn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e46a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df_full.shape)\n",
    "print(merged_df_no_outg.shape)\n",
    "print(merged_df_no_outg_prstn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62249ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_full=MECPOCollection.get_total_event_counts_for_merged_cpo_df(merged_df_full)\n",
    "merged_df_no_outg=MECPOCollection.get_total_event_counts_for_merged_cpo_df(merged_df_no_outg)\n",
    "merged_df_no_outg_prstn=MECPOCollection.get_total_event_counts_for_merged_cpo_df(merged_df_no_outg_prstn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bcf03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df_full.shape)\n",
    "print(merged_df_no_outg.shape)\n",
    "print(merged_df_no_outg_prstn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419689d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(set(merged_df_full.index).difference(set(counts_series_full.index)))==0)\n",
    "assert(len(set(merged_df_no_outg.index).difference(set(counts_series_no_outg.index)))==0)\n",
    "assert(len(set(merged_df_no_outg_prstn.index).difference(set(counts_series_no_outg_prstn.index)))==0)\n",
    "\n",
    "merged_df_full = pd.merge(\n",
    "    merged_df_full, \n",
    "    counts_series_full.to_frame(name=('nSNs', 'nSNs')), \n",
    "    left_index=True, right_index=True, how='inner')\n",
    "\n",
    "merged_df_no_outg = pd.merge(\n",
    "    merged_df_no_outg, \n",
    "    counts_series_no_outg.to_frame(name=('nSNs', 'nSNs')), \n",
    "    left_index=True, right_index=True, how='inner')\n",
    "\n",
    "merged_df_no_outg_prstn = pd.merge(\n",
    "    merged_df_no_outg_prstn, \n",
    "    counts_series_no_outg_prstn.to_frame(name=('nSNs', 'nSNs')), \n",
    "    left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b826bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df_full.shape)\n",
    "print(merged_df_no_outg.shape)\n",
    "print(merged_df_no_outg_prstn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df_full.shape)\n",
    "print(merged_df_full.index.get_level_values(0).nunique())\n",
    "print(merged_df_full.index.get_level_values(1).nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_full.index.get_level_values(1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf90da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df_full['01-05 Days']\n",
    "# merged_df_full['00-01 Days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bfd59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67754ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3b7fac1",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c5e72",
   "metadata": {},
   "source": [
    "# EEMSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494532a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b93a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_eemsp = True\n",
    "build_eemsp = False\n",
    "#-----\n",
    "summary_dict['merge_eemsp'] = merge_eemsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76036f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_eemsp:\n",
    "    trsf_pole_nbs = list(set(\n",
    "        merged_df_full.index.get_level_values(1).unique().tolist()+\n",
    "        merged_df_no_outg.index.get_level_values(1).unique().tolist()+\n",
    "        merged_df_no_outg_prstn.index.get_level_values(1).unique().tolist()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_eemsp:\n",
    "    if build_eemsp:\n",
    "        conn_eemsp = Utilities.get_eemsp_oracle_connection()\n",
    "        df_eemsp_OG = build_df_eemsp(conn_eemsp, trsf_pole_nbs, batch_size=1000, verbose=True, n_update=10)\n",
    "        df_eemsp_OG.to_pickle(os.path.join(save_dir_model_base, 'df_eemsp_OG.pkl'))\n",
    "    else:\n",
    "        df_eemsp_OG = pd.read_pickle(os.path.join(save_dir_model_base, 'df_eemsp_OG.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0b998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_eemsp:\n",
    "    df_eemsp = df_eemsp_OG.copy()\n",
    "    print(df_eemsp.shape)\n",
    "    print(df_eemsp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c008112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_eemsp:\n",
    "    # TODO!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Currently, simply taking the first entry wrt LOCATION_NB\n",
    "    df_eemsp = df_eemsp.sort_values(by=['LOCATION_NB', 'INSTALL_DT'], ignore_index=True)\n",
    "    df_eemsp = df_eemsp.groupby('LOCATION_NB', as_index=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f6f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_eemsp:\n",
    "    cols_of_interest_eemsp = [\n",
    "        'LOCATION_NB', \n",
    "        'MFGR_NM', \n",
    "        'INSTALL_DT', \n",
    "        'LAST_TRANS_DESC', \n",
    "        'EQTYPE_ID', \n",
    "        'COOLANT', \n",
    "        'INFO', \n",
    "        'KVA_SIZE',\n",
    "        'PHASE_CNT', \n",
    "        'PRIM_VOLTAGE', \n",
    "        'PROTECTION', \n",
    "        'PRU_NUMBER', \n",
    "        'SEC_VOLTAGE', \n",
    "        'SPECIAL_CHAR', \n",
    "        'TAPS', \n",
    "        'XFTYPE'\n",
    "    ]\n",
    "    # df_eemsp=df_eemsp[df_eemsp['REMOVAL_DT'].isna()]\n",
    "    df_eemsp=df_eemsp[cols_of_interest_eemsp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93861a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_eemsp:\n",
    "    assert(df_eemsp.shape[0]==df_eemsp['LOCATION_NB'].nunique())\n",
    "    print(f\"df_eemsp.shape[0]                 = {df_eemsp.shape[0]}\")\n",
    "    print(f\"len(trsf_pole_nbs)                = {len(trsf_pole_nbs)}\")\n",
    "    print(f\"Diff                              = {len(trsf_pole_nbs)-df_eemsp.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05ecb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36825e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_eemsp:\n",
    "    #-------------------------\n",
    "    print(f\"merged_df_full.shape          = {merged_df_full.shape}\")\n",
    "    print(f\"merged_df_no_outg.shape       = {merged_df_no_outg.shape}\")\n",
    "    print(f\"merged_df_no_outg_prstn.shape = {merged_df_no_outg_prstn.shape}\")\n",
    "    #-------------------------\n",
    "    df_eemsp = Utilities_df.prepend_level_to_MultiIndex(df_eemsp, level_val='EEMSP', axis=1)\n",
    "    #-------------------------\n",
    "    merged_df_full = pd.merge(\n",
    "        merged_df_full, \n",
    "        df_eemsp.set_index(('EEMSP', 'LOCATION_NB')), \n",
    "        left_on=merged_df_full.index.get_level_values(1), \n",
    "        right_index=True, \n",
    "        how='inner'\n",
    "    )\n",
    "    #-------------------------\n",
    "    merged_df_no_outg = pd.merge(\n",
    "        merged_df_no_outg, \n",
    "        df_eemsp.set_index(('EEMSP', 'LOCATION_NB')), \n",
    "        left_on=merged_df_no_outg.index.get_level_values(1), \n",
    "        right_index=True, \n",
    "        how='inner'\n",
    "    )\n",
    "    #-------------------------\n",
    "    merged_df_no_outg_prstn = pd.merge(\n",
    "        merged_df_no_outg_prstn, \n",
    "        df_eemsp.set_index(('EEMSP', 'LOCATION_NB')), \n",
    "        left_on=merged_df_no_outg_prstn.index.get_level_values(1), \n",
    "        right_index=True, \n",
    "        how='inner'\n",
    "    )\n",
    "    #-------------------------\n",
    "    print()\n",
    "    print(f\"merged_df_full.shape          = {merged_df_full.shape}\")\n",
    "    print(f\"merged_df_no_outg.shape       = {merged_df_no_outg.shape}\")\n",
    "    print(f\"merged_df_no_outg_prstn.shape = {merged_df_no_outg_prstn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cacba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING SCHEDULED OUTAGES\n",
    "\n",
    "print(f'merged_df_full.shape = {merged_df_full.shape}')\n",
    "\n",
    "merged_df_full =  MECPODf.get_cpo_df_subset_excluding_mjr_mnr_causes( \n",
    "    cpo_df=merged_df_full, \n",
    "    mjr_mnr_causes_to_exclude=None, \n",
    "    mjr_causes_to_exclude=None,\n",
    "    mnr_causes_to_exclude=['SCO', 'SO'], \n",
    "    outg_rec_nb_col='index'\n",
    ")\n",
    "\n",
    "print(f'merged_df_full.shape = {merged_df_full.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_full.index.get_level_values(0).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cdf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee23dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc90314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_outg_time_infos_df/no_outg_time_infos_prstn_df needed for splitting train/test according to time\n",
    "build_no_outg_time_infos_dfs=False\n",
    "\n",
    "if build_no_outg_time_infos_dfs:\n",
    "#     ede_data_dir_no_outg_2020 = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20221216\\20200101_20201231\\EndEvents_NoOutg'\n",
    "#     ede_data_dir_no_outg_2021 = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20221216\\20210101_20211231\\EndEvents_NoOutg'\n",
    "    ede_data_dir_no_outg_2022 = r'U:\\CloudData\\dovs_and_end_events_data\\20230512\\20220101_20221231\\NoOutgs\\EndEvents'\n",
    "    #-------------------------\n",
    "#     no_outg_time_infos_df_2020 = AMIEDE_DEV.get_no_outg_time_interval_infos_df_for_data_in_dir(ede_data_dir_no_outg_2020)\n",
    "#     no_outg_time_infos_df_2021 = AMIEDE_DEV.get_no_outg_time_interval_infos_df_for_data_in_dir(ede_data_dir_no_outg_2021)\n",
    "    no_outg_time_infos_df_2022 = AMIEDE_DEV.get_no_outg_time_interval_infos_df_for_data_in_dir(ede_data_dir_no_outg_2022)\n",
    "    #-------------------------\n",
    "#     no_outg_time_infos_df = pd.concat([no_outg_time_infos_df_2020, no_outg_time_infos_df_2021, no_outg_time_infos_df_2022])\n",
    "    no_outg_time_infos_df = no_outg_time_infos_df_2022\n",
    "    #-------------------------\n",
    "    no_outg_time_infos_df.to_pickle(os.path.join(save_dir_model_base, 'no_outg_time_infos_df.pkl'))\n",
    "else:\n",
    "    no_outg_time_infos_df = pd.read_pickle(os.path.join(save_dir_model_base, 'no_outg_time_infos_df.pkl'))\n",
    "#-----\n",
    "if 'is_first_after_outg' in no_outg_time_infos_df.index.names:\n",
    "    no_outg_time_infos_df = no_outg_time_infos_df.droplevel(level='is_first_after_outg', axis=0)\n",
    "#--------------------------------------------------    \n",
    "#--------------------------------------------------    \n",
    "if build_no_outg_time_infos_dfs:\n",
    "#     ede_data_dir_no_outg_prstn_2020 = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20230301\\20200101_20201231\\NoOutgs_Pristine\\EndEvents'\n",
    "#     ede_data_dir_no_outg_prstn_2021 = r'C:\\Users\\s346557\\Documents\\LocalData\\dovs_and_end_events_data\\20230301\\20210101_20211231\\NoOutgs_Pristine\\EndEvents'\n",
    "    ede_data_dir_no_outg_prstn_2022 = r'U:\\CloudData\\dovs_and_end_events_data\\20230301\\20220101_20221231\\NoOutgs_Pristine\\EndEvents'\n",
    "    #-------------------------\n",
    "#     no_outg_time_infos_df_prstn_2020 = AMIEDE_DEV.get_no_outg_time_interval_infos_df_for_data_in_dir(ede_data_dir_no_outg_prstn_2020)\n",
    "#     no_outg_time_infos_df_prstn_2021 = AMIEDE_DEV.get_no_outg_time_interval_infos_df_for_data_in_dir(ede_data_dir_no_outg_prstn_2021)\n",
    "    no_outg_time_infos_df_prstn_2022 = AMIEDE_DEV.get_no_outg_time_interval_infos_df_for_data_in_dir(ede_data_dir_no_outg_prstn_2022)\n",
    "    #-------------------------\n",
    "#     no_outg_time_infos_prstn_df = pd.concat([no_outg_time_infos_df_prstn_2020, no_outg_time_infos_df_prstn_2021, no_outg_time_infos_df_prstn_2022])\n",
    "    no_outg_time_infos_prstn_df = no_outg_time_infos_df_prstn_2022\n",
    "    #-------------------------\n",
    "    no_outg_time_infos_prstn_df.to_pickle(os.path.join(save_dir_model_base, 'no_outg_time_infos_prstn_df.pkl'))\n",
    "else:\n",
    "    no_outg_time_infos_prstn_df = pd.read_pickle(os.path.join(save_dir_model_base, 'no_outg_time_infos_prstn_df.pkl'))\n",
    "#-----\n",
    "if 'is_first_after_outg' in no_outg_time_infos_prstn_df.index.names:\n",
    "    no_outg_time_infos_prstn_df = no_outg_time_infos_prstn_df.droplevel(level='is_first_after_outg', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79991b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e1c2a41",
   "metadata": {},
   "source": [
    "# Add month info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_month = True\n",
    "summary_dict['include_month'] = include_month\n",
    "\n",
    "if include_month:\n",
    "    merged_df_full_wd = DOVSOutages.append_outg_dt_off_ts_full_to_df(\n",
    "        df=merged_df_full.copy(), \n",
    "        outg_rec_nb_idfr=('index', 'outg_rec_nb'), \n",
    "        dummy_col_levels_prefix='dummy_lvl_'\n",
    "    )\n",
    "    #-------------------------\n",
    "    merged_df_no_outg_wd = merge_cpx_df_w_time_infos(\n",
    "        cpx_df=merged_df_no_outg.copy(), \n",
    "        time_infos_df=no_outg_time_infos_df, \n",
    "        time_infos_drop_dupls_subset=['index', 't_min']\n",
    "    )\n",
    "    #-----\n",
    "    merged_df_no_outg_prstn_wd = merge_cpx_df_w_time_infos(\n",
    "        cpx_df=merged_df_no_outg_prstn.copy(), \n",
    "        time_infos_df=no_outg_time_infos_prstn_df, \n",
    "        time_infos_drop_dupls_subset=['index', 't_min']\n",
    "    )\n",
    "    #-------------------------\n",
    "    if ('is_outg', 'is_outg') in merged_df_full_wd.columns:\n",
    "        merged_df_full_wd = Utilities_df.move_cols_to_back(merged_df_full_wd, [('is_outg', 'is_outg')])\n",
    "    if ('is_outg', 'is_outg') in merged_df_no_outg_wd.columns:\n",
    "        merged_df_no_outg_wd = Utilities_df.move_cols_to_back(merged_df_no_outg_wd, [('is_outg', 'is_outg')])\n",
    "    if ('is_outg', 'is_outg') in merged_df_no_outg_prstn_wd.columns:\n",
    "        merged_df_no_outg_prstn_wd = Utilities_df.move_cols_to_back(merged_df_no_outg_prstn_wd, [('is_outg', 'is_outg')])\n",
    "    #-------------------------\n",
    "    # Change outage time to just month of outage\n",
    "    merged_df_full_wd[('dummy_lvl_0', 'outg_month')] = merged_df_full_wd[('dummy_lvl_0', 'DT_OFF_TS_FULL')].dt.month\n",
    "\n",
    "    merged_df_no_outg_wd[('dummy_lvl_0', 'outg_month')] = merged_df_no_outg_wd[('dummy_lvl_0', 't_min')].dt.month\n",
    "    merged_df_no_outg_prstn_wd[('dummy_lvl_0', 'outg_month')] = merged_df_no_outg_prstn_wd[('dummy_lvl_0', 't_min')].dt.month\n",
    "\n",
    "    #-------------------------\n",
    "    merged_df_full_wd = merged_df_full_wd.drop(columns=[('dummy_lvl_0', 'DT_OFF_TS_FULL')])\n",
    "    merged_df_no_outg_wd = merged_df_no_outg_wd.drop(columns=[('dummy_lvl_0', 't_min'), ('dummy_lvl_0', 't_max'), ('dummy_lvl_0', 'prem_nbs')])\n",
    "    merged_df_no_outg_prstn_wd = merged_df_no_outg_prstn_wd.drop(columns=[('dummy_lvl_0', 't_min'), ('dummy_lvl_0', 't_max'), ('dummy_lvl_0', 'prem_nbs')])\n",
    "    #-------------------------\n",
    "    merged_df_full=merged_df_full_wd.copy()\n",
    "    merged_df_no_outg=merged_df_no_outg_wd.copy()\n",
    "    merged_df_no_outg_prstn=merged_df_no_outg_prstn_wd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b54f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7c3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9c011c",
   "metadata": {},
   "source": [
    "# !~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NETWORK and PRIMARY trsf_pole_nbs\n",
    "# merged_df_no_outg=merged_df_no_outg.loc[~merged_df_no_outg.index.get_level_values(0).isin(['NETWORK', 'PRIMARY'])]\n",
    "# merged_df_no_outg_prstn=merged_df_no_outg_prstn.loc[~merged_df_no_outg_prstn.index.get_level_values(0).isin(['NETWORK', 'PRIMARY'])]\n",
    "merged_df_no_outg=merged_df_no_outg.loc[~merged_df_no_outg.index.get_level_values(1).isin(['NETWORK', 'PRIMARY'])]\n",
    "merged_df_no_outg_prstn=merged_df_no_outg_prstn.loc[~merged_df_no_outg_prstn.index.get_level_values(1).isin(['NETWORK', 'PRIMARY'])]\n",
    "#-----\n",
    "merged_df_full=merged_df_full[~merged_df_full.index.get_level_values(1).isin(['NETWORK', 'PRIMARY'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f81ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd880459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DOVS info to be used for setting target values\n",
    "merged_df_full_w_DOVS = DOVSOutages.append_outg_info_to_df(\n",
    "    df=merged_df_full.copy(), \n",
    "    outg_rec_nb_idfr=('index', 'outg_rec_nb'), \n",
    "    build_sql_function=DOVSOutages_SQL.build_sql_std_outage, \n",
    ")\n",
    "merged_df_full_w_DOVS=merged_df_full_w_DOVS[['outg_dummy_lvl_0']]\n",
    "merged_df_full_w_DOVS[('is_outg', 'is_outg')]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108122e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f92de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'from_outg' information so I can track how many are in target=1 and target=0 \n",
    "merged_df_full[('from_outg', 'from_outg')]          = 1\n",
    "merged_df_no_outg[('from_outg', 'from_outg')]       = 0\n",
    "merged_df_no_outg_prstn[('from_outg', 'from_outg')] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a39550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad5546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44544757",
   "metadata": {},
   "source": [
    "# Drop any time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb42544",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_keys_to_drop = None\n",
    "# an_keys_to_drop = ['00-01 Days']\n",
    "# an_keys_to_drop = ['01-06 Days']\n",
    "\n",
    "summary_dict['an_keys_to_drop'] = an_keys_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb5935",
   "metadata": {},
   "outputs": [],
   "source": [
    "if an_keys_to_drop is not None:\n",
    "    merged_df_full          = merged_df_full.drop(columns=an_keys_to_drop, level=0)\n",
    "    merged_df_no_outg       = merged_df_no_outg.drop(columns=an_keys_to_drop, level=0)\n",
    "    merged_df_no_outg_prstn = merged_df_no_outg_prstn.drop(columns=an_keys_to_drop, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6108941b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5096c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec897dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_0_train   = pd.to_datetime('2021-01-01')\n",
    "date_1_train   = pd.to_datetime('2022-01-01')\n",
    "#-----\n",
    "date_0_test    = pd.to_datetime('2022-01-01')\n",
    "date_1_test    = pd.to_datetime('2023-01-01')\n",
    "#-----\n",
    "date_0_HOLDOUT = pd.to_datetime('2022-06-01')\n",
    "date_1_HOLDOUT = pd.to_datetime('2023-01-01')\n",
    "\n",
    "# date_0_train   = pd.to_datetime('2021-01-01')\n",
    "# date_1_train   = pd.to_datetime('2022-01-01')\n",
    "# #-----\n",
    "# date_0_test    = pd.to_datetime('2022-01-01')\n",
    "# date_1_test    = pd.to_datetime('2022-06-01')\n",
    "# #-----\n",
    "# date_0_HOLDOUT = pd.to_datetime('2022-06-01')\n",
    "# date_1_HOLDOUT = pd.to_datetime('2023-01-01')\n",
    "\n",
    "# date_0_train   = pd.to_datetime('2020-01-01')\n",
    "# date_1_train   = pd.to_datetime('2022-01-01')\n",
    "# #-----\n",
    "# date_0_test    = pd.to_datetime('2022-01-01')\n",
    "# date_1_test    = pd.to_datetime('2022-01-01')\n",
    "# #-----\n",
    "# date_0_HOLDOUT = pd.to_datetime('2022-01-01')\n",
    "# date_1_HOLDOUT = pd.to_datetime('2023-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a451c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train_test_by_date = False\n",
    "split_train_test_by_outg = True \n",
    "random_state = None\n",
    "test_size=0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Timestamp is not JSON serializable, hence the need for strftime below\n",
    "summary_dict['date_0_train'] = date_0_train.strftime('%Y-%m-%d %H:%M:%S')\n",
    "summary_dict['date_1_train'] = date_1_train.strftime('%Y-%m-%d %H:%M:%S')\n",
    "#-----\n",
    "summary_dict['date_0_test'] = date_0_test.strftime('%Y-%m-%d %H:%M:%S')\n",
    "summary_dict['date_1_test'] = date_1_test.strftime('%Y-%m-%d %H:%M:%S')\n",
    "#-----\n",
    "summary_dict['date_0_HOLDOUT'] = date_0_HOLDOUT.strftime('%Y-%m-%d %H:%M:%S')\n",
    "summary_dict['date_1_HOLDOUT'] = date_1_HOLDOUT.strftime('%Y-%m-%d %H:%M:%S')\n",
    "#-----\n",
    "summary_dict['get_train_test_by_date'] = get_train_test_by_date\n",
    "summary_dict['split_train_test_by_outg'] = split_train_test_by_outg\n",
    "#-----\n",
    "summary_dict['random_state'] = random_state\n",
    "summary_dict['test_size'] = test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b2a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_train_test_by_date:\n",
    "    merged_df_full_train = get_cpx_outg_df_subset_by_outg_datetime(\n",
    "        cpx_outg_df=merged_df_full.copy(), \n",
    "        date_0 = date_0_train,\n",
    "        date_1 = date_1_train, \n",
    "        outg_rec_nb_idfr='index', \n",
    "        return_notin_also=False\n",
    "    )\n",
    "    merged_df_full_test = get_cpx_outg_df_subset_by_outg_datetime(\n",
    "        cpx_outg_df=merged_df_full.copy(), \n",
    "        date_0 = date_0_test,\n",
    "        date_1 = date_1_test, \n",
    "        outg_rec_nb_idfr='index', \n",
    "        return_notin_also=False\n",
    "    )\n",
    "else:\n",
    "    merged_df_full_train_test = get_cpx_outg_df_subset_by_outg_datetime(\n",
    "        cpx_outg_df=merged_df_full.copy(), \n",
    "        date_0 = date_0_train,\n",
    "        date_1 = date_1_test, \n",
    "        outg_rec_nb_idfr='index', \n",
    "        return_notin_also=False\n",
    "    )\n",
    "    #-----\n",
    "    if split_train_test_by_outg:\n",
    "        merged_df_full_train, merged_df_full_test = train_test_split_df_by_outage(\n",
    "            df=merged_df_full_train_test, \n",
    "            outg_rec_nb_idfr=('index', 'outg_rec_nb'), \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        merged_df_full_train, merged_df_full_test = train_test_split(\n",
    "            merged_df_full_train_test, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "#-------------------------    \n",
    "merged_df_full_HOLDOUT = get_cpx_outg_df_subset_by_outg_datetime(\n",
    "    cpx_outg_df=merged_df_full.copy(), \n",
    "    date_0 = date_0_HOLDOUT,\n",
    "    date_1 = date_1_HOLDOUT, \n",
    "    outg_rec_nb_idfr='index', \n",
    "    return_notin_also=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a95c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d63d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_train_test_by_date:\n",
    "    merged_df_no_outg_train = get_cpx_baseline_df_subset_by_datetime(\n",
    "        cpx_bsln_df = merged_df_no_outg.copy(), \n",
    "        bsln_time_infos_df = no_outg_time_infos_df.copy(), \n",
    "        date_0 = date_0_train,\n",
    "        date_1 = date_1_train, \n",
    "        bsln_time_infos_time_col='t_min', \n",
    "        return_notin_also=False, \n",
    "        merge_time_info_to_cpx_bsln_df=False\n",
    "    )\n",
    "    merged_df_no_outg_test = get_cpx_baseline_df_subset_by_datetime(\n",
    "        cpx_bsln_df = merged_df_no_outg.copy(), \n",
    "        bsln_time_infos_df = no_outg_time_infos_df.copy(), \n",
    "        date_0 = date_0_test,\n",
    "        date_1 = date_1_test, \n",
    "        bsln_time_infos_time_col='t_min', \n",
    "        return_notin_also=False, \n",
    "        merge_time_info_to_cpx_bsln_df=False\n",
    "    )\n",
    "else:\n",
    "    merged_df_no_outg_train_test = get_cpx_baseline_df_subset_by_datetime(\n",
    "        cpx_bsln_df = merged_df_no_outg.copy(), \n",
    "        bsln_time_infos_df = no_outg_time_infos_df.copy(), \n",
    "        date_0 = date_0_train,\n",
    "        date_1 = date_1_test, \n",
    "        bsln_time_infos_time_col='t_min', \n",
    "        return_notin_also=False, \n",
    "        merge_time_info_to_cpx_bsln_df=False\n",
    "    )\n",
    "    #-----\n",
    "    if split_train_test_by_outg:\n",
    "        merged_df_no_outg_train, merged_df_no_outg_test = train_test_split_df_by_outage(\n",
    "            df=merged_df_no_outg_train_test, \n",
    "            outg_rec_nb_idfr=('index', 'no_outg_rec_nb'), \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        merged_df_no_outg_train, merged_df_no_outg_test = train_test_split(\n",
    "            merged_df_no_outg_train_test, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "#-------------------------   \n",
    "merged_df_no_outg_HOLDOUT = get_cpx_baseline_df_subset_by_datetime(\n",
    "    cpx_bsln_df = merged_df_no_outg.copy(), \n",
    "    bsln_time_infos_df = no_outg_time_infos_df.copy(), \n",
    "    date_0 = date_0_HOLDOUT,\n",
    "    date_1 = date_1_HOLDOUT, \n",
    "    bsln_time_infos_time_col='t_min', \n",
    "    return_notin_also=False, \n",
    "    merge_time_info_to_cpx_bsln_df=False\n",
    ")\n",
    "#--------------------------------------------------   \n",
    "if get_train_test_by_date:\n",
    "    merged_df_no_outg_prstn_train = get_cpx_baseline_df_subset_by_datetime(\n",
    "        cpx_bsln_df = merged_df_no_outg_prstn.copy(), \n",
    "        bsln_time_infos_df = no_outg_time_infos_prstn_df.copy(), \n",
    "        date_0 = date_0_train,\n",
    "        date_1 = date_1_train, \n",
    "        bsln_time_infos_time_col='t_min', \n",
    "        return_notin_also=False, \n",
    "        merge_time_info_to_cpx_bsln_df=False\n",
    "    )\n",
    "    merged_df_no_outg_prstn_test = get_cpx_baseline_df_subset_by_datetime(\n",
    "        cpx_bsln_df = merged_df_no_outg_prstn.copy(), \n",
    "        bsln_time_infos_df = no_outg_time_infos_prstn_df.copy(), \n",
    "        date_0 = date_0_test,\n",
    "        date_1 = date_1_test, \n",
    "        bsln_time_infos_time_col='t_min', \n",
    "        return_notin_also=False, \n",
    "        merge_time_info_to_cpx_bsln_df=False\n",
    "    )\n",
    "else:\n",
    "    merged_df_no_outg_prstn_train_test = get_cpx_baseline_df_subset_by_datetime(\n",
    "        cpx_bsln_df = merged_df_no_outg_prstn.copy(), \n",
    "        bsln_time_infos_df = no_outg_time_infos_prstn_df.copy(), \n",
    "        date_0 = date_0_train,\n",
    "        date_1 = date_1_test, \n",
    "        bsln_time_infos_time_col='t_min', \n",
    "        return_notin_also=False, \n",
    "        merge_time_info_to_cpx_bsln_df=False\n",
    "    )\n",
    "    if split_train_test_by_outg:\n",
    "        merged_df_no_outg_prstn_train, merged_df_no_outg_prstn_test = train_test_split(\n",
    "            merged_df_no_outg_prstn_train_test, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        merged_df_no_outg_prstn_train, merged_df_no_outg_prstn_test = train_test_split_df_by_outage(\n",
    "            df=merged_df_no_outg_prstn_train_test, \n",
    "            outg_rec_nb_idfr=('index', 'no_outg_rec_nb'), \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "#-------------------------   \n",
    "merged_df_no_outg_prstn_HOLDOUT = get_cpx_baseline_df_subset_by_datetime(\n",
    "    cpx_bsln_df = merged_df_no_outg_prstn.copy(), \n",
    "    bsln_time_infos_df = no_outg_time_infos_prstn_df.copy(), \n",
    "    date_0 = date_0_HOLDOUT,\n",
    "    date_1 = date_1_HOLDOUT, \n",
    "    bsln_time_infos_time_col='t_min', \n",
    "    return_notin_also=False, \n",
    "    merge_time_info_to_cpx_bsln_df=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5bc47d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "addtnl_baseline_train = pd.concat([merged_df_no_outg_train, merged_df_no_outg_prstn_train])\n",
    "# addtnl_baseline_train = addtnl_baseline_train.sample(frac=0.10)\n",
    "\n",
    "addtnl_baseline_test = pd.concat([merged_df_no_outg_test, merged_df_no_outg_prstn_test])\n",
    "# addtnl_baseline_test = addtnl_baseline_test.sample(frac=0.10)\n",
    "\n",
    "addtnl_baseline_HOLDOUT = pd.concat([merged_df_no_outg_HOLDOUT, merged_df_no_outg_prstn_HOLDOUT])\n",
    "# addtnl_baseline_HOLDOUT = addtnl_baseline_HOLDOUT.sample(frac=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff99aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95afbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_df_train   = pd.concat([merged_df_full_train, addtnl_baseline_train])\n",
    "full_data_df_test    = pd.concat([merged_df_full_test, addtnl_baseline_test])\n",
    "full_data_df_HOLDOUT = pd.concat([merged_df_full_HOLDOUT, addtnl_baseline_HOLDOUT])\n",
    "\n",
    "#Shuffle the data\n",
    "full_data_df_train   = full_data_df_train.sample(frac=1)\n",
    "full_data_df_test    = full_data_df_test.sample(frac=1)\n",
    "full_data_df_HOLDOUT = full_data_df_HOLDOUT.sample(frac=1)\n",
    "\n",
    "full_data_df = pd.concat([full_data_df_train, full_data_df_test, full_data_df_HOLDOUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf029347",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ca611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE!!!!!!!!!!!!!\n",
    "# If using _v2, SHOULD NOT SCALE events_period column!!!!!!!!!!!!\n",
    "\n",
    "create_validation_set = False\n",
    "val_size = 0.10 #w.r.t to train size (i.e., w.r.t 1.0-test_size)\n",
    "\n",
    "run_scaler=True\n",
    "\n",
    "run_PCA = False\n",
    "pca_n_components=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict['create_validation_set'] = create_validation_set\n",
    "summary_dict['val_size']              = val_size\n",
    "summary_dict['run_scaler']            = run_scaler\n",
    "summary_dict['run_PCA']               = run_PCA\n",
    "summary_dict['pca_n_components']      = pca_n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59d93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_eemsp:\n",
    "    full_data_df         = full_data_df.drop(columns=[('key_0', '')])\n",
    "    full_data_df_train   = full_data_df_train.drop(columns=[('key_0', '')])\n",
    "    full_data_df_test    = full_data_df_test.drop(columns=[('key_0', '')])\n",
    "    full_data_df_HOLDOUT = full_data_df_HOLDOUT.drop(columns=[('key_0', '')])\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    cols_to_encode = full_data_df['EEMSP'].columns\n",
    "    for col in cols_to_encode:\n",
    "        le.fit(full_data_df[('EEMSP', col)])\n",
    "        #-----\n",
    "        full_data_df_train[('EEMSP', col)]   = le.transform(full_data_df_train[('EEMSP', col)])\n",
    "        full_data_df_test[('EEMSP', col)]    = le.transform(full_data_df_test[('EEMSP', col)])\n",
    "        full_data_df_HOLDOUT[('EEMSP', col)] = le.transform(full_data_df_HOLDOUT[('EEMSP', col)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae4643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688051f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83e219c",
   "metadata": {},
   "source": [
    "# Set target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38132d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_save_dir_base = r'C:\\Users\\s346557\\Documents\\AnalysisNote\\X_Results\\X_2_OutageMeterEvents\\Figures\\ConfusionMatrices'\n",
    "# # fig_save_subdir = 'Target_eq_1_ALL_w_EEMSP'\n",
    "# fig_save_subdir = 'Target_eq_1_ALL'\n",
    "# # fig_save_subdir = 'trsf_nb_eq_LocID_w_EEMSP'\n",
    "# fig_save_dir = os.path.join(fig_save_dir_base, fig_save_subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961effe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef54235",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_full_w_DOVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "outg_n_xfmrs = get_n_trsf_poles_per_outg(\n",
    "    df=merged_df_full, \n",
    "    outg_rec_nb_idfr='index_0', \n",
    "    trsf_pole_nb_idfr='index_1'\n",
    ")\n",
    "outgs_w_single_xfmr = outg_n_xfmrs[outg_n_xfmrs==1].index.tolist()\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e086247",
   "metadata": {},
   "outputs": [],
   "source": [
    "xfmr_equip_typ_nms_of_interest = ['TRANSFORMER, OH', 'TRANSFORMER, UG']\n",
    "#-----\n",
    "# slicer = DFSlicer(\n",
    "#     single_slicers = [\n",
    "#         dict(\n",
    "#             column=('outg_dummy_lvl_0', 'LOCATION_ID'), \n",
    "#             value=merged_df_full_w_DOVS.index.get_level_values(1), \n",
    "#             comparison_operator='=='\n",
    "#         ), \n",
    "#         dict(\n",
    "#             column=('outg_dummy_lvl_0', 'EQUIP_TYP_NM'), \n",
    "#             value=xfmr_equip_typ_nms_of_interest, \n",
    "#             comparison_operator='isin'\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "#-----\n",
    "# slicer = DFSlicer(\n",
    "#     single_slicers = [\n",
    "#         dict(\n",
    "#             column=('outg_dummy_lvl_0', 'LOCATION_ID'), \n",
    "#             value=merged_df_full_w_DOVS.index.get_level_values(1), \n",
    "#             comparison_operator='=='\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "#-----\n",
    "# slicer = DFSlicer(\n",
    "#     single_slicers = [\n",
    "#         dict(\n",
    "#             column=('outg_dummy_lvl_0', 'MNR_CAUSE_NM'), \n",
    "#             value='EQUIPMENT FAILURE', \n",
    "#             comparison_operator='=='\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "#-----\n",
    "slicer = DFSlicer()\n",
    "#-----\n",
    "# slicer = DFSlicer(\n",
    "#     single_slicers = [\n",
    "#         dict(\n",
    "#             column=('outg_dummy_lvl_0', 'EQUIP_TYP_NM'), \n",
    "#             value='CONDUCTOR OVERHEAD', \n",
    "#             comparison_operator='=='\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "#-----\n",
    "# slicer = DFSlicer(\n",
    "#     single_slicers = [\n",
    "#         dict(\n",
    "#             column=('outg_dummy_lvl_0', 'LOCATION_ID'), \n",
    "#             value=merged_df_full_w_DOVS.index.get_level_values(1), \n",
    "#             comparison_operator='=='\n",
    "#         ), \n",
    "#         dict(\n",
    "#             column=('outg_dummy_lvl_0', 'OUTG_REC_NB'), \n",
    "#             value=outgs_w_single_xfmr, \n",
    "#             comparison_operator='isin'\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "# merged_df_full_w_DOVS[('outg_dummy_lvl_0', 'OUTG_REC_NB')] = merged_df_full_w_DOVS.index.get_level_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06b211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict['slicer'] = slicer.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_full_w_DOVS_i = slicer.set_simple_column_value(df=merged_df_full_w_DOVS.copy(), column=('is_outg', 'is_outg'), value=1)\n",
    "full_outg_idxs_i = merged_df_full_w_DOVS_i[merged_df_full_w_DOVS_i[('is_outg', 'is_outg')]==1].index\n",
    "\n",
    "#NOTE: To achieve exclusion equal to that in set_target_val_1_by_idx, one could use:\n",
    "#        full_outg_idxs_exclude_i = merged_df_full_w_DOVS_i[merged_df_full_w_DOVS_i[('is_outg', 'is_outg')]==0].index\n",
    "#      then use .drop(index=list(set(full_data_df_i.index).intersection(set(full_outg_idxs_exclude_i))))\n",
    "#      However, as noted in the function, the methods in set_target_val_1_by_idx are probably safer/more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a8286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_others_from_outages=False\n",
    "summary_dict['remove_others_from_outages'] = remove_others_from_outages\n",
    "#-------------------------\n",
    "full_data_df_i         = full_data_df.copy()\n",
    "full_data_df_train_i   = full_data_df_train.copy()\n",
    "full_data_df_test_i    = full_data_df_test.copy()\n",
    "full_data_df_HOLDOUT_i = full_data_df_HOLDOUT.copy()\n",
    "#-------------------------\n",
    "full_data_df_i = set_target_val_1_by_idx(\n",
    "    df=full_data_df_i,\n",
    "    val_1_idxs=full_outg_idxs_i,\n",
    "    remove_others_from_outages=remove_others_from_outages, \n",
    "    target_col=('is_outg', 'is_outg'), \n",
    "    from_outg_col=('from_outg', 'from_outg')\n",
    ")\n",
    "#-----\n",
    "full_data_df_train_i = set_target_val_1_by_idx(\n",
    "    df=full_data_df_train_i,\n",
    "    val_1_idxs=full_outg_idxs_i,\n",
    "    remove_others_from_outages=remove_others_from_outages, \n",
    "    target_col=('is_outg', 'is_outg'), \n",
    "    from_outg_col=('from_outg', 'from_outg')\n",
    ")\n",
    "#-----\n",
    "full_data_df_test_i = set_target_val_1_by_idx(\n",
    "    df=full_data_df_test_i,\n",
    "    val_1_idxs=full_outg_idxs_i,\n",
    "    remove_others_from_outages=remove_others_from_outages, \n",
    "    target_col=('is_outg', 'is_outg'), \n",
    "    from_outg_col=('from_outg', 'from_outg')\n",
    ")\n",
    "#-----\n",
    "full_data_df_HOLDOUT_i = set_target_val_1_by_idx(\n",
    "    df=full_data_df_HOLDOUT_i,\n",
    "    val_1_idxs=full_outg_idxs_i,\n",
    "    remove_others_from_outages=remove_others_from_outages, \n",
    "    target_col=('is_outg', 'is_outg'), \n",
    "    from_outg_col=('from_outg', 'from_outg')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea0e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632f9e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "n_outg_target_1_train = full_data_df_train_i[('is_outg', 'is_outg')].sum()\n",
    "n_outg_target_0_train = full_data_df_train_i[\n",
    "    (full_data_df_train_i[('from_outg', 'from_outg')]==1) & \n",
    "    (full_data_df_train_i[('is_outg', 'is_outg')]==0)\n",
    "].shape[0]\n",
    "n_bsln_train = full_data_df_train_i[full_data_df_train_i[('from_outg', 'from_outg')]==0].shape[0]\n",
    "assert(full_data_df_train_i.shape[0]==n_outg_target_1_train+n_outg_target_0_train+n_bsln_train)\n",
    "pct_target_1_train = 100*n_outg_target_1_train/(n_outg_target_1_train+n_outg_target_0_train+n_bsln_train)\n",
    "#-----\n",
    "print('\\n----- TRAIN -----')\n",
    "print(f\"# direct outages (target = 1):      {n_outg_target_1_train}\")\n",
    "print(f\"# indirect outages (target = 0):    {n_outg_target_0_train}\")\n",
    "print(f\"\\t% direct = {100*n_outg_target_1_train/(n_outg_target_1_train+n_outg_target_0_train)}\")\n",
    "print(f\"# additional baseline (target = 0): {n_bsln_train}\")\n",
    "print(f\"%(target==1):                       {pct_target_1_train}%\")\n",
    "\n",
    "#--------------------------------------------------\n",
    "n_outg_target_1_test = full_data_df_test_i[('is_outg', 'is_outg')].sum()\n",
    "n_outg_target_0_test = full_data_df_test_i[\n",
    "    (full_data_df_test_i[('from_outg', 'from_outg')]==1) & \n",
    "    (full_data_df_test_i[('is_outg', 'is_outg')]==0)\n",
    "].shape[0]\n",
    "n_bsln_test = full_data_df_test_i[full_data_df_test_i[('from_outg', 'from_outg')]==0].shape[0]\n",
    "assert(full_data_df_test_i.shape[0]==n_outg_target_1_test+n_outg_target_0_test+n_bsln_test)\n",
    "pct_target_1_test = 100*n_outg_target_1_test/(n_outg_target_1_test+n_outg_target_0_test+n_bsln_test)\n",
    "#-----\n",
    "print('\\n----- TEST -----')\n",
    "print(f\"# direct outages (target = 1):      {n_outg_target_1_test}\")\n",
    "print(f\"# indirect outages (target = 0):    {n_outg_target_0_test}\")\n",
    "print(f\"\\t% direct = {100*n_outg_target_1_test/(n_outg_target_1_test+n_outg_target_0_test)}\")\n",
    "print(f\"# additional baseline (target = 0): {n_bsln_test}\")\n",
    "print(f\"%(target==1):                       {pct_target_1_test}%\")\n",
    "\n",
    "#--------------------------------------------------\n",
    "n_outg_target_1_HOLDOUT = full_data_df_HOLDOUT_i[('is_outg', 'is_outg')].sum()\n",
    "n_outg_target_0_HOLDOUT = full_data_df_HOLDOUT_i[\n",
    "    (full_data_df_HOLDOUT_i[('from_outg', 'from_outg')]==1) & \n",
    "    (full_data_df_HOLDOUT_i[('is_outg', 'is_outg')]==0)\n",
    "].shape[0]\n",
    "n_bsln_HOLDOUT = full_data_df_HOLDOUT_i[full_data_df_HOLDOUT_i[('from_outg', 'from_outg')]==0].shape[0]\n",
    "assert(full_data_df_HOLDOUT_i.shape[0]==n_outg_target_1_HOLDOUT+n_outg_target_0_HOLDOUT+n_bsln_HOLDOUT)\n",
    "pct_target_1_HOLDOUT = 100*n_outg_target_1_HOLDOUT/(n_outg_target_1_HOLDOUT+n_outg_target_0_HOLDOUT+n_bsln_HOLDOUT)\n",
    "#-----\n",
    "print('\\n----- HOLDOUT -----')\n",
    "print(f\"# direct outages (target = 1):      {n_outg_target_1_HOLDOUT}\")\n",
    "print(f\"# indirect outages (target = 0):    {n_outg_target_0_HOLDOUT}\")\n",
    "print(f\"\\t% direct = {100*n_outg_target_1_HOLDOUT/(n_outg_target_1_HOLDOUT+n_outg_target_0_HOLDOUT)}\")\n",
    "print(f\"# additional baseline (target = 0): {n_bsln_HOLDOUT}\")\n",
    "print(f\"%(target==1):                       {pct_target_1_HOLDOUT}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6258def",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    full_data_df_train_i[('is_outg', 'is_outg')].sum()==0 or \n",
    "    full_data_df_test_i[('is_outg', 'is_outg')].sum()==0 or \n",
    "    full_data_df_HOLDOUT_i[('is_outg', 'is_outg')].sum()==0\n",
    "):\n",
    "    print('Not enough target value==1 in train, test, and/or holdout')\n",
    "    print(f\"#target==1 train:   {full_data_df_train_i[('is_outg', 'is_outg')].sum()}\")\n",
    "    print(f\"#target==1 test:    {full_data_df_test_i[('is_outg', 'is_outg')].sum()}\")\n",
    "    print(f\"#target==1 holdout: {full_data_df_HOLDOUT_i[('is_outg', 'is_outg')].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_pct_target_1 = 25\n",
    "min_pct_target_1 = None\n",
    "summary_dict['min_pct_target_1'] = min_pct_target_1\n",
    "if min_pct_target_1 is not None:\n",
    "    full_data_df_train_i = ensure_target_val_1_min_pct(\n",
    "        df=full_data_df_train_i,\n",
    "        min_pct=min_pct_target_1,\n",
    "        target_col=('is_outg', 'is_outg'), \n",
    "        random_state=random_state\n",
    "    )\n",
    "    #-----\n",
    "    full_data_df_test_i = ensure_target_val_1_min_pct(\n",
    "        df=full_data_df_test_i,\n",
    "        min_pct=min_pct_target_1,\n",
    "        target_col=('is_outg', 'is_outg'), \n",
    "        random_state=random_state\n",
    "    )\n",
    "    #-----\n",
    "    full_data_df_HOLDOUT_i = ensure_target_val_1_min_pct(\n",
    "        df=full_data_df_HOLDOUT_i,\n",
    "        min_pct=min_pct_target_1,\n",
    "        target_col=('is_outg', 'is_outg'), \n",
    "        random_state=random_state\n",
    "    )\n",
    "    #-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a57eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "n_outg_target_1_train = full_data_df_train_i[('is_outg', 'is_outg')].sum()\n",
    "n_outg_target_0_train = full_data_df_train_i[\n",
    "    (full_data_df_train_i[('from_outg', 'from_outg')]==1) & \n",
    "    (full_data_df_train_i[('is_outg', 'is_outg')]==0)\n",
    "].shape[0]\n",
    "n_bsln_train = full_data_df_train_i[full_data_df_train_i[('from_outg', 'from_outg')]==0].shape[0]\n",
    "assert(full_data_df_train_i.shape[0]==n_outg_target_1_train+n_outg_target_0_train+n_bsln_train)\n",
    "pct_target_1_train = 100*n_outg_target_1_train/(n_outg_target_1_train+n_outg_target_0_train+n_bsln_train)\n",
    "#-----\n",
    "print('\\n----- TRAIN -----')\n",
    "print(f\"# direct outages (target = 1):      {n_outg_target_1_train}\")\n",
    "print(f\"# indirect outages (target = 0):    {n_outg_target_0_train}\")\n",
    "print(f\"\\t% direct = {100*n_outg_target_1_train/(n_outg_target_1_train+n_outg_target_0_train)}\")\n",
    "print(f\"# additional baseline (target = 0): {n_bsln_train}\")\n",
    "print(f\"%(target==1):                       {pct_target_1_train}%\")\n",
    "\n",
    "#--------------------------------------------------\n",
    "n_outg_target_1_test = full_data_df_test_i[('is_outg', 'is_outg')].sum()\n",
    "n_outg_target_0_test = full_data_df_test_i[\n",
    "    (full_data_df_test_i[('from_outg', 'from_outg')]==1) & \n",
    "    (full_data_df_test_i[('is_outg', 'is_outg')]==0)\n",
    "].shape[0]\n",
    "n_bsln_test = full_data_df_test_i[full_data_df_test_i[('from_outg', 'from_outg')]==0].shape[0]\n",
    "assert(full_data_df_test_i.shape[0]==n_outg_target_1_test+n_outg_target_0_test+n_bsln_test)\n",
    "pct_target_1_test = 100*n_outg_target_1_test/(n_outg_target_1_test+n_outg_target_0_test+n_bsln_test)\n",
    "#-----\n",
    "print('\\n----- TEST -----')\n",
    "print(f\"# direct outages (target = 1):      {n_outg_target_1_test}\")\n",
    "print(f\"# indirect outages (target = 0):    {n_outg_target_0_test}\")\n",
    "print(f\"\\t% direct = {100*n_outg_target_1_test/(n_outg_target_1_test+n_outg_target_0_test)}\")\n",
    "print(f\"# additional baseline (target = 0): {n_bsln_test}\")\n",
    "print(f\"%(target==1):                       {pct_target_1_test}%\")\n",
    "\n",
    "#--------------------------------------------------\n",
    "n_outg_target_1_HOLDOUT = full_data_df_HOLDOUT_i[('is_outg', 'is_outg')].sum()\n",
    "n_outg_target_0_HOLDOUT = full_data_df_HOLDOUT_i[\n",
    "    (full_data_df_HOLDOUT_i[('from_outg', 'from_outg')]==1) & \n",
    "    (full_data_df_HOLDOUT_i[('is_outg', 'is_outg')]==0)\n",
    "].shape[0]\n",
    "n_bsln_HOLDOUT = full_data_df_HOLDOUT_i[full_data_df_HOLDOUT_i[('from_outg', 'from_outg')]==0].shape[0]\n",
    "assert(full_data_df_HOLDOUT_i.shape[0]==n_outg_target_1_HOLDOUT+n_outg_target_0_HOLDOUT+n_bsln_HOLDOUT)\n",
    "pct_target_1_HOLDOUT = 100*n_outg_target_1_HOLDOUT/(n_outg_target_1_HOLDOUT+n_outg_target_0_HOLDOUT+n_bsln_HOLDOUT)\n",
    "#-----\n",
    "print('\\n----- HOLDOUT -----')\n",
    "print(f\"# direct outages (target = 1):      {n_outg_target_1_HOLDOUT}\")\n",
    "print(f\"# indirect outages (target = 0):    {n_outg_target_0_HOLDOUT}\")\n",
    "print(f\"\\t% direct = {100*n_outg_target_1_HOLDOUT/(n_outg_target_1_HOLDOUT+n_outg_target_0_HOLDOUT)}\")\n",
    "print(f\"# additional baseline (target = 0): {n_bsln_HOLDOUT}\")\n",
    "print(f\"%(target==1):                       {pct_target_1_HOLDOUT}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('from_outg', 'from_outg') in full_data_df_i.columns.tolist():\n",
    "    full_data_df_i = full_data_df_i.drop(columns=[('from_outg', 'from_outg')])\n",
    "if ('from_outg', 'from_outg') in full_data_df_train_i.columns.tolist():\n",
    "    full_data_df_train_i = full_data_df_train_i.drop(columns=[('from_outg', 'from_outg')])\n",
    "if ('from_outg', 'from_outg') in full_data_df_test_i.columns.tolist():\n",
    "    full_data_df_test_i = full_data_df_test_i.drop(columns=[('from_outg', 'from_outg')])\n",
    "if ('from_outg', 'from_outg') in full_data_df_HOLDOUT_i.columns.tolist():\n",
    "    full_data_df_HOLDOUT_i = full_data_df_HOLDOUT_i.drop(columns=[('from_outg', 'from_outg')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d98635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd269a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train_size = True\n",
    "red_test_size = 0.95 #Amount kept will be 1.0-red_test_size\n",
    "summary_dict['reduce_train_size'] = reduce_train_size\n",
    "summary_dict['red_test_size'] = red_test_size\n",
    "if reduce_train_size:\n",
    "    if split_train_test_by_outg:\n",
    "        full_data_df_train_i, _ = train_test_split_df_by_outage(\n",
    "            df=full_data_df_train_i, \n",
    "            outg_rec_nb_idfr='index_0', \n",
    "            test_size=red_test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        full_data_df_train_i, _ = train_test_split(\n",
    "            full_data_df_train_i, \n",
    "            test_size=red_test_size, \n",
    "            random_state=random_state\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fba3de",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb58344",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_df_train_i.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a314ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ugh = full_data_df_train_i.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fc331",
   "metadata": {},
   "outputs": [],
   "source": [
    "ugh[ugh>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba33a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_data_df_train_i.shape[0])\n",
    "full_data_df_train_i = full_data_df_train_i.dropna()\n",
    "print(full_data_df_train_i.shape[0])\n",
    "print()\n",
    "#-----\n",
    "print(full_data_df_test_i.shape[0])\n",
    "full_data_df_test_i = full_data_df_test_i.dropna()\n",
    "print(full_data_df_test_i.shape[0])\n",
    "print()\n",
    "#-----\n",
    "print(full_data_df_HOLDOUT_i.shape[0])\n",
    "full_data_df_HOLDOUT_i = full_data_df_HOLDOUT_i.dropna()\n",
    "print(full_data_df_HOLDOUT_i.shape[0])\n",
    "print()\n",
    "#-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_df_test_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b564a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_df_HOLDOUT_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc17335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data_df_train_i   = full_data_df_train_i.drop(columns=('dummy_lvl_0', 'outg_month'))\n",
    "# full_data_df_test_i    = full_data_df_test_i.drop(columns=('dummy_lvl_0', 'outg_month'))\n",
    "# full_data_df_HOLDOUT_i = full_data_df_HOLDOUT_i.drop(columns=('dummy_lvl_0', 'outg_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(full_data_df_train_i.columns.tolist()).symmetric_difference(set(full_data_df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "X_train_OG = full_data_df_train_i.iloc[:, :-1].copy()\n",
    "y_train    = full_data_df_train_i.iloc[:, -1].copy()\n",
    "\n",
    "X_test_OG = full_data_df_test_i.iloc[:, :-1].copy()\n",
    "y_test    = full_data_df_test_i.iloc[:, -1].copy()\n",
    "\n",
    "X_HOLDOUT = full_data_df_HOLDOUT_i.iloc[:, :-1]\n",
    "y_HOLDOUT = full_data_df_HOLDOUT_i.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c6b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(2,2, return_flattened_axes=True)\n",
    "#-------------------------\n",
    "sns.countplot(ax=axs[0], x = ('is_outg', 'is_outg'), data=full_data_df_i)\n",
    "axs[0].set_title('Full Data');\n",
    "axs[0].set_xlabel('Is Outage');\n",
    "axs[0].set_ylim([1.05*x for x in axs[0].get_ylim()])\n",
    "for p in axs[0].patches:\n",
    "    pct = (p.get_height()/full_data_df_i.shape[0]).round(5)\n",
    "    txt = f'{pct}%'\n",
    "    txt_x = p.get_x() \n",
    "    txt_y = 1.025*p.get_height()\n",
    "    axs[0].text(txt_x,txt_y,txt)\n",
    "#-------------------------\n",
    "sns.countplot(ax=axs[1], x = ('is_outg', 'is_outg'), data=y_train.to_frame())\n",
    "axs[1].set_title('Training Data');\n",
    "axs[1].set_xlabel('Is Outage');\n",
    "axs[1].set_ylim([1.05*x for x in axs[1].get_ylim()])\n",
    "for p in axs[1].patches:\n",
    "    pct = (p.get_height()/y_train.shape[0]).round(5)\n",
    "    txt = f'{pct}%'\n",
    "    txt_x = p.get_x() \n",
    "    txt_y = 1.025*p.get_height()\n",
    "    axs[1].text(txt_x,txt_y,txt)\n",
    "#-------------------------\n",
    "sns.countplot(ax=axs[2], x = ('is_outg', 'is_outg'), data=y_test.to_frame())\n",
    "axs[2].set_title('Testing Data');\n",
    "axs[2].set_xlabel('Is Outage');\n",
    "axs[2].set_ylim([1.05*x for x in axs[2].get_ylim()])\n",
    "for p in axs[2].patches:\n",
    "    pct = (p.get_height()/y_test.shape[0]).round(5)\n",
    "    txt = f'{pct}%'\n",
    "    txt_x = p.get_x() \n",
    "    txt_y = 1.025*p.get_height()\n",
    "    axs[2].text(txt_x,txt_y,txt)\n",
    "#-------------------------\n",
    "sns.countplot(ax=axs[3], x = ('is_outg', 'is_outg'), data=y_HOLDOUT.to_frame())\n",
    "axs[3].set_title('Holdout Data');\n",
    "axs[3].set_xlabel('Is Outage');\n",
    "axs[3].set_ylim([1.05*x for x in axs[3].get_ylim()])\n",
    "for p in axs[3].patches:\n",
    "    pct = (p.get_height()/y_HOLDOUT.shape[0]).round(5)\n",
    "    txt = f'{pct}%'\n",
    "    txt_x = p.get_x() \n",
    "    txt_y = 1.025*p.get_height()\n",
    "    axs[3].text(txt_x,txt_y,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45927394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67150a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "if create_validation_set:\n",
    "    if split_train_test_by_outg:\n",
    "        X_train_OG, X_val_OG, y_train, y_val = train_test_split_df_group(\n",
    "            X=X_train_OG, \n",
    "            y=y_train, \n",
    "            groups=X_train_OG.index.get_level_values(0), \n",
    "            test_size=val_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        X_train_OG, X_val_OG, y_train, y_val = train_test_split(X_train_OG, y_train, test_size=val_size, random_state=random_state)\n",
    "#-------------------------\n",
    "if run_scaler:\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train_OG)\n",
    "    #-----\n",
    "    if create_validation_set:\n",
    "        X_val   = scaler.transform(X_val_OG)\n",
    "    X_test  = scaler.transform(X_test_OG)\n",
    "    X_HOLDOUT   = scaler.transform(X_HOLDOUT)\n",
    "else:\n",
    "    X_train = X_train_OG\n",
    "    if create_validation_set:\n",
    "        X_val   = X_val_OG\n",
    "    X_test  = X_test_OG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_OG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_PCA:\n",
    "    # First, generate a PCA plot, showing the explained variance on the y-axis and number of components on x\n",
    "    # This is simply to check that the number of components kept looks correct\n",
    "    pca = PCA()\n",
    "    pca.fit(X_train)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.plot(cumsum)\n",
    "    \n",
    "    # Now, run the PCA with pca_n_components and perform transforms\n",
    "    pca=PCA(n_components=pca_n_components)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    print(f'PCA n-components       = {pca.n_components_}')\n",
    "    print(f'PCA explained variance = {pca.explained_variance_ratio_.sum()}')\n",
    "    #-----\n",
    "    if create_validation_set:\n",
    "        X_val      = pca.transform(X_val)\n",
    "    X_test     = pca.transform(X_test)\n",
    "    X_HOLDOUT  = pca.transform(X_HOLDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7773388",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e071f",
   "metadata": {},
   "source": [
    "# Dumb Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2dc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_clf = DumbClassifier()\n",
    "cross_val_score(dumb_clf, X_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd2d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = dumb_clf.predict(X_test)\n",
    "# print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "# print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "# print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9092fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455a1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7924f4a4",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchmetrics.classification import BinaryPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cb38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b36bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.float32)\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self,input_shape):\n",
    "#         super(Net,self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_shape,32)\n",
    "#         self.fc2 = nn.Linear(32,64)\n",
    "#         self.fc3 = nn.Linear(64,1)\n",
    "#     def forward(self,x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.sigmoid(self.fc3(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self,input_shape):\n",
    "#         super(Net,self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_shape,1024)\n",
    "#         self.fc2 = nn.Linear(1024,256)\n",
    "#         self.fc3 = nn.Linear(256,128)\n",
    "#         self.fc4 = nn.Linear(128,16)\n",
    "#         self.fc5 = nn.Linear(16,4)\n",
    "#         self.fc6 = nn.Linear(4,1)\n",
    "#     def forward(self,x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.relu(self.fc3(x))\n",
    "#         x = torch.relu(self.fc4(x))\n",
    "#         x = torch.relu(self.fc5(x))\n",
    "#         x = torch.sigmoid(self.fc6(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32,16)\n",
    "        self.fc5 = nn.Linear(16,4)\n",
    "        self.fc6 = nn.Linear(4,1)\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.sigmoid(self.fc6(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff345afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset   = dataset(X_train,y_train)\n",
    "testset    = dataset(X_test,y_test)\n",
    "# holdoutset = dataset(X_HOLDOUT, y_HOLDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader\n",
    "trainloader   = DataLoader(trainset,batch_size=64,shuffle=False)\n",
    "testloader    = DataLoader(testset,batch_size=64,shuffle=False)\n",
    "# holdoutloader = DataLoader(holdoutset,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd376fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca01e294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fc096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train\n",
    "y = y_train.to_numpy()\n",
    "print(\"shape of x: {}\\nshape of y: {}\".format(x.shape,y.shape))\n",
    "\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)\n",
    "\n",
    "trainset = dataset(x,y)\n",
    "# trainloader = DataLoader(trainset,batch_size=64,shuffle=False)\n",
    "trainloader = DataLoader(trainset,batch_size=X_train.shape[0],shuffle=False)\n",
    "\n",
    "#hyper parameters\n",
    "# learning_rate = 0.1\n",
    "# epochs = 700\n",
    "# # Model , Optimizer, Loss\n",
    "# model = Net(input_shape=x.shape[1])\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "# loss_fn = nn.BCELoss()\n",
    "\n",
    "#hyper parameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 10\n",
    "# Model , Optimizer, Loss\n",
    "model = Net(input_shape=X_train.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110e525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356646cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward loop\n",
    "losses = []\n",
    "accur = []\n",
    "precisions = []\n",
    "for i in range(epochs):\n",
    "    print(i)\n",
    "    for j,(x_t,y_t) in enumerate(trainloader):\n",
    "    \n",
    "        #calculate output\n",
    "        output = model(x_t)\n",
    "\n",
    "        #calculate loss\n",
    "        loss = loss_fn(output,y_t.reshape(-1,1))\n",
    "\n",
    "        #accuracy\n",
    "        predicted = model(torch.tensor(X_train,dtype=torch.float32))\n",
    "        acc = (predicted.reshape(-1).detach().numpy().round() == y_train).mean()\n",
    "        precision = BinaryPrecision()(predicted.reshape(-1).detach(), torch.tensor(y_train.values))\n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#     if i%50 == 0:\n",
    "#         losses.append(loss)\n",
    "#         accur.append(acc)\n",
    "#         print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(i,loss,acc))\n",
    "        losses.append(loss)\n",
    "        accur.append(acc)\n",
    "        precisions.append(precision)\n",
    "        print(\"epoch {}\\tloss : {}\\t accuracy : {}\\t precision : {}\".format(i,loss,acc,precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c791d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #forward loop\n",
    "# losses = []\n",
    "# accur = []\n",
    "# precisions = []\n",
    "\n",
    "# loss_hist = [0]*epochs\n",
    "# accuracy_hist=[0]*epochs\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     print(i)\n",
    "#     for j,(x_t,y_t) in enumerate(trainloader):\n",
    "    \n",
    "#         #calculate output\n",
    "#         output = model(x_t)\n",
    "\n",
    "#         #calculate loss\n",
    "#         loss = loss_fn(output,y_t.reshape(-1,1))\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         loss_hist[i] += loss.item()*y_t.size(0)\n",
    "#         is_correct = (torch.argmax(output, dim=1)==y_t).float()\n",
    "#         accuracy_hist[i] += is_correct.sum()\n",
    "#     loss_hist[i] /= len(trainloader.dataset)\n",
    "#     accuracy_hist[i] /= len(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c0d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(testset.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc231e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9201ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8bdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(y_pred, dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = (torch.argmax(y_pred, dim=1)==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('*****'*5)\n",
    "# print('TESTING DATASET')\n",
    "# print('*****'*5)\n",
    "# print(f\"#(target==1): {y_test.sum()}\")\n",
    "# print(f\"#(target==0): {y_test.shape[0]-y_test.sum()}\")\n",
    "# print(f\"%(target==1): {100*(y_test.sum()/(y_test.shape[0]))}\")\n",
    "# print('-----'*5)\n",
    "# print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred.detach().numpy()))\n",
    "# print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred.detach().numpy()))\n",
    "# print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred.detach().numpy()))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842a6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1e0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bd40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_dim = X_train.shape[1]\n",
    "\n",
    "#Layer size\n",
    "n_hidden1 = 300  # Number of hidden nodes\n",
    "n_hidden2 = 100\n",
    "n_output =  1   # Number of output nodes = for binary classifier\n",
    "\n",
    "\n",
    "class ChurnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChurnModel, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_input_dim, n_hidden1) \n",
    "        self.layer_2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.layer_out = nn.Linear(n_hidden2, n_output) \n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid =  nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(n_hidden1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(n_hidden2)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.layer_out(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "model = ChurnModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4baf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef6e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cacb7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Computation\n",
    "loss_func = nn.BCELoss()\n",
    "#Optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = []\n",
    "for epoch in range(epochs):\n",
    "    #Within each epoch run the subsets of data = batch sizes.\n",
    "    for xb, yb in trainloader:\n",
    "        y_pred = model(xb)            # Forward Propagation\n",
    "        loss = loss_func(y_pred.squeeze(), yb)  # Loss Computation\n",
    "        optimizer.zero_grad()         # Clearing all previous gradients, setting to zero \n",
    "        loss.backward()               # Back Propagation\n",
    "        optimizer.step()              # Updating the parameters \n",
    "    #print(\"Loss in iteration :\"+str(epoch)+\" is: \"+str(loss.item()))\n",
    "    print(loss.item())\n",
    "    train_loss.append(loss.item())\n",
    "print('Last iteration loss value: '+str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95282988",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(torch.from_numpy(X_test.astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.reshape(-1).detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred.reshape(-1).detach().numpy()>0.5).astype(int).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "((y_pred.reshape(-1).detach().numpy()>0.5).astype(int)==y_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7210cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "((y_pred.reshape(-1).detach().numpy()>0.5).astype(int)==y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583298d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred.reshape(-1).detach().numpy()>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*****'*5)\n",
    "print('TESTING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_test.sum()}\")\n",
    "print(f\"#(target==0): {y_test.shape[0]-y_test.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_test.sum()/(y_test.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c219e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee2ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
