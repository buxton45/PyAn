{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5755605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "# NOTE: To reload a class imported as, e.g., \n",
    "# from module import class\n",
    "# One must call:\n",
    "#   1. import module\n",
    "#   2. reload module\n",
    "#   3. from module import class\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "import string\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns, natsort_keygen\n",
    "from packaging import version\n",
    "import copy\n",
    "from functools import reduce\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm #e.g. for cmap=cm.jet\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "from EEMSP import EEMSP\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from MECPODf import MECPODf\n",
    "from MECPOAn import MECPOAn\n",
    "from MECPOCollection import MECPOCollection\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "#sys.path.insert(0, os.path.join(os.path.realpath('..'), 'Utilities'))\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "from Utilities_df import DFConstructType\n",
    "import Utilities_dt\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import Plot_Hist\n",
    "import Plot_Bar\n",
    "import GrubbsTest\n",
    "import DataFrameSubsetSlicer\n",
    "from DataFrameSubsetSlicer import DataFrameSubsetSlicer as DFSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import scipy\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c366f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_data_dfs_v2(full_data_df):\n",
    "    # Building final_data_df_v2 with this method (calling get_merged_cpo_dfs first) causes each index\n",
    "    #   in the final DF to be repeated the same amount of times.\n",
    "    # If I had built straight from individual DFs themselves (without calling get_merged_cpo_dfs), this would\n",
    "    #   not be the case, and in general the indices would have different numbers of repititions.\n",
    "    #   e.g., if a group only has events in the 01-05 Days period, that group's index would only occur once in the final DF\n",
    "    #         whereas if a group has events in all 6 periods, that group's index would be repeated 6 times.\n",
    "\n",
    "    full_data_dfs_v2 = []\n",
    "    time_pds = full_data_df.columns.get_level_values(0).unique()\n",
    "    time_pds = [x for x in time_pds if x!='is_outg']\n",
    "    #-------------------------\n",
    "    for time_pd in time_pds:\n",
    "        found_days = re.findall(r'(\\d{2})-(\\d{2}) Days', time_pd)\n",
    "        assert(len(found_days)==1)\n",
    "        found_days=found_days[0]\n",
    "        assert(len(found_days)==2)\n",
    "        days_min = float(found_days[0])\n",
    "        days_max = float(found_days[1])\n",
    "        days_avg = 0.5*(days_min+days_max)\n",
    "        #-----\n",
    "        full_data_df_i = full_data_df[[time_pd, 'is_outg']].copy()\n",
    "        full_data_df_i = full_data_df_i.droplevel(0, axis=1)\n",
    "        full_data_df_i['events_period'] = days_avg\n",
    "        full_data_dfs_v2.append(full_data_df_i)    \n",
    "    #-------------------------\n",
    "    for i in range(1, len(full_data_dfs_v2)):\n",
    "        assert(len(set(full_data_dfs_v2[i-1]).symmetric_difference(full_data_dfs_v2[i]))==0)\n",
    "    full_data_df_v2 = pd.concat(full_data_dfs_v2)\n",
    "    full_data_df_v2 = Utilities_df.move_cols_to_either_end(full_data_df_v2, ['events_period' ,'is_outg'], to_front=False)    \n",
    "    #-------------------------\n",
    "    return full_data_df_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911cc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DumbClassifier(BaseEstimator):\n",
    "    r\"\"\"\n",
    "    A dumb classifier which always predicts 1\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "    \n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds, x_min=None, x_max=None):\n",
    "    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n",
    "    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n",
    "    plt.xlim(x_min,x_max)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0,1], [0,1], 'k--') # Dashed diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb472e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_n_reasons_per_plot(n_reasons_per_plot_init, n_reasons):\n",
    "    r\"\"\"\n",
    "    Find a good n_reasons_per_plot value given an approximate initial value of n_reasons_per_plot.\n",
    "    -----\n",
    "    Given n_reasons_per_plot_init and n_reasons, n_plots can be found.\n",
    "    In the simple method, one would then simply put n_reasons_per_plot_init in the first i_plot=n_plots-1 plots, and \n",
    "      whatever remains in plot i_plot=n_plots.  This could lead to a very small number of plots in the final i_plot=n_plots\n",
    "    This function takes the calculated n_plots and adjusts n_reasons_per_plot_init to avoid this asymmetry in the final plot.\n",
    "    \n",
    "    e.g., suppose n_reasons_per_plot_init=20 and n_reasons=66\n",
    "      In this case, n_plots = 4\n",
    "      Simple method:\n",
    "        First 3 plots have 20 reasons each, final plot has 6 reasons.\n",
    "      This method:\n",
    "        First 3 plots have 17 reasons each, final plot has 15 reasons.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    n_plots = np.ceil(n_reasons/n_reasons_per_plot).astype(int)\n",
    "    n_reasons_per_plot_opt = np.ceil(n_reasons/n_plots).astype(int)\n",
    "    return n_reasons_per_plot_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aeb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_entries_by_binary_confusion_matrix_result(X, y_actl, y_pred):\n",
    "    r\"\"\"\n",
    "    Given the input feature vector X, actual (binary) y-values y_actl, and predicted \n",
    "    (binary) y-values y_pred, return X_tp, X_tn, X_fp, X_fn\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # True positives\n",
    "    # Are outages, predicted as outages\n",
    "    X_tp = X[(y_actl==1) & (y_pred==1)]\n",
    "\n",
    "    # True negatives\n",
    "    # Are not outages, predicted as not outages\n",
    "    X_tn = X[(y_actl==0) & (y_pred==0)]\n",
    "\n",
    "    # False positives\n",
    "    # Are not outages, predicted as outages\n",
    "    X_fp = X[(y_actl==0) & (y_pred==1)]\n",
    "\n",
    "    # False negatives\n",
    "    # Are outages, predicted as not outages\n",
    "    X_fn = X[(y_actl==1) & (y_pred==0)]\n",
    "    #-------------------------\n",
    "    return dict(\n",
    "        X_tp=X_tp, \n",
    "        X_tn=X_tn, \n",
    "        X_fp=X_fp, \n",
    "        X_fn=X_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_X_by_confusion_results_to_dfs(\n",
    "    X_by_confusion_result, \n",
    "    full_data_df, \n",
    "    run_PCA, \n",
    "    run_scaler\n",
    "):\n",
    "    r\"\"\"\n",
    "    X_tp, X_tn, X_fp, X_fn should be pd.DataFrames unless run_PCA==True or run_scaler==True.\n",
    "    If not DFs, convert them.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    expected_keys = ['X_tp', 'X_tn', 'X_fp', 'X_fn']\n",
    "    assert(len(set(X_by_confusion_result.keys()).symmetric_difference(set(expected_keys)))==0)\n",
    "    X_tp = X_by_confusion_result['X_tp']\n",
    "    X_tn = X_by_confusion_result['X_tn']\n",
    "    X_fp = X_by_confusion_result['X_fp']\n",
    "    X_fn = X_by_confusion_result['X_fn']\n",
    "    #-------------------------\n",
    "    assert(type(X_tp)==type(X_tn)==type(X_fp)==type(X_fn))\n",
    "    assert(X_tp.shape[1]==X_tn.shape[1]==X_fp.shape[1]==X_fn.shape[1])\n",
    "    n_cols = X_tp.shape[1]\n",
    "    # X_tp, X_tn, X_fp, X_fn should be pd.DataFrames unless run_PCA==True or run_scaler==True\n",
    "    if not isinstance(X_tp, pd.DataFrame):\n",
    "        assert(run_PCA or run_scaler)\n",
    "        if run_PCA:\n",
    "            assert(n_cols==pca.n_components_)\n",
    "            df_cols = [f'pca_comp_{i}' for i in range(pca.n_components_)]\n",
    "        else:\n",
    "            assert(n_cols==len(full_data_df.columns[:-1]))\n",
    "            df_cols = full_data_df.columns[:-1]\n",
    "        #----------\n",
    "        X_tp = pd.DataFrame(X_tp, columns=df_cols)\n",
    "        X_tn = pd.DataFrame(X_tn, columns=df_cols)\n",
    "        X_fp = pd.DataFrame(X_fp, columns=df_cols)\n",
    "        X_fn = pd.DataFrame(X_fn, columns=df_cols)\n",
    "    #-------------------------\n",
    "    return dict(\n",
    "        X_tp=X_tp, \n",
    "        X_tn=X_tn, \n",
    "        X_fp=X_fp, \n",
    "        X_fn=X_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877939f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_X_by_binary_confusion_result(\n",
    "    fig_num, \n",
    "    X_tp_w_args=None, \n",
    "    X_fn_w_args=None, \n",
    "    X_tn_w_args=None, \n",
    "    X_fp_w_args=None, \n",
    "    n_reasons_per_plot=20, \n",
    "    n_reasons_total_to_plot=None, \n",
    "    reason_order=None, \n",
    "    n_x=2, \n",
    "    include_xtick_labels_legend=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # At least one must not be None!\n",
    "    assert(\n",
    "        X_tp_w_args is not None or\n",
    "        X_fn_w_args is not None or\n",
    "        X_tn_w_args is not None or\n",
    "        X_fp_w_args is not None\n",
    "    )\n",
    "    #-------------------------\n",
    "    dflt_X_tp_args = dict(label='X_tp', color='green')\n",
    "    dflt_X_fn_args = dict(label='X_fn', edgecolor='green', fill=False, hatch='//')\n",
    "    dflt_X_tn_args = dict(label='X_tn', color='red')\n",
    "    dflt_X_fp_args = dict(label='X_fp', edgecolor='red', fill=False, hatch='//')\n",
    "    #-------------------------\n",
    "    Xs_w_args =    [     X_tp_w_args,    X_fn_w_args,    X_tn_w_args,    X_fp_w_args]\n",
    "    dflt_Xs_args = [dflt_X_tp_args, dflt_X_fn_args, dflt_X_tn_args, dflt_X_fp_args]\n",
    "    #-------------------------\n",
    "    dfs_w_args = []\n",
    "    for i, X_w_args in enumerate(Xs_w_args):\n",
    "        if X_w_args is not None:\n",
    "            assert(Utilities.is_object_one_of_types(X_w_args, [pd.DataFrame, list, tuple]))\n",
    "            if isinstance(X_w_args, pd.DataFrame):\n",
    "                X_w_args = [X_w_args, dict()]\n",
    "            assert(len(X_w_args)==2 and \n",
    "                   isinstance(X_w_args[0], pd.DataFrame) and\n",
    "                   isinstance(X_w_args[1], dict)\n",
    "                  )\n",
    "            X_w_args[1] = Utilities.supplement_dict_with_default_values(\n",
    "                to_supplmnt_dict=X_w_args[1], \n",
    "                default_values_dict=dflt_Xs_args[i], \n",
    "                inplace=True\n",
    "            )\n",
    "            dfs_w_args.append(X_w_args)\n",
    "    #-------------------------\n",
    "    # Make sure all DFs have same number of reasons (i.e., all have same number of columns)\n",
    "    for i in range(1, len(dfs_w_args)):\n",
    "        assert(dfs_w_args[i-1][0].shape[1]==dfs_w_args[i][0].shape[1])\n",
    "    #-----\n",
    "    if n_reasons_total_to_plot is None:\n",
    "        n_reasons=dfs_w_args[0][0].shape[1]\n",
    "    else:\n",
    "        n_reasons=n_reasons_total_to_plot\n",
    "    #-----\n",
    "    if get_optimal_n_reasons_per_plot:\n",
    "        n_reasons_per_plot = get_optimal_n_reasons_per_plot(n_reasons_per_plot, n_reasons)\n",
    "    n_y = np.ceil(n_reasons/(n_reasons_per_plot*n_x)).astype(int)\n",
    "    #-------------------------\n",
    "    if reason_order is None:\n",
    "        reason_order = dfs_w_args[0][0].columns.tolist()\n",
    "    reason_idxs = Utilities.get_batch_idx_pairs(n_reasons, n_reasons_per_plot)\n",
    "    fig, axs = Plot_General.default_subplots(n_x=n_x, n_y=n_y, fig_num=fig_num, return_flattened_axes=True)\n",
    "    #-----\n",
    "    # NOTE: If n_x > 1, then len(axs) can be greater than len(reason_idxs)\n",
    "    #       When this occurs, the final row of plots will have one or more empty \n",
    "    assert(len(axs)>=len(reason_idxs))\n",
    "    for i_plot in range(len(reason_idxs)):\n",
    "        idx_i_0 = reason_idxs[i_plot][0]\n",
    "        idx_i_1 = reason_idxs[i_plot][1]\n",
    "        #-----\n",
    "        Plot_Bar.plot_multiple_barplots(\n",
    "            ax=axs[i_plot], \n",
    "            dfs_w_args=dfs_w_args, \n",
    "            order=reason_order[idx_i_0:idx_i_1], \n",
    "            draw_side_by_side=True, \n",
    "            replace_xtick_labels_with_ints=True, \n",
    "            xtick_ints_offset=idx_i_0, \n",
    "            tick_args=[dict(axis='x', labelrotation=90, labelsize=15), \n",
    "                       dict(axis='y', labelsize=15)], \n",
    "            draw_legend=True\n",
    "        )\n",
    "    #--------------------------------------------------\n",
    "    if include_xtick_labels_legend:\n",
    "        xtick_elements = reason_order[:n_reasons]\n",
    "        xtick_rename_dict = {xtick_el:i+1 for i,xtick_el in enumerate(xtick_elements)}\n",
    "        subplot_layout_params = Plot_General.get_subplot_layout_params(fig)\n",
    "        xtick_labels_legend_textbox_kwargs = dict(\n",
    "            fig=fig, \n",
    "            xtick_rename_dict=xtick_rename_dict, \n",
    "            text_x_pos=1.02*subplot_layout_params['right'], \n",
    "            text_y_pos=subplot_layout_params['top'], \n",
    "            n_chars_per_line=30, \n",
    "            multi_line_offset=None, \n",
    "            new_org_separator=': ', \n",
    "            fontsize=10, \n",
    "            ha='left', \n",
    "            va='top',\n",
    "            n_lines_between_entries=1, \n",
    "            n_cols=2, \n",
    "            col_padding = 0.01\n",
    "        )\n",
    "        Plot_General.generate_xtick_labels_legend_textbox(\n",
    "            **xtick_labels_legend_textbox_kwargs\n",
    "        )  \n",
    "    #--------------------------------------------------\n",
    "    return fig,axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bc64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c218b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7118d21",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a05d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_sig_reasons(outg_df, no_outg_df, p_val_lim=0.05, equal_var=False):\n",
    "    r\"\"\"\n",
    "    Returns a list of columns which are statistically significant between outg_df and no_outg_df\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    sig_reasons = []\n",
    "    for reason in outg_df.columns:\n",
    "        ttr = scipy.stats.ttest_ind(outg_df[reason], no_outg_df[reason], equal_var=equal_var)\n",
    "        if ttr.pvalue < p_val_lim:\n",
    "            sig_reasons.append(reason)\n",
    "    #-------------------------\n",
    "    return sig_reasons\n",
    "\n",
    "\n",
    "def project_stat_sig_reasons(outg_df, no_outg_df, p_val_lim=0.05, equal_var=False):\n",
    "    r\"\"\"\n",
    "    Returns only the columns which are statistically different between outg_df and no_outg_df\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    sig_reasons = get_stat_sig_reasons(\n",
    "        outg_df=outg_df, \n",
    "        no_outg_df=no_outg_df, \n",
    "        p_val_lim=p_val_lim, \n",
    "        equal_var=equal_var\n",
    "    )\n",
    "    #-------------------------\n",
    "    return outg_df[sig_reasons], no_outg_df[sig_reasons]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d3212",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c415d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test_by_date(\n",
    "    df_outage, \n",
    "    df_no_outage, \n",
    "    outg_rec_nb_idfr, \n",
    "    train_dates,\n",
    "    test_dates=None, \n",
    "    y_col = ('is_outg', 'is_outg'), \n",
    "    random_state = None\n",
    "):\n",
    "    r\"\"\"\n",
    "    For now, df_outage train/test determined by date, and df_no_outage is selected to match the size of df_outage.\n",
    "    This is mainly because getting the time info of the no outage case is a hassle, and all no outage data are currently\n",
    "    from 2021 (I believe)\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    # Get outg_rec_nbs (series) and outg_rec_nbs_unq (list) from df\n",
    "    outg_rec_nbs = DOVSOutages.get_outg_rec_nbs_from_df(df=df_outage, idfr=outg_rec_nb_idfr)\n",
    "    assert(len(df_outage)==len(outg_rec_nbs)) # Important in ensuring proper merge at end\n",
    "    outg_rec_nbs_unq = outg_rec_nbs.unique().tolist()\n",
    "    #--------------------------------------------------\n",
    "    build_sql_function = DOVSOutages_SQL.build_sql_outage\n",
    "    build_sql_function_kwargs = dict(\n",
    "        outg_rec_nbs = outg_rec_nbs_unq, \n",
    "        field_to_split = 'outg_rec_nbs', \n",
    "        cols_of_interest=['OUTG_REC_NB', 'DT_OFF_TS']\n",
    "    )\n",
    "    dovs_outgs = DOVSOutages(\n",
    "        df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "        contstruct_df_args=None, \n",
    "        init_df_in_constructor=True,\n",
    "        build_sql_function=build_sql_function, \n",
    "        build_sql_function_kwargs=build_sql_function_kwargs, \n",
    "        build_consolidated=False\n",
    "    )\n",
    "    dovs_outgs_df = dovs_outgs.get_df()\n",
    "    #--------------------------------------------------\n",
    "    if test_dates is None:\n",
    "        test_dates = [train_dates[1], dovs_outgs_df['DT_OFF_TS'].max() + pd.Timedelta('1 second')]\n",
    "\n",
    "    dovs_outgs_df['train_set'] = False\n",
    "    dovs_outgs_df['test_set'] = False\n",
    "\n",
    "    dovs_outgs_df.loc[((dovs_outgs_df['DT_OFF_TS'] >= train_dates[0]) & (dovs_outgs_df['DT_OFF_TS'] < train_dates[1])), 'train_set'] = True\n",
    "    dovs_outgs_df.loc[((dovs_outgs_df['DT_OFF_TS'] >= test_dates[0]) & (dovs_outgs_df['DT_OFF_TS'] < test_dates[1])), 'test_set'] = True\n",
    "\n",
    "    # No entry should be contained in both train and test sets!\n",
    "    assert(all(dovs_outgs_df[['train_set', 'test_set']].sum(axis=1)<=1))\n",
    "\n",
    "    # Split up df_outage according to train/test split in dovs_outgs_df\n",
    "    df_outage_train = df_outage[outg_rec_nbs.isin(dovs_outgs_df[dovs_outgs_df['train_set']]['OUTG_REC_NB'])]\n",
    "    df_outage_test = df_outage[outg_rec_nbs.isin(dovs_outgs_df[dovs_outgs_df['test_set']]['OUTG_REC_NB'])]\n",
    "#     df_outage_train, df_outage_test = train_test_split(df_outage, test_size=0.33, random_state=random_state)\n",
    "\n",
    "    # Get the relative sizes the be used for selecting train/test for df_no_outage\n",
    "    train_size = df_outage_train.shape[0]/(df_outage_train.shape[0]+df_outage_test.shape[0])\n",
    "    test_size = 1.0-train_size\n",
    "\n",
    "    # Make the split for no outage\n",
    "    df_no_outage_train, df_no_outage_test = train_test_split(df_no_outage, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Combine outage and no outage\n",
    "    assert(all(df_outage_train.columns==df_no_outage_train.columns))\n",
    "    assert(all(df_outage_test.columns==df_no_outage_test.columns))\n",
    "    df_train = pd.concat([df_outage_train, df_no_outage_train])\n",
    "    df_test = pd.concat([df_outage_test, df_no_outage_test])\n",
    "\n",
    "    # Randomize order or rows\n",
    "    df_train = df_train.sample(frac=1, random_state=random_state)\n",
    "    df_test = df_test.sample(frac=1, random_state=random_state)\n",
    "\n",
    "    # Split X and y\n",
    "    X_train = df_train[[x for x in df_train.columns if x!=y_col]]\n",
    "    X_test  = df_test[[x for x in df_test.columns if x!=y_col]]\n",
    "\n",
    "    y_train = df_train[y_col]\n",
    "    y_test  = df_test[y_col]\n",
    "    #--------------------------------------------------\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!! REPLACE BY get_cpx_outg_df_subset_by_outg_datetime\n",
    "# PROBABLY NEED TO UPDATE get_cpo_df_subsets_by_outg_season as well!\n",
    "# def get_cpo_df_subset_by_outg_date(\n",
    "#     cpo_df, \n",
    "#     date_0,\n",
    "#     date_1, \n",
    "#     outg_rec_nb_idfr='index', \n",
    "#     return_notin_also=False\n",
    "# ):\n",
    "#     r\"\"\"\n",
    "#     Returns the subset of cpo_df whose associated outages are within [date_0, date_1)\n",
    "#     \"\"\"\n",
    "#     #-------------------------\n",
    "#     if not isinstance(date_0, datetime.datetime):\n",
    "#         date_0 = pd.to_datetime(date_0)\n",
    "#     if not isinstance(date_1, datetime.datetime):\n",
    "#         date_1 = pd.to_datetime(date_1)\n",
    "#     assert(isinstance(date_0, datetime.datetime))\n",
    "#     assert(isinstance(date_1, datetime.datetime))\n",
    "#     #-------------------------\n",
    "#     outg_rec_nbs = MECPODf.get_outg_rec_nbs_from_cpo_df(cpo_df=cpo_df, idfr=outg_rec_nb_idfr)\n",
    "#     assert(len(cpo_df)==len(outg_rec_nbs)) # Important in ensuring proper selection towards end of function\n",
    "#     outg_rec_nbs_unq = outg_rec_nbs.unique().tolist()\n",
    "#     #-------------------------\n",
    "#     build_sql_function = DOVSOutages_SQL.build_sql_outage\n",
    "#     build_sql_function_kwargs = dict(\n",
    "#         outg_rec_nbs = outg_rec_nbs_unq, \n",
    "#         field_to_split = 'outg_rec_nbs', \n",
    "#         cols_of_interest=['OUTG_REC_NB', 'DT_OFF_TS']\n",
    "#     )\n",
    "#     dovs_outgs = DOVSOutages(\n",
    "#         df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "#         contstruct_df_args=None, \n",
    "#         init_df_in_constructor=True,\n",
    "#         build_sql_function=build_sql_function, \n",
    "#         build_sql_function_kwargs=build_sql_function_kwargs, \n",
    "#         build_consolidated=False\n",
    "#     )\n",
    "#     dovs_outgs_df = dovs_outgs.get_df()\n",
    "#     #-------------------------\n",
    "#     subset_outg_rec_nbs = dovs_outgs_df.loc[(dovs_outgs_df['DT_OFF_TS'] >= date_0) & \n",
    "#                                             (dovs_outgs_df['DT_OFF_TS'] < date_1)]['OUTG_REC_NB'].unique().tolist()\n",
    "#     cpo_df_subset = cpo_df[outg_rec_nbs.isin(subset_outg_rec_nbs)].copy()\n",
    "#     #-------------------------\n",
    "#     if not return_notin_also:\n",
    "#         return cpo_df_subset\n",
    "#     else:\n",
    "#         cpo_df_notin = cpo_df[~outg_rec_nbs.isin(subset_outg_rec_nbs)].copy()\n",
    "#         return cpo_df_subset, cpo_df_notin\n",
    "\n",
    "# MOVED TO OutageModeler!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "def get_cpx_outg_df_subset_by_outg_datetime(\n",
    "    cpx_outg_df, \n",
    "    date_0,\n",
    "    date_1, \n",
    "    outg_rec_nb_idfr='index', \n",
    "    return_notin_also=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Returns the subset of cpx_outg_df whose associated outages are within [date_0, date_1)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if date_0 is None and date_1 is None:\n",
    "        if not return_notin_also:\n",
    "            return cpx_outg_df\n",
    "        else:\n",
    "            return cpx_outg_df, pd.DataFrame()\n",
    "    #-------------------------\n",
    "    if date_0 is None:\n",
    "        date_0 = pd.Timestamp.min\n",
    "    #-----\n",
    "    if date_1 is None:\n",
    "        date_1 = pd.Timestamp.max\n",
    "    #-------------------------\n",
    "    if not isinstance(date_0, datetime.datetime):\n",
    "        date_0 = pd.to_datetime(date_0)\n",
    "    if not isinstance(date_1, datetime.datetime):\n",
    "        date_1 = pd.to_datetime(date_1)\n",
    "    assert(isinstance(date_0, datetime.datetime))\n",
    "    assert(isinstance(date_1, datetime.datetime))\n",
    "    #-------------------------\n",
    "    contstruct_df_args=None\n",
    "    build_sql_function = DOVSOutages_SQL.build_sql_outage\n",
    "    build_sql_function_kwargs=dict(\n",
    "        datetime_col='DT_OFF_TS_FULL', \n",
    "        cols_of_interest=[\n",
    "            'OUTG_REC_NB', \n",
    "            dict(field_desc=f\"DOV.DT_ON_TS - DOV.STEP_DRTN_NB/(60*24)\", \n",
    "                 alias='DT_OFF_TS_FULL', table_alias_prefix=None)\n",
    "        ]\n",
    "    )\n",
    "    #-----\n",
    "    df_off_df = DOVSOutages.get_outg_info_for_df(\n",
    "        df=cpx_outg_df, \n",
    "        outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "        contstruct_df_args=contstruct_df_args, \n",
    "        build_sql_function=build_sql_function, \n",
    "        build_sql_function_kwargs=build_sql_function_kwargs, \n",
    "        set_outg_rec_nb_as_index=True\n",
    "    )\n",
    "    #-------------------------\n",
    "    outg_rec_nbs = MECPODf.get_outg_rec_nbs_from_cpo_df(cpo_df=cpx_outg_df, idfr=outg_rec_nb_idfr)\n",
    "    assert(len(cpx_outg_df)==len(outg_rec_nbs)) # Important in ensuring proper selection towards end of function\n",
    "    #-----\n",
    "    subset_outg_rec_nbs = df_off_df.loc[(df_off_df['DT_OFF_TS_FULL'] >= date_0) & \n",
    "                                        (df_off_df['DT_OFF_TS_FULL'] < date_1)].index.unique().tolist()\n",
    "    cpx_outg_df_subset = cpx_outg_df[outg_rec_nbs.isin(subset_outg_rec_nbs)].copy()\n",
    "    #-------------------------\n",
    "    if not return_notin_also:\n",
    "        return cpx_outg_df_subset\n",
    "    else:\n",
    "        cpx_outg_df_notin = cpx_outg_df[~outg_rec_nbs.isin(subset_outg_rec_nbs)].copy()\n",
    "        return cpx_outg_df_subset, cpx_outg_df_notin   \n",
    "    \n",
    "#TODO PLACE IN MECPODf.py, probably near get_rcpo_df_subset_by_mjr_mnr_causes\n",
    "def get_cpo_df_subsets_by_outg_season(\n",
    "    cpo_df, \n",
    "    outg_rec_nb_idfr='index'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Returns a dict whose keys are seasons and values are cpo_df subsets whose associated outages occur in the given season.\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    outg_rec_nbs = MECPODf.get_outg_rec_nbs_from_cpo_df(cpo_df=cpo_df, idfr=outg_rec_nb_idfr)\n",
    "    assert(len(cpo_df)==len(outg_rec_nbs)) # Important in ensuring proper selection towards end of function\n",
    "    outg_rec_nbs_unq = outg_rec_nbs.unique().tolist()\n",
    "    #-------------------------\n",
    "    build_sql_function = DOVSOutages_SQL.build_sql_outage\n",
    "    build_sql_function_kwargs = dict(\n",
    "        outg_rec_nbs = outg_rec_nbs_unq, \n",
    "        field_to_split = 'outg_rec_nbs', \n",
    "        cols_of_interest=['OUTG_REC_NB', 'DT_OFF_TS']\n",
    "    )\n",
    "    dovs_outgs = DOVSOutages(\n",
    "        df_construct_type=DFConstructType.kRunSqlQuery, \n",
    "        contstruct_df_args=None, \n",
    "        init_df_in_constructor=True,\n",
    "        build_sql_function=build_sql_function, \n",
    "        build_sql_function_kwargs=build_sql_function_kwargs, \n",
    "        build_consolidated=False\n",
    "    )\n",
    "    dovs_outgs_df = dovs_outgs.get_df()\n",
    "    #-------------------------\n",
    "    dovs_outgs_df['season']=np.nan\n",
    "    dovs_outgs_df.loc[dovs_outgs_df['DT_OFF_TS'].dt.month.isin([3,4,5]), 'season'] = 'spring'\n",
    "    dovs_outgs_df.loc[dovs_outgs_df['DT_OFF_TS'].dt.month.isin([6,7,8]), 'season'] = 'summer'\n",
    "    dovs_outgs_df.loc[dovs_outgs_df['DT_OFF_TS'].dt.month.isin([9,10,11]), 'season'] = 'autumn'\n",
    "    dovs_outgs_df.loc[dovs_outgs_df['DT_OFF_TS'].dt.month.isin([12,1,2]), 'season'] = 'winter'\n",
    "    # Make sure all entries assigned a season\n",
    "    assert(dovs_outgs_df['season'].isna().sum()==0)\n",
    "    #-------------------------\n",
    "    # Split up cpo_df according to seasons\n",
    "    cpo_df_spring = cpo_df[outg_rec_nbs.isin(dovs_outgs_df[dovs_outgs_df['season']=='spring']['OUTG_REC_NB'])]\n",
    "    cpo_df_summer = cpo_df[outg_rec_nbs.isin(dovs_outgs_df[dovs_outgs_df['season']=='summer']['OUTG_REC_NB'])]\n",
    "    cpo_df_autumn = cpo_df[outg_rec_nbs.isin(dovs_outgs_df[dovs_outgs_df['season']=='autumn']['OUTG_REC_NB'])]\n",
    "    cpo_df_winter = cpo_df[outg_rec_nbs.isin(dovs_outgs_df[dovs_outgs_df['season']=='winter']['OUTG_REC_NB'])]\n",
    "    assert(cpo_df_spring.shape[0]+cpo_df_summer.shape[0]+cpo_df_autumn.shape[0]+cpo_df_winter.shape[0]==cpo_df.shape[0])\n",
    "    #-------------------------\n",
    "    return dict(\n",
    "        spring=cpo_df_spring, \n",
    "        summer=cpo_df_summer, \n",
    "        autumn=cpo_df_autumn, \n",
    "        winter=cpo_df_winter\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def append_season_col_to_df(\n",
    "    df, \n",
    "    date_col, \n",
    "    placement_col='season', \n",
    "    seasons=None, \n",
    "    assert_all_classified=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if seasons is None:\n",
    "        seasons = dict(\n",
    "            spring=[3,4,5], \n",
    "            summer=[6,7,8], \n",
    "            autumn=[9,10,11], \n",
    "            winter=[12,1,2], \n",
    "        )\n",
    "    #-------------------------\n",
    "    df[placement_col] = np.nan\n",
    "    for season,months in seasons.items():\n",
    "        df.loc[df[date_col].dt.month.isin(months), placement_col] = season\n",
    "    #-------------------------\n",
    "    if assert_all_classified:\n",
    "        assert(df[placement_col].isna().sum()==0)\n",
    "    #-------------------------\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_df_subsets_by_seasons(\n",
    "    df, \n",
    "    date_col, \n",
    "    seasons=None, \n",
    "    assert_all_classified=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if seasons is None:\n",
    "        seasons = dict(\n",
    "            spring=[3,4,5], \n",
    "            summer=[6,7,8], \n",
    "            autumn=[9,10,11], \n",
    "            winter=[12,1,2], \n",
    "        )\n",
    "    #-------------------------\n",
    "    return_dict = {}\n",
    "    classified_count = 0\n",
    "    for season,months in seasons.items():\n",
    "        assert(season not in return_dict)\n",
    "        return_dict[season] = df[df[date_col].dt.month.isin(months)]\n",
    "        classified_count += return_dict[season].shape[0]\n",
    "    #-------------------------\n",
    "    if assert_all_classified:\n",
    "        assert(classified_count==df.shape[0])\n",
    "    #-------------------------\n",
    "    return return_dict\n",
    "\n",
    "\n",
    "def append_season_col_from_time_infos_to_df(\n",
    "    df, \n",
    "    time_infos_df, \n",
    "    date_col, \n",
    "    placement_col='season', \n",
    "    seasons=None, \n",
    "    missing_tolerance=0.05\n",
    "):\n",
    "    r\"\"\"\n",
    "    NOTE: If df.columns in MultiIndex, the function will pad placement_col by prepending levels with values=''\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    # Don't want any duplicate entries in time_infos_df, this could lead to additional rows being added to df\n",
    "    # NOTE: Assertion below is slightly too strong.  To be absolutely correct, only those indices which are also contained in\n",
    "    #       df need to be unique in DF.\n",
    "    assert(time_infos_df.index.nunique()==time_infos_df.shape[0])\n",
    "    #-------------------------\n",
    "    # Create placement_col in time_infos_df\n",
    "    time_infos_df = append_season_col_to_df(\n",
    "        df=time_infos_df.copy(), \n",
    "        date_col=date_col, \n",
    "        placement_col=placement_col, \n",
    "        seasons=seasons, \n",
    "        assert_all_classified=False\n",
    "    )\n",
    "    time_infos_df = time_infos_df[[placement_col]]\n",
    "    #-------------------------\n",
    "    # Make time_infos_df.columns have same number of levels as that of df so pd.merge doesn't complain\n",
    "    #   Needed extra levels are prepended with values = ''\n",
    "    assert(time_infos_df.columns.nlevels<=df.columns.nlevels)\n",
    "    if time_infos_df.columns.nlevels < df.columns.nlevels:\n",
    "        for i in range(df.columns.nlevels-time_infos_df.columns.nlevels):\n",
    "            time_infos_df = Utilities_df.prepend_level_to_MultiIndex(time_infos_df, '', axis=1)  \n",
    "    # Procedure above may have added levels to placement_col\n",
    "    # Update with the correct value\n",
    "    assert(time_infos_df.shape[1]==1)\n",
    "    placement_col = time_infos_df.columns[0]\n",
    "    #-------------------------\n",
    "    df_w_season = pd.merge(\n",
    "        df, \n",
    "        time_infos_df, \n",
    "        how='left', \n",
    "        left_index=True, \n",
    "        right_index=True\n",
    "    )\n",
    "    #-------------------------\n",
    "    missing_pct = df_w_season[placement_col].isna().sum()/df.shape[0]\n",
    "    if missing_pct>missing_tolerance:\n",
    "        print(f'missing_pct = {missing_pct} > missing_tolerance = {missing_tolerance}')\n",
    "        assert(0)\n",
    "    #-------------------------\n",
    "    return df_w_season\n",
    "\n",
    "\n",
    "def get_df_subsets_by_seasons_from_time_infos(\n",
    "    df, \n",
    "    time_infos_df, \n",
    "    date_col, \n",
    "    seasons=None, \n",
    "    missing_tolerance=0.05\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if seasons is None:\n",
    "        seasons = dict(\n",
    "            spring=[3,4,5], \n",
    "            summer=[6,7,8], \n",
    "            autumn=[9,10,11], \n",
    "            winter=[12,1,2], \n",
    "        )\n",
    "    #-------------------------\n",
    "    placement_col = Utilities.generate_random_string()\n",
    "    df_w_season = append_season_col_from_time_infos_to_df(\n",
    "        df=df, \n",
    "        time_infos_df=time_infos_df, \n",
    "        date_col=date_col, \n",
    "        placement_col=placement_col, \n",
    "        seasons=seasons, \n",
    "        missing_tolerance=missing_tolerance\n",
    "    )\n",
    "    # Note: append_season_col_from_time_infos_to_df alters placement_col if necessary to match the number of levels\n",
    "    #       in df.columns.  Find new value for placement_col\n",
    "    placement_col_idx = Utilities_df.find_idxs_in_highest_order_of_columns(df_w_season, placement_col)\n",
    "    # Should only be one\n",
    "    assert(len(placement_col_idx)==1)\n",
    "    placement_col_idx=placement_col_idx[0]\n",
    "    placement_col = df_w_season.columns[placement_col_idx]\n",
    "    #-------------------------\n",
    "    return_dict = {}\n",
    "    for season,months in seasons.items():\n",
    "        assert(season not in return_dict)\n",
    "        return_dict[season] = df_w_season[df_w_season[placement_col]==season].drop(columns=[placement_col])\n",
    "    #-------------------------\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d652a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO OutageModeler!\n",
    "def merge_cpx_df_w_time_infos(\n",
    "    cpx_df, \n",
    "    time_infos_df, \n",
    "    time_infos_drop_dupls_subset=['index', 't_min'], \n",
    "    dummy_lvl_base_name = 'dummy_lvl'\n",
    "):\n",
    "    r\"\"\"\n",
    "    cpx_df and time_infos_df must have same indices.\n",
    "    Typically, these are no_outg_rec_nb and trsf_pole_nb.\n",
    "    \n",
    "    time_infos_drop_dupls_subset:\n",
    "        Since data are collected over multiple files, sometimes a (no-)outage event is split over multiple files\n",
    "          with, e.g., different PNs.  So, duplicates must be dropped.\n",
    "        Typically, index needs to be included as well (index usually comprised of no_outg_rec_nb and trsf_pole_nb)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if time_infos_drop_dupls_subset is not None:\n",
    "        assert(Utilities.is_object_one_of_types(time_infos_drop_dupls_subset, [list, tuple]))\n",
    "        if 'index' in time_infos_drop_dupls_subset:\n",
    "            time_infos_drop_dupls_subset.remove('index')\n",
    "            time_infos_df = time_infos_df.reset_index().drop_duplicates(\n",
    "                subset=list(time_infos_df.index.names)+time_infos_drop_dupls_subset\n",
    "            ).set_index(time_infos_df.index.names)\n",
    "        else:\n",
    "            time_infos_df = time_infos_df.drop_duplicates(subset=time_infos_drop_dupls_subset)\n",
    "    #-------------------------\n",
    "    # In order to merge, cpx_df and time_infos_df must have same number of levels in columns\n",
    "    if cpx_df.columns.nlevels>1:\n",
    "        n_levels_to_add = cpx_df.columns.nlevels - time_infos_df.columns.nlevels\n",
    "        #-----\n",
    "        for i_new_lvl in range(n_levels_to_add):\n",
    "            # With each iteration, prepending a new level from n_levels_to_add-1 to 0\n",
    "            i_level_val = f'{dummy_lvl_base_name}_{(n_levels_to_add-1)-i_new_lvl}'\n",
    "            time_infos_df = Utilities_df.prepend_level_to_MultiIndex(\n",
    "                df=time_infos_df, \n",
    "                level_val=i_level_val, \n",
    "                level_name=None, \n",
    "                axis=1\n",
    "            )\n",
    "    assert(cpx_df.columns.nlevels==time_infos_df.columns.nlevels)\n",
    "    #-------------------------\n",
    "    # Apparently, pd.merge is smart enough to match index level names, so the following isn't strictly necessary!\n",
    "    # However, it doesn't hurt, and is good in practice\n",
    "    assert(len(set(cpx_df.index.names).symmetric_difference(set(time_infos_df.index.names)))==0)\n",
    "    if time_infos_df.index.names!=cpx_df.index.names:\n",
    "        time_infos_df = time_infos_df.reset_index().set_index(cpx_df.index.names)\n",
    "    #-----\n",
    "    cpx_df_wt = pd.merge(\n",
    "        cpx_df, \n",
    "        time_infos_df, \n",
    "        how='left', \n",
    "        left_index=True, \n",
    "        right_index=True\n",
    "    )\n",
    "    #-------------------------\n",
    "    return cpx_df_wt\n",
    "\n",
    "\n",
    "# MOVED TO OutageModeler!\n",
    "def get_cpx_baseline_df_subset_by_datetime(\n",
    "    cpx_bsln_df, \n",
    "    bsln_time_infos_df, \n",
    "    date_0,\n",
    "    date_1,\n",
    "    bsln_time_infos_time_col='t_min', \n",
    "    return_notin_also=False, \n",
    "    merge_time_info_to_cpx_bsln_df=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    cpx_bsln_df and bsln_time_infos_df must have same indices.\n",
    "    Typically, these are no_outg_rec_nb and trsf_pole_nb.\n",
    "    \n",
    "    NOTE: Have found merging can be taxing (from memory standpoint) when DFs are large.\n",
    "          Hence why default merge_time_info_to_cpx_bsln_df=False\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if date_0 is None and date_1 is None:\n",
    "        if not return_notin_also:\n",
    "            return cpx_outg_df\n",
    "        else:\n",
    "            return cpx_outg_df, pd.DataFrame()\n",
    "    #-------------------------\n",
    "    if date_0 is None:\n",
    "        date_0 = pd.Timestamp.min\n",
    "    #-----\n",
    "    if date_1 is None:\n",
    "        date_1 = pd.Timestamp.max\n",
    "    #-------------------------\n",
    "    if not isinstance(date_0, datetime.datetime):\n",
    "        date_0 = pd.to_datetime(date_0)\n",
    "    if not isinstance(date_1, datetime.datetime):\n",
    "        date_1 = pd.to_datetime(date_1)\n",
    "    assert(isinstance(date_0, datetime.datetime))\n",
    "    assert(isinstance(date_1, datetime.datetime))\n",
    "    #-------------------------\n",
    "    assert(len(set(cpx_bsln_df.index.names).symmetric_difference(set(bsln_time_infos_df.index.names)))==0)\n",
    "    if bsln_time_infos_df.index.names!=cpx_bsln_df.index.names:\n",
    "        bsln_time_infos_df = bsln_time_infos_df.reset_index().set_index(cpx_bsln_df.index.names)\n",
    "    #-------------------------\n",
    "    if bsln_time_infos_df.columns.nlevels>1 and not Utilities.is_object_one_of_types(bsln_time_infos_time_col, [list, tuple]):\n",
    "        bsln_time_infos_time_col = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "            df=bsln_time_infos_df, \n",
    "            col=bsln_time_infos_time_col\n",
    "        )\n",
    "    assert(bsln_time_infos_time_col in bsln_time_infos_df.columns.tolist())\n",
    "    #-------------------------\n",
    "    bsln_time_infos_df = bsln_time_infos_df[[bsln_time_infos_time_col]]\n",
    "    # Since collected over multiple files, sometimes a no-outage 'event' is split over multiple files\n",
    "    #   with, e.g., different PNs.  So, duplicates must be dropped\n",
    "    # Need to called reset_index because otherwise only bsln_time_infos_time_col will be considered, whereas\n",
    "    #   here a duplicate must have same no_outg_rec_nb, trsf_pole_nb, and bsln_time_infos_time_col!\n",
    "    bsln_time_infos_df = bsln_time_infos_df.reset_index().drop_duplicates().set_index(bsln_time_infos_df.index.names)\n",
    "    #-------------------------\n",
    "    # There should be an entry in bsln_time_infos_df for each in cpx_bsln_df\n",
    "    # The reverse is NOT true: If no events found for specifiec timeframe, no entries will exist in cpx_bsln_df\n",
    "    assert(len(set(cpx_bsln_df.index).difference(set(bsln_time_infos_df.index)))==0)\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    if not merge_time_info_to_cpx_bsln_df:\n",
    "        subset_idxs = bsln_time_infos_df.loc[\n",
    "            (bsln_time_infos_df[bsln_time_infos_time_col] >= date_0) &\n",
    "            (bsln_time_infos_df[bsln_time_infos_time_col] < date_1)\n",
    "        ].index.unique().tolist()\n",
    "        cpx_bsln_df_subset = cpx_bsln_df[cpx_bsln_df.index.isin(subset_idxs)].copy()\n",
    "        #-------------------------\n",
    "        if not return_notin_also:\n",
    "            return cpx_bsln_df_subset\n",
    "        else:\n",
    "            cpx_bsln_df_notin = cpx_bsln_df[~cpx_bsln_df.index.isin(subset_idxs)].copy()\n",
    "            return cpx_bsln_df_subset, cpx_bsln_df_notin\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    else:\n",
    "        cpx_bsln_df_wt = merge_cpx_df_w_time_infos(\n",
    "            cpx_df=cpx_bsln_df, \n",
    "            time_infos_df=bsln_time_infos_df, \n",
    "            time_infos_drop_dupls_subset=['index', bsln_time_infos_time_col]\n",
    "        )\n",
    "        #-------------------------\n",
    "        # Merging will add dummy levels if needed, so adjust bsln_time_infos_time_col if needed\n",
    "        if not bsln_time_infos_time_col in cpx_bsln_df_wt.columns.tolist():\n",
    "            bsln_time_infos_time_col = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "                df=cpx_bsln_df_wt, \n",
    "                col=bsln_time_infos_time_col\n",
    "            )\n",
    "        assert(bsln_time_infos_time_col in cpx_bsln_df_wt.columns.tolist())\n",
    "        #-------------------------\n",
    "        cpx_bsln_df_wt_subset = cpx_bsln_df_wt.loc[\n",
    "            (cpx_bsln_df_wt[bsln_time_infos_time_col] >= date_0) &\n",
    "            (cpx_bsln_df_wt[bsln_time_infos_time_col] < date_1)\n",
    "        ]\n",
    "        if not return_notin_also:\n",
    "            return cpx_bsln_df_wt_subset\n",
    "        else:\n",
    "            cpx_bsln_df_wt_notin = cpx_bsln_df_wt.loc[\n",
    "                ~((cpx_bsln_df_wt[bsln_time_infos_time_col] >= date_0) &\n",
    "                (cpx_bsln_df_wt[bsln_time_infos_time_col] < date_1))\n",
    "            ]\n",
    "            return cpx_bsln_df_wt_subset, cpx_bsln_df_wt_notin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05832868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_eemsp(conn_eemsp, trsf_pole_nbs, batch_size=1000, verbose=True, n_update=10):\n",
    "    return_df = pd.DataFrame()\n",
    "    n_batches = int(np.ceil(len(trsf_pole_nbs)/batch_size))\n",
    "    if verbose:\n",
    "        print(f'n_trsf_pole_nbs = {len(trsf_pole_nbs)}')\n",
    "        print(f'batch_size = {batch_size}')\n",
    "        print(f'n_batches = {n_batches}')\n",
    "    for i in range(n_batches):\n",
    "        if verbose and (i+1)%n_update==0:\n",
    "            print(f'{i+1}/{n_batches}')\n",
    "        i_beg = i*batch_size\n",
    "        i_end = (i+1)*batch_size\n",
    "        if i==n_batches-1:\n",
    "            i_end = len(trsf_pole_nbs)\n",
    "        sql_eemsp_i = EEMSP.build_sql_eemsp_oracle(trsf_pole_nbs[i_beg:i_end])\n",
    "        df_eemsp_i = pd.read_sql_query(sql_eemsp_i, conn_eemsp)\n",
    "        #-----\n",
    "        if return_df.shape[0]>0:\n",
    "            assert(all(df_eemsp_i.columns==return_df.columns))\n",
    "        return_df = pd.concat([return_df, df_eemsp_i])\n",
    "    return return_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576178f-2a7e-4efd-8b83-92f36784a7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "394f2c7f",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca4faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO OutageModeler!\n",
    "def set_target_val_1_by_idx(\n",
    "    df,\n",
    "    val_1_idxs,\n",
    "    remove_others_from_outages=False, \n",
    "    target_col=('is_outg', 'is_outg'), \n",
    "    from_outg_col=('from_outg', 'from_outg')\n",
    "):\n",
    "    r\"\"\"\n",
    "    Set the target value to 1 for those in df with indices found in val_1_idxs.\n",
    "\n",
    "    df:\n",
    "        pd.DataFrame object OR a list of such objects\n",
    "    \n",
    "    val_1_idxs:\n",
    "        A list containing the indices whose target values should be set to 1.\n",
    "        Note, in general, val_1_idxs can contain more indices than those found in df, \n",
    "          as an intersection will be used in the code.\n",
    "          \n",
    "    remove_others_from_outages:\n",
    "        If True, those with df[from_outg_col]==1 and df[target_col]==0 will be removed.\n",
    "        This is useful if one wants to use a subset of outages as the target and remove all other outages from the data.\n",
    "        \n",
    "    target_col\n",
    "    \n",
    "    from_outg_col:\n",
    "        Only used if remove_others_from_outages==True\n",
    "        \n",
    "    \"\"\"\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    assert(Utilities.is_object_one_of_types(df, [pd.DataFrame, list]))\n",
    "    if isinstance(df, list):\n",
    "        return_dfs = []\n",
    "        for df_i in df:\n",
    "            df_i_fnl = set_target_val_1_by_idx(\n",
    "                df                         = df_i, \n",
    "                val_1_idxs                 = val_1_idxs,\n",
    "                remove_others_from_outages = remove_others_from_outages, \n",
    "                target_col                 = target_col, \n",
    "                from_outg_col              = from_outg_col\n",
    "            )\n",
    "            return_dfs.append(df_i_fnl)\n",
    "        return return_dfs\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # First, set all target values to 0\n",
    "    df[target_col] = 0\n",
    "    \n",
    "    #-------------------------\n",
    "    # Set the target values to 1 for any indices in val_1_idxs\n",
    "    df.loc[list(set(df.index).intersection(set(val_1_idxs))), target_col] = 1\n",
    "    \n",
    "    #-------------------------\n",
    "    # Remove other outages not marked as target==1 if remove_others_from_outages==True\n",
    "    if remove_others_from_outages:\n",
    "        # Drop any entries which are from the outages collection but not marked as target==1\n",
    "        # NOTE: The method below is a little safer than finding the indices to drop and then calling .drop()\n",
    "        #         as this should be safe against duplicate indices, whereas the .drop method would not be.\n",
    "        #       However, I do not expect duplicate indices to occur, so either would probably be fine.\n",
    "        df = df[\n",
    "            ~(\n",
    "                (df[from_outg_col]==1) & \n",
    "                (df[target_col]==0)\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "    #-------------------------\n",
    "    return df\n",
    "\n",
    "# MOVED TO OutageModeler!\n",
    "def ensure_target_val_1_min_pct(\n",
    "    df,\n",
    "    min_pct,\n",
    "    target_col=('is_outg', 'is_outg'), \n",
    "    random_state=None, \n",
    "    assert_success=True, \n",
    "    return_discarded=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Make sure the collection of entries with target value==1 comprises at least min_pct of the overall collection.\n",
    "    If the percentage is below min_pct, entries are removed from the target value==0 collection until desired \n",
    "      percentage is reached.\n",
    "    If the percentage is already above min_pct, simply return the df.\n",
    "    \n",
    "    min_pct:\n",
    "        Should be between 0 and 100! (not, e.g., 0 and 1)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    pct = 100*(df[target_col]==1).sum()/df.shape[0]\n",
    "    #-------------------------\n",
    "    if pct >= min_pct:\n",
    "        return df\n",
    "    #-------------------------\n",
    "    # Need to determine how many entries from target==0 to keep \n",
    "    #   Define the number of target==1 values to be n_1 and the needed number of\n",
    "    #     target==0 values to be n_0\n",
    "    #   One must solve for n_0 in the following:  100*n_1/(n_1+n_0)=min_pct\n",
    "    #   ==> n_0 = (100-min_pct)*n_1/min_pct\n",
    "    n_1 = (df[target_col]==1).sum()\n",
    "    n_0 = np.floor((100-min_pct)*n_1/min_pct).astype(int)\n",
    "    #-------------------------\n",
    "    df_1 = df[df[target_col]==1]\n",
    "    df_0 = df[df[target_col]==0]\n",
    "    df_0_sub = df_0.sample(n=n_0, replace=False, random_state=random_state)\n",
    "    #-------------------------\n",
    "    # Join df_1 and df_0_sub and randomize order\n",
    "    return_df = pd.concat([df_1, df_0_sub])\n",
    "    return_df = return_df.sample(frac=1, random_state=random_state)\n",
    "    #-------------------------\n",
    "    # Ensure operation was successful\n",
    "    if assert_success:\n",
    "        pct = 100*(return_df[target_col]==1).sum()/return_df.shape[0]\n",
    "        assert(pct>=min_pct)\n",
    "    #-------------------------\n",
    "    if return_discarded:\n",
    "        df_0_discarded = df_0[~df_0.index.isin(df_0_sub.index)]\n",
    "        assert(df_0_sub.shape[0]+df_0_discarded.shape[0]==df_0.shape[0])\n",
    "        return return_df, df_0_discarded\n",
    "    else:\n",
    "        return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33459065",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbaef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO OutageModeler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "def train_test_split_df_by_outage(\n",
    "    df, \n",
    "    outg_rec_nb_idfr, \n",
    "    test_size, \n",
    "    random_state=None\n",
    "    \n",
    "):\n",
    "    r\"\"\"\n",
    "    This is simply a train-test split according to the outage groups.\n",
    "    i.e., this enforces that all entries for a given outage remain together (either all in train or all in test)\n",
    "          and never split across train/test\n",
    "    e.g., if outage 1 affects 5 transformers, all 5 transformers will be in train or all will be in test, it will never\n",
    "          occur that 3 are in train and 2 in test\n",
    "          \n",
    "          \n",
    "    outg_rec_nb_idfr:\n",
    "        This directs from where the outg_rec_nbs will be retrieved.\n",
    "        This should be a string, list, or tuple.\n",
    "        If the outg_rec_nbs are located in a column, idfr should simply be the column\n",
    "            - Single index columns --> simple string\n",
    "            - MultiIndex columns   --> appropriate tuple to identify column\n",
    "        If the outg_rec_nbs are located in the index:\n",
    "            - Single level index --> simple string 'index' or 'index_0'\n",
    "            - MultiIndex index:  --> \n",
    "                - string f'index_{level}', where level is the index level containing the outg_rec_nbs\n",
    "                - tuple of length 2, with 0th element ='index' and 1st element = idx_level_name where\n",
    "                    idx_level_name is the name of the index level containing the outg_rec_nbs \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    outg_rec_nbs = DOVSOutages.get_outg_rec_nbs_list_from_df(\n",
    "        df=df, \n",
    "        idfr=outg_rec_nb_idfr, \n",
    "        unique_only=False\n",
    "    )\n",
    "    #-----\n",
    "    split = gss.split(df, groups=outg_rec_nbs)\n",
    "    train_idxs, test_idxs = next(split)\n",
    "    #-----\n",
    "    df_train = df.iloc[train_idxs].copy()\n",
    "    df_test  = df.iloc[test_idxs].copy()\n",
    "    #-------------------------\n",
    "    # Make sure the operation worked as expected\n",
    "    outg_rec_nbs_train = DOVSOutages.get_outg_rec_nbs_list_from_df(\n",
    "        df=df_train, \n",
    "        idfr=outg_rec_nb_idfr, \n",
    "        unique_only=True\n",
    "    )\n",
    "    outg_rec_nbs_test = DOVSOutages.get_outg_rec_nbs_list_from_df(\n",
    "        df=df_test, \n",
    "        idfr=outg_rec_nb_idfr, \n",
    "        unique_only=True\n",
    "    )\n",
    "    assert(len(set(outg_rec_nbs_train).intersection(set(outg_rec_nbs_test)))==0)\n",
    "    #-------------------------\n",
    "    return df_train, df_test\n",
    "\n",
    "# MOVED TO Utilities_df!\n",
    "def train_test_split_df_group(\n",
    "    X,\n",
    "    y, \n",
    "    groups, \n",
    "    test_size, \n",
    "    random_state=None\n",
    "    \n",
    "):\n",
    "    r\"\"\"\n",
    "    This is simply a train-test split according to the outage groups.\n",
    "    i.e., this enforces that all entries for a given group remain together (either all in train or all in test)\n",
    "          and never split across train/test\n",
    "          \n",
    "    NOTE: If input is list, return value will be np.ndarray\n",
    "          If input is np.ndarray, output np.ndarray\n",
    "          If inputs pd.DataFrame/pd.Series, output pd.DataFrame/pd.Series\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # The methods expect X and y to be either np.ndarrays, lists or pd.DataFrame/pd.Series (respectively)\n",
    "    assert(Utilities.is_object_one_of_types(X, [np.ndarray, list, pd.DataFrame]))\n",
    "    assert(Utilities.is_object_one_of_types(y, [np.ndarray, list, pd.Series]))\n",
    "    assert(len(X)==len(y)) # next(split) would have failed if this wasn't true, but having it here \n",
    "    #                      makes it easier to locate and debug if it ever happens\n",
    "    #-------------------------\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    split = gss.split(X, y, groups=groups)\n",
    "    train_idxs, test_idxs = next(split)\n",
    "    #-------------------------\n",
    "    # In order to grab elements simply using list of indices (instead of looping through or whatever)\n",
    "    #   X and y must be np.ndarrays\n",
    "    if isinstance(X, list):\n",
    "        X = np.array(X)\n",
    "    if isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "    #-------------------------\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_train = X.iloc[train_idxs]\n",
    "        X_test  = X.iloc[test_idxs]\n",
    "    else:\n",
    "        X_train = X[train_idxs]\n",
    "        X_test  = X[test_idxs]\n",
    "    #-----\n",
    "    if isinstance(y, pd.Series):\n",
    "        y_train = y.iloc[train_idxs]\n",
    "        y_test  = y.iloc[test_idxs]\n",
    "    else:\n",
    "        y_train = y[train_idxs]\n",
    "        y_test  = y[test_idxs]\n",
    "    #-------------------------\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ddc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data_by_binary_confusion_matrix_result(\n",
    "    data_df, \n",
    "    y_pred, \n",
    "    y_col=('is_outg', 'is_outg')\n",
    "):\n",
    "    r\"\"\"\n",
    "    Classify each row in data_df by its confusion matrix result.\n",
    "    Returns a pd.Series object with index matching that of data_df and values equal\n",
    "      to confusion result (TP, FN, TN, FP)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(y_col in data_df.columns)\n",
    "    assert(data_df.shape[0]==len(y_pred))\n",
    "    #-------------------------\n",
    "    return_srs = data_df[[y_col]].copy()\n",
    "    return_srs.columns=['y']\n",
    "    #-------------------------\n",
    "    return_srs['y_pred'] = y_pred\n",
    "    #-------------------------\n",
    "    return_srs['confusion_result'] = ''\n",
    "    #-----\n",
    "    return_srs.loc[\n",
    "        (return_srs['y']      == 1) & \n",
    "        (return_srs['y_pred'] == 1), \n",
    "        'confusion_result'\n",
    "    ] = 'TP'\n",
    "    #-----\n",
    "    return_srs.loc[\n",
    "        (return_srs['y']      == 0) & \n",
    "        (return_srs['y_pred'] == 0), \n",
    "        'confusion_result'\n",
    "    ] = 'TN'\n",
    "    #-----\n",
    "    return_srs.loc[\n",
    "        (return_srs['y']      == 0) & \n",
    "        (return_srs['y_pred'] == 1), \n",
    "        'confusion_result'\n",
    "    ] = 'FP'\n",
    "    #-----\n",
    "    return_srs.loc[\n",
    "        (return_srs['y']      == 1) & \n",
    "        (return_srs['y_pred'] == 0), \n",
    "        'confusion_result'\n",
    "    ] = 'FN'\n",
    "    #-----\n",
    "    assert(return_srs['confusion_result'].isin(['TP', 'TN', 'FP', 'FN']).all())\n",
    "    return_srs=return_srs['confusion_result']\n",
    "    #-------------------------\n",
    "    return return_srs\n",
    "\n",
    "\n",
    "def get_df_subset_by_binary_confusion_matrix_result(\n",
    "    data_df, \n",
    "    y_pred, \n",
    "    y_col=('is_outg', 'is_outg')\n",
    "):\n",
    "    r\"\"\"\n",
    "    Get subsets of data_df for each confusion matrix result (TP, TN, FP, FN)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    cnfsn_res_srs = classify_data_by_binary_confusion_matrix_result(\n",
    "        data_df=data_df, \n",
    "        y_pred=y_pred, \n",
    "        y_col=y_col\n",
    "    )\n",
    "    assert(data_df.index.equals(cnfsn_res_srs.index))\n",
    "    #-------------------------\n",
    "    data_df_tp = data_df.loc[cnfsn_res_srs[cnfsn_res_srs=='TP'].index].copy()\n",
    "    data_df_tn = data_df.loc[cnfsn_res_srs[cnfsn_res_srs=='TN'].index].copy()\n",
    "    data_df_fp = data_df.loc[cnfsn_res_srs[cnfsn_res_srs=='FP'].index].copy()\n",
    "    data_df_fn = data_df.loc[cnfsn_res_srs[cnfsn_res_srs=='FN'].index].copy()\n",
    "    #-------------------------\n",
    "    return dict(\n",
    "        TP=data_df_tp, \n",
    "        TN=data_df_tn, \n",
    "        FP=data_df_fp, \n",
    "        FN=data_df_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dec3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outg_rec_nb_value_counts(\n",
    "    df, \n",
    "    outg_rec_nb_idfr\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    return DOVSOutages.get_outg_rec_nbs_from_df(\n",
    "        df=df, \n",
    "        idfr=outg_rec_nb_idfr\n",
    "    ).value_counts()\n",
    "\n",
    "def get_n_trsf_poles_per_outg(\n",
    "    df, \n",
    "    outg_rec_nb_idfr, \n",
    "    trsf_pole_nb_idfr\n",
    "):\n",
    "    r\"\"\"\n",
    "    Returns a series object whose index contains the outg_rec_nbs and values equal the number\n",
    "      of transformer pole numbers found for that outage\n",
    "      \n",
    "    IMPORTANT: One should use the full dataset, not, e.g. results broken up by confusion matrix results (e.g., df_TP)\n",
    "               The latter will not work because transformers belonging to the same outage may have different\n",
    "                 predictions, meaning some can be TP and others FN.\n",
    "               Thus, using df_TP would incorrectly underestimate the number of transformers per outage!\n",
    "      \n",
    "    outg_rec_nb_idfr/trsf_pole_nb_idfr:\n",
    "        These direct from where the outg_rec_nbs and trsf_pole_nbs will be retrieved.\n",
    "        !!! Explanations below for outg_rec_nbs_idfr, but hold true for trsf_pole_nb_idfr as well !!!\n",
    "        This should be a string, list, or tuple.\n",
    "        If the outg_rec_nbs are located in a column, idfr should simply be the column\n",
    "            - Single index columns --> simple string\n",
    "            - MultiIndex columns   --> appropriate tuple to identify column\n",
    "        If the outg_rec_nbs are located in the index:\n",
    "            - Single level index --> simple string 'index' or 'index_0'\n",
    "            - MultiIndex index:  --> \n",
    "                - string f'index_{level}', where level is the index level containing the outg_rec_nbs\n",
    "                - tuple of length 2, with 0th element ='index' and 1st element = idx_level_name where\n",
    "                    idx_level_name is the name of the index level containing the outg_rec_nbs \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    outg_rec_nbs  = DOVSOutages.get_outg_rec_nbs_from_df(df=df, idfr=outg_rec_nb_idfr)\n",
    "    trsf_pole_nbs = DOVSOutages.get_outg_rec_nbs_from_df(df=df, idfr=trsf_pole_nb_idfr)\n",
    "    #-----\n",
    "    assert(Utilities.is_object_one_of_types(outg_rec_nbs,  [pd.Index, pd.Series]))\n",
    "    assert(Utilities.is_object_one_of_types(trsf_pole_nbs, [pd.Index, pd.Series]))\n",
    "    #-------------------------\n",
    "    assert(len(outg_rec_nbs)==len(trsf_pole_nbs))\n",
    "    og_len = len(outg_rec_nbs)\n",
    "    #--------------------------------------------------\n",
    "    # If both pd.Index objects, create new pd.DataFrame housing the two\n",
    "    if isinstance(outg_rec_nbs, pd.Index) and isinstance(trsf_pole_nbs, pd.Index):\n",
    "        tmp_df = pd.DataFrame({\n",
    "            'outg_rec_nb'  : outg_rec_nbs, \n",
    "            'trsf_pole_nb' : trsf_pole_nbs\n",
    "        })\n",
    "        trsf_pole_nbs_per_outg = tmp_df.drop_duplicates()['outg_rec_nb'].value_counts()\n",
    "    #--------------------------------------------------\n",
    "    elif isinstance(outg_rec_nbs, pd.Series) and isinstance(trsf_pole_nbs, pd.Series):\n",
    "        tmp_df = pd.merge(outg_rec_nbs, trsf_pole_nbs, left_index=True, right_index=True, how='inner')\n",
    "        assert(tmp_df.shape[0]==og_len)\n",
    "        trsf_pole_nbs_per_outg = tmp_df.drop_duplicates().iloc[:,0].value_counts()\n",
    "    #--------------------------------------------------\n",
    "    else:\n",
    "        #-------------------------\n",
    "        # Headache below simply ensures the pd.Index object is found as an index (in exactly\n",
    "        #   one level of) the pd.Series object\n",
    "        if isinstance(outg_rec_nbs, pd.Index):\n",
    "            assert(isinstance(trsf_pole_nbs, pd.Series))\n",
    "            idx_obj = outg_rec_nbs\n",
    "            srs_obj = trsf_pole_nbs\n",
    "        else:\n",
    "            assert(isinstance(outg_rec_nbs, pd.Series))\n",
    "            assert(isinstance(trsf_pole_nbs, pd.Index))\n",
    "            idx_obj = trsf_pole_nbs\n",
    "            srs_obj = outg_rec_nbs\n",
    "        #-----\n",
    "        idxs_eq=0\n",
    "        for idx_level in range(srs_obj.index.nlevels):\n",
    "            if all(srs_obj.index.get_level_values(idx_level)==idx_obj):\n",
    "                idxs_eq+=1\n",
    "        assert(idxs_eq==1)    \n",
    "        #-------------------------\n",
    "        tmp_df = pd.DataFrame({\n",
    "            'outg_rec_nb'  : outg_rec_nbs, \n",
    "            'trsf_pole_nb' : trsf_pole_nbs\n",
    "        })\n",
    "        trsf_pole_nbs_per_outg = tmp_df.drop_duplicates()['outg_rec_nb'].value_counts()\n",
    "    #--------------------------------------------------\n",
    "    return trsf_pole_nbs_per_outg\n",
    "\n",
    "\n",
    "def get_outgs_w_single_xfmr(\n",
    "    df, \n",
    "    outg_rec_nb_idfr, \n",
    "    trsf_pole_nb_idfr\n",
    "):\n",
    "    r\"\"\"\n",
    "    Returns a list of outg_rec_nbs from df which affect a single transformer\n",
    "    \n",
    "    IMPORTANT: One should use the full dataset, not, e.g. results broken up by confusion matrix results (e.g., df_TP)\n",
    "               The latter will not work because transformers belonging to the same outage may have different\n",
    "                 predictions, meaning one can be tp and another fn.\n",
    "               Thus, using df_TP would incorrectly see such a situation as an outage with a single transformer!\n",
    "      \n",
    "    outg_rec_nb_idfr/trsf_pole_nb_idfr:\n",
    "        These direct from where the outg_rec_nbs and trsf_pole_nbs will be retrieved.\n",
    "        !!! Explanations below for outg_rec_nbs_idfr, but hold true for trsf_pole_nb_idfr as well !!!\n",
    "        This should be a string, list, or tuple.\n",
    "        If the outg_rec_nbs are located in a column, idfr should simply be the column\n",
    "            - Single index columns --> simple string\n",
    "            - MultiIndex columns   --> appropriate tuple to identify column\n",
    "        If the outg_rec_nbs are located in the index:\n",
    "            - Single level index --> simple string 'index' or 'index_0'\n",
    "            - MultiIndex index:  --> \n",
    "                - string f'index_{level}', where level is the index level containing the outg_rec_nbs\n",
    "                - tuple of length 2, with 0th element ='index' and 1st element = idx_level_name where\n",
    "                    idx_level_name is the name of the index level containing the outg_rec_nbs \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    n_trsf_poles_per_outg=get_n_trsf_poles_per_outg(\n",
    "        df=df, \n",
    "        outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "        trsf_pole_nb_idfr=trsf_pole_nb_idfr\n",
    "    )\n",
    "    return n_trsf_poles_per_outg[n_trsf_poles_per_outg==1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fd0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285113fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED to Utilities_df!!!!!!!\n",
    "# def get_idfr_loc(\n",
    "#     df, \n",
    "#     idfr\n",
    "# ):\n",
    "#     r\"\"\"\n",
    "#     Returns the identifier and whether or not it was found in index\n",
    "#     If idfr found in columns, essentially just reutnrs idfr\n",
    "#     If idfr found in index, returns the index level where it was found\n",
    "    \n",
    "#     idfr:\n",
    "#         This should be a string, list, or tuple.\n",
    "#         If column, idfr should simply be the column\n",
    "#             - Single index columns --> simple string\n",
    "#             - MultiIndex columns   --> appropriate tuple to identify column\n",
    "#         If in the index:\n",
    "#             - Single level index --> simple string 'index' or 'index_0'\n",
    "#             - MultiIndex index:  --> \n",
    "#                 - string f'index_{level}', where level is the index level containing the outg_rec_nbs\n",
    "#                 - tuple of length 2, with 0th element ='index' and 1st element = idx_level_name where\n",
    "#                     idx_level_name is the name of the index level containing the outg_rec_nbs \n",
    "#     \"\"\"\n",
    "#     #-------------------------\n",
    "#     assert(Utilities.is_object_one_of_types(idfr, [str, list, tuple]))\n",
    "#     # NOTE: pd doesn't like checking for idfr in df.columns if idfr is a list.  It is fine checking when\n",
    "#     #       it is a tuple, as tuples can represent columns.  Therefore, if idfr is a list, convert to tuple\n",
    "#     #       as this will fix the issue below and have no effect elsewhere.\n",
    "#     if isinstance(idfr, list):\n",
    "#         idfr = tuple(idfr)\n",
    "#     if idfr in df.columns:\n",
    "#         return idfr, False\n",
    "#     #-------------------------\n",
    "#     # If not in the columns (because return from function not executed above), outg_rec_nbs must be in the indices!\n",
    "#     # The if/else block below determines idfr_idx_lvl\n",
    "#     if isinstance(idfr, str):\n",
    "#         assert(idfr.startswith('index'))\n",
    "#         if idfr=='index':\n",
    "#             idfr_idx_lvl=0\n",
    "#         else:\n",
    "#             idfr_idx_lvl = re.findall(r'index_(\\d*)', idfr)\n",
    "#             assert(len(idfr_idx_lvl)==1)\n",
    "#             idfr_idx_lvl=idfr_idx_lvl[0]\n",
    "#             idfr_idx_lvl=int(idfr_idx_lvl)\n",
    "#     else:\n",
    "#         assert(len(idfr)==2)\n",
    "#         assert(idfr[0]=='index')\n",
    "#         idx_level_name = idfr[1]\n",
    "#         assert(idx_level_name in df.index.names)\n",
    "#         # Need to also make sure idx_level_name only occurs once, so no ambiguity!\n",
    "#         assert(df.index.names.count(idx_level_name)==1)\n",
    "#         idfr_idx_lvl = df.index.names.index(idx_level_name)\n",
    "#     #-------------------------\n",
    "#     assert(idfr_idx_lvl < df.index.nlevels)\n",
    "#     return (idfr_idx_lvl, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direct_xfmrs_from_df(\n",
    "    df, \n",
    "    outgs_w_single_xfmr=None, \n",
    "    xfmr_equip_typ_nms_of_interest=None, \n",
    "    outg_rec_nb_idfr='index_0', \n",
    "    trsf_pole_nb_idfr='index_1', \n",
    "    dovs_location_id_col=('outg_dummy_lvl_0', 'LOCATION_ID'), \n",
    "    dovs_equip_typ_nm_col=('outg_dummy_lvl_0', 'EQUIP_TYP_NM'), \n",
    "    return_indirect=False\n",
    "):\n",
    "    r\"\"\"\n",
    "    Direct transformers must always have their pole number (trsf_pole_nb) equal to the DOVS location ID (LOCATION_ID).\n",
    "    An additional constrain on the equipment type name (EQUIP_TYP_NM in DOVS) may be enforced by setting the\n",
    "      xfmr_equip_typ_nms_of_interest parameters (e.g., xfmr_equip_typ_nms_of_interest = ['TRANSFORMER, OH', 'TRANSFORMER, UG'])\n",
    "    Transformers from outages affecting only a single transformer may also be considered as direct by setting the\n",
    "      outgs_w_single_xfmr parameter (using, e.g., the get_outgs_w_single_xfmr method).\n",
    "    See LOGIC EXPLANATIONS below\n",
    "      \n",
    "      \n",
    "    outgs_w_single_xfmr: \n",
    "        Should be a list containing outg_rec_nbs.\n",
    "        Can be obtained, e.g., from the get_outgs_w_single_xfmr method.\n",
    "        IMPORTANT: When obtaining outgs_w_single_xfmr, one should use the full dataset, not, e.g. results broken up by confusion \n",
    "                     matrix results (e.g., df_TP).\n",
    "                   The latter will not work because transformers belonging to the same outage may have different\n",
    "                     predictions, meaning one can be tp and another fn.\n",
    "                   Thus, using df_TP would incorrectly see such a situation as an outage with a single transformer!\n",
    "                   SHORT: Even if using this function on the subset df_TP, one should obtain outgs_w_single_xfmr from df.\n",
    "                   \n",
    "    return_indirect:\n",
    "        If true, instead of returning the direct transformers, the indirect will be returned.\n",
    "        Implemented simply using the apply_not parameter in DFSlicer (i.e., use the logic defined below to obtain\n",
    "          slicing_booleans, but slice with ~slicing_booleans instead)\n",
    "  \n",
    "    ----- LOGIC EXPLANATIONS -----\n",
    "    If outgs_w_single_xfmr is None and xfmr_equip_typ_nms_of_interest is None, the logic is as follows:\n",
    "        trsf_pole_nb==LOCATION_ID\n",
    "    -----\n",
    "    If outgs_w_single_xfmr and xfmr_equip_typ_nms_of_interest are supplied, the logic is as follows:\n",
    "        (trsf_pole_nb==LOCATION_ID & EQUIP_TYP_NM in xfmr_equip_typ_nms_of_interest) | OUTG_REC_NB in outgs_w_single_xfmr\n",
    "    -----\n",
    "    If outgs_w_single_xfmr is supplied and xfmr_equip_typ_nms_of_interest is None, the logic is as follows:\n",
    "        trsf_pole_nb==LOCATION_ID | OUTG_REC_NB in outgs_w_single_xfmr\n",
    "    -----\n",
    "    If xfmr_equip_typ_nms_of_interest is supplied and outgs_w_single_xfmr is None, the logic is as follows:\n",
    "        trsf_pole_nb==LOCATION_ID & EQUIP_TYP_NM in xfmr_equip_typ_nms_of_interest\n",
    "    -----\n",
    "    NOTE: If needed, one could easily include a parameter, e.g., join_outgs_w_single_xfmr, to allow one to join\n",
    "            outgs_w_single_xfmr with 'or' (default) or 'and'\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    outg_rec_nb_idfr_loc  = Utilities_df.get_idfr_loc(df, outg_rec_nb_idfr)\n",
    "    trsf_pole_nb_idfr_loc = Utilities_df.get_idfr_loc(df, trsf_pole_nb_idfr)\n",
    "\n",
    "    # If either is found in the index, reset_index will need to be called for DFSlicer to be used\n",
    "    if outg_rec_nb_idfr_loc[1] or trsf_pole_nb_idfr_loc[1]:\n",
    "        if outg_rec_nb_idfr_loc[1]:\n",
    "            df.index = df.index.set_names(Utilities.generate_random_string(), level=outg_rec_nb_idfr_loc[0])\n",
    "            outg_rec_nb_idfr_loc = (df.index.names[outg_rec_nb_idfr_loc[0]], False)\n",
    "        #-----\n",
    "        if trsf_pole_nb_idfr_loc[1]:\n",
    "            df.index = df.index.set_names(Utilities.generate_random_string(), level=trsf_pole_nb_idfr_loc[0])\n",
    "            trsf_pole_nb_idfr_loc = (df.index.names[trsf_pole_nb_idfr_loc[0]], False)\n",
    "        #-----\n",
    "        df = df.reset_index()\n",
    "    #-------------------------\n",
    "    assert(outg_rec_nb_idfr_loc[0] in df.columns)\n",
    "    assert(trsf_pole_nb_idfr_loc[0] in df.columns)\n",
    "    #--------------------------------------------------\n",
    "    slicer = DFSlicer(\n",
    "        single_slicers = [\n",
    "            dict(\n",
    "                column=dovs_location_id_col, \n",
    "                value=df[trsf_pole_nb_idfr_loc[0]], \n",
    "                comparison_operator='=='\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    #-----\n",
    "    if xfmr_equip_typ_nms_of_interest is not None:\n",
    "        slicer.add_single_slicer(\n",
    "            dict(\n",
    "                column=dovs_equip_typ_nm_col, \n",
    "                value=xfmr_equip_typ_nms_of_interest, \n",
    "                comparison_operator='isin'\n",
    "            )\n",
    "        )\n",
    "    #-------------------------\n",
    "    slicers = [slicer]\n",
    "    #-------------------------\n",
    "    if outgs_w_single_xfmr is not None:\n",
    "        # Need a second slicer, since this will be added with | (whereas first two\n",
    "        #   are combined with &)\n",
    "        slicer_2 = DFSlicer(\n",
    "            single_slicers = [\n",
    "                dict(\n",
    "                    column=outg_rec_nb_idfr_loc[0], \n",
    "                    value=outgs_w_single_xfmr, \n",
    "                    comparison_operator='isin'\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        slicers.append(slicer_2)\n",
    "    #--------------------------------------------------\n",
    "    return_df = DFSlicer.combine_slicers_and_perform_slicing(\n",
    "        df=df, \n",
    "        slicers=slicers, \n",
    "        join_slicers='or', \n",
    "        apply_not=return_indirect\n",
    "    )\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direct_xfmrs_in_tp_fn_from_df(\n",
    "    data_df, \n",
    "    y_pred, \n",
    "    y_col=('is_outg', 'is_outg'), \n",
    "    outgs_w_single_xfmr=None, \n",
    "    xfmr_equip_typ_nms_of_interest=None, \n",
    "    outg_rec_nb_idfr='index_0', \n",
    "    trsf_pole_nb_idfr='index_1'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Returns the subset of direct (and indirect) transformers from data_df in the true positive (tp) and \n",
    "      false negative (fn) groups.\n",
    "    Return object is dictionary with keys = TP_dir, TP_indir, FN_dir, FN_indir\n",
    "    NOTE: Can only return results for TP and FN because the idea of a direct transformer only exists\n",
    "            for real outages (no such analog in baseline data)\n",
    "            \n",
    "    Direct transformers must always have their pole number (trsf_pole_nb) equal to the DOVS location ID (LOCATION_ID).\n",
    "    An additional constrain on the equipment type name (EQUIP_TYP_NM in DOVS) may be enforced by setting the\n",
    "      xfmr_equip_typ_nms_of_interest parameters (e.g., xfmr_equip_typ_nms_of_interest = ['TRANSFORMER, OH', 'TRANSFORMER, UG'])\n",
    "    Transformers from outages affecting only a single transformer may also be considered as direct by setting the\n",
    "      outgs_w_single_xfmr parameter (using, e.g., the get_outgs_w_single_xfmr method).\n",
    "    See LOGIC EXPLANATIONS below\n",
    "      \n",
    "      \n",
    "    outgs_w_single_xfmr: \n",
    "        Should be a list containing outg_rec_nbs.\n",
    "        Can be obtained, e.g., from the get_outgs_w_single_xfmr method.\n",
    "        IMPORTANT: When obtaining outgs_w_single_xfmr, one should use the full dataset, not, e.g. results broken up by confusion \n",
    "                     matrix results (e.g., df_TP).\n",
    "                   The latter will not work because transformers belonging to the same outage may have different\n",
    "                     predictions, meaning one can be tp and another fn.\n",
    "                   Thus, using df_TP would incorrectly see such a situation as an outage with a single transformer!\n",
    "                   SHORT: Even if using this function on the subset df_TP, one should obtain outgs_w_single_xfmr from df.\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    data_df_by_cnfsn_res = get_df_subset_by_binary_confusion_matrix_result(\n",
    "        data_df=data_df, \n",
    "        y_pred=y_pred, \n",
    "        y_col=y_col\n",
    "    )\n",
    "    #-----\n",
    "    data_df_tp = data_df_by_cnfsn_res['TP']\n",
    "    data_df_fn = data_df_by_cnfsn_res['FN']\n",
    "    #--------------------------------------------------\n",
    "    data_df_tp = DOVSOutages.append_outg_info_to_df(\n",
    "        df=data_df_tp, \n",
    "        outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "        build_sql_function=DOVSOutages_SQL.build_sql_std_outage, \n",
    "    )\n",
    "    #-----\n",
    "    data_df_fn = DOVSOutages.append_outg_info_to_df(\n",
    "        df=data_df_fn, \n",
    "        outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "        build_sql_function=DOVSOutages_SQL.build_sql_std_outage, \n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    dovs_location_id_col = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "        df=data_df_tp, \n",
    "        col='LOCATION_ID'\n",
    "    )\n",
    "    dovs_equip_typ_nm_col = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "        df=data_df_tp, \n",
    "        col='EQUIP_TYP_NM'\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    if data_df_tp.shape[0]>0:\n",
    "        assert(dovs_location_id_col in data_df_tp.columns and dovs_equip_typ_nm_col in data_df_tp.columns)\n",
    "        #-----\n",
    "        data_df_tp_dir = get_direct_xfmrs_from_df(\n",
    "            df=data_df_tp, \n",
    "            outgs_w_single_xfmr=outgs_w_single_xfmr, \n",
    "            xfmr_equip_typ_nms_of_interest=xfmr_equip_typ_nms_of_interest, \n",
    "            outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "            trsf_pole_nb_idfr=trsf_pole_nb_idfr, \n",
    "            dovs_location_id_col=dovs_location_id_col, \n",
    "            dovs_equip_typ_nm_col=dovs_equip_typ_nm_col, \n",
    "            return_indirect=False\n",
    "        )\n",
    "        #-----\n",
    "        data_df_tp_indir = get_direct_xfmrs_from_df(\n",
    "            df=data_df_tp, \n",
    "            outgs_w_single_xfmr=outgs_w_single_xfmr, \n",
    "            xfmr_equip_typ_nms_of_interest=xfmr_equip_typ_nms_of_interest, \n",
    "            outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "            trsf_pole_nb_idfr=trsf_pole_nb_idfr, \n",
    "            dovs_location_id_col=dovs_location_id_col, \n",
    "            dovs_equip_typ_nm_col=dovs_equip_typ_nm_col, \n",
    "            return_indirect=True\n",
    "        )\n",
    "    else:\n",
    "        data_df_tp_dir   = pd.DataFrame()\n",
    "        data_df_tp_indir = pd.DataFrame()\n",
    "    #-----\n",
    "    if data_df_fn.shape[0]>0:\n",
    "        assert(dovs_location_id_col in data_df_fn.columns and dovs_equip_typ_nm_col in data_df_fn.columns)\n",
    "        #-----\n",
    "        data_df_fn_dir = get_direct_xfmrs_from_df(\n",
    "            df=data_df_fn, \n",
    "            outgs_w_single_xfmr=outgs_w_single_xfmr, \n",
    "            xfmr_equip_typ_nms_of_interest=xfmr_equip_typ_nms_of_interest, \n",
    "            outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "            trsf_pole_nb_idfr=trsf_pole_nb_idfr, \n",
    "            dovs_location_id_col=dovs_location_id_col, \n",
    "            dovs_equip_typ_nm_col=dovs_equip_typ_nm_col, \n",
    "            return_indirect=False\n",
    "        )\n",
    "        #-----\n",
    "        data_df_fn_indir = get_direct_xfmrs_from_df(\n",
    "            df=data_df_fn, \n",
    "            outgs_w_single_xfmr=outgs_w_single_xfmr, \n",
    "            xfmr_equip_typ_nms_of_interest=xfmr_equip_typ_nms_of_interest, \n",
    "            outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "            trsf_pole_nb_idfr=trsf_pole_nb_idfr, \n",
    "            dovs_location_id_col=dovs_location_id_col, \n",
    "            dovs_equip_typ_nm_col=dovs_equip_typ_nm_col, \n",
    "            return_indirect=True\n",
    "        )\n",
    "    else:\n",
    "        data_df_fn_dir   = pd.DataFrame()\n",
    "        data_df_fn_indir = pd.DataFrame()\n",
    "    #--------------------------------------------------\n",
    "    return_dict = dict(\n",
    "        TP_dir   = data_df_tp_dir, \n",
    "        TP_indir = data_df_tp_indir, \n",
    "        FN_dir   = data_df_fn_dir, \n",
    "        FN_indir = data_df_fn_indir, \n",
    "    )\n",
    "    return return_dict\n",
    "\n",
    "\n",
    "def get_n_direct_xfmrs_in_tp_fn(\n",
    "    data_df, \n",
    "    y_pred, \n",
    "    y_col=('is_outg', 'is_outg'), \n",
    "    outgs_w_single_xfmr=None, \n",
    "    xfmr_equip_typ_nms_of_interest=None, \n",
    "    outg_rec_nb_idfr='index_0', \n",
    "    trsf_pole_nb_idfr='index_1'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Returns the number of direct (and indirect) transformers in the true positive (tp) and \n",
    "      false negative (fn) groups.\n",
    "    Return object is dictionary with keys = TP_dir, TP_indir, FN_dir, FN_indir\n",
    "    NOTE: Can only return results for TP and FN because the idea of a direct transformer only exists\n",
    "            for real outages (no such analog in baseline data)\n",
    "            \n",
    "    Direct transformers must always have their pole number (trsf_pole_nb) equal to the DOVS location ID (LOCATION_ID).\n",
    "    An additional constrain on the equipment type name (EQUIP_TYP_NM in DOVS) may be enforced by setting the\n",
    "      xfmr_equip_typ_nms_of_interest parameters (e.g., xfmr_equip_typ_nms_of_interest = ['TRANSFORMER, OH', 'TRANSFORMER, UG'])\n",
    "    Transformers from outages affecting only a single transformer may also be considered as direct by setting the\n",
    "      outgs_w_single_xfmr parameter (using, e.g., the get_outgs_w_single_xfmr method).\n",
    "    See LOGIC EXPLANATIONS below\n",
    "      \n",
    "      \n",
    "    outgs_w_single_xfmr: \n",
    "        Should be a list containing outg_rec_nbs.\n",
    "        Can be obtained, e.g., from the get_outgs_w_single_xfmr method.\n",
    "        IMPORTANT: When obtaining outgs_w_single_xfmr, one should use the full dataset, not, e.g. results broken up by confusion \n",
    "                     matrix results (e.g., df_TP).\n",
    "                   The latter will not work because transformers belonging to the same outage may have different\n",
    "                     predictions, meaning one can be tp and another fn.\n",
    "                   Thus, using df_TP would incorrectly see such a situation as an outage with a single transformer!\n",
    "                   SHORT: Even if using this function on the subset df_TP, one should obtain outgs_w_single_xfmr from df.\n",
    "    \"\"\"\n",
    "    #--------------------------------------------------\n",
    "    dfs_dict = get_direct_xfmrs_in_tp_fn_from_df(\n",
    "        data_df=data_df, \n",
    "        y_pred=y_pred, \n",
    "        y_col=y_col, \n",
    "        outgs_w_single_xfmr=outgs_w_single_xfmr, \n",
    "        xfmr_equip_typ_nms_of_interest=xfmr_equip_typ_nms_of_interest, \n",
    "        outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "        trsf_pole_nb_idfr=trsf_pole_nb_idfr\n",
    "    )\n",
    "    #--------------------------------------------------\n",
    "    return_dict = dict(\n",
    "        TP_dir   = dfs_dict['TP_dir'].shape[0], \n",
    "        TP_indir = dfs_dict['TP_indir'].shape[0], \n",
    "        FN_dir   = dfs_dict['FN_dir'].shape[0], \n",
    "        FN_indir = dfs_dict['FN_indir'].shape[0], \n",
    "    )\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0efcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95854d5c",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Time info related methods\n",
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d587e420-0650-4b82-8c41-8c1a40f4854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_baseline_time_infos_df(\n",
    "    bsln_time_infos_df\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    time_infos_df_bsln = bsln_time_infos_df.copy()\n",
    "    time_infos_df_bsln = time_infos_df_bsln.reset_index()\n",
    "    #-----\n",
    "    necessary_cols = ['no_outg_rec_nb', 'trsf_pole_nb', 't_min', 't_max']\n",
    "    assert(len(set(necessary_cols).difference(set(time_infos_df_bsln.columns)))==0)\n",
    "    #-----\n",
    "    time_infos_df_bsln = time_infos_df_bsln.set_index(['no_outg_rec_nb', 'trsf_pole_nb'])\n",
    "    time_infos_df_bsln = time_infos_df_bsln[['t_min', 't_max']]\n",
    "    #-----\n",
    "    time_infos_df_bsln.index.names = ['outg_rec_nb', 'trsf_pole_nb']\n",
    "    #-------------------------\n",
    "    return time_infos_df_bsln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6cda1-e9db-44ae-a028-52815d11b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_time_infos_df(\n",
    "    ede_data_dirs_bsln,\n",
    "    standardize=True, \n",
    "    save_path=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    bsln_time_infos_dfs = []\n",
    "    #-----\n",
    "    for ede_dir_i in ede_data_dirs_bsln:\n",
    "        time_infos_df_i = MECPOAn.get_bsln_time_interval_infos_df_for_data_in_dir(\n",
    "            data_dir=ede_dir_i, \n",
    "            make_addtnl_groupby_idx=True, \n",
    "            include_summary_paths=False\n",
    "        )\n",
    "        bsln_time_infos_dfs.append(time_infos_df_i)\n",
    "    #-----\n",
    "    bsln_time_infos_df = pd.concat(bsln_time_infos_dfs)\n",
    "    #-------------------------\n",
    "    # Index names should be 'trsf_pole_nb', 'no_outg_rec_nb', and possibly 'is_first_after_outg' \n",
    "    #   (typically in that order, but not necessarily)\n",
    "    assert(len(set(bsln_time_infos_df.index.names).difference(set(['trsf_pole_nb', 'no_outg_rec_nb', 'is_first_after_outg'])))==0)\n",
    "    # Typically, want index as ['no_outg_rec_nb', 'trsf_pole_nb', 'is_first_after_outg']\n",
    "    assert(bsln_time_infos_df.index.nlevels==2 or bsln_time_infos_df.index.nlevels==3)\n",
    "    return_index_levels = ['no_outg_rec_nb', 'trsf_pole_nb']\n",
    "    if bsln_time_infos_df.index.nlevels==3:\n",
    "        return_index_levels.append('is_first_after_outg')\n",
    "    assert(len(set(bsln_time_infos_df.index.names).symmetric_difference(set(return_index_levels)))==0)\n",
    "    if bsln_time_infos_df.index.names!=return_index_levels:\n",
    "        bsln_time_infos_df = bsln_time_infos_df.reset_index().set_index(return_index_levels)    \n",
    "    #-------------------------\n",
    "    if standardize:\n",
    "        bsln_time_infos_df = standardize_baseline_time_infos_df(\n",
    "            bsln_time_infos_df = bsln_time_infos_df\n",
    "        )\n",
    "    #-------------------------\n",
    "    if save_path:\n",
    "        bsln_time_infos_df.to_pickle(save_path)\n",
    "    #-------------------------\n",
    "    return bsln_time_infos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f85aac-145f-483b-a2ec-94507b7cc12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f77360-5416-4124-980e-a1a162b37136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95587357-2607-48bc-89bf-eb51192bef5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc2939-fb89-41d2-866f-a7503f506b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526f563-fc1b-4438-b198-f3a35fbd8afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badd8632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE build_baseline_time_infos_df instead (should be exactly the same)\n",
    "def build_no_outg_time_infos_df(\n",
    "    ede_data_dirs_no_outg,\n",
    "    save_path=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    no_outg_time_infos_dfs = []\n",
    "    #-----\n",
    "    for ede_dir_i in ede_data_dirs_no_outg:\n",
    "        time_infos_df_i = MECPOAn.get_bsln_time_interval_infos_df_for_data_in_dir(\n",
    "            data_dir=ede_dir_i, \n",
    "            make_addtnl_groupby_idx=True, \n",
    "            include_summary_paths=False\n",
    "        )\n",
    "        no_outg_time_infos_dfs.append(time_infos_df_i)\n",
    "    #-----\n",
    "    no_outg_time_infos_df = pd.concat(no_outg_time_infos_dfs)\n",
    "    #-------------------------\n",
    "    # Index names should be 'trsf_pole_nb', 'no_outg_rec_nb', and possibly 'is_first_after_outg' \n",
    "    #   (typically in that order, but not necessarily)\n",
    "    assert(len(set(no_outg_time_infos_df.index.names).difference(set(['trsf_pole_nb', 'no_outg_rec_nb', 'is_first_after_outg'])))==0)\n",
    "    # Typically, want index as ['no_outg_rec_nb', 'trsf_pole_nb', 'is_first_after_outg']\n",
    "    assert(no_outg_time_infos_df.index.nlevels==2 or no_outg_time_infos_df.index.nlevels==3)\n",
    "    return_index_levels = ['no_outg_rec_nb', 'trsf_pole_nb']\n",
    "    if no_outg_time_infos_df.index.nlevels==3:\n",
    "        return_index_levels.append('is_first_after_outg')\n",
    "    assert(len(set(no_outg_time_infos_df.index.names).symmetric_difference(set(return_index_levels)))==0)\n",
    "    if no_outg_time_infos_df.index.names!=return_index_levels:\n",
    "        no_outg_time_infos_df = no_outg_time_infos_df.reset_index().set_index(return_index_levels)    \n",
    "    #-------------------------\n",
    "    if save_path:\n",
    "        no_outg_time_infos_df.to_pickle(save_path)\n",
    "    #-------------------------\n",
    "    return no_outg_time_infos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2004bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO MECPOAn\n",
    "def build_outg_time_infos_df(\n",
    "    rcpx_df, \n",
    "    outg_rec_nb_idfr=('index', 'outg_rec_nb'), \n",
    "    dummy_col_levels_prefix='dummy_lvl_',     \n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    tmp_og_cols = rcpx_df.columns.tolist()\n",
    "    #-------------------------\n",
    "    time_infos_df_outg = DOVSOutages.append_outg_dt_off_ts_full_to_df(\n",
    "        df=rcpx_df.copy(), \n",
    "        outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "        dummy_col_levels_prefix=dummy_col_levels_prefix, \n",
    "        include_dt_on_ts=True\n",
    "    )\n",
    "    #-------------------------\n",
    "    time_info_cols = list(set(time_infos_df_outg.columns.tolist()).difference(set(tmp_og_cols)))\n",
    "    time_infos_df_outg = time_infos_df_outg[time_info_cols]\n",
    "    if time_infos_df_outg.columns.nlevels>1:\n",
    "        assert(time_infos_df_outg.columns.nlevels==2)\n",
    "        assert(time_infos_df_outg.columns.get_level_values(0).nunique()==1)\n",
    "        time_infos_df_outg.columns = time_infos_df_outg.columns.droplevel(0)\n",
    "    #-------------------------\n",
    "    assert(len(set(['DT_OFF_TS_FULL', 'DT_ON_TS']).difference(set(time_infos_df_outg.columns)))==0)\n",
    "    time_infos_df_outg = time_infos_df_outg.rename(columns={\n",
    "        'DT_OFF_TS_FULL':'t_min', \n",
    "        'DT_ON_TS':'t_max'\n",
    "    })\n",
    "    #-------------------------\n",
    "    return time_infos_df_outg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe724a8-32f4-4d7d-a060-53538fd4a668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c85139-8a21-4969-aa69-6481643dd7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac3287-9ab8-4e4b-b55c-45bb5c3c8f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c216fb7-877a-40ed-bb5e-f04e999f3d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f0523-5d0f-480e-98fc-1f8a6c25424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_time_infos_df_for_eemsp(\n",
    "    bsln_time_infos_df\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    time_infos_df_bsln = bsln_time_infos_df.copy()\n",
    "    time_infos_df_bsln = time_infos_df_bsln.reset_index()\n",
    "    #-----\n",
    "    necessary_cols = ['no_outg_rec_nb', 'trsf_pole_nb', 't_min', 't_max']\n",
    "    assert(len(set(necessary_cols).difference(set(time_infos_df_bsln.columns)))==0)\n",
    "    #-----\n",
    "    time_infos_df_bsln = time_infos_df_bsln.set_index(['no_outg_rec_nb', 'trsf_pole_nb'])\n",
    "    time_infos_df_bsln = time_infos_df_bsln[['t_min', 't_max']]\n",
    "    #-----\n",
    "    time_infos_df_bsln.index.names = ['outg_rec_nb', 'trsf_pole_nb']\n",
    "    #-------------------------\n",
    "    return time_infos_df_bsln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE build_baseline_time_infos_df_for_eemsp instead (should be exactly the same)\n",
    "def build_no_outg_time_infos_df_for_eemsp(\n",
    "    no_outg_time_infos_df\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    time_infos_df_no_outg = no_outg_time_infos_df.copy()\n",
    "    time_infos_df_no_outg = time_infos_df_no_outg.reset_index()\n",
    "    #-----\n",
    "    necessary_cols = ['no_outg_rec_nb', 'trsf_pole_nb', 't_min', 't_max']\n",
    "    assert(len(set(necessary_cols).difference(set(time_infos_df_no_outg.columns)))==0)\n",
    "    #-----\n",
    "    time_infos_df_no_outg = time_infos_df_no_outg.set_index(['no_outg_rec_nb', 'trsf_pole_nb'])\n",
    "    time_infos_df_no_outg = time_infos_df_no_outg[['t_min', 't_max']]\n",
    "    #-----\n",
    "    time_infos_df_no_outg.index.names = ['outg_rec_nb', 'trsf_pole_nb']\n",
    "    #-------------------------\n",
    "    return time_infos_df_no_outg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_time_infos_df_for_eemsp(\n",
    "    time_infos_df_outg, \n",
    "    time_infos_df_otbl, \n",
    "    time_infos_df_prbl\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(\n",
    "        time_infos_df_outg.index.names == \n",
    "        time_infos_df_otbl.index.names == \n",
    "        ['outg_rec_nb', 'trsf_pole_nb']\n",
    "    )\n",
    "    #-----\n",
    "    assert(len(set(['t_min', 't_max']).difference(set(time_infos_df_outg.columns)))==0)\n",
    "    assert(len(set(['t_min', 't_max']).difference(set(time_infos_df_otbl.columns)))==0)\n",
    "    #-------------------------\n",
    "    time_infos_df_outg = time_infos_df_outg[['t_min', 't_max']]\n",
    "    time_infos_df_otbl = time_infos_df_otbl[['t_min', 't_max']]\n",
    "    #-------------------------\n",
    "    time_infos_df = pd.concat([time_infos_df_outg, time_infos_df_otbl])\n",
    "    #-------------------------\n",
    "    if time_infos_df_prbl is not None:\n",
    "        assert(time_infos_df_prbl.index.names == time_infos_df_outg.index.names)\n",
    "        assert(len(set(['t_min', 't_max']).difference(set(time_infos_df_prbl.columns)))==0)\n",
    "        time_infos_df_prbl = time_infos_df_prbl[['t_min', 't_max']]\n",
    "        #-----\n",
    "        time_infos_df = pd.concat([time_infos_df, time_infos_df_prbl])\n",
    "    #-------------------------\n",
    "    return time_infos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31369dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebfa98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6d722c3",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------\n",
    "# EEMSP\n",
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ec7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BOTH FUNCTIONS MOVED TO EEMSP.py!!!!!\n",
    "# def reduce1_eemsp_for_outg_trsf_i(\n",
    "#     time_infos_df_i, \n",
    "#     df_eemsp, \n",
    "#     outg_rec_nb_idfr, \n",
    "#     trsf_pole_nb_idfr, \n",
    "#     dt_min_col, \n",
    "#     dt_max_col, \n",
    "#     eemsp_location_nb_col = 'LOCATION_NB', \n",
    "#     eemsp_install_dt_col  = 'INSTALL_DT', \n",
    "#     eemsp_removal_dt_col  = 'REMOVAL_DT', \n",
    "#     return_eemsp_outg_rec_nb_col = 'OUTG_REC_NB_TO_MERGE'\n",
    "# ):\n",
    "#     r\"\"\"\n",
    "#     For a particular (outg_rec_nb, trsf_pole_nb) group, find the corresponding EEMSP entries.\n",
    "#     Typically, EEMSP will have multiple entries for each transformer, so the point of this function\n",
    "#       is to find the correct entries at the time of the outage.\n",
    "#     After this procedure, all entries will be time-appropriate, but there may still be multiple entries\n",
    "#       for a particular (outg_rec_nb, trsf_pole_nb) group.\n",
    "#     Multiple entries will occur, e.g., if there are multiple transformers on the particular trsf_pole_nb\n",
    "#       (at this point, I do not know how to determine to which transformer a meter is connected, only the\n",
    "#       trsf_pole_nb).\n",
    "#     For reductions down to one entry, see reduce2_eemsp_for_outg_trsf_i\n",
    "      \n",
    "#     NOTE: For this case, the outg_rec_nb_idfr/trsf_pole_nb_idfr should be equal to a column.\n",
    "#           One cannot use, e.g., 'index_0'\n",
    "#           This function is not really meant to be used on its own, and the correct formatting of time_infos_df_i\n",
    "#             is taken care of by the calling function\n",
    "#     \"\"\"\n",
    "#     #-------------------------\n",
    "#     assert(isinstance(time_infos_df_i, pd.Series))\n",
    "#     #-------------------------\n",
    "#     outg_rec_nb  = time_infos_df_i[outg_rec_nb_idfr]\n",
    "#     trsf_pole_nb = time_infos_df_i[trsf_pole_nb_idfr]\n",
    "#     dt_min       = time_infos_df_i[dt_min_col]\n",
    "#     if dt_max_col is not None:\n",
    "#         dt_max = time_infos_df_i[dt_max_col]\n",
    "#     else:\n",
    "#         dt_max = dt_min\n",
    "#     #-------------------------\n",
    "#     df_eemsp_i = df_eemsp[df_eemsp[eemsp_location_nb_col]==trsf_pole_nb]\n",
    "#     #-----\n",
    "#     df_eemsp_i = df_eemsp_i[\n",
    "#         (df_eemsp_i[eemsp_install_dt_col] <= dt_min) & \n",
    "#         (df_eemsp_i[eemsp_removal_dt_col].fillna(pd.Timestamp.max) > dt_max)\n",
    "#     ]\n",
    "#     df_eemsp_i = df_eemsp_i.drop_duplicates()\n",
    "#     #-------------------------    \n",
    "#     df_eemsp_i[return_eemsp_outg_rec_nb_col] = outg_rec_nb\n",
    "#     #-------------------------\n",
    "#     return df_eemsp_i\n",
    "\n",
    "\n",
    "# def reduce1_eemsp_for_outg_trsf(\n",
    "#     time_infos_df, \n",
    "#     df_eemsp, \n",
    "#     outg_rec_nb_idfr  = 'index_0', \n",
    "#     trsf_pole_nb_idfr = 'index_1', \n",
    "\n",
    "#     dt_min_col = ('dummy_lvl_0', 'DT_OFF_TS_FULL'), \n",
    "#     dt_max_col = None, \n",
    "    \n",
    "#     eemsp_location_nb_col = 'LOCATION_NB', \n",
    "#     eemsp_install_dt_col  = 'INSTALL_DT', \n",
    "#     eemsp_removal_dt_col  = 'REMOVAL_DT', \n",
    "#     return_eemsp_outg_rec_nb_col = 'OUTG_REC_NB_TO_MERGE', \n",
    "#     verbose=True,\n",
    "#     n_update=1000\n",
    "# ):\n",
    "#     r\"\"\"\n",
    "#     For each (outg_rec_nb, trsf_pole_nb) group, find the corresponding EEMSP entries.\n",
    "#     Typically, EEMSP will have multiple entries for each transformer/pole, so the point of this function\n",
    "#       is to find the correct entries at the time of the outage.\n",
    "#     After this procedure, all entries will be time-appropriate, but there may still be multiple entries\n",
    "#       for a particular (outg_rec_nb, trsf_pole_nb) group.\n",
    "#     Multiple entries will occur, e.g., if there are multiple transformers on the particular trsf_pole_nb\n",
    "#       (at this point, I do not know how to determine to which transformer a meter is connected, only the\n",
    "#       trsf_pole_nb).\n",
    "#     For reductions down to one entry, see reduce2_eemsp_for_outg_trsf_i\n",
    "    \n",
    "#     If the outg_rec_nbs/trsf_pole_nbs are stored in the indices, and the indices are named,\n",
    "#       one can simply supply the corresponding names.\n",
    "#     Otherwise, one can always supply, e.g., index_0 and index_1\n",
    "\n",
    "#     If only a single time is to be used (e.g., the outage starting time), set only dt_min_col\n",
    "#     If two times (e.g., outage starting and stopping) set both dt_min_col and dt_max_col\n",
    "#     \"\"\"\n",
    "#     #----------------------------------------------------------------------------------------------------\n",
    "#     #----------------------------------------------------------------------------------------------------\n",
    "#     # 1. First, determine where exactly outg_rec_nb and trsf_pole_nb are located in time_infos_df.\n",
    "#     # 2. If in indices, call reset_index()\n",
    "#     # 3. Set grp_by_cols, and make sure each combination of outg_rec_nb, trsf_pole_nb \n",
    "#     #    has a single datetime entry.\n",
    "#     #-------------------------\n",
    "#     outg_rec_nb_idfr_loc  = Utilities_df.get_idfr_loc(time_infos_df, outg_rec_nb_idfr)\n",
    "#     trsf_pole_nb_idfr_loc = Utilities_df.get_idfr_loc(time_infos_df, trsf_pole_nb_idfr)\n",
    "#     #--------------------------------------------------\n",
    "#     # If outg_rec_nbs in index (i.e., outg_rec_nb_idfr_loc[1]==True), grab the index name and set \n",
    "#     #   outg_rec_nb_idfr equal to it.\n",
    "#     #   If no index name, give it a random name\n",
    "#     # If outg_rec_nbs in a column (i.e., outg_rec_nb_idfr_loc[1]==True), simply set outg_rec_nb_idfr\n",
    "#     #   equal to it (i.e., set equal to outg_rec_nb_idfr_loc[0])\n",
    "#     if outg_rec_nb_idfr_loc[1]:\n",
    "#         if time_infos_df.index.names[outg_rec_nb_idfr_loc[0]]:\n",
    "#             outg_rec_nb_idfr = time_infos_df.index.names[outg_rec_nb_idfr_loc[0]]\n",
    "#         else:\n",
    "#             tmp_outg_rec_nb_name = 'outg_rec_nb_'+Utilities.generate_random_string(str_len=4)\n",
    "#             time_infos_df.index = time_infos_df.index.set_names(tmp_outg_rec_nb_name, level=outg_rec_nb_idfr_loc[0])\n",
    "#             outg_rec_nb_idfr = tmp_outg_rec_nb_name\n",
    "#     else:\n",
    "#         outg_rec_nb_idfr = outg_rec_nb_idfr_loc[0]\n",
    "#     #-------------------------\n",
    "#     # Do the same thing for trsf_pole_nbs\n",
    "#     if trsf_pole_nb_idfr_loc[1]:\n",
    "#         if time_infos_df.index.names[trsf_pole_nb_idfr_loc[0]]:\n",
    "#             trsf_pole_nb_idfr = time_infos_df.index.names[trsf_pole_nb_idfr_loc[0]]\n",
    "#         else:\n",
    "#             tmp_trsf_pole_nb_name = 'trsf_pole_nb_'+Utilities.generate_random_string(str_len=4)\n",
    "#             time_infos_df.index = time_infos_df.index.set_names(tmp_trsf_pole_nb_name, level=trsf_pole_nb_idfr_loc[0])\n",
    "#             trsf_pole_nb_idfr = tmp_trsf_pole_nb_name\n",
    "#     else:\n",
    "#         trsf_pole_nb_idfr = trsf_pole_nb_idfr_loc[0]\n",
    "#     #--------------------------------------------------\n",
    "#     grp_by_cols = [outg_rec_nb_idfr, trsf_pole_nb_idfr, dt_min_col]\n",
    "#     if dt_max_col is None:\n",
    "#         dt_max_col = dt_min_col\n",
    "#     else:\n",
    "#         grp_by_cols.append(dt_max_col)\n",
    "#     #-------------------------\n",
    "#     # Even if outg_rec_nbs and/or trsf_pole_nbs are in the index, the above methods ensure the indices will be \n",
    "#     #   named in such a case.\n",
    "#     # Therefore, one can use .groupby with the index names or columns\n",
    "#     # NOTE: Each combination of outg_rec_nb, trsf_pole_nb should only have a single datetime entry!\n",
    "#     #       The assertion below enforces that\n",
    "#     n_groups = time_infos_df.groupby(grp_by_cols).ngroups\n",
    "#     assert(\n",
    "#         n_groups == time_infos_df.groupby([outg_rec_nb_idfr, trsf_pole_nb_idfr]).ngroups\n",
    "#     )    \n",
    "    \n",
    "#     #----------------------------------------------------------------------------------------------------\n",
    "#     #----------------------------------------------------------------------------------------------------\n",
    "#     # Form groups_df, which will have columns housing the outg_rec_nb, trsf_pole_nb, and the time info (one\n",
    "#     #   additional column if dt_max_col is None, two additional columns if not None)\n",
    "#     #-------------------------\n",
    "#     #--------------------------------------------------\n",
    "#     # Using the groupby method is much easier, but also much slower!\n",
    "#     #   group by method: groups = list(time_infos_df.groupby(grp_by_cols).groups.keys())\n",
    "#     # So, instead, I will use .value_counts method after calling reset_index()\n",
    "#     # This is annoying because if outg_rec_nb or trsf_pole_nb in index, I must call reset_index\n",
    "#     # Furthermore, if time_infos_df has MultiIndex columns, the identifiers will change from strings to tuples\n",
    "#     # In such a case, pandas still is able to grab time_infos_df[[outg_rec_nb_idfr, trsf_pole_nb_idfr]]\n",
    "#     #   i.e., is smart enough to find, e.g., ['outg_rec_nb', 'trsf_pole_nb'], even though the columns\n",
    "#     #   are technically [('outg_rec_nb', ''), ('trsf_pole_nb', '')]\n",
    "#     # But, pandas is NOT smart enough to grab time_infos_df[grp_by_cols] \n",
    "#     #   i.e., too dumb to find, e.g., ['outg_rec_nb', 'trsf_pole_nb', ('dummy_lvl_0', 'DT_OFF_TS_FULL')] when\n",
    "#     #   the columns are technically  [('outg_rec_nb', ''), ('trsf_pole_nb', ''), ('dummy_lvl_0', 'DT_OFF_TS_FULL')]\n",
    "#     #-------------------------\n",
    "#     if outg_rec_nb_idfr_loc[1] and time_infos_df.columns.nlevels>1:\n",
    "#         outg_rec_nb_idfr = tuple([outg_rec_nb_idfr] + ['' for _ in range(time_infos_df.columns.nlevels-1)])\n",
    "#         grp_by_cols[0] = outg_rec_nb_idfr\n",
    "#     #-----\n",
    "#     if trsf_pole_nb_idfr_loc[1] and time_infos_df.columns.nlevels>1:\n",
    "#         trsf_pole_nb_idfr = tuple([trsf_pole_nb_idfr] + ['' for _ in range(time_infos_df.columns.nlevels-1)])\n",
    "#         grp_by_cols[1] = trsf_pole_nb_idfr\n",
    "#     #-------------------------\n",
    "#     groups_df = time_infos_df.reset_index()[grp_by_cols].value_counts()\n",
    "#     groups_df=groups_df.reset_index().drop(columns='count')\n",
    "#     assert(groups_df.shape[0]==n_groups)\n",
    "    \n",
    "#     #----------------------------------------------------------------------------------------------------\n",
    "#     #----------------------------------------------------------------------------------------------------\n",
    "#     # Chop down size of df_eemsp first\n",
    "#     # Get rid of any with install date after last time or removal date before first time\n",
    "#     df_eemsp=df_eemsp[~(\n",
    "#         (df_eemsp[eemsp_install_dt_col]>time_infos_df[dt_max_col].max()) |\n",
    "#         (df_eemsp[eemsp_removal_dt_col]<time_infos_df[dt_min_col].min())\n",
    "#     )]\n",
    "#     df_eemsp = df_eemsp.drop_duplicates()\n",
    "#     #----------------------------------------------------------------------------------------------------\n",
    "#     #----------------------------------------------------------------------------------------------------\n",
    "#     # Iterate through groups_df, and grab the appropriate entries from df_eemsp\n",
    "#     eemsp_dfs = []\n",
    "#     for i,(idx_i, df_i) in enumerate(groups_df.iterrows()):\n",
    "#         if verbose and i%n_update==0:\n",
    "#             print(f\"{i} of {groups_df.shape[0]}\")\n",
    "#         eemsp_df_i = reduce1_eemsp_for_outg_trsf_i(\n",
    "#             time_infos_df_i=df_i, \n",
    "#             df_eemsp=df_eemsp, \n",
    "#             outg_rec_nb_idfr=outg_rec_nb_idfr, \n",
    "#             trsf_pole_nb_idfr=trsf_pole_nb_idfr, \n",
    "#             dt_min_col=dt_min_col, \n",
    "#             dt_max_col=dt_max_col, \n",
    "#             eemsp_location_nb_col=eemsp_location_nb_col, \n",
    "#             eemsp_install_dt_col= eemsp_install_dt_col, \n",
    "#             eemsp_removal_dt_col= eemsp_removal_dt_col, \n",
    "#             return_eemsp_outg_rec_nb_col=return_eemsp_outg_rec_nb_col\n",
    "#         )\n",
    "#         eemsp_dfs.append(eemsp_df_i)\n",
    "#     return_df_eemsp = pd.concat(eemsp_dfs)\n",
    "#     return return_df_eemsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e45b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7947a52",
   "metadata": {},
   "source": [
    "# BEG OLD\n",
    "Still need to implement new versions in other notebooks, so old are temporarilly kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ambiguous_from_df_eemsp_OLD(\n",
    "    df_eemsp, \n",
    "    outg_rec_nb_col='OUTG_REC_NB_TO_MERGE', \n",
    "    location_nb_col='LOCATION_NB'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Any (outg_rec_nb, location_nb) group with multiple entries will be removed\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure outg_rec_nb_col, location_nb_col, and columns in numeric_cols/dt_cols are present in df_eemsp\n",
    "    # NOTE: If simple string given for columns whereas df_eemsp has MultiIndex columns, the\n",
    "    #       following should remdy such a situation by changing string to appropriate tuple\n",
    "    outg_rec_nb_col = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "        df=df_eemsp, \n",
    "        col=outg_rec_nb_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    location_nb_col = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "        df=df_eemsp, \n",
    "        col=location_nb_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    no_ambiguity = (df_eemsp[[outg_rec_nb_col, location_nb_col]].value_counts()==1)\n",
    "    no_ambiguity = no_ambiguity[no_ambiguity==True].index\n",
    "    #-------------------------\n",
    "    df_eemsp_singles = df_eemsp.set_index(\n",
    "        [outg_rec_nb_col, location_nb_col]\n",
    "    ).loc[no_ambiguity].reset_index()\n",
    "    #-------------------------\n",
    "    return df_eemsp_singles\n",
    "\n",
    "\n",
    "def reduce2_eemsp_for_outg_trsf_i_OLD(\n",
    "    df_eemsp_i, \n",
    "    agg_dict, \n",
    "    return_idx_order, \n",
    "    mult_strategy='agg', \n",
    "    include_n_eemsp=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Helper function fo reduce2_eemsp_for_outg_trsf\n",
    "    In reality, only mult_strategy used here should be 'agg' ('first' is handled in \n",
    "      reduce2_eemsp_for_outg_trsf with the nth() function)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # NOTE: If mult_strategy=='exclude' in reduce2_eemsp_for_outg_trsf, the workflow\n",
    "    #         should not utilize this function!\n",
    "    assert(mult_strategy in ['agg', 'first'])\n",
    "    #-------------------------\n",
    "    if df_eemsp_i.shape[0]==1:\n",
    "        srs_i = df_eemsp_i.iloc[0]\n",
    "    else:\n",
    "        if mult_strategy=='agg':\n",
    "            srs_i = df_eemsp_i.agg(agg_dict)\n",
    "        else:\n",
    "            assert(mult_strategy=='first')\n",
    "            srs_i = df_eemsp_i.iloc[0]\n",
    "    #-------------------------\n",
    "    srs_i=srs_i.reindex(index=return_idx_order)\n",
    "    #-------------------------\n",
    "    if include_n_eemsp:\n",
    "        srs_i['n_eemsp'] = df_eemsp_i.shape[0]\n",
    "    #-------------------------\n",
    "    return srs_i\n",
    "\n",
    "\n",
    "def reduce2_eemsp_for_outg_trsf_OLD(\n",
    "    df_eemsp, \n",
    "    mult_strategy='agg', \n",
    "    include_n_eemsp=True, \n",
    "    outg_rec_nb_col='OUTG_REC_NB_TO_MERGE', \n",
    "    location_nb_col='LOCATION_NB', \n",
    "    numeric_cols = ['KVA_SIZE'], \n",
    "    dt_cols = ['INSTALL_DT', 'REMOVAL_DT'], \n",
    "    ignore_cols = ['SERIAL_NB'], \n",
    "    cat_cols_as_strings=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    To be run after reduce1_eemsp_for_outg_trsf_i!!!!!\n",
    "    The intent of this function is to reduce df_eemsp down to one row per outg_rec_nb_col/location_nb_col group\n",
    "    \n",
    "    mult_strategy:\n",
    "        Dictates how (outg_rec_nb, trsf_pole_nb) groups with multiple EEMSP entries will be handled.\n",
    "        Can be 'agg', 'first', or 'exclude'\n",
    "        'agg':\n",
    "            For (outg_rec_nb, trsf_pole_nb) groups with multiple entries, aggregate\n",
    "        'first':\n",
    "            For (outg_rec_nb, trsf_pole_nb) groups with multiple entries, take the first\n",
    "        'exclude':\n",
    "            Exclude (outg_rec_nb, trsf_pole_nb) groups with multiple entries\n",
    "            \n",
    "    cat_cols_as_strings:\n",
    "        Categorical columns can either be aggregated as (sorted) lists of unique values (cat_cols_as_strings==False),\n",
    "          or as strings (cat_cols_as_strings==True)\n",
    "        If cat_cols_as_strings==True, a given string is simply the (sorted) lists of unique values joined by commas\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(mult_strategy in ['agg', 'first', 'exclude'])\n",
    "    #-------------------------\n",
    "    # Copy below probably not necessary\n",
    "    df_eemsp = df_eemsp.copy()\n",
    "    #-------------------------\n",
    "    # Make sure outg_rec_nb_col, location_nb_col, and columns in numeric_cols/dt_cols are present in df_eemsp\n",
    "    # NOTE: If simple string given for columns whereas df_eemsp has MultiIndex columns, the\n",
    "    #       following should remdy such a situation by changing string to appropriate tuple\n",
    "    outg_rec_nb_col = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "        df=df_eemsp, \n",
    "        col=outg_rec_nb_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    location_nb_col = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "        df=df_eemsp, \n",
    "        col=location_nb_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    for i,col in enumerate(numeric_cols):\n",
    "        if col not in df_eemsp.columns:\n",
    "            numeric_cols[i] = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "                df=df_eemsp, \n",
    "                col=col\n",
    "            )\n",
    "        #-----    \n",
    "        if not is_numeric_dtype(df_eemsp[numeric_cols[i]].dtype):\n",
    "            df_eemsp = Utilities_df.convert_col_type(\n",
    "                df=df_eemsp, \n",
    "                column=numeric_cols[i], \n",
    "                to_type=float\n",
    "            )\n",
    "    #-------------------------\n",
    "    for i,col in enumerate(dt_cols):\n",
    "        if col not in df_eemsp.columns:\n",
    "            dt_cols[i] = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "                df=df_eemsp, \n",
    "                col=col\n",
    "            )\n",
    "        #-----    \n",
    "        if not is_datetime64_dtype(df_eemsp[dt_cols[i]].dtype):\n",
    "            df_eemsp = Utilities_df.convert_col_type(\n",
    "                df=df_eemsp, \n",
    "                column=dt_cols[i], \n",
    "                to_type=datetime.datetime\n",
    "            )\n",
    "    #-------------------------\n",
    "    if ignore_cols:\n",
    "        # ignore_cols may or may not actually be contained in df_eemsp, hence the need for try/except\n",
    "        ignore_cols_OG = copy.deepcopy(ignore_cols)\n",
    "        ignore_cols = []\n",
    "        for i,col in enumerate(ignore_cols_OG):\n",
    "            try:\n",
    "                col = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "                    df=df_eemsp, \n",
    "                    col=col\n",
    "                )\n",
    "                ignore_cols.append(col)\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        ignore_cols=[]\n",
    "    #-------------------------\n",
    "    # All columns not in numeric_cols + dt_cols will be considered categorical\n",
    "    cat_cols = [\n",
    "        x for x in df_eemsp.columns \n",
    "        if x not in numeric_cols+dt_cols+ignore_cols+[outg_rec_nb_col, location_nb_col]\n",
    "    ]\n",
    "    #--------------------------------------------------\n",
    "    # Build the aggregation dictionary and aggregate\n",
    "    numeric_dict = {k:np.mean for k in numeric_cols}\n",
    "    #-----\n",
    "    dt_dict      = {k:np.mean for k in dt_cols}\n",
    "    #-----\n",
    "    # For whatever reason, using list(set(x)) in this case (using dictionary\n",
    "    #   with column keys and function values) returns a list of all the unique\n",
    "    #   characters in the column.\n",
    "    # Instead, I must use list(set(x.tolist())) \n",
    "    cat_dict     = {k:lambda x: natsorted(list(set(x.tolist()))) for k in cat_cols}\n",
    "    if cat_cols_as_strings:\n",
    "        # NOTE: Need .astype(str) below because Python apparently only likes joining lists of strings!\n",
    "        cat_dict = {k:lambda x: ', '.join(natsorted(list(set(x.astype(str).tolist())))) for k in cat_cols}\n",
    "    #-------------------------\n",
    "    agg_dict = (numeric_dict | dt_dict | cat_dict)\n",
    "    #-------------------------\n",
    "    agg_dict[outg_rec_nb_col] = lambda x: x.tolist()[0]\n",
    "    agg_dict[location_nb_col] = lambda x: x.tolist()[0]\n",
    "    #-------------------------\n",
    "    return_idx_order = (\n",
    "        [outg_rec_nb_col, location_nb_col] + \n",
    "        natsorted(numeric_dict.keys()) + \n",
    "        natsorted(dt_dict.keys()) + \n",
    "        natsorted(cat_dict.keys())\n",
    "    )\n",
    "    assert(len(set(agg_dict.keys()).symmetric_difference(set(return_idx_order)))==0)\n",
    "    #-------------------------\n",
    "    if mult_strategy=='exclude':\n",
    "        if ignore_cols:\n",
    "            df_eemsp = df_eemsp.drop(columns=ignore_cols)\n",
    "        return_df = remove_ambiguous_from_df_eemsp_OLD(\n",
    "            df_eemsp=df_eemsp, \n",
    "            outg_rec_nb_col=outg_rec_nb_col, \n",
    "            location_nb_col=location_nb_col\n",
    "        )\n",
    "    else:\n",
    "        if mult_strategy=='first':\n",
    "            if ignore_cols:\n",
    "                df_eemsp = df_eemsp.drop(columns=ignore_cols)\n",
    "            if dt_cols:\n",
    "                df_eemsp = df_eemsp.sort_values(by=dt_cols, ascending=False)\n",
    "            # NOTE: I want to use nth() below, NOT first()\n",
    "            #       Apparently, first doesn't grab the first row of each group, \n",
    "            #         it returns the first non-null entry of each column (so, if null values\n",
    "            #         exist, this will be a mixture of 2 or more rows)\n",
    "            return_df = df_eemsp.groupby(\n",
    "                [outg_rec_nb_col, location_nb_col], \n",
    "                as_index=False, \n",
    "                group_keys=False, \n",
    "                dropna=False\n",
    "            ).nth(0)\n",
    "            #----------\n",
    "            if include_n_eemsp:\n",
    "                n_eemsp_df = df_eemsp.groupby(\n",
    "                    [outg_rec_nb_col, location_nb_col], \n",
    "                    as_index=False, \n",
    "                    group_keys=False, \n",
    "                    dropna=False\n",
    "                ).size()\n",
    "                assert('size' in n_eemsp_df.columns)\n",
    "                n_eemsp_df = n_eemsp_df.rename(columns={'size':'n_eemsp'})\n",
    "                #-----\n",
    "                tmp_shape = return_df.shape\n",
    "                return_df = pd.merge(\n",
    "                    return_df, \n",
    "                    n_eemsp_df, \n",
    "                    left_on= [outg_rec_nb_col, location_nb_col], \n",
    "                    right_on=[outg_rec_nb_col, location_nb_col], \n",
    "                    how='inner'\n",
    "                )\n",
    "                #-----\n",
    "                assert(return_df.shape[0]==tmp_shape[0])\n",
    "                assert(return_df.shape[1]==tmp_shape[1]+1)\n",
    "            \n",
    "        else:\n",
    "            assert(mult_strategy=='agg')\n",
    "            return_df = df_eemsp.groupby(\n",
    "                [outg_rec_nb_col, location_nb_col], \n",
    "                as_index=False, \n",
    "                group_keys=False, \n",
    "                dropna=False\n",
    "            ).apply(\n",
    "                lambda x: reduce2_eemsp_for_outg_trsf_i_OLD(\n",
    "                    df_eemsp_i=x, \n",
    "                    agg_dict=agg_dict, \n",
    "                    return_idx_order=return_idx_order, \n",
    "                    mult_strategy=mult_strategy, \n",
    "                    include_n_eemsp=include_n_eemsp\n",
    "                )\n",
    "            )\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b637d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rcpx_with_eemsp_OLD(\n",
    "    df_rcpx, \n",
    "    df_eemsp, \n",
    "    outg_rec_nb_idfr_rcpx ='index_0', \n",
    "    trsf_pole_nb_idfr_rcpx='index_1', \n",
    "    outg_rec_nb_idfr_eemsp='OUTG_REC_NB_TO_MERGE', \n",
    "    location_nb_idfr_eemsp='LOCATION_NB', \n",
    "    set_index=True, \n",
    "    drop_eemsp_merge_cols=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    set_index:\n",
    "        If True, the index of return_df will be set equal to the contents of \n",
    "          outg_rec_nb_idfr_rcpx and trsf_pole_nb_idfr_rcpx\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    outg_rec_nb_idfr_rcpx_loc  = Utilities_df.get_idfr_loc(df_rcpx, outg_rec_nb_idfr_rcpx)\n",
    "    trsf_pole_nb_idfr_rcpx_loc = Utilities_df.get_idfr_loc(df_rcpx, trsf_pole_nb_idfr_rcpx)\n",
    "    #-----\n",
    "    outg_rec_nb_idfr_eemsp_loc = Utilities_df.get_idfr_loc(df_eemsp, outg_rec_nb_idfr_eemsp)\n",
    "    location_nb_idfr_eemsp_loc = Utilities_df.get_idfr_loc(df_eemsp, location_nb_idfr_eemsp)\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # If either outg_rec_nb_idfr_rcpx or trsf_pole_nb_idfr_rcpx found in index, must call reset_index()\n",
    "    # Reason: If one did not call reset_index and instead used, e.g., left_index=True, the actual values\n",
    "    #           in the indices will not be included in the merged df\n",
    "    # If found in the index, use name of index level as identifier.  If no name exists, give it a random one\n",
    "    if outg_rec_nb_idfr_rcpx_loc[1] or trsf_pole_nb_idfr_rcpx_loc[1]:\n",
    "        #-----\n",
    "        if outg_rec_nb_idfr_rcpx_loc[1]:\n",
    "            if df_rcpx.index.names[outg_rec_nb_idfr_rcpx_loc[0]]:\n",
    "                outg_rec_nb_idfr_rcpx = df_rcpx.index.names[outg_rec_nb_idfr_rcpx_loc[0]]\n",
    "            else:\n",
    "                tmp_name = 'outg_rec_nb_'+Utilities.generate_random_string(str_len=4)\n",
    "                df_rcpx.index = df_rcpx.index.set_names(tmp_name, level=outg_rec_nb_idfr_rcpx_loc[0])\n",
    "                outg_rec_nb_idfr_rcpx = tmp_name\n",
    "        #-----\n",
    "        if trsf_pole_nb_idfr_rcpx_loc[1]:\n",
    "            if df_rcpx.index.names[trsf_pole_nb_idfr_rcpx_loc[0]]:\n",
    "                trsf_pole_nb_idfr_rcpx = df_rcpx.index.names[trsf_pole_nb_idfr_rcpx_loc[0]]\n",
    "            else:\n",
    "                tmp_name = 'trsf_pole_nb_'+Utilities.generate_random_string(str_len=4)\n",
    "                df_rcpx.index = df_rcpx.index.set_names(tmp_name, level=trsf_pole_nb_idfr_rcpx_loc[0])\n",
    "                trsf_pole_nb_idfr_rcpx = tmp_name\n",
    "        #-----\n",
    "        df_rcpx = df_rcpx.reset_index()\n",
    "    else:\n",
    "        outg_rec_nb_idfr_rcpx  = outg_rec_nb_idfr_rcpx_loc[0]\n",
    "        trsf_pole_nb_idfr_rcpx = trsf_pole_nb_idfr_rcpx_loc[0]\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    # Same for outg_rec_nb_idfr_eemsp_loc, location_nb_idfr_eemsp_loc with df_eemsp\n",
    "    if outg_rec_nb_idfr_eemsp_loc[1] or location_nb_idfr_eemsp_loc[1]:\n",
    "        #-----\n",
    "        if outg_rec_nb_idfr_eemsp_loc[1]:\n",
    "            if df_eemsp.index.names[outg_rec_nb_idfr_eemsp_loc[0]]:\n",
    "                outg_rec_nb_idfr_eemsp = df_eemsp.index.names[outg_rec_nb_idfr_eemsp_loc[0]]\n",
    "            else:\n",
    "                tmp_name = 'outg_rec_nb_'+Utilities.generate_random_string(str_len=4)\n",
    "                df_eemsp.index = df_eemsp.index.set_names(tmp_name, level=outg_rec_nb_idfr_eemsp_loc[0])\n",
    "                outg_rec_nb_idfr_eemsp = tmp_name\n",
    "        #-----\n",
    "        if location_nb_idfr_eemsp_loc[1]:\n",
    "            if df_eemsp.index.names[location_nb_idfr_eemsp_loc[0]]:\n",
    "                location_nb_idfr_eemsp = df_eemsp.index.names[location_nb_idfr_eemsp_loc[0]]\n",
    "            else:\n",
    "                tmp_name = 'location_nb_'+Utilities.generate_random_string(str_len=4)\n",
    "                df_eemsp.index = df_eemsp.index.set_names(tmp_name, level=location_nb_idfr_eemsp_loc[0])\n",
    "                location_nb_idfr_eemsp = tmp_name\n",
    "        #-----\n",
    "        df_eemsp = df_eemsp.reset_index()\n",
    "    else:\n",
    "        outg_rec_nb_idfr_eemsp  = outg_rec_nb_idfr_eemsp_loc[0]\n",
    "        location_nb_idfr_eemsp = location_nb_idfr_eemsp_loc[0]\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # Make sure the necessary merge columns are found in the DFs\n",
    "    if outg_rec_nb_idfr_rcpx not in df_rcpx.columns:\n",
    "        print(f\"outg_rec_nb_idfr_rcpx = {outg_rec_nb_idfr_rcpx} not in  df_rcpx.columns!\")\n",
    "        print('CRASH IMMINENT')\n",
    "        assert(0)\n",
    "    #-----\n",
    "    if trsf_pole_nb_idfr_rcpx not in df_rcpx.columns:\n",
    "        print(f\"trsf_pole_nb_idfr_rcpx = {trsf_pole_nb_idfr_rcpx} not in  df_rcpx.columns!\")\n",
    "        print('CRASH IMMINENT')\n",
    "        assert(0)\n",
    "    #----------\n",
    "    if outg_rec_nb_idfr_eemsp not in df_eemsp.columns:\n",
    "        print(f\"outg_rec_nb_idfr_eemsp = {outg_rec_nb_idfr_eemsp} not in  df_eemsp.columns!\")\n",
    "        print('CRASH IMMINENT')\n",
    "        assert(0)\n",
    "    #-----\n",
    "    if location_nb_idfr_eemsp not in df_eemsp.columns:\n",
    "        print(f\"location_nb_idfr_eemsp = {location_nb_idfr_eemsp} not in  df_eemsp.columns!\")\n",
    "        print('CRASH IMMINENT')\n",
    "        assert(0)\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # In order to merge, df_rcpx and df_eemsp must have the same number of levels of columns\n",
    "    if df_rcpx.columns.nlevels != df_eemsp.columns.nlevels:\n",
    "        if df_rcpx.columns.nlevels > df_eemsp.columns.nlevels:\n",
    "            n_levels_to_add = df_rcpx.columns.nlevels - df_eemsp.columns.nlevels\n",
    "            #-----\n",
    "            df_eemsp = Utilities_df.prepend_levels_to_MultiIndex(\n",
    "                df=df_eemsp, \n",
    "                n_levels_to_add=n_levels_to_add, \n",
    "                dummy_col_levels_prefix='EEMSP_'\n",
    "            )\n",
    "            #-----\n",
    "            # Get new MultiIndex versions of outg_rec_nb_idfr_eemsp and location_nb_idfr_eemsp\n",
    "            outg_rec_nb_idfr_eemsp = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "                df=df_eemsp, \n",
    "                col=outg_rec_nb_idfr_eemsp\n",
    "            )\n",
    "            #-------------------------\n",
    "            location_nb_idfr_eemsp = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "                df=df_eemsp, \n",
    "                col=location_nb_idfr_eemsp\n",
    "            )\n",
    "        elif df_rcpx.columns.nlevels < df_eemsp.columns.nlevels:\n",
    "            n_levels_to_add = df_eemsp.columns.nlevels - df_rcpx.columns.nlevels\n",
    "            #-----\n",
    "            df_rcpx = Utilities_df.prepend_levels_to_MultiIndex(\n",
    "                df=df_rcpx, \n",
    "                n_levels_to_add=n_levels_to_add, \n",
    "                dummy_col_levels_prefix='RCPX_'\n",
    "            )\n",
    "            #-----\n",
    "            # Get new MultiIndex versions of outg_rec_nb_idfr_rcpx and trsf_pole_nb_idfr_rcpx\n",
    "            outg_rec_nb_idfr_rcpx = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "                df=df_rcpx, \n",
    "                col=outg_rec_nb_idfr_rcpx\n",
    "            )\n",
    "            #-------------------------\n",
    "            trsf_pole_nb_idfr_rcpx = Utilities_df.find_single_col_in_multiindex_cols(\n",
    "                df=df_rcpx, \n",
    "                col=trsf_pole_nb_idfr_rcpx\n",
    "            )\n",
    "        else:\n",
    "            assert(0)\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # Merge\n",
    "    assert(\n",
    "        df_rcpx[[outg_rec_nb_idfr_rcpx, trsf_pole_nb_idfr_rcpx]].dtypes.tolist() ==\n",
    "        df_eemsp[[outg_rec_nb_idfr_eemsp, location_nb_idfr_eemsp]].dtypes.tolist()\n",
    "    )\n",
    "    #-----\n",
    "    return_df = pd.merge(\n",
    "        df_rcpx, \n",
    "        df_eemsp, \n",
    "        left_on=[outg_rec_nb_idfr_rcpx, trsf_pole_nb_idfr_rcpx], \n",
    "        right_on=[outg_rec_nb_idfr_eemsp, location_nb_idfr_eemsp], \n",
    "        how='inner'\n",
    "    )\n",
    "    #-------------------------\n",
    "    if drop_eemsp_merge_cols:\n",
    "        # NOTE: If rcpx and eemsp columns are the same, after merge only one will remain, and \n",
    "        #       do not want to drop in such a case\n",
    "        cols_to_drop=[]\n",
    "        if outg_rec_nb_idfr_eemsp != outg_rec_nb_idfr_rcpx:\n",
    "            cols_to_drop.append(outg_rec_nb_idfr_eemsp)\n",
    "        if location_nb_idfr_eemsp != trsf_pole_nb_idfr_rcpx:\n",
    "            cols_to_drop.append(location_nb_idfr_eemsp)\n",
    "        #-----\n",
    "        if cols_to_drop:\n",
    "            return_df = return_df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    #-------------------------\n",
    "    if set_index:\n",
    "        return_df = return_df.set_index([outg_rec_nb_idfr_rcpx, trsf_pole_nb_idfr_rcpx])\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a3b0e",
   "metadata": {},
   "source": [
    "# END OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO EEMSP.py\n",
    "def remove_ambiguous_from_df_eemsp(\n",
    "    df_eemsp, \n",
    "    grp_by_cols=['OUTG_REC_NB_TO_MERGE', 'LOCATION_NB']\n",
    "):\n",
    "    r\"\"\"\n",
    "    Any group with multiple entries will be removed\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # Make sure grp_by_cols are present in df_eemsp\n",
    "    # NOTE: If simple string given for columns whereas df_eemsp has MultiIndex columns, the\n",
    "    #       following should remdy such a situation by changing string to appropriate tuple\n",
    "    grp_by_cols = [Utilities_df.find_single_col_in_multiindex_cols(df=df_eemsp, col=x) for x in grp_by_cols]\n",
    "    #-------------------------\n",
    "    no_ambiguity = (df_eemsp[grp_by_cols].value_counts()==1)\n",
    "    no_ambiguity = no_ambiguity[no_ambiguity==True].index\n",
    "    #-------------------------\n",
    "    df_eemsp_singles = df_eemsp.set_index(grp_by_cols).loc[no_ambiguity].reset_index()\n",
    "    #-------------------------\n",
    "    return df_eemsp_singles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce146d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO EEMSP.py\n",
    "def reduce2_eemsp_for_outg_trsf_i(\n",
    "    df_eemsp_i, \n",
    "    agg_dict, \n",
    "    return_idx_order, \n",
    "    mult_strategy='agg', \n",
    "    include_n_eemsp=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    Helper function for reduce2_eemsp_for_outg_trsf\n",
    "    In reality, only mult_strategy used here should be 'agg' ('first' is handled in \n",
    "      reduce2_eemsp_for_outg_trsf with the nth() function)\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # NOTE: If mult_strategy=='exclude' in reduce2_eemsp_for_outg_trsf, the workflow\n",
    "    #         should not utilize this function!\n",
    "    assert(mult_strategy in ['agg', 'first'])\n",
    "    #-------------------------\n",
    "    if df_eemsp_i.shape[0]==1:\n",
    "        srs_i = df_eemsp_i.iloc[0]\n",
    "    else:\n",
    "        if mult_strategy=='agg':\n",
    "            srs_i = df_eemsp_i.agg(agg_dict)\n",
    "        else:\n",
    "            assert(mult_strategy=='first')\n",
    "            srs_i = df_eemsp_i.iloc[0]\n",
    "    #-------------------------\n",
    "    srs_i=srs_i.reindex(index=return_idx_order)\n",
    "    #-------------------------\n",
    "    if include_n_eemsp:\n",
    "        srs_i['n_eemsp'] = df_eemsp_i.shape[0]\n",
    "    #-------------------------\n",
    "    return srs_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO EEMSP.py\n",
    "def reduce2_eemsp_for_outg_trsf(\n",
    "    df_eemsp, \n",
    "    mult_strategy='agg', \n",
    "    include_n_eemsp=True, \n",
    "    grp_by_cols=['OUTG_REC_NB_TO_MERGE', 'LOCATION_NB'], \n",
    "    numeric_cols = ['KVA_SIZE'], \n",
    "    dt_cols = ['INSTALL_DT', 'REMOVAL_DT'], \n",
    "    ignore_cols = ['SERIAL_NB'], \n",
    "    cat_cols_as_strings=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    To be run after reduce1_eemsp_for_outg_trsf_i!!!!!\n",
    "    The intent of this function is to reduce df_eemsp down to one row per group (typically location_nb_col\n",
    "      and/or outg_rec_nb_col)\n",
    "    \n",
    "    mult_strategy:\n",
    "        Dictates how (outg_rec_nb, trsf_pole_nb) groups with multiple EEMSP entries will be handled.\n",
    "        Can be 'agg', 'first', or 'exclude'\n",
    "        'agg':\n",
    "            For (outg_rec_nb, trsf_pole_nb) groups with multiple entries, aggregate\n",
    "        'first':\n",
    "            For (outg_rec_nb, trsf_pole_nb) groups with multiple entries, take the first\n",
    "        'exclude':\n",
    "            Exclude (outg_rec_nb, trsf_pole_nb) groups with multiple entries\n",
    "            \n",
    "    cat_cols_as_strings:\n",
    "        Categorical columns can either be aggregated as (sorted) lists of unique values (cat_cols_as_strings==False),\n",
    "          or as strings (cat_cols_as_strings==True)\n",
    "        If cat_cols_as_strings==True, a given string is simply the (sorted) lists of unique values joined by commas\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(mult_strategy in ['agg', 'first', 'exclude'])\n",
    "    #-------------------------\n",
    "    # Copy below probably not necessary\n",
    "    df_eemsp = df_eemsp.copy()\n",
    "    #-------------------------\n",
    "    # Make sure grp_by_cols and columns in numeric_cols/dt_cols are present in df_eemsp\n",
    "    # NOTE: If simple string given for columns whereas df_eemsp has MultiIndex columns, the\n",
    "    #       following should remdy such a situation by changing string to appropriate tuple\n",
    "    grp_by_cols  = [Utilities_df.find_single_col_in_multiindex_cols(df=df_eemsp, col=x) for x in grp_by_cols]\n",
    "    numeric_cols = [Utilities_df.find_single_col_in_multiindex_cols(df=df_eemsp, col=x) for x in numeric_cols]\n",
    "    dt_cols      = [Utilities_df.find_single_col_in_multiindex_cols(df=df_eemsp, col=x) for x in dt_cols]\n",
    "    #-------------------------\n",
    "    # Make sure numeric_cols are numeric types and dt_cols are datetime\n",
    "    for i,col in enumerate(numeric_cols):\n",
    "        if not is_numeric_dtype(df_eemsp[numeric_cols[i]].dtype):\n",
    "            df_eemsp = Utilities_df.convert_col_type(\n",
    "                df=df_eemsp, \n",
    "                column=numeric_cols[i], \n",
    "                to_type=float\n",
    "            )\n",
    "    #-----\n",
    "    for i,col in enumerate(dt_cols):\n",
    "        if not is_datetime64_dtype(df_eemsp[dt_cols[i]].dtype):\n",
    "            df_eemsp = Utilities_df.convert_col_type(\n",
    "                df=df_eemsp, \n",
    "                column=dt_cols[i], \n",
    "                to_type=datetime.datetime\n",
    "            )\n",
    "    #-------------------------\n",
    "    if ignore_cols:\n",
    "        # ignore_cols may or may not actually be contained in df_eemsp, hence the need for try/except\n",
    "        ignore_cols_OG = copy.deepcopy(ignore_cols)\n",
    "        ignore_cols = []\n",
    "        for i,col in enumerate(ignore_cols_OG):\n",
    "            try:\n",
    "                col = Utilities_df.find_single_col_in_multiindex_cols(df=df_eemsp, col=col)\n",
    "                ignore_cols.append(col)\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        ignore_cols=[]\n",
    "    #-------------------------\n",
    "    # All columns not in numeric_cols + dt_cols will be considered categorical\n",
    "    cat_cols = [\n",
    "        x for x in df_eemsp.columns \n",
    "        if x not in numeric_cols+dt_cols+ignore_cols+grp_by_cols\n",
    "    ]\n",
    "    #--------------------------------------------------\n",
    "    # Build the aggregation dictionary and aggregate\n",
    "    numeric_dict = {k:np.mean for k in numeric_cols}\n",
    "    #-----\n",
    "    dt_dict      = {k:np.mean for k in dt_cols}\n",
    "    #-----\n",
    "    # For whatever reason, using list(set(x)) in this case (using dictionary\n",
    "    #   with column keys and function values) returns a list of all the unique\n",
    "    #   characters in the column.\n",
    "    # Instead, I must use list(set(x.tolist())) \n",
    "    cat_dict     = {k:lambda x: natsorted(list(set(x.tolist()))) for k in cat_cols}\n",
    "    if cat_cols_as_strings:\n",
    "        # NOTE: Need .astype(str) below because Python apparently only likes joining lists of strings!\n",
    "        cat_dict = {k:lambda x: ', '.join(natsorted(list(set(x.astype(str).tolist())))) for k in cat_cols}\n",
    "    #-------------------------\n",
    "    agg_dict = (numeric_dict | dt_dict | cat_dict)\n",
    "    #-------------------------\n",
    "    for col_i in grp_by_cols:\n",
    "        agg_dict[col_i] = lambda x: x.tolist()[0]\n",
    "    #-------------------------\n",
    "    return_idx_order = (\n",
    "        grp_by_cols + \n",
    "        natsorted(numeric_dict.keys()) + \n",
    "        natsorted(dt_dict.keys()) + \n",
    "        natsorted(cat_dict.keys())\n",
    "    )\n",
    "    assert(len(set(agg_dict.keys()).symmetric_difference(set(return_idx_order)))==0)\n",
    "    #-------------------------\n",
    "    if mult_strategy=='exclude':\n",
    "        if ignore_cols:\n",
    "            df_eemsp = df_eemsp.drop(columns=ignore_cols)\n",
    "        return_df = remove_ambiguous_from_df_eemsp(\n",
    "            df_eemsp=df_eemsp, \n",
    "            grp_by_cols=grp_by_cols\n",
    "        )\n",
    "    else:\n",
    "        if mult_strategy=='first':\n",
    "            if ignore_cols:\n",
    "                df_eemsp = df_eemsp.drop(columns=ignore_cols)\n",
    "            if dt_cols:\n",
    "                df_eemsp = df_eemsp.sort_values(by=dt_cols, ascending=False)\n",
    "            # NOTE: I want to use nth() below, NOT first()\n",
    "            #       Apparently, first doesn't grab the first row of each group, \n",
    "            #         it returns the first non-null entry of each column (so, if null values\n",
    "            #         exist, this will be a mixture of 2 or more rows)\n",
    "            return_df = df_eemsp.groupby(\n",
    "                grp_by_cols, \n",
    "                as_index=False, \n",
    "                group_keys=False, \n",
    "                dropna=False\n",
    "            ).nth(0)\n",
    "            #----------\n",
    "            if include_n_eemsp:\n",
    "                n_eemsp_df = df_eemsp.groupby(\n",
    "                    grp_by_cols, \n",
    "                    as_index=False, \n",
    "                    group_keys=False, \n",
    "                    dropna=False\n",
    "                ).size()\n",
    "                assert('size' in n_eemsp_df.columns)\n",
    "                n_eemsp_df = n_eemsp_df.rename(columns={'size':'n_eemsp'})\n",
    "                #-----\n",
    "                tmp_shape = return_df.shape\n",
    "                return_df = pd.merge(\n",
    "                    return_df, \n",
    "                    n_eemsp_df, \n",
    "                    left_on= grp_by_cols, \n",
    "                    right_on=grp_by_cols, \n",
    "                    how='inner'\n",
    "                )\n",
    "                #-----\n",
    "                assert(return_df.shape[0]==tmp_shape[0])\n",
    "                assert(return_df.shape[1]==tmp_shape[1]+1)\n",
    "            \n",
    "        else:\n",
    "            assert(mult_strategy=='agg')\n",
    "            return_df = df_eemsp.groupby(\n",
    "                grp_by_cols, \n",
    "                as_index=False, \n",
    "                group_keys=False, \n",
    "                dropna=False\n",
    "            ).apply(\n",
    "                lambda x: reduce2_eemsp_for_outg_trsf_i(\n",
    "                    df_eemsp_i=x, \n",
    "                    agg_dict=agg_dict, \n",
    "                    return_idx_order=return_idx_order, \n",
    "                    mult_strategy=mult_strategy, \n",
    "                    include_n_eemsp=include_n_eemsp\n",
    "                )\n",
    "            )\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64ad8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a69203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED to Utilities_df!!!!!!!\n",
    "# def prep_df_for_merge(\n",
    "#     df, \n",
    "#     merge_on, \n",
    "#     inplace=False\n",
    "# ):\n",
    "#     r\"\"\"\n",
    "#     Helper function for new version of merge_rcpx_with_eemsp.\n",
    "#     This will locate where the merge_on are in df, i.e., if in columns of indices.\n",
    "#     If any are in index, .reset_index() will be called on df.\n",
    "#         All index levels are ensured to have an identifiable name before reset_index is called\n",
    "        \n",
    "#     Returns: df, merge_on\n",
    "#         Where df may possibly be modified (reset_index called) and merge_on will content the full and \n",
    "#           correct columns to merge on.\n",
    "#     \"\"\"\n",
    "#     #-------------------------\n",
    "#     if not inplace:\n",
    "#         df = df.copy()\n",
    "#     #-------------------------\n",
    "#     assert(isinstance(merge_on, list))\n",
    "#     #-------------------------\n",
    "#     merge_on_locs = [get_idfr_loc(df, x) for x in merge_on]\n",
    "#     #-------------------------\n",
    "#     # Determine whether any of the data to be used in the merge come from the index\n",
    "#     # Reason: When merging, it is easier to have all the merging fields in the index of both\n",
    "#     #           dfs or in the columns of both dfs, but not a mixture.\n",
    "#     #         e.g., if left_index=True and right_on=[col_1, ..., col_m], the merged df will have\n",
    "#     #           index equal to [0, n-1], i.e., the index of df_1 will not be included in final result    \n",
    "#     merge_needs_idx = any([True if x[1]==True else False for x in merge_on_locs])\n",
    "    \n",
    "#     #-------------------------\n",
    "#     # If any merge_on are found in index, .reset_index() is going to be called.\n",
    "#     # To make life easier (by making the indices traceable/identifiable), make sure all index levels have a name\n",
    "#     # If an index level does not have a name, it will be named f'idx_{level}'\n",
    "#     # Before calling reset_index, if any of the index level names is already found in the columns (probably shouldn't \n",
    "#     #   happen, but can) all of the index names will be given an random suffix\n",
    "#     if merge_needs_idx:\n",
    "#         # Name any unnamed\n",
    "#         df.index.names = [name_i if name_i else f'idx_{i}' for i,name_i in enumerate(df.index.names)]\n",
    "#         # Add random suffix if needed\n",
    "#         if any([name_i in df.columns for name_i in df.index.names]):\n",
    "#             rand_pfx = Utilities.generate_random_string(str_len=4)\n",
    "#             df.index.names = [f\"{name_i}_{rand_pfx}\" for name_i in df.index.names]\n",
    "#         #-------------------------\n",
    "#         # Update merge_on using merge_on_locs:\n",
    "#         #   NOTE: Those with merge_on_locs[i][1]==True are found in index, others are not\n",
    "#         #   For those found in columns, the column (i.e., merge_on_locs[i][0]) is simply used\n",
    "#         #   For those found in index, the index level (=merge_on_locs[i][0]) name is used\n",
    "#         merge_on = [df.index.names[x[0]] if x[1]==True else x[0] \n",
    "#                     for x in merge_on_locs]\n",
    "#         #-------------------------\n",
    "#         # As promised, reset the index\n",
    "#         df = df.reset_index()\n",
    "#     else:\n",
    "#         merge_on = [x[0] for x in merge_on_locs]\n",
    "\n",
    "#     # If df has MultiIndex columns, make sure merge_on contains the full MutliIndex column name\n",
    "#     #     When df has MultiIndex columns and merge_needs_idx was run above, this procedure is\n",
    "#     #       explicitly necessary (e.g., when n_levels=2, after reset_index, idx_1 ==> (idx_1, '') column)\n",
    "#     #     This will also be necessary if user lazilly inputs merge_col_i when, e.g., in reality the \n",
    "#     #       full columns is (merge_needs_idx, '')\n",
    "#     merge_on = [Utilities_df.find_single_col_in_multiindex_cols(df=df, col=x) for x in merge_on]\n",
    "\n",
    "#     # Make sure all columns are found\n",
    "#     assert(all([x in df.columns.tolist() for x in merge_on]))\n",
    "    \n",
    "#     #-------------------------\n",
    "#     return df, merge_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOVED TO EEMSP.py\n",
    "def merge_rcpx_with_eemsp(\n",
    "    df_rcpx, \n",
    "    df_eemsp, \n",
    "    merge_on_rcpx, \n",
    "    merge_on_eems,\n",
    "    set_index=True, \n",
    "    drop_eemsp_merge_cols=True\n",
    "):\n",
    "    r\"\"\"\n",
    "    merge_on_rcpx/merge_on_eems:\n",
    "        These should both be lists of equal length.\n",
    "        Pairs to be merged should have the same index between the two lists\n",
    "    \n",
    "    set_index:\n",
    "        If True, the index of return_df will be set to merge_on_rcpx\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    # I will likely be manipulating df_rcpx and df_eemsp.\n",
    "    # I probably don't want to change the DFs outside of this function, so copy\n",
    "    df_rcpx = df_rcpx.copy()\n",
    "    df_eemsp = df_eemsp.copy()\n",
    "    \n",
    "    #-------------------------\n",
    "    # merge_on_rcpx and merge_on_eems must be lists of the same length\n",
    "    assert(isinstance(merge_on_rcpx, list) and isinstance(merge_on_eems, list))\n",
    "    assert(len(merge_on_rcpx)==len(merge_on_eems))\n",
    "    \n",
    "    #-------------------------\n",
    "    # Call reset_index if needed and identify full/true merge_on values\n",
    "    df_rcpx, merge_on_rcpx = Utilities_df.prep_df_for_merge(\n",
    "        df=df_rcpx, \n",
    "        merge_on=merge_on_rcpx, \n",
    "        inplace=True\n",
    "    )\n",
    "    #-----\n",
    "    df_eemsp, merge_on_eems = Utilities_df.prep_df_for_merge(\n",
    "        df=df_eemsp, \n",
    "        merge_on=merge_on_eems, \n",
    "        inplace=True\n",
    "    )\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # In order to merge, df_rcpx and df_eemsp must have the same number of levels of columns\n",
    "    if df_rcpx.columns.nlevels != df_eemsp.columns.nlevels:\n",
    "        if df_rcpx.columns.nlevels > df_eemsp.columns.nlevels:\n",
    "            n_levels_to_add = df_rcpx.columns.nlevels - df_eemsp.columns.nlevels\n",
    "            #-----\n",
    "            df_eemsp = Utilities_df.prepend_levels_to_MultiIndex(\n",
    "                df=df_eemsp, \n",
    "                n_levels_to_add=n_levels_to_add, \n",
    "                dummy_col_levels_prefix='EEMSP_'\n",
    "            )\n",
    "            #-----\n",
    "            # Get new MultiIndex versions of merge_on_eems\n",
    "            merge_on_eems = [Utilities_df.find_single_col_in_multiindex_cols(df=df_eemsp, col=x) for x in merge_on_eems]\n",
    "        elif df_rcpx.columns.nlevels < df_eemsp.columns.nlevels:\n",
    "            n_levels_to_add = df_eemsp.columns.nlevels - df_rcpx.columns.nlevels\n",
    "            #-----\n",
    "            df_rcpx = Utilities_df.prepend_levels_to_MultiIndex(\n",
    "                df=df_rcpx, \n",
    "                n_levels_to_add=n_levels_to_add, \n",
    "                dummy_col_levels_prefix='RCPX_'\n",
    "            )\n",
    "            #-----\n",
    "            # Get new MultiIndex versions of merge_on_rcpx\n",
    "            merge_on_rcpx = [Utilities_df.find_single_col_in_multiindex_cols(df=df_rcpx, col=x) for x in merge_on_rcpx]        \n",
    "        else:\n",
    "            assert(0)\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # Merge\n",
    "    assert(\n",
    "        df_rcpx[merge_on_rcpx].dtypes.tolist() ==\n",
    "        df_eemsp[merge_on_eems].dtypes.tolist()\n",
    "    )\n",
    "    #-----\n",
    "    return_df = pd.merge(\n",
    "        df_rcpx, \n",
    "        df_eemsp, \n",
    "        left_on=merge_on_rcpx, \n",
    "        right_on=merge_on_eems, \n",
    "        how='inner'\n",
    "    )\n",
    "    #-------------------------\n",
    "    if drop_eemsp_merge_cols:\n",
    "        # NOTE: If rcpx and eemsp columns are the same, after merge only one will remain, and \n",
    "        #       do not want to drop in such a case!\n",
    "        cols_to_drop=[]\n",
    "        for i_col in range(len(merge_on_rcpx)):\n",
    "            if merge_on_eems[i_col] != merge_on_rcpx[i_col]:\n",
    "                cols_to_drop.append(merge_on_eems[i_col])\n",
    "        #-----\n",
    "        if cols_to_drop:\n",
    "            return_df = return_df.drop(columns=cols_to_drop)\n",
    "    #-------------------------\n",
    "    if set_index:\n",
    "        return_df = return_df.set_index(merge_on_rcpx)\n",
    "        #-----\n",
    "        # Resolve any ugly resulting index names, e.g., [('outg_rec_nb', ''), ('trsf_pole_nb', '')]\n",
    "        #   i.e., if name_i is a tuple where 0th element is not empty but all other are, then change\n",
    "        #         name to 0th element\n",
    "        fnl_idx_names = []\n",
    "        for idx_name_i in return_df.index.names:\n",
    "            if(\n",
    "                isinstance(idx_name_i, tuple) and \n",
    "                idx_name_i[0] and\n",
    "                not any([True if idx_name_i[i] else False for i in range(1, len(idx_name_i))])\n",
    "            ):\n",
    "                fnl_idx_names.append(idx_name_i[0])\n",
    "            else:\n",
    "                fnl_idx_names.append(idx_name_i)\n",
    "        return_df.index.names = fnl_idx_names\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de5462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e989b36",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------\n",
    "# PLOTTING AND POST-PROCESSING\n",
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def get_outg_confusion_matrix_text_colors(\n",
    "    cmd\n",
    "):\n",
    "    r\"\"\"\n",
    "    Basically taken from sklearn.metrics.ConfusionMatrixDisplay.plot\n",
    "    Colors are needed so my additional text matches what's provided by sklearn method.\n",
    "    \n",
    "    SHOULD BE CALLED AFTER sklearn.metrics.ConfusionMatrixDisplay.plot!\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    cm = cmd.confusion_matrix\n",
    "    assert(cm.shape[0]==2)\n",
    "    n_classes = cm.shape[0]\n",
    "    #-------------------------\n",
    "    cmap_min, cmap_max = cmd.im_.cmap(0), cmd.im_.cmap(1.0)\n",
    "    thresh = (cm.max() + cm.min()) / 2.0\n",
    "    #-------------------------\n",
    "    colors = np.empty((2,2), dtype=object)\n",
    "    for i,j in product(range(2), range(2)):\n",
    "        color = cmap_max if cm[i, j] < thresh else cmap_min\n",
    "        colors[i,j] = color\n",
    "    #-------------------------\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_outg_confusion_matrix(\n",
    "    y, \n",
    "    y_pred, \n",
    "    title=None, \n",
    "    normalize=None, \n",
    "    scientific=True, \n",
    "    ax=None, \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    n_dir_indir_tp_fn=None\n",
    "):\n",
    "    r\"\"\"\n",
    "    Visualize confusion matrix for outages using sklearn.metrics.ConfusionMatrixDisplay\n",
    "    \n",
    "    n_dir_indir_tp_fn:\n",
    "        If included, plot the number of direct and indirect transformers in the FN and TP cells.\n",
    "        n_dir_indir_tp_fn should be a dict with keys = ['FN_dir', 'FN_indir', 'TP_dir', 'TP_indir'] and\n",
    "          values equal to the associated number of transformers\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    cmd = ConfusionMatrixDisplay(\n",
    "        confusion_matrix(y, y_pred, normalize=normalize), \n",
    "        display_labels=['No Outg.','Outage']\n",
    "    )\n",
    "    #-----\n",
    "    if scientific:\n",
    "        cmd.plot(values_format='.3e', ax=ax, text_kw=text_kw)\n",
    "    else:\n",
    "        cmd.plot(values_format='', ax=ax, text_kw=text_kw)\n",
    "    #-------------------------\n",
    "    cmd.ax_.set_xlabel('Predicted', fontsize='xx-large')\n",
    "    cmd.ax_.set_ylabel('True', fontsize='xx-large')\n",
    "    cmd.ax_.set_title(title, fontsize='xx-large', fontweight='semibold')\n",
    "    #ax.set_size_inches(12, 10)\n",
    "    #-------------------------\n",
    "    if scientific:\n",
    "        txt_fmt     = '{:.4e}'\n",
    "        pct_txt_fmt = txt_fmt\n",
    "    else:\n",
    "        txt_fmt     = '{}'\n",
    "        pct_txt_fmt = '{:.4f}'\n",
    "    #-----\n",
    "    ax = cmd.ax_\n",
    "    ax.text(1.5, 0.9, \"# Entries:    {}\".format(txt_fmt).format(y.shape[0]), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.8, \"# Outages:  {}\".format(txt_fmt).format(y.sum()), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.7, \"# Baseline:  {}\".format(txt_fmt).format(y.shape[0]-y.sum()), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.6, \"% Outages:  {}\".format(pct_txt_fmt).format(100*y.sum()/y.shape[0]), fontsize=14, transform=ax.transAxes)\n",
    "    #-------------------------\n",
    "    ax.text(1.5, 0.4, \"ACCURACY:  {:.4f}\".format(accuracy_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.3, \"PRECISION:  {:.4f}\".format(precision_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.2, \"RECALL:       {:.4f}\".format(recall_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    #--------------------------------------------------\n",
    "    # Include TN, FP, FN, TP labels\n",
    "    # NOTES:\n",
    "    #   Text stored in cmd.text_ is stored in row-major fashion\n",
    "    #   Therefore, when plotting the value, the y-value corresponds to the 0th\n",
    "    #     index and the x-value corresponds to the 1st index\n",
    "    #\n",
    "    #   Axes are defined with limits:\n",
    "    #     x_lim = (-0.5, 1.5)\n",
    "    #     y_lim = (1.5, -0.5)\n",
    "    #   The axes are defined such that: \n",
    "    #     top-left corner     = (-0.5, -0.5)\n",
    "    #     bottom-right corner = (1.5, 1.5) \n",
    "    #-----\n",
    "    colors = get_outg_confusion_matrix_text_colors(cmd)\n",
    "    cat_fontsize = text_kw.get('fontsize', 'xx-large')\n",
    "    #-----\n",
    "    cmd.ax_.text(0,   -0.25, 'TN', ha='center', va='center', color=colors[0,0], fontweight='bold', fontsize=cat_fontsize)\n",
    "    cmd.ax_.text(1.0, -0.25, 'FP', ha='center', va='center', color=colors[0,1], fontweight='bold', fontsize=cat_fontsize)\n",
    "\n",
    "    cmd.ax_.text(0,    0.75, 'FN', ha='center', va='center', color=colors[1,0], fontweight='bold', fontsize=cat_fontsize)\n",
    "    cmd.ax_.text(1.0,  0.75, 'TP', ha='center', va='center', color=colors[1,1], fontweight='bold', fontsize=cat_fontsize)    \n",
    "    #--------------------------------------------------\n",
    "    if n_dir_indir_tp_fn is not None:\n",
    "        assert(len(set(['FN_dir', 'FN_indir', 'TP_dir', 'TP_indir']).difference(set(n_dir_indir_tp_fn.keys())))==0)\n",
    "        #----------\n",
    "        # FN\n",
    "        cmd.ax_.text(0,   1.25,  \"#Dir = {}\".format(txt_fmt).format(n_dir_indir_tp_fn['FN_dir']), \n",
    "                     ha='center', va='center', fontsize='medium', color=colors[1,0])\n",
    "        cmd.ax_.text(0,   1.375, \"#Indir = {}\".format(txt_fmt).format(n_dir_indir_tp_fn['FN_indir']), \n",
    "                     ha='center', va='center', fontsize='medium', color=colors[1,0])\n",
    "        #----------\n",
    "        # TP\n",
    "        cmd.ax_.text(1.0, 1.25,  \"#Dir = {}\".format(txt_fmt).format(n_dir_indir_tp_fn['TP_dir']), \n",
    "                     ha='center', va='center', fontsize='medium', color=colors[1,1])\n",
    "        cmd.ax_.text(1.0, 1.375, \"#Indir = {}\".format(txt_fmt).format(n_dir_indir_tp_fn['TP_indir']), \n",
    "                     ha='center', va='center', fontsize='medium', color=colors[1,1])\n",
    "    #--------------------------------------------------\n",
    "    #return ax, cmd.ax_\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fd0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
