{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444718d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "#reload(Utilities)\n",
    "#reload(clm)\n",
    "\n",
    "import sys, os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_dtype, is_timedelta64_dtype\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted, ns\n",
    "from packaging import version\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyodbc\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, os.path.realpath('..'))\n",
    "import Utilities_config\n",
    "#-----\n",
    "import CommonLearningMethods as clm\n",
    "#-----\n",
    "from MeterPremise import MeterPremise\n",
    "#-----\n",
    "from AMI_SQL import AMI_SQL\n",
    "from AMINonVee_SQL import AMINonVee_SQL\n",
    "from AMIEndEvents_SQL import AMIEndEvents_SQL\n",
    "from AMIUsgInst_SQL import AMIUsgInst_SQL\n",
    "from DOVSOutages_SQL import DOVSOutages_SQL\n",
    "#-----\n",
    "from GenAn import GenAn\n",
    "from AMINonVee import AMINonVee\n",
    "from AMIEndEvents import AMIEndEvents\n",
    "from AMIUsgInst import AMIUsgInst\n",
    "from DOVSOutages import DOVSOutages\n",
    "#---------------------------------------------------------------------\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import dates\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_sql_aids_dir())\n",
    "import Utilities_sql\n",
    "import TableInfos\n",
    "from TableInfos import TableInfo\n",
    "from SQLElement import SQLElement\n",
    "from SQLElementsCollection import SQLElementsCollection\n",
    "from SQLSelect import SQLSelectElement, SQLSelect\n",
    "from SQLFrom import SQLFrom\n",
    "from SQLWhere import SQLWhereElement, SQLWhere\n",
    "from SQLJoin import SQLJoin, SQLJoinCollection\n",
    "from SQLGroupBy import SQLGroupByElement, SQLGroupBy\n",
    "from SQLHaving import SQLHaving\n",
    "from SQLOrderBy import SQLOrderByElement, SQLOrderBy\n",
    "from SQLQuery import SQLQuery\n",
    "from SQLQueryGeneric import SQLQueryGeneric\n",
    "#---------------------------------------------------------------------\n",
    "sys.path.insert(0, Utilities_config.get_utilities_dir())\n",
    "import Utilities\n",
    "import Utilities_df\n",
    "import Utilities_dt\n",
    "from Utilities_df import DFConstructType\n",
    "import Plot_General\n",
    "import Plot_Box_sns\n",
    "import GrubbsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve\n",
    "#-----\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#-----\n",
    "import scipy\n",
    "from scipy import signal\n",
    "#-----\n",
    "from tensorflow import keras\n",
    "#-----\n",
    "# from tslearn.shapelets import LearningShapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42305eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c866bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def get_outg_confusion_matrix_text_colors(\n",
    "    cmd\n",
    "):\n",
    "    r\"\"\"\n",
    "    Basically taken from sklearn.metrics.ConfusionMatrixDisplay.plot\n",
    "    Colors are needed so my additional text matches what's provided by sklearn method.\n",
    "    \n",
    "    SHOULD BE CALLED AFTER sklearn.metrics.ConfusionMatrixDisplay.plot!\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    cm = cmd.confusion_matrix\n",
    "    assert(cm.shape[0]==2)\n",
    "    n_classes = cm.shape[0]\n",
    "    #-------------------------\n",
    "    cmap_min, cmap_max = cmd.im_.cmap(0), cmd.im_.cmap(1.0)\n",
    "    thresh = (cm.max() + cm.min()) / 2.0\n",
    "    #-------------------------\n",
    "    colors = np.empty((2,2), dtype=object)\n",
    "    for i,j in product(range(2), range(2)):\n",
    "        color = cmap_max if cm[i, j] < thresh else cmap_min\n",
    "        colors[i,j] = color\n",
    "    #-------------------------\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(\n",
    "    y, \n",
    "    y_pred, \n",
    "    title=None, \n",
    "    normalize=None, \n",
    "    scientific=True, \n",
    "    ax=None, \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='Outage', \n",
    "    target_eq_0_name='Baseline'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Visualize confusion matrix for outages using sklearn.metrics.ConfusionMatrixDisplay\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    cmd = ConfusionMatrixDisplay(\n",
    "        confusion_matrix(y, y_pred, normalize=normalize), \n",
    "        display_labels=[target_eq_0_name, target_eq_1_name]\n",
    "    )\n",
    "    #-----\n",
    "    if scientific:\n",
    "        cmd.plot(values_format='.3e', ax=ax, text_kw=text_kw)\n",
    "    else:\n",
    "        cmd.plot(values_format='', ax=ax, text_kw=text_kw)\n",
    "    #-------------------------\n",
    "    cmd.ax_.set_xlabel('Predicted', fontsize='xx-large')\n",
    "    cmd.ax_.set_ylabel('True', fontsize='xx-large')\n",
    "    cmd.ax_.set_title(title, fontsize='xx-large', fontweight='semibold')\n",
    "    #ax.set_size_inches(12, 10)\n",
    "    #-------------------------\n",
    "    if scientific:\n",
    "        txt_fmt     = '{:.4e}'\n",
    "        pct_txt_fmt = txt_fmt\n",
    "    else:\n",
    "        txt_fmt     = '{}'\n",
    "        pct_txt_fmt = '{:.4f}'\n",
    "    #-----\n",
    "    ax = cmd.ax_\n",
    "    ax.text(1.5, 0.9, \"# Entries:  {}\".format(txt_fmt).format(y.shape[0]), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.8, \"# {}:  {}\".format(target_eq_1_name, txt_fmt).format(y.sum()), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.7, \"# {}: {}\".format(target_eq_0_name, txt_fmt).format(y.shape[0]-y.sum()), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.6, \"% {}:  {}\".format(target_eq_1_name, pct_txt_fmt).format(100*y.sum()/y.shape[0]), fontsize=14, transform=ax.transAxes)\n",
    "    #-------------------------\n",
    "    ax.text(1.5, 0.4, \"ACCURACY:  {:.4f}\".format(accuracy_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.3, \"PRECISION:  {:.4f}\".format(precision_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.2, \"RECALL:       {:.4f}\".format(recall_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    ax.text(1.5, 0.1, \"F1:               {:.4f}\".format(f1_score(y, y_pred)), fontsize=14, transform=ax.transAxes)\n",
    "    #--------------------------------------------------\n",
    "    # Include TN, FP, FN, TP labels\n",
    "    # NOTES:\n",
    "    #   Text stored in cmd.text_ is stored in row-major fashion\n",
    "    #   Therefore, when plotting the value, the y-value corresponds to the 0th\n",
    "    #     index and the x-value corresponds to the 1st index\n",
    "    #\n",
    "    #   Axes are defined with limits:\n",
    "    #     x_lim = (-0.5, 1.5)\n",
    "    #     y_lim = (1.5, -0.5)\n",
    "    #   The axes are defined such that: \n",
    "    #     top-left corner     = (-0.5, -0.5)\n",
    "    #     bottom-right corner = (1.5, 1.5) \n",
    "    #-----\n",
    "    colors = get_outg_confusion_matrix_text_colors(cmd)\n",
    "    cat_fontsize = text_kw.get('fontsize', 'xx-large')\n",
    "    #-----\n",
    "    cmd.ax_.text(0,   -0.25, 'TN', ha='center', va='center', color=colors[0,0], fontweight='bold', fontsize=cat_fontsize)\n",
    "    cmd.ax_.text(1.0, -0.25, 'FP', ha='center', va='center', color=colors[0,1], fontweight='bold', fontsize=cat_fontsize)\n",
    "\n",
    "    cmd.ax_.text(0,    0.75, 'FN', ha='center', va='center', color=colors[1,0], fontweight='bold', fontsize=cat_fontsize)\n",
    "    cmd.ax_.text(1.0,  0.75, 'TP', ha='center', va='center', color=colors[1,1], fontweight='bold', fontsize=cat_fontsize)    \n",
    "    #--------------------------------------------------\n",
    "    #return ax, cmd.ax_\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be91e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c3b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ev_submeter_in_pair(\n",
    "    ami_df_i, \n",
    "    pct_0_thresh_main=0.1, \n",
    "    pct_0_thresh_subm=0.6, \n",
    "    enforce_corr=True, \n",
    "    corr_thresh=0.5, \n",
    "    #remove_undetermined=True, \n",
    "    value_col='value', \n",
    "    time_col='index', \n",
    "    PN_col='aep_premise_nb', \n",
    "    SN_col='serialnumber'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a pair of meter connected to a single premise, this tries to determine which is the EV submeter in the pair, \n",
    "      and keeps that which is not.\n",
    "    Basically, the submeter only monitors the EV, whereas the other monitors the usage of the entire premise.\n",
    "    Thus, the EV signal should be 0 for a majority of the time, and the two meters should be correlated.\n",
    "    \n",
    "    This is a somewhat specialized function for use with engineering the EV dataset\n",
    "    \n",
    "    ami_df_i:\n",
    "        An AMI DF containing data for a single premise which has two meters\n",
    "        \n",
    "    pct_0_thresh_main:\n",
    "        The maximum allowed percentage of 0 values for a meter to be considered the main meter\n",
    "    pct_0_thresh_subm:\n",
    "        The minimum allowed percentage of 0 values for a meter to be considered the submeter\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(ami_df_i[PN_col].nunique()==1)\n",
    "    assert(pct_0_thresh_main < pct_0_thresh_subm)\n",
    "    #-------------------------\n",
    "    SNs_i = ami_df_i[SN_col].unique().tolist()\n",
    "    assert(len(SNs_i)<=2)\n",
    "    #-------------------------\n",
    "    if len(SNs_i)==1:\n",
    "        return ami_df_i\n",
    "    #-------------------------\n",
    "    ami_df_i_1 = ami_df_i[ami_df_i[SN_col]==SNs_i[0]]\n",
    "    ami_df_i_2 = ami_df_i[ami_df_i[SN_col]==SNs_i[1]]\n",
    "    if time_col=='index':\n",
    "        ami_df_i_1 = ami_df_i_1.sort_index()\n",
    "        ami_df_i_2 = ami_df_i_2.sort_index()\n",
    "    else:\n",
    "        ami_df_i_1 = ami_df_i_1.sort_values(by=[time_col])\n",
    "        ami_df_i_2 = ami_df_i_2.sort_values(by=[time_col])\n",
    "    #--------------------------------------------------\n",
    "    # NOTE: I have seen some SNs which have a bunch of 0.002 values, which, for the purposes here should be treated as 0s\n",
    "    #       This is why I use <0.005 instead of ==0 and >=0.005 instead of !=0 below\n",
    "    #--------------------------------------------------\n",
    "    pct_1 = (ami_df_i_1[value_col]<0.005).sum()/ami_df_i_1.shape[0]\n",
    "    pct_2 = (ami_df_i_2[value_col]<0.005).sum()/ami_df_i_2.shape[0]\n",
    "    #-------------------------\n",
    "    if pct_1 <= pct_0_thresh_main:\n",
    "        defn_1 = 1\n",
    "    elif pct_1 >= pct_0_thresh_subm:\n",
    "        defn_1 = 0\n",
    "    else:\n",
    "        defn_1 = -1\n",
    "    #-----\n",
    "    if pct_2 <= pct_0_thresh_main:\n",
    "        defn_2 = 1\n",
    "    elif pct_2 >= pct_0_thresh_subm:\n",
    "        defn_2 = 0\n",
    "    else:\n",
    "        defn_2 = -1\n",
    "    #-------------------------\n",
    "    if defn_1+defn_2==1:\n",
    "        pass_pcts=True\n",
    "    else:\n",
    "        pass_pcts=False\n",
    "    #-------------------------\n",
    "    if not pass_pcts:\n",
    "        return\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    pass_corr=True\n",
    "    if enforce_corr:\n",
    "        if time_col=='index':\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_index=True, right_index=True, how='outer')\n",
    "        else:\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_on=time_col, right_on=time_col, how='outer')\n",
    "        # For the correlation only using values which are non-zero (and aligning, meaning non-NaN)\n",
    "        ami_df_i_12 = ami_df_i_12[(ami_df_i_12[f'{value_col}_x']>=0.005) & (ami_df_i_12[f'{value_col}_y']>=0.005)]\n",
    "        corr_val = ami_df_i_12[f'{value_col}_x'].corr(ami_df_i_12[f'{value_col}_y'])\n",
    "        if corr_val >= corr_thresh:\n",
    "            pass_corr = True\n",
    "        else:\n",
    "            pass_corr = False\n",
    "            \n",
    "    #--------------------------------------------------\n",
    "    if not (pass_pcts and pass_corr):\n",
    "        return\n",
    "    #-------------------------\n",
    "    if defn_1==1:\n",
    "        assert(defn_2==0) # Sanity check, not needed\n",
    "        return ami_df_i[ami_df_i[SN_col]==SNs_i[0]]\n",
    "    else:\n",
    "        assert(defn_1==0 and defn_2==1) # Sanity check, not needed\n",
    "        return ami_df_i[ami_df_i[SN_col]==SNs_i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_pairs_w_submeter(\n",
    "    ami_df_i, \n",
    "    pct_0_thresh_main=0.1, \n",
    "    pct_0_thresh_subm=0.6, \n",
    "    enforce_corr=True, \n",
    "    corr_thresh=0.5, \n",
    "    #remove_undetermined=True, \n",
    "    value_col='value', \n",
    "    time_col='index', \n",
    "    PN_col='aep_premise_nb', \n",
    "    SN_col='serialnumber'\n",
    "):\n",
    "    r\"\"\"\n",
    "    Given a pair of meter connected to a single premise, this tries to determine which is the EV submeter in the pair, \n",
    "      and keeps that which is not.\n",
    "    Basically, the submeter only monitors the EV, whereas the other monitors the usage of the entire premise.\n",
    "    Thus, the EV signal should be 0 for a majority of the time, and the two meters should be correlated.\n",
    "    \n",
    "    This is a somewhat specialized function for use with engineering the EV dataset\n",
    "    \n",
    "    ami_df_i:\n",
    "        An AMI DF containing data for a single premise which has two meters\n",
    "        \n",
    "    pct_0_thresh_main:\n",
    "        The maximum allowed percentage of 0 values for a meter to be considered the main meter\n",
    "    pct_0_thresh_subm:\n",
    "        The minimum allowed percentage of 0 values for a meter to be considered the submeter\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(ami_df_i[PN_col].nunique()==1)\n",
    "    assert(pct_0_thresh_main < pct_0_thresh_subm)\n",
    "    #-------------------------\n",
    "    SNs_i = ami_df_i[SN_col].unique().tolist()\n",
    "    assert(len(SNs_i)<=2)\n",
    "    #-------------------------\n",
    "    if len(SNs_i)==1:\n",
    "        return \n",
    "    #-------------------------\n",
    "    ami_df_i_1 = ami_df_i[ami_df_i[SN_col]==SNs_i[0]]\n",
    "    ami_df_i_2 = ami_df_i[ami_df_i[SN_col]==SNs_i[1]]\n",
    "    if time_col=='index':\n",
    "        ami_df_i_1 = ami_df_i_1.sort_index()\n",
    "        ami_df_i_2 = ami_df_i_2.sort_index()\n",
    "    else:\n",
    "        ami_df_i_1 = ami_df_i_1.sort_values(by=[time_col])\n",
    "        ami_df_i_2 = ami_df_i_2.sort_values(by=[time_col])\n",
    "    #--------------------------------------------------\n",
    "    # NOTE: I have seen some SNs which have a bunch of 0.002 values, which, for the purposes here should be treated as 0s\n",
    "    #       This is why I use <0.005 instead of ==0 and >=0.005 instead of !=0 below\n",
    "    #--------------------------------------------------\n",
    "    pct_1 = (ami_df_i_1[value_col]<0.005).sum()/ami_df_i_1.shape[0]\n",
    "    pct_2 = (ami_df_i_2[value_col]<0.005).sum()/ami_df_i_2.shape[0]\n",
    "    #-------------------------\n",
    "    if pct_1 <= pct_0_thresh_main:\n",
    "        defn_1 = 1\n",
    "    elif pct_1 >= pct_0_thresh_subm:\n",
    "        defn_1 = 0\n",
    "    else:\n",
    "        defn_1 = -1\n",
    "    #-----\n",
    "    if pct_2 <= pct_0_thresh_main:\n",
    "        defn_2 = 1\n",
    "    elif pct_2 >= pct_0_thresh_subm:\n",
    "        defn_2 = 0\n",
    "    else:\n",
    "        defn_2 = -1\n",
    "    #-------------------------\n",
    "    if defn_1+defn_2==1:\n",
    "        pass_pcts=True\n",
    "    else:\n",
    "        pass_pcts=False\n",
    "    #-------------------------\n",
    "    if not pass_pcts:\n",
    "        return\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    pass_corr=True\n",
    "    if enforce_corr:\n",
    "        if time_col=='index':\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_index=True, right_index=True, how='outer')\n",
    "        else:\n",
    "            ami_df_i_12 = pd.merge(ami_df_i_1[value_col], ami_df_i_2[value_col], left_on=time_col, right_on=time_col, how='outer')\n",
    "        # For the correlation only using values which are non-zero (and aligning, meaning non-NaN)\n",
    "        ami_df_i_12 = ami_df_i_12[(ami_df_i_12[f'{value_col}_x']>=0.005) & (ami_df_i_12[f'{value_col}_y']>=0.005)]\n",
    "        corr_val = ami_df_i_12[f'{value_col}_x'].corr(ami_df_i_12[f'{value_col}_y'])\n",
    "        if corr_val >= corr_thresh:\n",
    "            pass_corr = True\n",
    "        else:\n",
    "            pass_corr = False\n",
    "            \n",
    "    #--------------------------------------------------\n",
    "    if not (pass_pcts and pass_corr):\n",
    "        return\n",
    "    #-------------------------\n",
    "    if defn_1==1:\n",
    "        assert(defn_2==0) # Sanity check, not needed\n",
    "        ami_df_i_main = ami_df_i[ami_df_i[SN_col]==SNs_i[0]].copy()\n",
    "        ami_df_i_subm = ami_df_i[ami_df_i[SN_col]==SNs_i[1]].copy()\n",
    "    else:\n",
    "        assert(defn_1==0 and defn_2==1) # Sanity check, not needed\n",
    "        ami_df_i_main =  ami_df_i[ami_df_i[SN_col]==SNs_i[1]].copy()\n",
    "        ami_df_i_subm =  ami_df_i[ami_df_i[SN_col]==SNs_i[0]].copy()\n",
    "    #-------------------------\n",
    "    if time_col=='index':\n",
    "        return_ami_df_i = pd.merge(\n",
    "            ami_df_i_main, \n",
    "            ami_df_i_subm[[value_col, SN_col]], \n",
    "            left_index=True, \n",
    "            right_index=True, \n",
    "            how='inner'\n",
    "        )\n",
    "    else:\n",
    "        return_ami_df_i = pd.merge(\n",
    "            ami_df_i_main, \n",
    "            ami_df_i_subm[[value_col, time_col, SN_col]], \n",
    "            left_on=time_col, \n",
    "            right_on=time_col, \n",
    "            how='inner'\n",
    "        )\n",
    "    #----------\n",
    "    return_ami_df_i = return_ami_df_i.rename(columns={\n",
    "        f'{value_col}_x': f'{value_col}_main', \n",
    "        f'{value_col}_y': f'{value_col}_subm', \n",
    "        f'{SN_col}_x': f'{SN_col}_main', \n",
    "        f'{SN_col}_y': f'{SN_col}_subm', \n",
    "    })\n",
    "    #----------\n",
    "    return_ami_df_i[f'{value_col}_delt'] = return_ami_df_i[f'{value_col}_main']-return_ami_df_i[f'{value_col}_subm']\n",
    "    #-------------------------\n",
    "    return return_ami_df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a43e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "021a06aa",
   "metadata": {},
   "source": [
    "# SEE WEBPAGES\n",
    "https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data\n",
    "https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data/43512887#43512887\n",
    "https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data/56451135#56451135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ca9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class real_time_peak_detection():\n",
    "#     def __init__(self, array, lag, threshold, influence):\n",
    "#         self.y = list(array)\n",
    "#         self.length = len(self.y)\n",
    "#         self.lag = lag\n",
    "#         self.threshold = threshold\n",
    "#         self.influence = influence\n",
    "#         self.signals = [0] * len(self.y)\n",
    "#         self.filteredY = np.array(self.y).tolist()\n",
    "#         self.avgFilter = [0] * len(self.y)\n",
    "#         self.stdFilter = [0] * len(self.y)\n",
    "#         self.avgFilter[self.lag - 1] = np.mean(self.y[0:self.lag]).tolist()\n",
    "#         self.stdFilter[self.lag - 1] = np.std(self.y[0:self.lag]).tolist()\n",
    "\n",
    "#     def thresholding_algo(self, new_value):\n",
    "#         self.y.append(new_value)\n",
    "#         i = len(self.y) - 1\n",
    "#         self.length = len(self.y)\n",
    "#         if i < self.lag:\n",
    "#             return 0\n",
    "#         elif i == self.lag:\n",
    "#             self.signals = [0] * len(self.y)\n",
    "#             self.filteredY = np.array(self.y).tolist()\n",
    "#             self.avgFilter = [0] * len(self.y)\n",
    "#             self.stdFilter = [0] * len(self.y)\n",
    "#             self.avgFilter[self.lag] = np.mean(self.y[0:self.lag]).tolist()\n",
    "#             self.stdFilter[self.lag] = np.std(self.y[0:self.lag]).tolist()\n",
    "#             return 0\n",
    "\n",
    "#         self.signals += [0]\n",
    "#         self.filteredY += [0]\n",
    "#         self.avgFilter += [0]\n",
    "#         self.stdFilter += [0]\n",
    "\n",
    "#         if abs(self.y[i] - self.avgFilter[i - 1]) > (self.threshold * self.stdFilter[i - 1]):\n",
    "\n",
    "#             if self.y[i] > self.avgFilter[i - 1]:\n",
    "#                 self.signals[i] = 1\n",
    "#             else:\n",
    "#                 self.signals[i] = -1\n",
    "\n",
    "#             self.filteredY[i] = self.influence * self.y[i] + \\\n",
    "#                 (1 - self.influence) * self.filteredY[i - 1]\n",
    "#             self.avgFilter[i] = np.mean(self.filteredY[(i - self.lag):i])\n",
    "#             self.stdFilter[i] = np.std(self.filteredY[(i - self.lag):i])\n",
    "#         else:\n",
    "#             self.signals[i] = 0\n",
    "#             self.filteredY[i] = self.y[i]\n",
    "#             self.avgFilter[i] = np.mean(self.filteredY[(i - self.lag):i])\n",
    "#             self.stdFilter[i] = np.std(self.filteredY[(i - self.lag):i])\n",
    "\n",
    "#         return self.signals[i]\n",
    "\n",
    "\n",
    "def thresholding_algo_OLD(y, lag, threshold, influence):\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    for i in range(lag, len(y)):\n",
    "        if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter [i-1]:\n",
    "            if y[i] > avgFilter[i-1]:\n",
    "                signals[i] = 1\n",
    "            else:\n",
    "                signals[i] = -1\n",
    "\n",
    "            filteredY[i] = influence * y[i] + (1 - influence) * filteredY[i-1]\n",
    "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "        else:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))\n",
    "\n",
    "\n",
    "def thresholding_algo(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.mean(y)),\n",
    "                    stdFilter = np.asarray(np.std(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.mean(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77458f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding_algo_median(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.median(y)),\n",
    "                    stdFilter = np.asarray(np.std(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.median(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.median(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.median(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f60856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(y):\n",
    "    return np.mean(np.abs(y - np.mean(y)))\n",
    "\n",
    "def thresholding_algo_mad(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.mean(y)),\n",
    "                    stdFilter = np.asarray(mad(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = mad(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.mean(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = mad(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))\n",
    "\n",
    "\n",
    "def thresholding_algo_median_mad(\n",
    "    y, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    #-----\n",
    "    if len(y) < lag:\n",
    "        return dict(signals = np.asarray(np.zeros(len(y))),\n",
    "                    avgFilter = np.asarray(np.median(y)),\n",
    "                    stdFilter = np.asarray(mad(y)))\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.median(y[0:lag])\n",
    "    stdFilter[lag - 1] = mad(y[0:lag])\n",
    "    non_signal_Y = []\n",
    "    for i in range(lag, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "            non_signal_Y.append(y[i])    \n",
    "        else:\n",
    "            if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter[i-1]:\n",
    "                if y[i] > avgFilter[i-1]:\n",
    "                    signals[i] = 1\n",
    "                else:\n",
    "                    signals[i] = -1\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * np.median(non_signal_Y)\n",
    "            else:\n",
    "                signals[i] = 0\n",
    "                filteredY[i] = y[i]\n",
    "                non_signal_Y.append(y[i])\n",
    "        avgFilter[i] = np.median(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = mad(filteredY[(i-lag+1):i+1])\n",
    "\n",
    "    return dict(signals = np.asarray(signals),\n",
    "                avgFilter = np.asarray(avgFilter),\n",
    "                stdFilter = np.asarray(stdFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024cf62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c4433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ef061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_signal_groups_in_df_i(\n",
    "    df_i, \n",
    "    SN_col='serialnumber',\n",
    "    signal_col='signals', \n",
    "    return_signal_group_col='signal_grp'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)    \n",
    "    #-------------------------    \n",
    "    drop_idx = False\n",
    "    if df_i.index.name in df_i.columns:\n",
    "        drop_idx = True\n",
    "    signals_df_i = df_i.reset_index(drop=drop_idx)[df_i.reset_index(drop=drop_idx)[signal_col]==1]\n",
    "    #-----\n",
    "    tmp_signal_grp_col = Utilities.generate_random_string()\n",
    "    signals_df_i[tmp_signal_grp_col] = np.nan\n",
    "    #-----\n",
    "    tmp_idx_col = Utilities.generate_random_string()\n",
    "    signals_df_i[tmp_idx_col] = signals_df_i.index\n",
    "    #-----\n",
    "    signals_df_i[tmp_signal_grp_col] = signals_df_i[tmp_idx_col].diff().ne(1).cumsum()\n",
    "    #-------------------------\n",
    "    return_df = df_i.copy()\n",
    "    return_df[return_signal_group_col] = np.nan\n",
    "    return_df.iloc[signals_df_i.index,-1] = signals_df_i[tmp_signal_grp_col]\n",
    "    #-------------------------\n",
    "    return return_df\n",
    "\n",
    "def find_signals_and_set_signal_groups_in_df_i(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber',\n",
    "    signal_col='signals', \n",
    "    return_signal_group_col='signal_grp'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    rtpd_i = thresholding_algo(\n",
    "        y=df_i[value_col].tolist(), \n",
    "        lag=lag,\n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold\n",
    "    )\n",
    "    df_i[signal_col] = rtpd_i['signals']    \n",
    "    #-------------------------\n",
    "    return_df = set_signal_groups_in_df_i(\n",
    "        df_i=df_i, \n",
    "        SN_col=SN_col,\n",
    "        signal_col=signal_col, \n",
    "        return_signal_group_col=return_signal_group_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signals_and_build_df_i_signals_gpd(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col='signals'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    assert(df_i.index.name == time_col)\n",
    "    df_i = df_i.sort_index()\n",
    "    #-------------------------\n",
    "    signal_group_col='signal_grp'\n",
    "    df_i = find_signals_and_set_signal_groups_in_df_i(\n",
    "        df_i=df_i, \n",
    "        lag=lag, \n",
    "        threshold=threshold,\n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col,\n",
    "        signal_col=signal_col, \n",
    "        return_signal_group_col=signal_group_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    df_i_signals = df_i[df_i[signal_group_col].notna()]\n",
    "    df_i_signals = df_i_signals.drop(columns=[signal_col])\n",
    "    df_i_signals[signal_group_col] = df_i_signals[signal_group_col].astype(int)\n",
    "    df_i_signals = df_i_signals.reset_index()\n",
    "    assert(time_col in df_i_signals.columns.tolist())\n",
    "    #-------------------------\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "\n",
    "    df_i_signals_gpd = df_i_signals.groupby([signal_group_col]).agg({\n",
    "        time_col:['mean', 'std', 'min', 'max'], \n",
    "        value_col:['mean', 'std', 'max']\n",
    "    })\n",
    "    df_i_signals_gpd=df_i_signals_gpd.sort_values(by=[(time_col, 'mean')])\n",
    "    df_i_signals_gpd[(time_col, 'max_m_min')] = df_i_signals_gpd[(time_col, 'max')] - df_i_signals_gpd[(time_col, 'min')]\n",
    "    #-------------------------\n",
    "    return df_i_signals_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c2944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_SN(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col='signals'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "    \n",
    "    df_i_signals_gpd = find_signals_and_build_df_i_signals_gpd(\n",
    "        df_i=df_i, \n",
    "        lag=lag, \n",
    "        threshold=threshold,\n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col, \n",
    "        time_col=time_col, \n",
    "        signal_col=signal_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    peak_max_mean     = df_i_signals_gpd[(value_col, 'max')].mean()\n",
    "    peak_width_mean   = df_i_signals_gpd[(time_col, 'max_m_min')].mean()\n",
    "    peak_spacing_mean = df_i_signals_gpd[(time_col, 'mean')].diff().mean()\n",
    "    # Below, the date being used is of no matter, any random date works, it's simply\n",
    "    #   to make pd.to_datetime happy\n",
    "    if df_i_signals_gpd.shape[0]>0:\n",
    "        peak_hour_mean    = pd.to_datetime(\n",
    "            '2023-01-01 ' + df_i_signals_gpd[(time_col, 'mean')].dt.strftime('%H:%M:%S'), \n",
    "            format=\"%Y-%m-%d %H:%M:%S\"\n",
    "        ).mean().round('H').time().hour\n",
    "    else:\n",
    "        peak_hour_mean = np.nan\n",
    "    #-------------------------\n",
    "    features_srs = pd.Series({\n",
    "        'peak_max_mean':     peak_max_mean, \n",
    "        'peak_width_mean':   peak_width_mean, \n",
    "        'peak_spacing_mean': peak_spacing_mean, \n",
    "        'peak_hour_mean':    peak_hour_mean, \n",
    "    })\n",
    "    features_srs.name = df_i[SN_col].unique()[0]\n",
    "    return features_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1477341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_SN_v2(\n",
    "    df_i, \n",
    "    lag, \n",
    "    threshold,\n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col='signals'\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    assert(df_i.index.name == time_col)\n",
    "    df_i = df_i.sort_index()\n",
    "    #-------------------------\n",
    "    signal_group_col='signal_grp'\n",
    "    df_i = find_signals_and_set_signal_groups_in_df_i(\n",
    "        df_i=df_i, \n",
    "        lag=lag, \n",
    "        threshold=threshold,\n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col, \n",
    "        SN_col=SN_col,\n",
    "        signal_col=signal_col, \n",
    "        return_signal_group_col=signal_group_col\n",
    "    )\n",
    "    #-------------------------\n",
    "    df_i_signals = df_i[df_i[signal_group_col].notna()]\n",
    "    df_i_signals = df_i_signals.drop(columns=[signal_col])\n",
    "    df_i_signals[signal_group_col] = df_i_signals[signal_group_col].astype(int)\n",
    "    #-----   \n",
    "    drop_idx = False\n",
    "    if df_i_signals.index.name in df_i_signals.columns:\n",
    "        drop_idx = True\n",
    "    df_i_signals = df_i_signals.reset_index(drop=drop_idx)\n",
    "    assert(time_col in df_i_signals.columns.tolist())\n",
    "    #-------------------------\n",
    "    # Features split into:\n",
    "    #   1. Using all data in peaks pooled together\n",
    "    #   2. First grouping by signal group, then extracting features\n",
    "    #-------------------------\n",
    "    # Features (1):\n",
    "    peak_mean = df_i_signals[value_col].mean()\n",
    "    peak_std  = df_i_signals[value_col].std()\n",
    "    #-------------------------\n",
    "    # Features (2):\n",
    "    #-----\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "\n",
    "    df_i_signals_gpd = df_i_signals.groupby([signal_group_col]).agg({\n",
    "        time_col:['mean', 'std', 'min', 'max'], \n",
    "        value_col:['mean', 'std', 'max']\n",
    "    })\n",
    "    df_i_signals_gpd=df_i_signals_gpd.sort_values(by=[(time_col, 'mean')])\n",
    "    df_i_signals_gpd[(time_col, 'max_m_min')] = df_i_signals_gpd[(time_col, 'max')] - df_i_signals_gpd[(time_col, 'min')]\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    peak_max_mean     = df_i_signals_gpd[(value_col, 'max')].mean()\n",
    "    peak_max_std      = df_i_signals_gpd[(value_col, 'max')].std()\n",
    "    #-----\n",
    "    peak_width_mean   = df_i_signals_gpd[(time_col, 'max_m_min')].mean()\n",
    "    peak_width_std    = df_i_signals_gpd[(time_col, 'max_m_min')].std()\n",
    "    #-----\n",
    "    peak_spacing_mean = df_i_signals_gpd[(time_col, 'mean')].diff().mean()\n",
    "    peak_spacing_std  = df_i_signals_gpd[(time_col, 'mean')].diff().std()\n",
    "    #-----\n",
    "    # Below, the date being used is of no matter, any random date works, it's simply\n",
    "    #   to make pd.to_datetime happy\n",
    "    if df_i_signals_gpd.shape[0]>0:\n",
    "        peak_hour_mean    = pd.to_datetime(\n",
    "            '2023-01-01 ' + df_i_signals_gpd[(time_col, 'mean')].dt.strftime('%H:%M:%S'), \n",
    "            format=\"%Y-%m-%d %H:%M:%S\"\n",
    "        ).mean().round('H').time().hour\n",
    "    else:\n",
    "        peak_hour_mean = np.nan\n",
    "    #-------------------------\n",
    "    features_srs = pd.Series({\n",
    "        'peak_mean':         peak_mean, \n",
    "        'peak_std':          peak_std, \n",
    "        #-----\n",
    "        'peak_max_mean':     peak_max_mean, \n",
    "        'peak_max_std':      peak_max_std, \n",
    "        #-----\n",
    "        'peak_width_mean':   peak_width_mean, \n",
    "        'peak_width_std':    peak_width_std, \n",
    "        #-----\n",
    "        'peak_spacing_mean': peak_spacing_mean,\n",
    "        'peak_spacing_std':  peak_spacing_std,\n",
    "        #-----\n",
    "        'peak_hour_mean':    peak_hour_mean, \n",
    "    })\n",
    "    features_srs.name = df_i[SN_col].unique()[0]\n",
    "    return features_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_in_peak_df(\n",
    "    peak_df\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    if 'peak_mean' in peak_df.columns:\n",
    "        peak_df['peak_mean']         = peak_df['peak_mean'].fillna(0)\n",
    "    #-----\n",
    "    if 'peak_std' in peak_df.columns:\n",
    "        peak_df['peak_std']          = peak_df['peak_std'].fillna(-1)\n",
    "    #-------------------------\n",
    "    if 'peak_max_mean' in peak_df.columns:\n",
    "        peak_df['peak_max_mean']     = peak_df['peak_max_mean'].fillna(0)\n",
    "    #-----\n",
    "    if 'peak_max_std' in peak_df.columns:\n",
    "        peak_df['peak_max_std']      = peak_df['peak_max_std'].fillna(-1)\n",
    "    #-------------------------    \n",
    "    if 'peak_width_mean' in peak_df.columns:\n",
    "        peak_df['peak_width_mean']   = peak_df['peak_width_mean'].fillna(pd.Timedelta(0))\n",
    "#         peak_df['peak_width_mean']   = peak_df['peak_width_mean'].fillna(pd.Timedelta.max)\n",
    "    #-----\n",
    "    if 'peak_width_std' in peak_df.columns:\n",
    "        peak_df['peak_width_std']    = peak_df['peak_width_std'].fillna(pd.Timedelta(-1))\n",
    "#         peak_df['peak_width_std']    = peak_df['peak_width_std'].fillna(pd.Timedelta(0))\n",
    "#         peak_df['peak_width_std']    = peak_df['peak_width_std'].fillna(pd.Timedelta(pd.Timedelta.max))\n",
    "    #-------------------------\n",
    "    if 'peak_spacing_mean' in peak_df.columns:\n",
    "        peak_df['peak_spacing_mean'] = peak_df['peak_spacing_mean'].fillna(pd.Timedelta(0))\n",
    "#         peak_df['peak_spacing_mean'] = peak_df['peak_spacing_mean'].fillna(pd.Timedelta.max)\n",
    "    #-----\n",
    "    if 'peak_spacing_std' in peak_df.columns:\n",
    "        peak_df['peak_spacing_std']  = peak_df['peak_spacing_std'].fillna(pd.Timedelta(-1))\n",
    "#         peak_df['peak_spacing_std']  = peak_df['peak_spacing_std'].fillna(pd.Timedelta(0))\n",
    "#         peak_df['peak_spacing_std']  = peak_df['peak_spacing_std'].fillna(pd.Timedelta(pd.Timedelta.max))\n",
    "    #-------------------------\n",
    "    if 'peak_hour_mean' in peak_df.columns:\n",
    "        peak_df['peak_hour_mean']    = peak_df['peak_hour_mean'].fillna(-1)\n",
    "    #-------------------------\n",
    "    return peak_df\n",
    "\n",
    "\n",
    "def convert_to_seconds_in_peak_df(\n",
    "    peak_df\n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    cols_to_convert = ['peak_width_mean', 'peak_width_std', 'peak_spacing_mean', 'peak_spacing_std']\n",
    "    for col in cols_to_convert:\n",
    "        if col in peak_df.columns:\n",
    "            peak_df[col]   = peak_df[col].dt.total_seconds()/60\n",
    "    #-------------------------\n",
    "    return peak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76505f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2e52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2680486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aebfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_it_funky(df_i, n_entries=500, val_col='value', SN_col='serialnumber'):\n",
    "    if df_i.shape[0] < n_entries:\n",
    "        return None\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    vals_i = df_i[val_col].sort_index().iloc[:n_entries].tolist()\n",
    "    return_i = pd.Series(vals_i)\n",
    "    return_i.name = df_i[SN_col].unique()[0]\n",
    "    return return_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68197eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510bbac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_peaks_OLD(\n",
    "    df, \n",
    "    value_col, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    r\"\"\"\n",
    "    Do not use too small a value for lag!!!!!\n",
    "        This can cause the std values to become very small, causing much to be identified as peaks.\n",
    "        It's not a huge deal when signal_abs_threshold!=0, because this usually gives the algorithm enough\n",
    "          data to work out the rolling mean and std reliably.\n",
    "    \"\"\"\n",
    "    #-----\n",
    "    y = df[value_col].tolist()\n",
    "    #-----\n",
    "    if len(y) <= lag:\n",
    "        return np.asarray(y)\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    \n",
    "\n",
    "    # To avoid possibility of data starting with peak, which can mess up methods (especially if\n",
    "    #   lag is small), start with first value (after lag) less than the mean\n",
    "    # NOTE: The mean here includes all data, so includes peaks as well, so should be larger than the\n",
    "    #         smoothed mean\n",
    "    i_beg = np.argmax((y[lag:] < np.mean(y))) + lag\n",
    "    \n",
    "    non_signal_Y = []\n",
    "    for i in range(i_beg, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = y[i]\n",
    "            filteredY[i] = y[i]\n",
    "        else:\n",
    "            # Only looking for peaks, so don't use abs(y-avg)\n",
    "            if y[i] - avgFilter[i-1] > threshold * stdFilter[i-1]:\n",
    "                #-------------------------\n",
    "                # First, set the signal equal to the rolling average\n",
    "                #-----\n",
    "                # If avgFilter[i-1]==0, try np.mean(avgFilter)\n",
    "                # If both==0, use raw value, y[i]\n",
    "                if avgFilter[i-1]>0:\n",
    "                    signals[i] = avgFilter[i-1]\n",
    "                else:\n",
    "                    if np.mean(avgFilter)>0:\n",
    "                        signals[i] = np.mean(avgFilter)\n",
    "                    else:\n",
    "                        print('Cannot use any averages!')\n",
    "                        signals[i] = y[i]\n",
    "                #-------------------------\n",
    "                # Next, set filteredY\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    tmp_idx = np.random.randint(i-lag, i, 1)[0]\n",
    "                    tmp_mean = filteredY[tmp_idx]\n",
    "                    tmp_std = stdFilter[tmp_idx]\n",
    "                    tmp_val  = np.random.normal(tmp_mean, tmp_std, 1)\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * tmp_val\n",
    "            else:\n",
    "                signals[i] = y[i]\n",
    "                filteredY[i] = y[i]\n",
    "        #-----\n",
    "        # Note: As opposed to other thresholding functions, since any peaks have been reduced to average\n",
    "        #       values, all filteredY are also non_signal_Y (so can be appended here, instead of separately\n",
    "        #       in if/else statements as in other thresholding functions)\n",
    "        non_signal_Y.append(filteredY[i])\n",
    "        #-----\n",
    "        avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "        stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "        \n",
    "    #-------------------------\n",
    "    # Now, go back and perform calculation for initial lag entries using the non_signal_Y\n",
    "    #   for average values\n",
    "    for i in range(0,i_beg):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = y[i]\n",
    "        else:\n",
    "            if y[i] - np.mean(non_signal_Y) > threshold * np.std(non_signal_Y):\n",
    "                # Instead of using the mean of non_signal_Y, randomly draw a mean from avgFilter\n",
    "                signals[i] = np.random.choice(a=avgFilter, size=1)\n",
    "            else:\n",
    "                signals[i] = y[i]\n",
    "\n",
    "    return np.asarray(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_peaks(\n",
    "    df, \n",
    "    value_col, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold=1.0\n",
    "):\n",
    "    r\"\"\"\n",
    "    Do not use too small a value for lag!!!!!\n",
    "        This can cause the std values to become very small, causing much to be identified as peaks.\n",
    "        It's not a huge deal when signal_abs_threshold!=0, because this usually gives the algorithm enough\n",
    "          data to work out the rolling mean and std reliably.\n",
    "          \n",
    "    NOTE: While dealing with the first lag entries after i_beg, we are still susceptible to peaks contaminating\n",
    "            the data (causing higher means and, more importantly, higher standard deviations)\n",
    "          THEREFORE, if we are within this regime, instead of using mean and standard devation, I will instead\n",
    "            use the median and interquartile range (which are much more robust against outliers)\n",
    "    \"\"\"\n",
    "    #-----\n",
    "    y = df[value_col].tolist()\n",
    "    #-----\n",
    "    if len(y) <= lag:\n",
    "        return np.asarray(y)\n",
    "    #-----\n",
    "    signals = np.zeros(len(y))\n",
    "    filteredY = np.array(y)\n",
    "    avgFilter = [0]*len(y)\n",
    "    stdFilter = [0]*len(y)\n",
    "    avgFilter[lag - 1] = np.mean(y[0:lag])\n",
    "    stdFilter[lag - 1] = np.std(y[0:lag])\n",
    "    \n",
    "\n",
    "    # To avoid possibility of data starting with peak, which can mess up methods (especially if\n",
    "    #   lag is small), start with first value (after lag) less than the mean\n",
    "    # NOTE: The mean here includes all data, so includes peaks as well, so should be larger than the\n",
    "    #         smoothed mean\n",
    "    i_beg = np.argmax((y[lag:] < np.mean(y))) + lag\n",
    "    avgFilter[i_beg - 1] = np.median(y[i_beg-lag:i_beg])\n",
    "    stdFilter[i_beg - 1] = scipy.stats.iqr(y[i_beg-lag:i_beg])\n",
    "    \n",
    "    non_signal_Y = []\n",
    "    for i in range(i_beg, len(y)):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = y[i]\n",
    "            filteredY[i] = y[i]\n",
    "        else:\n",
    "            # Only looking for peaks, so don't use abs(y-avg)\n",
    "            if y[i] - avgFilter[i-1] > threshold * stdFilter[i-1]:\n",
    "                #-------------------------\n",
    "                # First, set the signal equal to the rolling average\n",
    "                #-----\n",
    "                # If avgFilter[i-1]==0, try np.mean(avgFilter)\n",
    "                # If both==0, use raw value, y[i]\n",
    "                if avgFilter[i-1]>0:\n",
    "                    signals[i] = avgFilter[i-1]\n",
    "                else:\n",
    "                    if np.mean(avgFilter)>0:\n",
    "                        signals[i] = np.mean(avgFilter)\n",
    "                    else:\n",
    "                        print('Cannot use any averages!')\n",
    "                        signals[i] = y[i]\n",
    "                #-------------------------\n",
    "                # Next, set filteredY\n",
    "                if len(non_signal_Y)==0:\n",
    "                    filteredY[i] = influence * y[i]\n",
    "                else:\n",
    "                    tmp_idx = np.random.randint(i-lag, i, 1)[0]\n",
    "                    tmp_mean = filteredY[tmp_idx]\n",
    "                    tmp_std = stdFilter[tmp_idx]\n",
    "                    tmp_val  = np.random.normal(tmp_mean, tmp_std, 1)\n",
    "                    filteredY[i] = influence * y[i] + (1 - influence) * tmp_val\n",
    "            else:\n",
    "                signals[i] = y[i]\n",
    "                filteredY[i] = y[i]\n",
    "        #-----\n",
    "        # Note: As opposed to other thresholding functions, since any peaks have been reduced to average\n",
    "        #       values, all filteredY are also non_signal_Y (so can be appended here, instead of separately\n",
    "        #       in if/else statements as in other thresholding functions)\n",
    "        non_signal_Y.append(filteredY[i])\n",
    "        #-----\n",
    "        if i-i_beg > lag:\n",
    "            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])\n",
    "        else:\n",
    "            avgFilter[i] = np.median(filteredY[(i-lag+1):i+1])\n",
    "            stdFilter[i] = scipy.stats.iqr(filteredY[(i-lag+1):i+1])            \n",
    "        \n",
    "    #-------------------------\n",
    "    # Now, go back and perform calculation for initial lag entries using the non_signal_Y\n",
    "    #   for average values\n",
    "    for i in range(0,i_beg):\n",
    "        if y[i] < signal_abs_threshold:\n",
    "            signals[i] = y[i]\n",
    "        else:\n",
    "            if y[i] - np.mean(non_signal_Y) > threshold * np.std(non_signal_Y):\n",
    "                # Instead of using the mean of non_signal_Y, randomly draw a mean from avgFilter\n",
    "                signals[i] = np.random.choice(a=avgFilter, size=1)\n",
    "            else:\n",
    "                signals[i] = y[i]\n",
    "\n",
    "    return np.asarray(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefde3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding_algo_df_i(\n",
    "    df_i, \n",
    "    value_col, \n",
    "    lag, \n",
    "    threshold, \n",
    "    influence, \n",
    "    signal_abs_threshold, \n",
    "    smooth_rolling_window, \n",
    "    SN_col='serialnumber', \n",
    "    PN_col='aep_premise_nb', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col_out='signal', \n",
    "    signal_pos_col_out='signal_pos', \n",
    "    signal_binary_col_out='signal_binary', \n",
    "    signal_pos_binary_col_out='signal_pos_binary',  \n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    if df_i.index.name != time_col:\n",
    "        assert(time_col in df_i.columns)\n",
    "        df_i = df_i.set_index(time_col, drop=True)\n",
    "    assert(df_i.index.name == time_col)\n",
    "    df_i = df_i.sort_index()\n",
    "    #-------------------------\n",
    "    # First, smooth out the peaks so the un-biased rolling mean and std can be formed\n",
    "    smooth = smooth_peaks(\n",
    "        df=df_i, \n",
    "        value_col=value_col, \n",
    "        lag=lag, \n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold\n",
    "    )\n",
    "    smooth_col = f'{value_col}_smooth'\n",
    "    df_i[smooth_col] = smooth\n",
    "    #-------------------------\n",
    "    # Generate smooth rolling mean and std\n",
    "    smooth_roll_mean_col = f'{value_col}_smooth_roll_mean'\n",
    "    smooth_roll_std_col  = f'{value_col}_smooth_roll_std'\n",
    "    df_i[smooth_roll_mean_col] = df_i[smooth_col].rolling(smooth_rolling_window, center=True).mean()\n",
    "    df_i[smooth_roll_std_col]  = df_i[smooth_col].rolling(smooth_rolling_window, center=True).std()\n",
    "    #-----\n",
    "    # Drop the NaN values at front and back of df_i naturally resulting from rolling window methods\n",
    "    df_i = df_i.dropna(subset=[smooth_roll_mean_col, smooth_roll_std_col]).copy()\n",
    "    #-------------------------\n",
    "    # Generate the signal column, which is defined for each point as the number of (rolling) std deviations \n",
    "    #   away from the (rolling) mean.\n",
    "    # Also generate the binary version of the signal, which is simply whether or not the number of std away from\n",
    "    #   the mean is outside of the threshold.\n",
    "    # Finally, build the positive versions of the above two, which set any values less than the mean equal to 0 (since\n",
    "    #   we are looking for positive peaks)\n",
    "    #-----\n",
    "    df_i[signal_col_out] = (df_i[value_col] - df_i[smooth_roll_mean_col])/df_i[smooth_roll_std_col]\n",
    "\n",
    "    # Probably really only want points which are above the rolling mean, since we're looking for peaks\n",
    "    # For now, keep both signal and signal_pos\n",
    "    df_i[signal_pos_col_out] = df_i[signal_col_out]\n",
    "    df_i.loc[df_i[signal_pos_col_out]<0, signal_pos_col_out] = 0\n",
    "\n",
    "    # Binary results are yes/no of whether peak is found\n",
    "    df_i[signal_binary_col_out]     = np.abs(df_i[signal_col_out]) > threshold\n",
    "    df_i[signal_pos_binary_col_out] = np.abs(df_i[signal_pos_col_out]) > threshold\n",
    "    \n",
    "    #-------------------------\n",
    "    # Generate a couple test series for possible use\n",
    "    df_i['test_final'] = df_i[value_col]\n",
    "    df_i.loc[df_i[signal_binary_col_out]==0, 'test_final'] = df_i.loc[df_i[signal_binary_col_out]==0, smooth_roll_mean_col]\n",
    "    #-----\n",
    "    df_i['test_final2'] = df_i[value_col]\n",
    "    df_i.loc[df_i[signal_binary_col_out]==0, 'test_final2'] = 0\n",
    "    #-----\n",
    "    df_i['test_final_signal'] = df_i[signal_col_out]\n",
    "    df_i.loc[df_i[signal_binary_col_out]==0, 'test_final_signal'] = 0\n",
    "    \n",
    "    #-------------------------\n",
    "    # To save space, don't keep all of columns in df_i, only really keep those build here (plus SN, PN, value)\n",
    "    cols_to_keep = [\n",
    "        SN_col, \n",
    "        PN_col, \n",
    "        value_col, \n",
    "        smooth_col, \n",
    "        smooth_roll_mean_col, \n",
    "        smooth_roll_std_col, \n",
    "        signal_col_out, \n",
    "        signal_pos_col_out, \n",
    "        signal_binary_col_out, \n",
    "        signal_pos_binary_col_out, \n",
    "        'test_final', \n",
    "        'test_final2', \n",
    "        'test_final_signal'\n",
    "    ]\n",
    "    return df_i[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20ec64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_peak_features_for_df_i(\n",
    "    df_i, \n",
    "    signal_group_col='signal_grp', \n",
    "    value_col='value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    #-------------------------\n",
    "    assert(df_i[SN_col].nunique()==1)\n",
    "    #-------------------------\n",
    "    if time_col not in df_i.columns:\n",
    "        assert(time_col==df_i.index.name)\n",
    "        df_i[time_col] = df_i.index\n",
    "    assert(time_col in df_i.columns)\n",
    "    #-------------------------\n",
    "    df_i_signals = df_i[df_i[signal_group_col].notna()].copy()\n",
    "    df_i_signals[signal_group_col] = df_i_signals[signal_group_col].astype(int)\n",
    "    df_i_signals = df_i_signals.reset_index(drop=True)\n",
    "    #-------------------------\n",
    "    # Features split into:\n",
    "    #   1. Using all data in peaks pooled together\n",
    "    #   2. First grouping by signal group, then extracting features\n",
    "    #-------------------------\n",
    "    # Features (1):\n",
    "    peak_mean = df_i_signals[value_col].mean()\n",
    "    peak_std  = df_i_signals[value_col].std()\n",
    "    #-------------------------\n",
    "    # Features (2):\n",
    "    #-----\n",
    "    # I think I like the mean of the max, time width of the peak, and the spacing of the peaks\n",
    "    # For the width of the peak, there needs to be a more sophisticated approach.\n",
    "    #   e.g., if the peak is one data point, the width is 0\n",
    "    #   For now, however, I will simply take max-min\n",
    "\n",
    "    df_i_signals_gpd = df_i_signals.groupby([signal_group_col]).agg({\n",
    "        time_col:['mean', 'std', 'min', 'max'], \n",
    "        value_col:['mean', 'std', 'max']\n",
    "    })\n",
    "    df_i_signals_gpd=df_i_signals_gpd.sort_values(by=[(time_col, 'mean')])\n",
    "    df_i_signals_gpd[(time_col, 'max_m_min')] = df_i_signals_gpd[(time_col, 'max')] - df_i_signals_gpd[(time_col, 'min')]\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    peak_max_mean     = df_i_signals_gpd[(value_col, 'max')].mean()\n",
    "    peak_max_std      = df_i_signals_gpd[(value_col, 'max')].std()\n",
    "    #-----\n",
    "    peak_width_mean   = df_i_signals_gpd[(time_col, 'max_m_min')].mean()\n",
    "    peak_width_std    = df_i_signals_gpd[(time_col, 'max_m_min')].std()\n",
    "    #-----\n",
    "    peak_spacing_mean = df_i_signals_gpd[(time_col, 'mean')].diff().mean()\n",
    "    peak_spacing_std  = df_i_signals_gpd[(time_col, 'mean')].diff().std()\n",
    "    #-----\n",
    "    # Below, the date being used is of no matter, any random date works, it's simply\n",
    "    #   to make pd.to_datetime happy\n",
    "    if df_i_signals_gpd.shape[0]>0:\n",
    "        peak_hour_mean    = pd.to_datetime(\n",
    "            '2023-01-01 ' + df_i_signals_gpd[(time_col, 'mean')].dt.strftime('%H:%M:%S'), \n",
    "            format=\"%Y-%m-%d %H:%M:%S\"\n",
    "        ).mean().round('H').time().hour\n",
    "    else:\n",
    "        peak_hour_mean = np.nan\n",
    "    #-------------------------\n",
    "    features_srs = pd.Series({\n",
    "        'peak_mean':         peak_mean, \n",
    "        'peak_std':          peak_std, \n",
    "        #-----\n",
    "        'peak_max_mean':     peak_max_mean, \n",
    "        'peak_max_std':      peak_max_std, \n",
    "        #-----\n",
    "        'peak_width_mean':   peak_width_mean, \n",
    "        'peak_width_std':    peak_width_std, \n",
    "        #-----\n",
    "        'peak_spacing_mean': peak_spacing_mean,\n",
    "        'peak_spacing_std':  peak_spacing_std,\n",
    "        #-----\n",
    "        'peak_hour_mean':    peak_hour_mean, \n",
    "    })\n",
    "    features_srs.name = df_i[SN_col].unique()[0]\n",
    "    return features_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe00d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11240477",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_ami_df = False\n",
    "# files_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Data'\n",
    "# pkl_save_dir =  r'C:\\Users\\s346557\\Documents\\LocalData\\EVs'\n",
    "\n",
    "files_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Recent_Data\\Data'\n",
    "pkl_save_dir =  r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Recent_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472da5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if build_ami_df:\n",
    "    ami_df = GenAn.read_df_from_csv_dir_batches(\n",
    "        files_dir=files_dir, \n",
    "        file_path_glob=r'*.csv'\n",
    "    )\n",
    "    #-------------------------\n",
    "    ami_df = ami_df[\n",
    "        (ami_df['aep_derived_uom']=='KWH') & \n",
    "        (ami_df['aep_srvc_qlty_idntfr']=='TOTAL')\n",
    "    ].copy()\n",
    "    #-----\n",
    "    ami_df['timezoneoffset'] = ami_df['starttimeperiod'].str[-6:]\n",
    "    #-------------------------\n",
    "    ami_df = AMINonVee.perform_std_initiation_and_cleaning(\n",
    "        ami_df, \n",
    "        timestamp_col=None\n",
    "    )\n",
    "    #-----\n",
    "    ami_df = Utilities_dt.strip_tz_info_and_convert_to_dt(\n",
    "        df=ami_df, \n",
    "        time_col='starttimeperiod', \n",
    "        placement_col='starttimeperiod_local', \n",
    "        run_quick=True, \n",
    "        n_strip=6, \n",
    "        inplace=False\n",
    "    )\n",
    "    ami_df = Utilities_dt.strip_tz_info_and_convert_to_dt(\n",
    "        df=ami_df, \n",
    "        time_col='endtimeperiod', \n",
    "        placement_col='endtimeperiod_local', \n",
    "        run_quick=True, \n",
    "        n_strip=6, \n",
    "        inplace=False\n",
    "    )\n",
    "    ami_df=ami_df.set_index('starttimeperiod_local', drop=False)\n",
    "    #-------------------------\n",
    "    ami_df.to_pickle(os.path.join(pkl_save_dir, 'ami_df.pkl'))\n",
    "else:\n",
    "    ami_df = pd.read_pickle(os.path.join(pkl_save_dir, 'ami_df.pkl'))\n",
    "#-------------------------\n",
    "all_trff_dfs = pd.read_pickle(os.path.join(pkl_save_dir, 'all_trff_dfs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6cf5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc95004",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df['aep_premise_nb'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebed084",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trff_dfs['PREM_NB'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trff_dfs_found     = all_trff_dfs[all_trff_dfs['PREM_NB'].isin(ami_df['aep_premise_nb'].unique().tolist())]\n",
    "all_trff_dfs_not_found = all_trff_dfs[~all_trff_dfs['PREM_NB'].isin(ami_df['aep_premise_nb'].unique().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_trff_dfs_found['PREM_NB'].nunique())\n",
    "print(all_trff_dfs_not_found['PREM_NB'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ddda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ami_df['aep_premise_nb'].nunique())\n",
    "print(ami_df['serialnumber'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df = ami_df.groupby('aep_premise_nb', as_index=False, group_keys=False).apply(\n",
    "    lambda x: remove_ev_submeter_in_pair(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b404b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ami_df['aep_premise_nb'].nunique())\n",
    "print(ami_df['serialnumber'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a186e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_resamples = AMINonVee.build_time_resampled_dfs(\n",
    "    ami_df, \n",
    "    base_freq='15T', \n",
    "    freqs=['H', '2H', '3H', '4H'], \n",
    "    other_grouper_cols=['serialnumber', 'aep_premise_nb']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7450ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df[['starttimeperiod_local', 'starttimeperiod_utc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77292cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c14ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evs_prems = all_trff_dfs[all_trff_dfs['EV']==1]['PREM_NB'].unique().tolist()\n",
    "non_prems = all_trff_dfs[all_trff_dfs['EV']==0]['PREM_NB'].unique().tolist()\n",
    "\n",
    "# aep_premise_nb in ami_df is of type object (i.e., a string), whereas PREM_NB in trff_df is int64\n",
    "evs_prems = [str(x) for x in evs_prems]\n",
    "non_prems = [str(x) for x in non_prems]\n",
    "#-----\n",
    "ami_df_evs = ami_df[ami_df['aep_premise_nb'].isin(evs_prems)].copy()\n",
    "ami_df_non = ami_df[ami_df['aep_premise_nb'].isin(non_prems)].copy()\n",
    "#----\n",
    "assert(ami_df.shape[0]==ami_df_evs.shape[0]+ami_df_non.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd03e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ami_df_evs.shape[0] = {ami_df_evs.shape[0]}')\n",
    "print(f'ami_df_non.shape[0] = {ami_df_non.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a009f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ami_df_evs['aep_premise_nb'].nunique() = {ami_df_evs['aep_premise_nb'].nunique()}\")\n",
    "print(f\"ami_df_non['aep_premise_nb'].nunique() = {ami_df_non['aep_premise_nb'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ee146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54218922",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_resamples_evs = {}\n",
    "ami_df_resamples_non = {}\n",
    "for freq_i, dfs_i in ami_df_resamples.items():\n",
    "    df_i = dfs_i['df']\n",
    "    ami_df_evs_i = df_i[df_i['aep_premise_nb'].isin(evs_prems)].copy()\n",
    "    ami_df_non_i = df_i[df_i['aep_premise_nb'].isin(non_prems)].copy()   \n",
    "    #-----\n",
    "    assert(df_i.shape[0]==ami_df_evs_i.shape[0]+ami_df_non_i.shape[0])\n",
    "    assert(freq_i not in ami_df_resamples_evs.keys())\n",
    "    assert(freq_i not in ami_df_resamples_non.keys())\n",
    "    ami_df_resamples_evs[freq_i] = ami_df_evs_i\n",
    "    ami_df_resamples_non[freq_i] = ami_df_non_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35b94b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1800ac4",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea781c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32327a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df = ami_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ami_df[ami_df['serialnumber']=='786854657'].sort_index().copy()\n",
    "\n",
    "rtpd = thresholding_algo(\n",
    "    y=tmp_df['value'].tolist(), \n",
    "    lag=12,\n",
    "    threshold=4, \n",
    "    influence=0.0, \n",
    "    signal_abs_threshold=1.0\n",
    ")\n",
    "tmp_df['signals'] = rtpd['signals']\n",
    "#-------------------------\n",
    "fig, ax = Plot_General.default_subplots()\n",
    "sns.lineplot(ax=ax, x='date', y='value', hue='serialnumber', data=tmp_df)\n",
    "sns.scatterplot(ax=ax, x='date', y='value', hue='serialnumber', data=tmp_df[tmp_df['signals']==1], palette='hls')\n",
    "ax.grid(True, which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e798d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = tmp_df[:1000].copy()\n",
    "#-----\n",
    "df2 = tmp_df[tmp_df['signals']==1].copy()\n",
    "df2 = df2[df1.index.min():df1.index.max()]\n",
    "#-------------------------\n",
    "fig, ax = Plot_General.default_subplots()\n",
    "sns.lineplot(ax=ax, x='date', y='value', hue='serialnumber', data=df1)\n",
    "sns.scatterplot(ax=ax, x='date', y='value', hue='serialnumber', data=df2, palette='hls')\n",
    "ax.grid(True, which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da00e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = ami_df[ami_df['serialnumber']=='786854657'].sort_index().copy()\n",
    "value_col = 'value'\n",
    "lag=12\n",
    "threshold=4\n",
    "influence=0.0 \n",
    "signal_abs_threshold=1.0\n",
    "smooth_rolling_window=12\n",
    "#-------------------------\n",
    "df_i = thresholding_algo_df_i(\n",
    "    df_i=df_i, \n",
    "    value_col=value_col, \n",
    "    lag=lag, \n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold, \n",
    "    smooth_rolling_window=smooth_rolling_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760da86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38873d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "fig, ax = Plot_General.default_subplots()\n",
    "sns.lineplot(ax=ax, x='starttimeperiod_local', y='value', hue='serialnumber', data=df_i)\n",
    "sns.scatterplot(ax=ax, x='starttimeperiod_local', y='value', hue='serialnumber', data=df_i[df_i['signal_binary']==1], palette='hls')\n",
    "ax.grid(True, which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_i[:1000].copy()\n",
    "#-----\n",
    "df2 = df_i[df_i['signal_binary']==1].copy()\n",
    "df2 = df2[df1.index.min():df1.index.max()]\n",
    "#-------------------------\n",
    "fig, ax = Plot_General.default_subplots()\n",
    "sns.lineplot(ax=ax, x='starttimeperiod_local', y='value', hue='serialnumber', data=df1)\n",
    "sns.scatterplot(ax=ax, x='starttimeperiod_local', y='value', hue='serialnumber', data=df2, palette='hls')\n",
    "ax.grid(True, which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2665d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19165ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaebfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4e0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae31a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af78d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc579f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a96e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d5394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e43e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf26aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af510a9",
   "metadata": {},
   "source": [
    "# PLOTS FOR ALL!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_num = 0\n",
    "\n",
    "# df_evs = ami_df_resamples_evs['4H']\n",
    "# df_non = ami_df_resamples_non['4H']\n",
    "# #-----\n",
    "# value_col = 'mean_TRS value'\n",
    "# SN_col = 'serialnumber'\n",
    "# time_col = 'date'\n",
    "# #-----\n",
    "# lag = 24\n",
    "# threshold = 5\n",
    "# influence = 0.0\n",
    "# signal_abs_threshold=1.0\n",
    "# #-----\n",
    "# save_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Figures\\4H_L24_T5_ST1_reduced'\n",
    "\n",
    "# # df_evs = ami_df_evs\n",
    "# # df_non = ami_df_non\n",
    "# # #-----\n",
    "# # value_col = 'value'\n",
    "# # SN_col = 'serialnumber'\n",
    "# # time_col = 'date'\n",
    "# # #-----\n",
    "# # lag = 48\n",
    "# # threshold = 5\n",
    "# # influence = 0.0\n",
    "# # signal_abs_threshold=1.0\n",
    "# # #-----\n",
    "# # save_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Figures\\15T_L48_T5_ST1'\n",
    "\n",
    "\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "\n",
    "# pdf = PdfPages(os.path.join(save_dir, 'EVs_w_signals.pdf'))\n",
    "# for SN_i in df_evs[SN_col].unique().tolist():\n",
    "#     df_i = df_evs[df_evs[SN_col]==SN_i].copy()\n",
    "#     #-----\n",
    "#     if df_i.shape[0]<=lag:\n",
    "#         continue\n",
    "#     #-----\n",
    "#     # If there are a bunch of zero values at the beginning of df_i, this will throw off\n",
    "#     #   the calculation.\n",
    "#     # The following line gets ride of any leading zeros\n",
    "#     df_i = df_i.iloc[df_i[value_col].ne(0).argmax():]\n",
    "#     #-----\n",
    "#     rtpd_i = thresholding_algo(\n",
    "#         y=df_i[value_col].tolist(), \n",
    "#         lag=lag,\n",
    "#         threshold=threshold, \n",
    "#         influence=influence, \n",
    "#         signal_abs_threshold=signal_abs_threshold\n",
    "#     )\n",
    "#     df_i['signals'] = rtpd_i['signals']\n",
    "#     #-----\n",
    "#     fig, ax = Plot_General.default_subplots(fig_num=fig_num)\n",
    "#     sns.lineplot(ax=ax, x=time_col, y=value_col, data=df_i).set(title=f'SN = {SN_i}')\n",
    "#     sns.scatterplot(ax=ax, x=time_col, y=value_col, data=df_i[df_i['signals']==1], color='red')\n",
    "#     ax.axhline(y=signal_abs_threshold, color='red')\n",
    "#     pdf.savefig(fig, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "#     fig_num+=1\n",
    "# pdf.close()\n",
    "# #-------------------------\n",
    "# pdf = PdfPages(os.path.join(save_dir, 'Non_w_signals.pdf'))\n",
    "# for SN_i in df_non[SN_col].unique().tolist():\n",
    "#     df_i = df_non[df_non[SN_col]==SN_i].copy()\n",
    "#     #-----\n",
    "#     if df_i.shape[0]<=lag:\n",
    "#         continue\n",
    "#     #-----\n",
    "#     # If there are a bunch of zero values at the beginning of df_i, this will throw off\n",
    "#     #   the calculation.\n",
    "#     # The following line gets ride of any leading zeros\n",
    "#     df_i = df_i.iloc[df_i[value_col].ne(0).argmax():]\n",
    "#     #-----\n",
    "#     rtpd_i = thresholding_algo(\n",
    "#         y=df_i[value_col].tolist(), \n",
    "#         lag=lag,\n",
    "#         threshold=threshold, \n",
    "#         influence=influence, \n",
    "#         signal_abs_threshold=signal_abs_threshold\n",
    "#     )\n",
    "#     df_i['signals'] = rtpd_i['signals']\n",
    "#     #-----\n",
    "#     fig, ax = Plot_General.default_subplots(fig_num=fig_num)\n",
    "#     sns.lineplot(ax=ax, x=time_col, y=value_col, data=df_i).set(title=f'SN = {SN_i}')\n",
    "#     sns.scatterplot(ax=ax, x=time_col, y=value_col, data=df_i[df_i['signals']==1], color='red')\n",
    "#     ax.axhline(y=signal_abs_threshold, color='red')\n",
    "#     pdf.savefig(fig, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "#     fig_num+=1\n",
    "# pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_num = 0\n",
    "\n",
    "# # df_evs = ami_df_resamples_evs['H'].reset_index()\n",
    "# # df_non = ami_df_resamples_non['H'].reset_index()\n",
    "# # #-----\n",
    "# # value_col = 'mean_TRS value'\n",
    "# # SN_col = 'serialnumber'\n",
    "# # time_col = 'starttimeperiod_local'\n",
    "# # #-----\n",
    "# # lag=24\n",
    "# # threshold=5\n",
    "# # influence=0\n",
    "# # signal_abs_threshold=1.0\n",
    "# # smooth_rolling_window=12\n",
    "# # #-----\n",
    "# # save_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Recent_Data\\Figures\\H_L24_T5_ST1_SRW12_reduced'\n",
    "\n",
    "# df_evs = ami_df_evs\n",
    "# df_non = ami_df_non\n",
    "# #-----\n",
    "# value_col = 'value'\n",
    "# SN_col = 'serialnumber'\n",
    "# time_col = 'starttimeperiod_local'\n",
    "# #-----\n",
    "# lag = 48\n",
    "# threshold = 5\n",
    "# influence = 0.0\n",
    "# signal_abs_threshold=1.0\n",
    "# smooth_rolling_window=24\n",
    "# #-----\n",
    "# save_dir = r'C:\\Users\\s346557\\Documents\\LocalData\\EVs\\Recent_Data\\Figures\\15T_L48_T5_ST1_SRW24_reduced'\n",
    "\n",
    "\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "\n",
    "# pdf = PdfPages(os.path.join(save_dir, 'EVs_w_signals.pdf'))\n",
    "# for SN_i in df_evs[SN_col].unique().tolist():\n",
    "#     df_i = df_evs[df_evs[SN_col]==SN_i].copy()\n",
    "#     #-----\n",
    "#     if df_i.shape[0]<=lag:\n",
    "#         continue\n",
    "#     #-----\n",
    "#     # If there are a bunch of zero values at the beginning of df_i, this will throw off\n",
    "#     #   the calculation.\n",
    "#     # The following line gets ride of any leading zeros\n",
    "# #     df_i = df_i.iloc[df_i[value_col].ne(0).argmax():]\n",
    "#     #-----\n",
    "#     df_i = thresholding_algo_df_i(\n",
    "#         df_i=df_i, \n",
    "#         value_col=value_col, \n",
    "#         lag=lag, \n",
    "#         threshold=threshold, \n",
    "#         influence=influence, \n",
    "#         signal_abs_threshold=signal_abs_threshold, \n",
    "#         smooth_rolling_window=smooth_rolling_window\n",
    "#     )\n",
    "#     #-----\n",
    "#     fig, ax = Plot_General.default_subplots(n_x=1, n_y=1)\n",
    "#     df_i[value_col].plot(ax=ax, color='black')\n",
    "#     df_i[df_i['signal_binary']==True].reset_index().plot.scatter(ax=ax, x='starttimeperiod_local', y=value_col, color='black')\n",
    "#     df_i['test_final'].plot(ax=ax, color='red', linestyle='dotted')\n",
    "#     df_i['test_final2'].plot(ax=ax, color='green', linestyle='dashed')\n",
    "#     ax.axhline(y=signal_abs_threshold, color='red')\n",
    "#     ax.set_title(f'SN = {SN_i}')\n",
    "#     pdf.savefig(fig, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "#     fig_num+=1\n",
    "# pdf.close()\n",
    "# #-------------------------\n",
    "# pdf = PdfPages(os.path.join(save_dir, 'Non_w_signals.pdf'))\n",
    "# for SN_i in df_non[SN_col].unique().tolist():\n",
    "#     df_i = df_non[df_non[SN_col]==SN_i].copy()\n",
    "#     #-----\n",
    "#     if df_i.shape[0]<=lag:\n",
    "#         continue\n",
    "#     #-----\n",
    "#     # If there are a bunch of zero values at the beginning of df_i, this will throw off\n",
    "#     #   the calculation.\n",
    "#     # The following line gets ride of any leading zeros\n",
    "# #     df_i = df_i.iloc[df_i[value_col].ne(0).argmax():]\n",
    "#     #-----\n",
    "#     df_i = thresholding_algo_df_i(\n",
    "#         df_i=df_i, \n",
    "#         value_col=value_col, \n",
    "#         lag=lag, \n",
    "#         threshold=threshold, \n",
    "#         influence=influence, \n",
    "#         signal_abs_threshold=signal_abs_threshold, \n",
    "#         smooth_rolling_window=smooth_rolling_window\n",
    "#     )\n",
    "#     #-----\n",
    "#     fig, ax = Plot_General.default_subplots(n_x=1, n_y=1)\n",
    "#     df_i[value_col].plot(ax=ax, color='black')\n",
    "#     df_i[df_i['signal_binary']==True].reset_index().plot.scatter(ax=ax, x='starttimeperiod_local', y=value_col, color='black')\n",
    "#     df_i['test_final'].plot(ax=ax, color='red', linestyle='dotted')\n",
    "#     df_i['test_final2'].plot(ax=ax, color='green', linestyle='dashed')\n",
    "#     ax.axhline(y=signal_abs_threshold, color='red')\n",
    "#     pdf.savefig(fig, bbox_inches='tight')\n",
    "#     ax.set_title(f'SN = {SN_i}')\n",
    "#     plt.close(fig)\n",
    "#     fig_num+=1\n",
    "# pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7e9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f20a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6137b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5b4d7b8",
   "metadata": {},
   "source": [
    "# OLD METHOD WITH PEAKS (15T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_evs_fit = ami_df_evs.drop(columns='starttimeperiod_local').copy()\n",
    "ami_df_non_fit = ami_df_non.drop(columns='starttimeperiod_local').copy()\n",
    "value_col = 'value'\n",
    "lag=48\n",
    "threshold=10\n",
    "influence=0.0\n",
    "signal_abs_threshold=2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df_evs = ami_df_evs_fit.groupby(['serialnumber']).apply(\n",
    "    lambda x: extract_features_for_SN_v2(\n",
    "        df_i=x, \n",
    "        lag=lag, \n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col\n",
    "    )\n",
    ")\n",
    "#-----\n",
    "peak_df_non = ami_df_non_fit.groupby(['serialnumber']).apply(\n",
    "    lambda x: extract_features_for_SN_v2(\n",
    "        df_i=x, \n",
    "        lag=lag, \n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col\n",
    "    )\n",
    ")\n",
    "#-------------------------\n",
    "peak_df_evs['target']=1\n",
    "peak_df_non['target']=0\n",
    "#-------------------------\n",
    "peak_df=pd.concat([peak_df_evs, peak_df_non])\n",
    "peak_df=peak_df.sample(frac=1)\n",
    "#-------------------------\n",
    "peak_df_OG = peak_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b972ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df = peak_df_OG.copy()\n",
    "#-------------------------\n",
    "peak_df = fill_na_in_peak_df(peak_df)\n",
    "peak_df = convert_to_seconds_in_peak_df(peak_df)\n",
    "#-------------------------\n",
    "peak_df_train, peak_df_test = train_test_split(peak_df, test_size=0.33, random_state=42)\n",
    "#-------------------------\n",
    "X_train = peak_df_train[[x for x in peak_df_train.columns.tolist() if x!='target']]\n",
    "y_train = peak_df_train['target']\n",
    "#-----\n",
    "X_test = peak_df_test[[x for x in peak_df_test.columns.tolist() if x!='target']]\n",
    "y_test = peak_df_test['target']\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52747393",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators = 10, max_depth=None, n_jobs=None)\n",
    "start = time.time()\n",
    "forest_clf.fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = forest_clf.predict(X_train)\n",
    "print('*****'*5)\n",
    "print('TRAINING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_train.sum()}\")\n",
    "print(f\"#(target==0): {y_train.shape[0]-y_train.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_train.sum()/(y_train.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_train, y_pred_train))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_train, y_pred_train))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_train, y_pred_train))\n",
    "print()\n",
    "\n",
    "y_pred = forest_clf.predict(X_test)\n",
    "print('*****'*5)\n",
    "print('TESTING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_test.sum()}\")\n",
    "print(f\"#(target==0): {y_test.shape[0]-y_test.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_test.sum()/(y_test.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_test, y_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(\n",
    "    n_x=1, \n",
    "    n_y=2, \n",
    "    fig_num=fig_num, \n",
    "    unit_figsize_width=6., \n",
    "    unit_figsize_height=4., \n",
    ")\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.4))\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_train, \n",
    "    y_pred=y_pred_train, \n",
    "    title='Train', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[0], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_test, \n",
    "    y_pred=y_pred, \n",
    "    title='Test', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[1], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "fig_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d4c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e231a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(peak_df.columns[:-1])==len(forest_clf.feature_importances_))\n",
    "importances = list(zip(peak_df.columns[:-1], forest_clf.feature_importances_))\n",
    "importances_srtd = sorted(importances, key=lambda x: x[1], reverse=True)\n",
    "importances_srtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43051148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9bc68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUCKME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1206b740",
   "metadata": {},
   "source": [
    "# OLD METHOD WITH PEAKS (H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_evs_fit = ami_df_resamples_evs['H'].copy()\n",
    "ami_df_non_fit = ami_df_resamples_non['H'].copy()\n",
    "value_col = 'mean_TRS value'\n",
    "lag=24\n",
    "threshold=5\n",
    "influence=0.0\n",
    "signal_abs_threshold=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49147632",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df_evs = ami_df_evs_fit.groupby(['serialnumber']).apply(\n",
    "    lambda x: extract_features_for_SN_v2(\n",
    "        df_i=x, \n",
    "        lag=lag, \n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col\n",
    "    )\n",
    ")\n",
    "#-----\n",
    "peak_df_non = ami_df_non_fit.groupby(['serialnumber']).apply(\n",
    "    lambda x: extract_features_for_SN_v2(\n",
    "        df_i=x, \n",
    "        lag=lag, \n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        value_col=value_col\n",
    "    )\n",
    ")\n",
    "#-------------------------\n",
    "peak_df_evs['target']=1\n",
    "peak_df_non['target']=0\n",
    "#-------------------------\n",
    "peak_df=pd.concat([peak_df_evs, peak_df_non])\n",
    "peak_df=peak_df.sample(frac=1)\n",
    "#-------------------------\n",
    "peak_df_OG = peak_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df = peak_df_OG.copy()\n",
    "#-------------------------\n",
    "peak_df = fill_na_in_peak_df(peak_df)\n",
    "peak_df = convert_to_seconds_in_peak_df(peak_df)\n",
    "#-------------------------\n",
    "peak_df_train, peak_df_test = train_test_split(peak_df, test_size=0.33, random_state=42)\n",
    "#-------------------------\n",
    "X_train = peak_df_train[[x for x in peak_df_train.columns.tolist() if x!='target']]\n",
    "y_train = peak_df_train['target']\n",
    "#-----\n",
    "X_test = peak_df_test[[x for x in peak_df_test.columns.tolist() if x!='target']]\n",
    "y_test = peak_df_test['target']\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators = 10, max_depth=None, n_jobs=None)\n",
    "start = time.time()\n",
    "forest_clf.fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "y_pred_train = forest_clf.predict(X_train)\n",
    "print('*****'*5)\n",
    "print('TRAINING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_train.sum()}\")\n",
    "print(f\"#(target==0): {y_train.shape[0]-y_train.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_train.sum()/(y_train.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_train, y_pred_train))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_train, y_pred_train))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_train, y_pred_train))\n",
    "print()\n",
    "\n",
    "y_pred = forest_clf.predict(X_test)\n",
    "print('*****'*5)\n",
    "print('TESTING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_test.sum()}\")\n",
    "print(f\"#(target==0): {y_test.shape[0]-y_test.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_test.sum()/(y_test.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_test, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ee384",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(\n",
    "    n_x=1, \n",
    "    n_y=2, \n",
    "    fig_num=fig_num, \n",
    "    unit_figsize_width=6., \n",
    "    unit_figsize_height=4., \n",
    ")\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.4))\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_train, \n",
    "    y_pred=y_pred_train, \n",
    "    title='Train', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[0], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_test, \n",
    "    y_pred=y_pred, \n",
    "    title='Test', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[1], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "fig_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0db420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440d136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f1f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8465cb98",
   "metadata": {},
   "source": [
    "# GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55086426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # lags = [8, 12, 24, 48, 96, 192]\n",
    "# # thresholds = [2, 5, 8, 10, 15]\n",
    "# # signal_abs_thresholds = [0, 1, 1.5, 2, 2.5]\n",
    "\n",
    "# lags = [8, 12, 24, 48, 96, 192]\n",
    "# thresholds = [2, 5, 8, 10, 15]\n",
    "# signal_abs_thresholds = [0, 1, 2]\n",
    "\n",
    "# for lag in lags:\n",
    "#     for threshold in thresholds:\n",
    "#         for signal_abs_threshold in signal_abs_thresholds:\n",
    "#             peak_df_evs = ami_df_evs.drop(columns='starttimeperiod_local').groupby(['serialnumber']).apply(\n",
    "#                 lambda x: extract_features_for_SN(\n",
    "#                     df_i=x, \n",
    "#                     lag=lag, \n",
    "#                     threshold=threshold, \n",
    "#                     influence=0.0, \n",
    "#                     signal_abs_threshold=signal_abs_threshold, \n",
    "#                     value_col='value'\n",
    "#                 )\n",
    "#             )\n",
    "#             #-----\n",
    "#             peak_df_non = ami_df_non.drop(columns='starttimeperiod_local').groupby(['serialnumber']).apply(\n",
    "#                 lambda x: extract_features_for_SN(\n",
    "#                     df_i=x, \n",
    "#                     lag=lag, \n",
    "#                     threshold=threshold, \n",
    "#                     influence=0.0, \n",
    "#                     signal_abs_threshold=signal_abs_threshold, \n",
    "#                     value_col='value'\n",
    "#                 )\n",
    "#             )\n",
    "#             #-------------------------\n",
    "#             peak_df_evs['target']=1\n",
    "#             peak_df_non['target']=0\n",
    "#             #-----\n",
    "#             peak_df=pd.concat([peak_df_evs, peak_df_non])\n",
    "#             peak_df=peak_df.sample(frac=1)\n",
    "#             #-------------------------            \n",
    "#             peak_df = fill_na_in_peak_df(peak_df)\n",
    "#             peak_df = convert_to_seconds_in_peak_df(peak_df)\n",
    "#             #-------------------------\n",
    "#             peak_df_train, peak_df_test = train_test_split(peak_df, test_size=0.33, random_state=42)\n",
    "#             #-----\n",
    "#             X_train = peak_df_train[[x for x in peak_df_train.columns.tolist() if x!='target']]\n",
    "#             y_train = peak_df_train['target']\n",
    "#             #-----\n",
    "#             X_test = peak_df_test[[x for x in peak_df_test.columns.tolist() if x!='target']]\n",
    "#             y_test = peak_df_test['target']\n",
    "#             #-------------------------\n",
    "#             forest_clf = RandomForestClassifier(n_estimators = 10, max_depth=None, n_jobs=None)\n",
    "#             start = time.time()\n",
    "#             forest_clf.fit(X_train, y_train)\n",
    "#             print(time.time()-start)\n",
    "#             #-------------------------\n",
    "#             print('\\n\\n')\n",
    "#             print(f'lag                  = {lag}')\n",
    "#             print(f'threshold            = {threshold}')\n",
    "#             print(f'signal_abs_threshold = {signal_abs_threshold}')\n",
    "#             #-------------------------\n",
    "#             y_pred_train = forest_clf.predict(X_train)\n",
    "#             print('*****'*5)\n",
    "#             print('TRAINING DATASET')\n",
    "#             print('*****'*5)\n",
    "#             print(f\"#(target==1): {y_train.sum()}\")\n",
    "#             print(f\"#(target==0): {y_train.shape[0]-y_train.sum()}\")\n",
    "#             print(f\"%(target==1): {100*(y_train.sum()/(y_train.shape[0]))}\")\n",
    "#             print('-----'*5)\n",
    "#             print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_train, y_pred_train))\n",
    "#             print(\"PRECISION OF THE MODEL: \", precision_score(y_train, y_pred_train))\n",
    "#             print(\"RECALL    OF THE MODEL: \", recall_score(y_train, y_pred_train))\n",
    "#             print(\"F1        OF THE MODEL: \", f1_score(y_train, y_pred_train))\n",
    "#             print()\n",
    "\n",
    "#             y_pred = forest_clf.predict(X_test)\n",
    "#             print('*****'*5)\n",
    "#             print('TESTING DATASET')\n",
    "#             print('*****'*5)\n",
    "#             print(f\"#(target==1): {y_test.sum()}\")\n",
    "#             print(f\"#(target==0): {y_test.shape[0]-y_test.sum()}\")\n",
    "#             print(f\"%(target==1): {100*(y_test.sum()/(y_test.shape[0]))}\")\n",
    "#             print('-----'*5)\n",
    "#             print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "#             print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "#             print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "#             print(\"F1        OF THE MODEL: \", f1_score(y_test, y_pred))\n",
    "#             print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100deb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0d90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2ed88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb2dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea417082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b9cf299",
   "metadata": {},
   "source": [
    "# peak_df scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a34cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(\n",
    "    peak_df['peak_max_mean'].tolist(), \n",
    "    peak_df['peak_width_mean'].tolist(), \n",
    "    peak_df['peak_spacing_mean'].tolist(), \n",
    "    color='red'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f773f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3418ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16, 9))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(\n",
    "    peak_df[peak_df['target']==1]['peak_max_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==1]['peak_width_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==1]['peak_spacing_mean'].tolist(), \n",
    "    color='red'\n",
    ")\n",
    "# ax.scatter(\n",
    "#     peak_df[peak_df['target']==0]['peak_max_mean'].tolist(), \n",
    "#     peak_df[peak_df['target']==0]['peak_width_mean'].tolist(), \n",
    "#     peak_df[peak_df['target']==0]['peak_spacing_mean'].tolist(), \n",
    "#     color='blue'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16, 9))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "# ax.scatter(\n",
    "#     peak_df[peak_df['target']==1]['peak_max_mean'].tolist(), \n",
    "#     peak_df[peak_df['target']==1]['peak_width_mean'].tolist(), \n",
    "#     peak_df[peak_df['target']==1]['peak_spacing_mean'].tolist(), \n",
    "#     color='red'\n",
    "# )\n",
    "ax.scatter(\n",
    "    peak_df[peak_df['target']==0]['peak_max_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==0]['peak_width_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==0]['peak_spacing_mean'].tolist(), \n",
    "    color='blue'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61271f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16, 9))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(\n",
    "    peak_df[peak_df['target']==1]['peak_max_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==1]['peak_width_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==1]['peak_spacing_mean'].tolist(), \n",
    "    color='red'\n",
    ")\n",
    "ax.scatter(\n",
    "    peak_df[peak_df['target']==0]['peak_max_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==0]['peak_width_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==0]['peak_spacing_mean'].tolist(), \n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('peak_max_mean', fontweight ='bold')\n",
    "ax.set_ylabel('peak_width_mean', fontweight ='bold')\n",
    "ax.set_zlabel('peak_spacing_mean', fontweight ='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da911581",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16, 9))\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(\n",
    "    peak_df[peak_df['target']==1]['peak_max_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==1]['peak_width_mean'].tolist(), \n",
    "    color='red'\n",
    ")\n",
    "ax.scatter(\n",
    "    peak_df[peak_df['target']==0]['peak_max_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==0]['peak_width_mean'].tolist(), \n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('peak_max_mean', fontweight ='bold')\n",
    "ax.set_ylabel('peak_width_mean', fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b09da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16, 9))\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(\n",
    "    peak_df[peak_df['target']==0]['peak_max_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==0]['peak_width_mean'].tolist(), \n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('peak_max_mean', fontweight ='bold')\n",
    "ax.set_ylabel('peak_width_mean', fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107cb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16, 9))\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(\n",
    "    peak_df[peak_df['target']==1]['peak_max_mean'].tolist(), \n",
    "    peak_df[peak_df['target']==1]['peak_width_mean'].tolist(), \n",
    "    color='red'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('peak_max_mean', fontweight ='bold')\n",
    "ax.set_ylabel('peak_width_mean', fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf3b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6085cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9959d267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44943b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af607c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c748d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18f4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a5d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7448c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c150d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98680af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f072117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506629dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6c801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68674530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aacdc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898b2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ccc4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33907348",
   "metadata": {},
   "source": [
    "# OLD METHOD WITH KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_evs_fit = ami_df_resamples_evs['H'].copy()\n",
    "ami_df_non_fit = ami_df_resamples_non['H'].copy()\n",
    "value_col = 'mean_TRS value'\n",
    "n_entries=200\n",
    "\n",
    "# ami_df_evs_fit = ami_df_evs.drop(columns='starttimeperiod_local').copy()\n",
    "# ami_df_non_fit = ami_df_non.drop(columns='starttimeperiod_local').copy()\n",
    "# value_col = 'value'\n",
    "# n_entries=500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b88fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_evs = ami_df_evs_fit.groupby(['serialnumber']).apply(\n",
    "    lambda x: make_it_funky(\n",
    "        df_i=x, \n",
    "        n_entries=n_entries, \n",
    "        val_col=value_col, \n",
    "        SN_col='serialnumber'\n",
    "    )\n",
    ")\n",
    "#-----\n",
    "keras_df_non = ami_df_non_fit.groupby(['serialnumber']).apply(\n",
    "    lambda x: make_it_funky(\n",
    "        df_i=x, \n",
    "        n_entries=n_entries, \n",
    "        val_col=value_col, \n",
    "        SN_col='serialnumber'\n",
    "    )\n",
    ")\n",
    "#-------------------------\n",
    "keras_df_evs['target']=1\n",
    "keras_df_non['target']=0\n",
    "#-------------------------\n",
    "keras_df = pd.concat([keras_df_evs, keras_df_non])\n",
    "#-------------------------\n",
    "keras_df=keras_df.dropna()\n",
    "#-------------------------\n",
    "keras_df_train, keras_df_test = train_test_split(keras_df, test_size=0.33, random_state=42)\n",
    "#-------------------------\n",
    "X_train = keras_df_train[[x for x in keras_df_train.columns.tolist() if x != 'target']].values\n",
    "y_train = keras_df_train['target'].values\n",
    "X_train=X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "#-----\n",
    "X_test = keras_df_test[[x for x in keras_df_test.columns.tolist() if x != 'target']].values\n",
    "y_test = keras_df_test['target'].values\n",
    "X_test=X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4cce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4857a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a02640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_model(input_shape):\n",
    "#     num_classes=2\n",
    "#     input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "#     conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "#     conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "#     conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "#     conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "#     conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "#     conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "#     conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "#     conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "#     conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "#     gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "#     output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "#     return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    num_classes=2\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=5, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=10, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "    \n",
    "    conv4 = keras.layers.Conv1D(filters=64, kernel_size=10, padding=\"same\")(conv3)\n",
    "    conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = keras.layers.ReLU()(conv4)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv4)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d44287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(input_shape=X_train.shape[1:])\n",
    "# keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a33841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# epochs = 500\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"best_model.h5\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "print(\"Train accuracy\", train_acc)\n",
    "print(\"Train loss\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f353337",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train = np.argmax(y_pred_train, axis=1)\n",
    "#-----\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_train, y_pred_train))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_train, y_pred_train))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5478e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "#-----\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0006fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(\n",
    "    n_x=1, \n",
    "    n_y=2, \n",
    "    fig_num=fig_num, \n",
    "    unit_figsize_width=6., \n",
    "    unit_figsize_height=4., \n",
    ")\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.4))\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_train, \n",
    "    y_pred=y_pred_train, \n",
    "    title='Train', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[0], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_test, \n",
    "    y_pred=y_pred, \n",
    "    title='Test', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[1], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "fig_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf036d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e1325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd6c6a4a",
   "metadata": {},
   "source": [
    "# NEW METHOD PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = ami_df_resamples_evs['H'][ami_df_resamples_evs['H']['serialnumber']=='786854657'].sort_index().copy()\n",
    "value_col = 'mean_TRS value'\n",
    "\n",
    "# df_i = ami_df_evs[ami_df_evs['serialnumber']=='786854657'].sort_index().copy()\n",
    "# value_col = 'value'\n",
    "\n",
    "# df_i = ami_df_non[ami_df_non['serialnumber']=='780852455'].sort_index().iloc[:1000].copy()\n",
    "# value_col = 'value'\n",
    "\n",
    "# df_i = ami_df_resamples_non['H'][ami_df_resamples_non['H']['serialnumber']=='780852455'].sort_index().copy()\n",
    "# value_col = 'mean_TRS value'\n",
    "\n",
    "df_i_OG = df_i.copy()\n",
    "\n",
    "lag=24\n",
    "threshold=5\n",
    "influence=0\n",
    "signal_abs_threshold=1.0\n",
    "smooth_rolling_window=12\n",
    "\n",
    "smooth_col = f'{value_col}_smooth'\n",
    "smooth_roll_mean_col = f'{value_col}_smooth_roll_mean'\n",
    "smooth_roll_std_col  = f'{value_col}_smooth_roll_std'\n",
    "\n",
    "signal_col_out='signal'\n",
    "signal_pos_col_out='signal_pos'\n",
    "signal_binary_col_out='signal_binary'\n",
    "signal_pos_binary_col_out='signal_pos_binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb9a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = thresholding_algo_df_i(\n",
    "    df_i=df_i, \n",
    "    value_col=value_col, \n",
    "    lag=lag, \n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold, \n",
    "    smooth_rolling_window=smooth_rolling_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d1de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e409701",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "\n",
    "df_i[value_col].plot(ax=axs[0], color='blue')\n",
    "df_i[smooth_col].plot(ax=axs[0], color='green')\n",
    "df_i[smooth_roll_mean_col].plot(ax=axs[0], color='red')\n",
    "\n",
    "df_i[signal_col_out].plot(ax=axs[1], color='blue')\n",
    "df_i[signal_pos_col_out].plot(ax=axs[1], color='green')\n",
    "df_i[df_i[signal_binary_col_out]==1].reset_index().plot.scatter(ax=axs[1], x='starttimeperiod_local', y=signal_col_out, color='blue', alpha=0.5)\n",
    "df_i[df_i[signal_pos_binary_col_out]==1].reset_index().plot.scatter(ax=axs[1], x='starttimeperiod_local', y=signal_pos_col_out, color='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4465fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "\n",
    "df_i[value_col].plot(ax=axs[0], color='blue')\n",
    "df_i['test_final'].plot(ax=axs[0], color='purple')\n",
    "\n",
    "df_i[signal_col_out].plot(ax=axs[1], color='blue')\n",
    "df_i['test_final_signal'].plot(ax=axs[1], color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e1cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad3f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b3787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21bb758e",
   "metadata": {},
   "source": [
    "# NEW METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_evs_fit = ami_df_resamples_evs['H'].copy()\n",
    "ami_df_non_fit = ami_df_resamples_non['H'].copy()\n",
    "#-----\n",
    "value_col = 'mean_TRS value'\n",
    "# lag=12\n",
    "# threshold=5\n",
    "# influence=0\n",
    "# signal_abs_threshold=0.5\n",
    "# smooth_rolling_window=12\n",
    "\n",
    "# lag                   = 24\n",
    "# threshold             = 3\n",
    "# influence             = 0 \n",
    "# signal_abs_threshold  = 1.0\n",
    "# smooth_rolling_window = 24\n",
    "\n",
    "lag                   = 24\n",
    "threshold             = 2\n",
    "influence             = 0 \n",
    "signal_abs_threshold  = 1.0\n",
    "smooth_rolling_window = 48\n",
    "\n",
    "# lag                   = 48\n",
    "# threshold             = 2\n",
    "# influence             = 0 \n",
    "# signal_abs_threshold  = 2.0\n",
    "# smooth_rolling_window = 36\n",
    "\n",
    "funky_n_entries=200\n",
    "# fit_signal_col_keras='test_final'\n",
    "fit_signal_col_keras='test_final2'\n",
    "\n",
    "# fit_signal_col_peaks = 'test_final'\n",
    "fit_signal_col_peaks = 'signal'\n",
    "\n",
    "\n",
    "# ami_df_evs_fit = ami_df_evs.copy()\n",
    "# ami_df_non_fit = ami_df_non.copy()\n",
    "# #-----\n",
    "# value_col = 'value'\n",
    "# lag=48\n",
    "# threshold=5\n",
    "# influence=0\n",
    "# signal_abs_threshold=1.0\n",
    "# smooth_rolling_window=48\n",
    "# funky_n_entries=500\n",
    "# fit_signal_col_keras='test_final'\n",
    "\n",
    "# # fit_signal_col_peaks = 'test_final'\n",
    "# fit_signal_col_peaks = 'signal'\n",
    "\n",
    "\n",
    "\n",
    "signal_col_out='signal'\n",
    "signal_pos_col_out='signal_pos'\n",
    "signal_binary_col_out='signal_binary'\n",
    "signal_pos_binary_col_out='signal_pos_binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817927d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "ami_df_evs_fit = ami_df_evs_fit.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "    lambda x: thresholding_algo_df_i(\n",
    "        df_i=x, \n",
    "        value_col=value_col, \n",
    "        lag=lag, \n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        smooth_rolling_window=smooth_rolling_window\n",
    "    )\n",
    ")\n",
    "ami_df_non_fit = ami_df_non_fit.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "    lambda x: thresholding_algo_df_i(\n",
    "        df_i=x, \n",
    "        value_col=value_col, \n",
    "        lag=lag, \n",
    "        threshold=threshold, \n",
    "        influence=influence, \n",
    "        signal_abs_threshold=signal_abs_threshold, \n",
    "        smooth_rolling_window=smooth_rolling_window\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_evs_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b723a51",
   "metadata": {},
   "source": [
    "## NEW METHOD KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_signal_col_keras='test_final'\n",
    "# fit_signal_col_keras=value_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "keras_df_evs = ami_df_evs_fit.reset_index(drop=True).groupby(['serialnumber']).apply(\n",
    "    lambda x: make_it_funky(\n",
    "        df_i=x, \n",
    "        n_entries=funky_n_entries, \n",
    "        val_col=fit_signal_col_keras, \n",
    "        SN_col='serialnumber'\n",
    "    )\n",
    ")\n",
    "keras_df_non = ami_df_non_fit.reset_index(drop=True).groupby(['serialnumber']).apply(\n",
    "    lambda x: make_it_funky(\n",
    "        df_i=x, \n",
    "        n_entries=funky_n_entries, \n",
    "        val_col=fit_signal_col_keras, \n",
    "        SN_col='serialnumber'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#-------------------------\n",
    "keras_df_evs['target']=1\n",
    "keras_df_non['target']=0\n",
    "keras_df = pd.concat([keras_df_evs, keras_df_non])\n",
    "keras_df=keras_df.dropna()\n",
    "#-------------------------\n",
    "keras_df_train, keras_df_test = train_test_split(keras_df, test_size=0.33, random_state=42)\n",
    "#-------------------------\n",
    "X_train = keras_df_train[[x for x in keras_df_train.columns.tolist() if x != 'target']].values\n",
    "y_train = keras_df_train['target'].values\n",
    "X_train=X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "#-----\n",
    "X_test = keras_df_test[[x for x in keras_df_test.columns.tolist() if x != 'target']].values\n",
    "y_test = keras_df_test['target'].values\n",
    "X_test=X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f787ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    num_classes=2\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=16, kernel_size=25, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=8, kernel_size=10, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "    \n",
    "    conv4 = keras.layers.Conv1D(filters=4, kernel_size=10, padding=\"same\")(conv3)\n",
    "    conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = keras.layers.ReLU()(conv4)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv4)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    num_classes=2\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=16, kernel_size=25, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=8, kernel_size=25, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "    \n",
    "    conv4 = keras.layers.Conv1D(filters=4, kernel_size=50, padding=\"same\")(conv3)\n",
    "    conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = keras.layers.ReLU()(conv4)\n",
    "    \n",
    "    conv5 = keras.layers.Conv1D(filters=4, kernel_size=20, padding=\"same\")(conv4)\n",
    "    conv5 = keras.layers.BatchNormalization()(conv5)\n",
    "    conv5 = keras.layers.ReLU()(conv5)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv5)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eba6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(input_shape=X_train.shape[1:])\n",
    "# keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01efbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# epochs = 500\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        r'C:\\Users\\s346557\\Downloads\\tmp1\\best_model.h5', save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "#     keras.callbacks.ReduceLROnPlateau(\n",
    "#         monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "#     ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d53b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(r'C:\\Users\\s346557\\Downloads\\tmp1\\best_model.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480306ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "print(\"Train accuracy\", train_acc)\n",
    "print(\"Train loss\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d25b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train = np.argmax(y_pred_train, axis=1)\n",
    "#-----\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_train, y_pred_train))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_train, y_pred_train))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab1735",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "#-----\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(\n",
    "    n_x=1, \n",
    "    n_y=2, \n",
    "    fig_num=fig_num, \n",
    "    unit_figsize_width=6., \n",
    "    unit_figsize_height=4., \n",
    ")\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.4))\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_train, \n",
    "    y_pred=y_pred_train, \n",
    "    title='Train', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[0], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_test, \n",
    "    y_pred=y_pred, \n",
    "    title='Test', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[1], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "fig_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb33142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_name = f\"keras_cmd_L{lag}_T{threshold}_ST{str(signal_abs_threshold).replace('.', '_')}_SRW{smooth_rolling_window}.png\"\n",
    "# Plot_General.save_fig(\n",
    "#     fig=fig, \n",
    "#     save_dir=r'C:\\Users\\s346557\\Documents\\Presentations\\EVs\\Figures', \n",
    "#     save_name=save_name, \n",
    "#     bbox_inches='tight'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb0ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(keras_df_test.shape[0]==len(y_pred))\n",
    "keras_df_test_w_preds = keras_df_test.copy()\n",
    "keras_df_test_w_preds['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f64106",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_test_w_preds[\n",
    "    (keras_df_test_w_preds['target']==1) & \n",
    "    (keras_df_test_w_preds['y_pred']==0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff47be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras_df_test_w_preds[\n",
    "    (keras_df_test_w_preds['target']==1) & \n",
    "    (keras_df_test_w_preds['y_pred']==0)\n",
    "].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = ami_df_evs_fit[ami_df_evs_fit['serialnumber']=='996575104'].copy()\n",
    "value_col = 'mean_TRS value'\n",
    "\n",
    "# lag=24\n",
    "# threshold=5\n",
    "# influence=0\n",
    "# signal_abs_threshold=1.0\n",
    "# smooth_rolling_window=12\n",
    "\n",
    "lag=24\n",
    "threshold=3\n",
    "influence=0\n",
    "signal_abs_threshold=0.0\n",
    "smooth_rolling_window=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2187191",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = thresholding_algo_df_i(\n",
    "    df_i=df_i, \n",
    "    value_col=value_col, \n",
    "    lag=lag, \n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold, \n",
    "    smooth_rolling_window=smooth_rolling_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6247d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Plot_General.default_subplots(n_x=1, n_y=1)\n",
    "df_i[value_col].plot(ax=ax, color='black')\n",
    "df_i[df_i['signal_binary']==True].reset_index().plot.scatter(ax=ax, x='starttimeperiod_local', y=value_col, color='black')\n",
    "df_i['test_final'].plot(ax=ax, color='red', linestyle='dotted')\n",
    "df_i['test_final2'].plot(ax=ax, color='green', linestyle='dashed')\n",
    "ax.axhline(y=signal_abs_threshold, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983c17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf16018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ad9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_test_w_preds[\n",
    "    (keras_df_test_w_preds['target']==0) & \n",
    "    (keras_df_test_w_preds['y_pred']==1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3c666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = ami_df_non_fit[ami_df_non_fit['serialnumber']=='766690041'].copy()\n",
    "value_col = 'mean_TRS value'\n",
    "\n",
    "# lag=24\n",
    "# threshold=5\n",
    "# influence=0\n",
    "# signal_abs_threshold=1.0\n",
    "# smooth_rolling_window=12\n",
    "\n",
    "lag=24\n",
    "threshold=5\n",
    "influence=0\n",
    "signal_abs_threshold=1.0\n",
    "smooth_rolling_window=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = thresholding_algo_df_i(\n",
    "    df_i=df_i, \n",
    "    value_col=value_col, \n",
    "    lag=lag, \n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold, \n",
    "    smooth_rolling_window=smooth_rolling_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0de997",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Plot_General.default_subplots(n_x=1, n_y=1)\n",
    "df_i[value_col].plot(ax=ax, color='black')\n",
    "df_i[df_i['signal_binary']==True].reset_index().plot.scatter(ax=ax, x='starttimeperiod_local', y=value_col, color='black')\n",
    "df_i['test_final'].plot(ax=ax, color='red', linestyle='dotted')\n",
    "df_i['test_final2'].plot(ax=ax, color='green', linestyle='dashed')\n",
    "ax.axhline(y=signal_abs_threshold, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845b973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce8dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c331115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0bfcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52588a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_test_w_preds[keras_df_test_w_preds['target']==0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_test_w_preds[keras_df_test_w_preds['target']==1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_test_w_preds[\n",
    "    (keras_df_test_w_preds['target']==0) & \n",
    "    (keras_df_test_w_preds['y_pred']==0)\n",
    "].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a293eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_test_w_preds[\n",
    "    (keras_df_test_w_preds['target']==0) & \n",
    "    (keras_df_test_w_preds['y_pred']==1)\n",
    "].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eea006",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_test_w_preds[\n",
    "    (keras_df_test_w_preds['target']==1) & \n",
    "    (keras_df_test_w_preds['y_pred']==1)\n",
    "].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3b007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85f595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_df_test_w_preds[\n",
    "    (keras_df_test_w_preds['target']==1) & \n",
    "    (keras_df_test_w_preds['y_pred']==0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_i = ami_df_resamples_evs['H'][ami_df_resamples_evs['H']['serialnumber']=='786854657'].sort_index().copy()\n",
    "# value_col = 'mean_TRS value'\n",
    "\n",
    "df_i = ami_df_evs[ami_df_evs['serialnumber']=='761789064'].sort_index().copy()\n",
    "value_col = 'value'\n",
    "\n",
    "# df_i = ami_df_non[ami_df_non['serialnumber']=='780852455'].sort_index().iloc[:1000].copy()\n",
    "# value_col = 'value'\n",
    "\n",
    "# df_i = ami_df_resamples_non['H'][ami_df_resamples_non['H']['serialnumber']=='780852455'].sort_index().copy()\n",
    "# value_col = 'mean_TRS value'\n",
    "\n",
    "\n",
    "lag=24\n",
    "threshold=5\n",
    "influence=0\n",
    "signal_abs_threshold=1.0\n",
    "smooth_rolling_window=12\n",
    "\n",
    "smooth_col = f'{value_col}_smooth'\n",
    "smooth_roll_mean_col = f'{value_col}_smooth_roll_mean'\n",
    "smooth_roll_std_col  = f'{value_col}_smooth_roll_std'\n",
    "\n",
    "signal_col_out='signal'\n",
    "signal_pos_col_out='signal_pos'\n",
    "signal_binary_col_out='signal_binary'\n",
    "signal_pos_binary_col_out='signal_pos_binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = thresholding_algo_df_i(\n",
    "    df_i=df_i, \n",
    "    value_col=value_col, \n",
    "    lag=lag, \n",
    "    threshold=threshold, \n",
    "    influence=influence, \n",
    "    signal_abs_threshold=signal_abs_threshold, \n",
    "    smooth_rolling_window=smooth_rolling_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f383c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "\n",
    "df_i[value_col].plot(ax=axs[0], color='blue')\n",
    "df_i[smooth_col].plot(ax=axs[0], color='green')\n",
    "df_i[smooth_roll_mean_col].plot(ax=axs[0], color='red')\n",
    "\n",
    "df_i[signal_col_out].plot(ax=axs[1], color='blue')\n",
    "df_i[signal_pos_col_out].plot(ax=axs[1], color='green')\n",
    "df_i[df_i[signal_binary_col_out]==1].reset_index().plot.scatter(ax=axs[1], x='starttimeperiod_local', y=signal_col_out, color='blue', alpha=0.5)\n",
    "df_i[df_i[signal_pos_binary_col_out]==1].reset_index().plot.scatter(ax=axs[1], x='starttimeperiod_local', y=signal_pos_col_out, color='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(n_x=1, n_y=2)\n",
    "\n",
    "df_i[value_col].plot(ax=axs[0], color='blue')\n",
    "df_i['test_final'].plot(ax=axs[0], color='purple')\n",
    "\n",
    "df_i[signal_col_out].plot(ax=axs[1], color='blue')\n",
    "df_i['test_final_signal'].plot(ax=axs[1], color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cc4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a511c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24929b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_evs_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656ab29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = ami_df_evs_fit[ami_df_evs_fit['serialnumber']=='786854657'].copy()\n",
    "signal_group_col='signal_grp'\n",
    "signal_col='signal_binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_signal_group_col='signal_grp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = set_signal_groups_in_df_i(\n",
    "    df_i=df_i, \n",
    "    SN_col='serialnumber',\n",
    "    signal_col=signal_col, \n",
    "    return_signal_group_col=return_signal_group_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955f897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c27e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_features_df_i = build_peak_features_for_df_i(\n",
    "    df_i, \n",
    "    signal_group_col='signal_grp', \n",
    "    value_col='mean_TRS value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51752b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_features_df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067a5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ac16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i2 = ami_df_evs[ami_df_evs['serialnumber']=='786854657'].drop(columns=['starttimeperiod_local'])\n",
    "dev_features_df_i2 = extract_features_for_SN_v2(\n",
    "    df_i=df_i2, \n",
    "    lag=24, \n",
    "    threshold=5,\n",
    "    influence=0, \n",
    "    signal_abs_threshold=1.0, \n",
    "    value_col='value', \n",
    "    SN_col='serialnumber', \n",
    "    time_col='starttimeperiod_local', \n",
    "    signal_col='signals'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0217209",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_features_df_i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed723e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862acbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd704d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81300ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4c7297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ce507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7fc1aec",
   "metadata": {},
   "source": [
    "## NEW METHOD PEAKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f99f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df_evs = ami_df_evs_fit.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "    lambda x: set_signal_groups_in_df_i(\n",
    "        df_i=x, \n",
    "        SN_col='serialnumber',\n",
    "        signal_col='signal_binary', \n",
    "        return_signal_group_col='signal_grp'        \n",
    "    )\n",
    ")\n",
    "#-----\n",
    "peak_df_non = ami_df_non_fit.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "    lambda x: set_signal_groups_in_df_i(\n",
    "        df_i=x, \n",
    "        SN_col='serialnumber',\n",
    "        signal_col='signal_binary', \n",
    "        return_signal_group_col='signal_grp'        \n",
    "    )\n",
    ")\n",
    "#-------------------------\n",
    "peak_df_evs = peak_df_evs.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "    lambda x: build_peak_features_for_df_i(\n",
    "        df_i=x, \n",
    "        signal_group_col='signal_grp', \n",
    "        value_col=fit_signal_col_peaks,\n",
    "        SN_col='serialnumber', \n",
    "        time_col='starttimeperiod_local',        \n",
    "    )\n",
    ")\n",
    "#-----\n",
    "peak_df_non = peak_df_non.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "    lambda x: build_peak_features_for_df_i(\n",
    "        df_i=x, \n",
    "        signal_group_col='signal_grp', \n",
    "        value_col=fit_signal_col_peaks, \n",
    "        SN_col='serialnumber', \n",
    "        time_col='starttimeperiod_local',        \n",
    "    )\n",
    ")\n",
    "#-------------------------\n",
    "peak_df_evs['target']=1\n",
    "peak_df_non['target']=0\n",
    "#-------------------------\n",
    "peak_df=pd.concat([peak_df_evs, peak_df_non])\n",
    "peak_df=peak_df.sample(frac=1)\n",
    "#-------------------------\n",
    "peak_df_OG = peak_df.copy()\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de16596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df = peak_df_OG.copy()\n",
    "#-------------------------\n",
    "peak_df = fill_na_in_peak_df(peak_df)\n",
    "peak_df = convert_to_seconds_in_peak_df(peak_df)\n",
    "peak_df = peak_df.drop(columns=['serialnumber'])\n",
    "#-------------------------\n",
    "# peak_df = peak_df.drop(columns=[\n",
    "#     'peak_mean', 'peak_std', \n",
    "#     'peak_max_std', \n",
    "#     'peak_width_std', \n",
    "#     'peak_spacing_std'\n",
    "# ])\n",
    "#-------------------------\n",
    "# Remove inf values\n",
    "print(f\"Before inf removal, peak_df.shape[0] = {peak_df.shape[0]}\")\n",
    "peak_df = peak_df.loc[np.isinf(peak_df[[x for x in peak_df.columns if x!='serialnumber']]).sum(axis=1)==0].copy()\n",
    "print(f\"After inf removal, peak_df.shape[0] = {peak_df.shape[0]}\")\n",
    "#-------------------------\n",
    "peak_df_train, peak_df_test = train_test_split(peak_df, test_size=0.33, random_state=42)\n",
    "#-------------------------\n",
    "X_train = peak_df_train[[x for x in peak_df_train.columns.tolist() if x!='target']]\n",
    "y_train = peak_df_train['target']\n",
    "#-----\n",
    "X_test = peak_df_test[[x for x in peak_df_test.columns.tolist() if x!='target']]\n",
    "y_test = peak_df_test['target']\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee3907",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest_clf = RandomForestClassifier(n_estimators = 10, max_depth=None, n_jobs=None)\n",
    "# forest_clf = RandomForestClassifier(\n",
    "#     n_estimators = 100, \n",
    "#     criterion='log_loss', \n",
    "#     max_depth=40,\n",
    "#     max_features='log2', \n",
    "#     bootstrap=True, \n",
    "#     n_jobs=None\n",
    "# )\n",
    "forest_clf = RandomForestClassifier(\n",
    "    n_estimators = 10, \n",
    "    criterion='log_loss', \n",
    "    max_depth=40,\n",
    "    max_features='log2', \n",
    "    bootstrap=True, \n",
    "    n_jobs=None\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "forest_clf.fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88c837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = forest_clf.predict(X_train)\n",
    "print('*****'*5)\n",
    "print('TRAINING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_train.sum()}\")\n",
    "print(f\"#(target==0): {y_train.shape[0]-y_train.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_train.sum()/(y_train.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_train, y_pred_train))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_train, y_pred_train))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_train, y_pred_train))\n",
    "print()\n",
    "\n",
    "y_pred = forest_clf.predict(X_test)\n",
    "print('*****'*5)\n",
    "print('TESTING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_test.sum()}\")\n",
    "print(f\"#(target==0): {y_test.shape[0]-y_test.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_test.sum()/(y_test.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_test, y_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(forest_clf, X_train, y_train, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ad731",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(forest_clf, X_train, y_train, cv=10, scoring='precision').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b562a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(forest_clf, X_train, y_train, cv=10, scoring='recall').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead486d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(forest_clf, X_train, y_train, cv=10, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e353f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_validate(forest_clf, X_train, y_train, cv=10, scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f57a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy:  {res['test_accuracy'].mean()}\")\n",
    "print(f\"precision: {res['test_precision'].mean()}\")\n",
    "print(f\"recall:    {res['test_recall'].mean()}\")\n",
    "print(f\"f1:        {res['test_f1'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c71fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879ba34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(\n",
    "    n_x=1, \n",
    "    n_y=2, \n",
    "    fig_num=fig_num, \n",
    "    unit_figsize_width=6., \n",
    "    unit_figsize_height=4., \n",
    ")\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.4))\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_train, \n",
    "    y_pred=y_pred_train, \n",
    "    title='Train', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[0], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_test, \n",
    "    y_pred=y_pred, \n",
    "    title='Test', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[1], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "fig_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_name = f\"forest_L{lag}_T{threshold}_ST{str(signal_abs_threshold).replace('.', '_')}_SRW{smooth_rolling_window}.png\"\n",
    "# Plot_General.save_fig(\n",
    "#     fig=fig, \n",
    "#     save_dir=r'C:\\Users\\s346557\\Documents\\Presentations\\EVs\\Figures', \n",
    "#     save_name=save_name, \n",
    "#     bbox_inches='tight'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76524887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(peak_df.columns[:-1])==len(forest_clf.feature_importances_))\n",
    "importances = list(zip(peak_df.columns[:-1], forest_clf.feature_importances_))\n",
    "importances_srtd = sorted(importances, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a0ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_srtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698dce66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e384264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_param_grid = { \n",
    "    'n_estimators': [100, 200, 500, 1000, 2000],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth' : [10, 20, 30, 40, 50],\n",
    "    'criterion' :['gini', 'entropy', 'log_loss'], \n",
    "    'bootstrap':[True,False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81acabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_grid_search = GridSearchCV(\n",
    "    forest_clf, forest_param_grid, cv=3, \n",
    "    scoring=None, \n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "forest_grid_search.fit(X_train, y_train)\n",
    "print(time.time()-start)\n",
    "\n",
    "print(forest_grid_search.best_params_)\n",
    "print(forest_grid_search.best_estimator_)\n",
    "y_pred = forest_grid_search.best_estimator_.predict(X_test)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1c511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132a1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "173bccc7",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e3c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186bd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a16657",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f844dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a svm Classifier\n",
    "clf = svm.SVC()\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = clf.predict(X_train)\n",
    "print('*****'*5)\n",
    "print('TRAINING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_train.sum()}\")\n",
    "print(f\"#(target==0): {y_train.shape[0]-y_train.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_train.sum()/(y_train.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_train, y_pred_train))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_train, y_pred_train))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_train, y_pred_train))\n",
    "print()\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print('*****'*5)\n",
    "print('TESTING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_test.sum()}\")\n",
    "print(f\"#(target==0): {y_test.shape[0]-y_test.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_test.sum()/(y_test.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_test, y_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92368332",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(\n",
    "    n_x=1, \n",
    "    n_y=2, \n",
    "    fig_num=fig_num, \n",
    "    unit_figsize_width=6., \n",
    "    unit_figsize_height=4., \n",
    ")\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.4))\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_train, \n",
    "    y_pred=y_pred_train, \n",
    "    title='Train', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[0], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_test, \n",
    "    y_pred=y_pred, \n",
    "    title='Test', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[1], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "fig_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_name = f\"svc_L{lag}_T{threshold}_ST{str(signal_abs_threshold).replace('.', '_')}_SRW{smooth_rolling_window}.png\"\n",
    "# Plot_General.save_fig(\n",
    "#     fig=fig, \n",
    "#     save_dir=r'C:\\Users\\s346557\\Documents\\Presentations\\EVs\\Figures', \n",
    "#     save_name=save_name, \n",
    "#     bbox_inches='tight'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada85e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afff225",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "\n",
    "# # NOTE: Can be a single dict, or a list of dicts\n",
    "svc_param_grid = { \n",
    "    'C': [0.000001, 0.001, 0.1, 0, 1.0, 10, 100], \n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [3, 5, 7, 9, 12], # Only for poly!\n",
    "    'gamma':['scale', 'auto'] # Only for rbf, poly, and sigmoid!\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7038c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid_search = GridSearchCV(\n",
    "    clf, svc_param_grid, cv=3, \n",
    "    scoring=None, \n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "svc_grid_search.fit(X_train, y_train)\n",
    "print(time.time()-start)\n",
    "\n",
    "print(svc_grid_search.best_params_)\n",
    "print(svc_grid_search.best_estimator_)\n",
    "y_pred = svc_grid_search.best_estimator_.predict(X_test)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d386d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c0c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(C=0.1)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d9b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = logreg.predict(X_train)\n",
    "print('*****'*5)\n",
    "print('TRAINING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_train.sum()}\")\n",
    "print(f\"#(target==0): {y_train.shape[0]-y_train.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_train.sum()/(y_train.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_train, y_pred_train))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_train, y_pred_train))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_train, y_pred_train))\n",
    "print()\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('*****'*5)\n",
    "print('TESTING DATASET')\n",
    "print('*****'*5)\n",
    "print(f\"#(target==1): {y_test.sum()}\")\n",
    "print(f\"#(target==0): {y_test.shape[0]-y_test.sum()}\")\n",
    "print(f\"%(target==1): {100*(y_test.sum()/(y_test.shape[0]))}\")\n",
    "print('-----'*5)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))\n",
    "print(\"F1        OF THE MODEL: \", f1_score(y_test, y_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = Plot_General.default_subplots(\n",
    "    n_x=1, \n",
    "    n_y=2, \n",
    "    fig_num=fig_num, \n",
    "    unit_figsize_width=6., \n",
    "    unit_figsize_height=4., \n",
    ")\n",
    "Plot_General.adjust_subplots_args(fig, dict(hspace=0.4))\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_train, \n",
    "    y_pred=y_pred_train, \n",
    "    title='Train', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[0], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "cmd = draw_confusion_matrix(\n",
    "    y=y_test, \n",
    "    y_pred=y_pred, \n",
    "    title='Test', \n",
    "    normalize=None, \n",
    "    scientific=False, \n",
    "    ax=axs[1], \n",
    "    text_kw=dict(fontsize='xx-large'), \n",
    "    target_eq_1_name='EV', \n",
    "    target_eq_0_name='Not EV'\n",
    ")\n",
    "#-----\n",
    "fig_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab606823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_name = f\"logreg_L{lag}_T{threshold}_ST{str(signal_abs_threshold).replace('.', '_')}_SRW{smooth_rolling_window}.png\"\n",
    "# Plot_General.save_fig(\n",
    "#     fig=fig, \n",
    "#     save_dir=r'C:\\Users\\s346557\\Documents\\Presentations\\EVs\\Figures', \n",
    "#     save_name=save_name, \n",
    "#     bbox_inches='tight'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b59ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b584dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "# # NOTE: Can be a single dict, or a list of dicts\n",
    "logreg_param_grid = { \n",
    "    'C': [0.000001, 0.001, 0.1, 0, 1.0, 10, 100, 10000], \n",
    "    'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    'dual': [True, False], \n",
    "    'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca811e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_grid_search = GridSearchCV(\n",
    "    logreg, logreg_param_grid, cv=3, \n",
    "    scoring=None, \n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "logreg_grid_search.fit(X_train, y_train)\n",
    "print(time.time()-start)\n",
    "\n",
    "print(logreg_grid_search.best_params_)\n",
    "print(logreg_grid_search.best_estimator_)\n",
    "y_pred = logreg_grid_search.best_estimator_.predict(X_test)\n",
    "print(\"ACCURACY  OF THE MODEL: \", accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION OF THE MODEL: \", precision_score(y_test, y_pred))\n",
    "print(\"RECALL    OF THE MODEL: \", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754519a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e050f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6783eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f569e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d0e34d4",
   "metadata": {},
   "source": [
    "# GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef790be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    num_classes=2\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=16, kernel_size=25, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=8, kernel_size=10, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "    \n",
    "    conv4 = keras.layers.Conv1D(filters=4, kernel_size=10, padding=\"same\")(conv3)\n",
    "    conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = keras.layers.ReLU()(conv4)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv4)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b703f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_df_empty = pd.DataFrame(columns=[\n",
    "    'lag', \n",
    "    'threshold', \n",
    "    'signal_abs_threshold', \n",
    "    'smooth_rolling_window', \n",
    "    #-----\n",
    "    'acc_train_keras', \n",
    "    'prec_train_keras', \n",
    "    'rec_train_keras', \n",
    "    'f1_train_keras', \n",
    "    \n",
    "    'acc_test_keras', \n",
    "    'prec_test_keras', \n",
    "    'rec_test_keras', \n",
    "    'f1_test_keras', \n",
    "    #-----\n",
    "    'acc_train_forest', \n",
    "    'prec_train_forest', \n",
    "    'rec_train_forest', \n",
    "    'f1_train_forest', \n",
    "    \n",
    "    'acc_test_forest', \n",
    "    'prec_test_forest', \n",
    "    'rec_test_forest', \n",
    "    'f1_test_forest', \n",
    "    #-----\n",
    "    'acc_train_svc', \n",
    "    'prec_train_svc', \n",
    "    'rec_train_svc', \n",
    "    'f1_train_svc', \n",
    "    \n",
    "    'acc_test_svc', \n",
    "    'prec_test_svc', \n",
    "    'rec_test_svc', \n",
    "    'f1_test_svc', \n",
    "    #-----\n",
    "    'acc_train_logreg', \n",
    "    'prec_train_logreg', \n",
    "    'rec_train_logreg', \n",
    "    'f1_train_logreg', \n",
    "    \n",
    "    'acc_test_logreg', \n",
    "    'prec_test_logreg', \n",
    "    'rec_test_logreg', \n",
    "    'f1_test_logreg'    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40aa9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_evs_fit = ami_df_resamples_evs['H'].copy()\n",
    "ami_df_non_fit = ami_df_resamples_non['H'].copy()\n",
    "#-----\n",
    "value_col = 'mean_TRS value'\n",
    "funky_n_entries=200\n",
    "\n",
    "fit_signal_col_keras='test_final'\n",
    "# fit_signal_col_keras='test_final2'\n",
    "\n",
    "fit_signal_col_peaks = 'test_final'\n",
    "# fit_signal_col_peaks = 'signal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [24, 36, 48]\n",
    "thresholds = [2, 3, 5, 10]\n",
    "signal_abs_thresholds = [0, 1, 2]\n",
    "smooth_rolling_windows = [24, 36, 48]\n",
    "\n",
    "# lags = [24, 48]\n",
    "# thresholds = [3]\n",
    "# signal_abs_thresholds = [1]\n",
    "# smooth_rolling_windows = [24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa50af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lists = [\n",
    "    lags, \n",
    "    thresholds, \n",
    "    signal_abs_thresholds, \n",
    "    smooth_rolling_windows\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "grid_results_df = grid_results_df_empty.copy()\n",
    "for i, grid_i in enumerate(list(itertools.product(*grid_lists))):\n",
    "    start_i = time.time()\n",
    "    lag                   = grid_i[0]\n",
    "    threshold             = grid_i[1]\n",
    "    signal_abs_threshold  = grid_i[2]\n",
    "    smooth_rolling_window = grid_i[3]\n",
    "    #-------------------------\n",
    "    print(f\"{i} of {len(list(itertools.product(*grid_lists)))}\")\n",
    "    print(f'lag                   = {lag}')\n",
    "    print(f'threshold             = {threshold}')\n",
    "    print(f'signal_abs_threshold  = {signal_abs_threshold}')\n",
    "    print(f'smooth_rolling_window = {smooth_rolling_window}')\n",
    "    #-------------------------\n",
    "    res_i = dict()\n",
    "    res_i['lag']                   = lag\n",
    "    res_i['threshold']             = threshold\n",
    "    res_i['signal_abs_threshold']  = signal_abs_threshold\n",
    "    res_i['smooth_rolling_window'] = smooth_rolling_window\n",
    "    #-------------------------\n",
    "    ami_df_evs_fit = ami_df_evs_fit.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "        lambda x: thresholding_algo_df_i(\n",
    "            df_i=x, \n",
    "            value_col=value_col, \n",
    "            lag=lag, \n",
    "            threshold=threshold, \n",
    "            influence=influence, \n",
    "            signal_abs_threshold=signal_abs_threshold, \n",
    "            smooth_rolling_window=smooth_rolling_window\n",
    "        )\n",
    "    )\n",
    "    ami_df_non_fit = ami_df_non_fit.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "        lambda x: thresholding_algo_df_i(\n",
    "            df_i=x, \n",
    "            value_col=value_col, \n",
    "            lag=lag, \n",
    "            threshold=threshold, \n",
    "            influence=influence, \n",
    "            signal_abs_threshold=signal_abs_threshold, \n",
    "            smooth_rolling_window=smooth_rolling_window\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # NEW METHOD KERAS\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    keras_df_evs = ami_df_evs_fit.reset_index(drop=True).groupby(['serialnumber']).apply(\n",
    "        lambda x: make_it_funky(\n",
    "            df_i=x, \n",
    "            n_entries=funky_n_entries, \n",
    "            val_col=fit_signal_col_keras, \n",
    "            SN_col='serialnumber'\n",
    "        )\n",
    "    )\n",
    "    keras_df_non = ami_df_non_fit.reset_index(drop=True).groupby(['serialnumber']).apply(\n",
    "        lambda x: make_it_funky(\n",
    "            df_i=x, \n",
    "            n_entries=funky_n_entries, \n",
    "            val_col=fit_signal_col_keras, \n",
    "            SN_col='serialnumber'\n",
    "        )\n",
    "    )\n",
    "    #-------------------------\n",
    "    keras_df_evs['target']=1\n",
    "    keras_df_non['target']=0\n",
    "    keras_df = pd.concat([keras_df_evs, keras_df_non])\n",
    "    keras_df=keras_df.dropna()\n",
    "    #-------------------------\n",
    "    keras_df_train, keras_df_test = train_test_split(keras_df, test_size=0.33, random_state=42)\n",
    "    #-------------------------\n",
    "    X_train = keras_df_train[[x for x in keras_df_train.columns.tolist() if x != 'target']].values\n",
    "    y_train = keras_df_train['target'].values\n",
    "    X_train=X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    #-----\n",
    "    X_test = keras_df_test[[x for x in keras_df_test.columns.tolist() if x != 'target']].values\n",
    "    y_test = keras_df_test['target'].values\n",
    "    X_test=X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    #-------------------------\n",
    "    model = make_model(input_shape=X_train.shape[1:])\n",
    "    #-------------------------\n",
    "    epochs = 500\n",
    "    batch_size = 32\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "        ),\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "    ]\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_split=0.2,\n",
    "        verbose=0,\n",
    "    )\n",
    "    #-------------------------\n",
    "    model = keras.models.load_model(\"best_model.h5\")\n",
    "    #-------------------------\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_train = np.argmax(y_pred_train, axis=1)\n",
    "    #-----\n",
    "    res_i['acc_train_keras']  = accuracy_score(y_train, y_pred_train)\n",
    "    res_i['prec_train_keras'] = precision_score(y_train, y_pred_train)\n",
    "    res_i['rec_train_keras']  = recall_score(y_train, y_pred_train)\n",
    "    res_i['f1_train_keras']   = f1_score(y_train, y_pred_train)\n",
    "    #-------------------------\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    #-----\n",
    "    res_i['acc_test_keras']  = accuracy_score(y_test, y_pred)\n",
    "    res_i['prec_test_keras'] = precision_score(y_test, y_pred)\n",
    "    res_i['rec_test_keras']  = recall_score(y_test, y_pred)\n",
    "    res_i['f1_test_keras']   = f1_score(y_test, y_pred)\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # NEW METHOD PEAKS\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    peak_df_evs = ami_df_evs_fit.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "        lambda x: set_signal_groups_in_df_i(\n",
    "            df_i=x, \n",
    "            SN_col='serialnumber',\n",
    "            signal_col='signal_binary', \n",
    "            return_signal_group_col='signal_grp'        \n",
    "        )\n",
    "    )\n",
    "    #-----\n",
    "    peak_df_non = ami_df_non_fit.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "        lambda x: set_signal_groups_in_df_i(\n",
    "            df_i=x, \n",
    "            SN_col='serialnumber',\n",
    "            signal_col='signal_binary', \n",
    "            return_signal_group_col='signal_grp'        \n",
    "        )\n",
    "    )\n",
    "    #-------------------------\n",
    "    peak_df_evs = peak_df_evs.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "        lambda x: build_peak_features_for_df_i(\n",
    "            df_i=x, \n",
    "            signal_group_col='signal_grp', \n",
    "            value_col=fit_signal_col_peaks,\n",
    "            SN_col='serialnumber', \n",
    "            time_col='starttimeperiod_local',        \n",
    "        )\n",
    "    )\n",
    "    #-----\n",
    "    peak_df_non = peak_df_non.groupby(['serialnumber'], as_index=False, group_keys=False).apply(\n",
    "        lambda x: build_peak_features_for_df_i(\n",
    "            df_i=x, \n",
    "            signal_group_col='signal_grp', \n",
    "            value_col=fit_signal_col_peaks, \n",
    "            SN_col='serialnumber', \n",
    "            time_col='starttimeperiod_local',        \n",
    "        )\n",
    "    )\n",
    "    #-------------------------\n",
    "    peak_df_evs['target']=1\n",
    "    peak_df_non['target']=0\n",
    "    #-------------------------\n",
    "    peak_df=pd.concat([peak_df_evs, peak_df_non])\n",
    "    peak_df=peak_df.sample(frac=1)\n",
    "    #-------------------------\n",
    "    # peak_df_OG = peak_df.copy()\n",
    "    #-------------------------\n",
    "    # peak_df = peak_df_OG.copy()\n",
    "    #-------------------------\n",
    "    peak_df = fill_na_in_peak_df(peak_df)\n",
    "    peak_df = convert_to_seconds_in_peak_df(peak_df)\n",
    "    peak_df = peak_df.drop(columns=['serialnumber'])\n",
    "    #-------------------------\n",
    "    # peak_df = peak_df.drop(columns=[\n",
    "    #     'peak_mean', 'peak_std', \n",
    "    #     'peak_max_std', \n",
    "    #     'peak_width_std', \n",
    "    #     'peak_spacing_std'\n",
    "    # ])\n",
    "    #-------------------------\n",
    "    # Remove inf values\n",
    "    print(f\"Before inf removal, peak_df.shape[0] = {peak_df.shape[0]}\")\n",
    "    peak_df = peak_df.loc[np.isinf(peak_df[[x for x in peak_df.columns if x!='serialnumber']]).sum(axis=1)==0].copy()\n",
    "    print(f\"After inf removal, peak_df.shape[0] = {peak_df.shape[0]}\")\n",
    "    #-------------------------\n",
    "    peak_df_train, peak_df_test = train_test_split(peak_df, test_size=0.33, random_state=42)\n",
    "    #-------------------------\n",
    "    X_train = peak_df_train[[x for x in peak_df_train.columns.tolist() if x!='target']]\n",
    "    y_train = peak_df_train['target']\n",
    "    #-----\n",
    "    X_test = peak_df_test[[x for x in peak_df_test.columns.tolist() if x!='target']]\n",
    "    y_test = peak_df_test['target']\n",
    "    #--------------------------------------------------\n",
    "    # Random Forest\n",
    "    #--------------------------------------------------\n",
    "    forest_clf = RandomForestClassifier(n_estimators = 10, max_depth=None, n_jobs=None)\n",
    "    forest_clf.fit(X_train, y_train)\n",
    "    #-------------------------\n",
    "    y_pred_train = forest_clf.predict(X_train)\n",
    "    res_i['acc_train_forest']  = accuracy_score(y_train, y_pred_train)\n",
    "    res_i['prec_train_forest'] = precision_score(y_train, y_pred_train)\n",
    "    res_i['rec_train_forest']  = recall_score(y_train, y_pred_train)\n",
    "    res_i['f1_train_forest']   = f1_score(y_train, y_pred_train)\n",
    "    #-------------------------\n",
    "    y_pred = forest_clf.predict(X_test)\n",
    "    res_i['acc_test_forest']  = accuracy_score(y_test, y_pred)\n",
    "    res_i['prec_test_forest'] = precision_score(y_test, y_pred)\n",
    "    res_i['rec_test_forest']  = recall_score(y_test, y_pred)\n",
    "    res_i['f1_test_forest']   = f1_score(y_test, y_pred)\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    # SVC Forest\n",
    "    #--------------------------------------------------\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    #-------------------------\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    #-------------------------\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    res_i['acc_train_svc']  = accuracy_score(y_train, y_pred_train)\n",
    "    res_i['prec_train_svc'] = precision_score(y_train, y_pred_train)\n",
    "    res_i['rec_train_svc']  = recall_score(y_train, y_pred_train)\n",
    "    res_i['f1_train_svc']   = f1_score(y_train, y_pred_train)\n",
    "    #-------------------------\n",
    "    y_pred = clf.predict(X_test)\n",
    "    res_i['acc_test_svc']  = accuracy_score(y_test, y_pred)\n",
    "    res_i['prec_test_svc'] = precision_score(y_test, y_pred)\n",
    "    res_i['rec_test_svc']  = recall_score(y_test, y_pred)\n",
    "    res_i['f1_test_svc']   = f1_score(y_test, y_pred)\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    # Logistic Regression\n",
    "    #--------------------------------------------------\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "    #-------------------------\n",
    "    y_pred_train = logreg.predict(X_train)\n",
    "    res_i['acc_train_logreg']  = accuracy_score(y_train, y_pred_train)\n",
    "    res_i['prec_train_logreg'] = precision_score(y_train, y_pred_train)\n",
    "    res_i['rec_train_logreg']  = recall_score(y_train, y_pred_train)\n",
    "    res_i['f1_train_logreg']   = f1_score(y_train, y_pred_train)\n",
    "    #-------------------------\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    res_i['acc_test_logreg']  = accuracy_score(y_test, y_pred)\n",
    "    res_i['prec_test_logreg'] = precision_score(y_test, y_pred)\n",
    "    res_i['rec_test_logreg']  = recall_score(y_test, y_pred)\n",
    "    res_i['f1_test_logreg']   = f1_score(y_test, y_pred)\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    grid_results_df = pd.concat([grid_results_df, pd.DataFrame(res_i, index=[grid_results_df.shape[0]])])\n",
    "    print(time.time()-start_i)\n",
    "    print()\n",
    "    \n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_df.sort_values(by=['prec_test_keras'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d75358",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_df.sort_values(by=['f1_test_keras'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ad114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ae2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_df.sort_values(by=['prec_test_forest'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d9203",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_df.sort_values(by=['prec_test_svc'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b825a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_df.sort_values(by=['f1_test_logreg'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54199533",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fa700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14c13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63774deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05544745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634cf512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.od import OutlierProphet\n",
    "from alibi_detect.utils.fetching import fetch_detector\n",
    "from alibi_detect.utils.saving import save_detector, load_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279215c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_df_evs['serialnumber'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a51ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7ebb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
